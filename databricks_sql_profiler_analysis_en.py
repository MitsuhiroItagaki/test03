# Databricks notebook source
# MAGIC %md
# MAGIC # Databricks SQL Profiler Analysis Tool
# MAGIC
# MAGIC This notebook reads Databricks SQL profiler JSON log files and extracts metrics necessary for bottleneck identification and improvement recommendations.
# MAGIC
# MAGIC ## üöÄ Performance Optimization Updates (2024)
# MAGIC - **Fixed duplicate EXPLAIN execution** for original queries
# MAGIC - **Implemented caching mechanism** to prevent redundant database calls  
# MAGIC - **Optimized iterative optimization process** to avoid repeated EXPLAIN COST execution
# MAGIC - **Added global cache** for EXPLAIN results across multiple analysis functions
# MAGIC - **Reduced file I/O operations** through intelligent cache reuse
# MAGIC
# MAGIC ### Key Improvements:
# MAGIC 1. Original query EXPLAIN results are cached and reused across optimization attempts
# MAGIC 2. Fallback processing checks for existing cached results before re-execution  
# MAGIC 3. Analysis functions prioritize cached data over file system searches
# MAGIC 4. Performance degradation analysis no longer triggers duplicate EXPLAIN calls
# MAGIC
# MAGIC ### Expected Benefits:
# MAGIC - **3x faster execution** for iterative optimization (max 3 attempts)
# MAGIC - **Reduced database load** and network traffic
# MAGIC - **Elimination of duplicate output files**
# MAGIC - **More efficient resource utilization**
# MAGIC
# MAGIC ## Feature Overview
# MAGIC
# MAGIC 1. **SQL Profiler JSON File Loading**
# MAGIC    - Analysis of profiler logs output by Databricks
# MAGIC    - Extraction of execution plan metrics stored in the `graphs` key
# MAGIC
# MAGIC 2. **Key Metrics Extraction**
# MAGIC    - Query basic information (ID, status, execution time, etc.)
# MAGIC    - Overall performance (execution time, data volume, cache efficiency, etc.)
# MAGIC    - Stage and node detailed metrics
# MAGIC    - Bottleneck indicator calculation
# MAGIC
# MAGIC 3. **AI-powered Bottleneck Analysis**
# MAGIC    - Configurable LLM endpoints (Databricks, OpenAI, Azure OpenAI, Anthropic)
# MAGIC    - Bottleneck identification from extracted metrics
# MAGIC    - Specific improvement recommendations
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC **Prerequisites:**
# MAGIC - LLM endpoint configuration (Databricks Model Serving or external API)
# MAGIC - Required API key setup
# MAGIC - SQL profiler JSON file preparation (DBFS or FileStore)
# MAGIC
# MAGIC ---

# COMMAND ----------

# MAGIC %md
# MAGIC # üîß Configuration & Setup Section
# MAGIC
# MAGIC **This section performs basic tool configuration**
# MAGIC
# MAGIC üìã **Configuration Contents:**
# MAGIC - Analysis target file specification
# MAGIC - LLM endpoint configuration
# MAGIC - Analysis function definitions
# MAGIC
# MAGIC ‚ö†Ô∏è **Important:** Execute all cells in this section before running the main processing

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìÅ Analysis Target File Configuration
# MAGIC
# MAGIC **First, specify the SQL profiler JSON file to be analyzed.**
# MAGIC
# MAGIC This cell performs the following configurations:
# MAGIC - üìÇ SQL profiler JSON file path configuration
# MAGIC - üìã Examples of supported file path formats
# MAGIC - ‚öôÔ∏è Basic environment configuration

# COMMAND ----------

# üìÅ SQL Profiler JSON File Path Configuration
# 
# Please change the JSON_FILE_PATH below to your actual file path:

# Notebook environment file path configuration (please select from the following options)

# Option 1: Pre-tuning plan file (recommended)
JSON_FILE_PATH = 'query-profile_01f0703c-c975-1f48-ad71-ba572cc57272.json'

# Option 2: To use other JSON files, uncomment and edit the following
# JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/nophoton.json'
# JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/POC1.json'
# JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/your_file.json'

# Command line environment (optional)
import sys
if len(sys.argv) > 1 and not sys.argv[1].startswith('-'):
    # Use only when command line argument is not a flag (doesn't start with -)
    JSON_FILE_PATH = sys.argv[1]

# üåê Output language setting (OUTPUT_LANGUAGE: 'ja' = Japanese, 'en' = English)
OUTPUT_LANGUAGE = 'en'

# üîç EXPLAIN statement execution setting (EXPLAIN_ENABLED: 'Y' = execute, 'N' = do not execute)
EXPLAIN_ENABLED = 'Y'

# üêõ Debug mode setting (DEBUG_ENABLED: 'Y' = keep intermediate files, 'N' = keep final files only)
DEBUG_ENABLED = 'Y'

# üóÇÔ∏è Catalog and database configuration (used when executing EXPLAIN statements)
CATALOG = 'tpcds'
DATABASE = 'tpcds_sf1000_delta_lc'

# COMMAND ----------

def save_debug_query_trial(query: str, attempt_num: int, trial_type: str, query_id: str = None, error_info: str = None) -> str:
    """
    Save queries under optimization attempt by attempt when DEBUG_ENABLED=Y
    
    Args:
        query: Generated query
        attempt_num: Trial number (1, 2, 3, ...)
        trial_type: Trial type ('initial', 'performance_improvement', 'error_correction')
        query_id: Query ID (optional)
        error_info: Error information (optional)
    
    Returns:
        Saved file path (empty string if not saved)
    """
    debug_enabled = globals().get('DEBUG_ENABLED', 'N')
    if debug_enabled.upper() != 'Y':
        return ""
    
    try:
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        
        # Generate query ID from trial number if not specified
        if not query_id:
            query_id = f"trial_{attempt_num}"
        
        # Generate filename: debug_trial_{attempt_num}_{trial_type}_{timestamp}.sql
        filename = f"debug_trial_{attempt_num:02d}_{trial_type}_{timestamp}.sql"
        
        # Prepare metadata information
        metadata_header = f"""-- üêõ DEBUG: Optimization trial query (DEBUG_ENABLED=Y)
-- üìã Trial number: {attempt_num}
-- üéØ Trial type: {trial_type}
-- üïê Generated time: {timestamp}
-- üîç Query ID: {query_id}
"""
        
        # Add error information if available
        if error_info:
            metadata_header += f"""-- ‚ö†Ô∏è  Error information: {error_info[:200]}{'...' if len(error_info) > 200 else ''}
"""
        
        metadata_header += f"""-- üìÑ Generated file: {filename}
-- ================================================

"""
        
        # File saving
        full_content = metadata_header + query
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        print(f"üêõ DEBUG save completed: {filename} (attempt {attempt_num}: {trial_type})")
        return filename
        
    except Exception as e:
        print(f"‚ö†Ô∏è DEBUG save error: {str(e)}")
        return ""

# üß† Structured extraction settings (STRUCTURED_EXTRACTION_ENABLED: 'Y' = use structured extraction, 'N' = use traditional truncation)
# Controls the processing method for Physical Plan and EXPLAIN COST
# - 'Y': Structured extraction of important information only (recommended: high precision & high efficiency)
# - 'N': Traditional truncation based on character limits (for fallback)
STRUCTURED_EXTRACTION_ENABLED = 'Y'

# üîÑ Maximum retry count settings for automatic error correction (MAX_RETRIES: default 2 times)
# Number of retries when EXPLAIN execution of LLM-generated optimized queries encounters errors
# - 1st attempt: EXPLAIN execution with initial generated query
# - 2nd attempt and beyond: Re-input error information to LLM to generate corrected query and re-execute
# - When maximum attempts reached: Use original working query for file generation
MAX_RETRIES = 3

# üöÄ Iterative optimization maximum attempt count settings (MAX_OPTIMIZATION_ATTEMPTS: default 3 times)
# Number of improvement attempts when performance degradation is detected
# - 1st attempt: Initial optimization query generation and performance verification
# - 2nd attempt and beyond: Corrected query generation and verification based on degradation cause analysis
# - When maximum attempts reached: Use original query
# Note: This is a separate parameter from syntax error correction (MAX_RETRIES)
MAX_OPTIMIZATION_ATTEMPTS = 3

# üí° Usage examples:
# OUTPUT_LANGUAGE = 'ja'  # Output files in Japanese
# OUTPUT_LANGUAGE = 'en'  # Output files in English

# üåê Multilingual message dictionary
MESSAGES = {
    'ja': {
        'bottleneck_title': 'Databricks SQL Profiler Bottleneck Analysis Results',
        'query_id': 'Query ID',
        'analysis_time': 'Analysis Date/Time',
        'execution_time': 'Execution Time',
        'sql_optimization_report': 'SQL Optimization Report',
        'optimization_time': 'Optimization Date/Time',
        'original_file': 'Original File',
        'optimized_file': 'Optimized File',
        'optimization_analysis': 'Optimization Analysis Results',
        'performance_metrics': 'Performance Metrics Reference Information',
        'read_data': 'Data Read',
        'spill': 'Spill',
        'top10_processes': 'TOP10 Most Time-Consuming Processes'
    },
    'en': {
        'bottleneck_title': 'Databricks SQL Profiler Bottleneck Analysis Results',
        'query_id': 'Query ID',
        'analysis_time': 'Analysis Time',
        'execution_time': 'Execution Time',
        'sql_optimization_report': 'SQL Optimization Report',
        'optimization_time': 'Optimization Time',
        'original_file': 'Original File',
        'optimized_file': 'Optimized File',
        'optimization_analysis': 'Optimization Analysis Results',
        'performance_metrics': 'Performance Metrics Reference',
        'read_data': 'Data Read',
        'spill': 'Spill',
        'top10_processes': 'Top 10 Most Time-Consuming Processes'
    }
}

def get_message(key: str) -> str:
    """Get multilingual message"""
    return MESSAGES.get(OUTPUT_LANGUAGE, MESSAGES['ja']).get(key, key)

# üìã Supported file path format examples:
# Unity Catalog Volumes:
# JSON_FILE_PATH = '/Volumes/catalog/schema/volume/profiler.json'
# 
# FileStore (recommended):
# JSON_FILE_PATH = '/FileStore/shared_uploads/your_username/profiler_log.json'
# 
# DBFS:
# JSON_FILE_PATH = '/dbfs/FileStore/shared_uploads/your_username/profiler_log.json'
# 
# DBFS URI:
# JSON_FILE_PATH = 'dbfs:/FileStore/shared_uploads/your_username/profiler_log.json'

print("üìÅ „ÄêAnalysis Target File Configuration Completed„Äë")
print("=" * 50)
print(f"üìÑ Target file: {JSON_FILE_PATH}")
print("=" * 50)

# ‚öôÔ∏è Basic environment configuration
import json
try:
    import pandas as pd
except ImportError:
    print("Warning: pandas is not installed, some features may not work")
    pd = None
from typing import Dict, List, Any, Optional
from datetime import datetime

print("‚úÖ Basic library import completed")
print("üöÄ Please proceed to the next cell")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ü§ñ LLM Endpoint Configuration
# MAGIC
# MAGIC This cell performs the following configurations:
# MAGIC - LLM provider selection (Databricks/OpenAI/Azure/Anthropic)
# MAGIC - Connection settings for each provider
# MAGIC - Required library imports

# COMMAND ----------

# ü§ñ LLM Endpoint Configuration
LLM_CONFIG = {
    # Endpoint type: 'databricks', 'openai', 'azure_openai', 'anthropic'
    "provider": "databricks",
    
    # Databricks Model Serving configuration (high-speed execution priority)
    "databricks": {
        "endpoint_name": "databricks-claude-3-7-sonnet",  # Model Serving endpoint name
        "max_tokens": 131072,  # 128K tokens (Claude 3.7 Sonnet maximum limit)
        "temperature": 0.0,    # For deterministic output (0.1‚Üí0.0)
        # "thinking_enabled": False,  # Extended thinking mode (default: disabled - high-speed execution priority) - Claude 3 Sonnet only
        # "thinking_budget_tokens": 65536  # Thinking token budget 64K tokens (used only when enabled) - Claude 3 Sonnet only
    },
    
    # OpenAI configuration (optimized for complete SQL generation)
    "openai": {
        "api_key": "",  # OpenAI API key (can also use environment variable OPENAI_API_KEY)
        "model": "gpt-4o",  # gpt-4o, gpt-4-turbo, gpt-3.5-turbo
        "max_tokens": 16000,  # Maximum within OpenAI limits
        "temperature": 0.0    # For deterministic output (0.1‚Üí0.0)
    },
    
    # Azure OpenAI configuration (optimized for complete SQL generation)
    "azure_openai": {
        "api_key": "",  # Azure OpenAI API key (can also use environment variable AZURE_OPENAI_API_KEY)
        "endpoint": "",  # https://your-resource.openai.azure.com/
        "deployment_name": "",  # Deployment name
        "api_version": "2024-02-01",
        "max_tokens": 16000,  # Maximum within Azure OpenAI limits
        "temperature": 0.0    # For deterministic output (0.1‚Üí0.0)
    },
    
    # Anthropic configuration (optimized for complete SQL generation)
    "anthropic": {
        "api_key": "",  # Anthropic API key (can also use environment variable ANTHROPIC_API_KEY)
        "model": "claude-3-5-sonnet-20241022",  # claude-3-5-sonnet-20241022, claude-3-opus-20240229
        "max_tokens": 16000,  # Maximum within Anthropic limits
        "temperature": 0.0    # For deterministic output (0.1‚Üí0.0)
    }
}

print("ü§ñ LLM endpoint configuration completed")
print(f"ü§ñ LLM Provider: {LLM_CONFIG['provider']}")

if LLM_CONFIG['provider'] == 'databricks':
    print(f"üîó Databricks endpoint: {LLM_CONFIG['databricks']['endpoint_name']}")
    thinking_status = "Enabled" if LLM_CONFIG['databricks'].get('thinking_enabled', False) else "Disabled"
    thinking_budget = LLM_CONFIG['databricks'].get('thinking_budget_tokens', 65536)
    max_tokens = LLM_CONFIG['databricks'].get('max_tokens', 131072)
    print(f"üß† Extended thinking mode: {thinking_status} (budget: {thinking_budget:,} tokens)")
    print(f"üìä Maximum tokens: {max_tokens:,} tokens ({max_tokens//1024}K)")
    if not LLM_CONFIG['databricks'].get('thinking_enabled', False):
        print("‚ö° Fast execution mode: Skip thinking process for rapid result generation")
elif LLM_CONFIG['provider'] == 'openai':
    print(f"üîó OpenAI model: {LLM_CONFIG['openai']['model']}")
elif LLM_CONFIG['provider'] == 'azure_openai':
    print(f"üîó Azure OpenAI deployment: {LLM_CONFIG['azure_openai']['deployment_name']}")
elif LLM_CONFIG['provider'] == 'anthropic':
    print(f"üîó Anthropic model: {LLM_CONFIG['anthropic']['model']}")

print()
print("üí° LLM provider switching examples:")
print('   LLM_CONFIG["provider"] = "openai"      # Switch to OpenAI GPT-4')
print('   LLM_CONFIG["provider"] = "anthropic"   # Switch to Anthropic Claude')
print('   LLM_CONFIG["provider"] = "azure_openai" # Switch to Azure OpenAI')
print()
print("üß† Databricks extended thinking mode configuration examples:")
print('   LLM_CONFIG["databricks"]["thinking_enabled"] = False  # Disable extended thinking mode (default, fast execution)')
print('   LLM_CONFIG["databricks"]["thinking_enabled"] = True   # Enable extended thinking mode (detailed analysis only)')
print('   LLM_CONFIG["databricks"]["thinking_budget_tokens"] = 65536  # Thinking token budget (64K)')
print('   LLM_CONFIG["databricks"]["max_tokens"] = 131072  # Maximum tokens (128K)')
print()

# Import necessary libraries
try:
    import requests
except ImportError:
    print("Warning: requests is not installed, some features may not work")
    requests = None
import os
try:
    from pyspark.sql import SparkSession
except ImportError:
    print("Warning: pyspark is not installed")
    SparkSession = None
    print("‚úÖ Spark Version: Not available")

# Safely retrieve Databricks Runtime information
try:
    if spark is not None:
        runtime_version = spark.conf.get('spark.databricks.clusterUsageTags.sparkVersion')
    print(f"‚úÖ Databricks Runtime: {runtime_version}")
except Exception:
    try:
        # Retrieve DBR information using alternative method
        dbr_version = spark.conf.get('spark.databricks.clusterUsageTags.clusterName', 'Unknown')
        print(f"‚úÖ Databricks Cluster: {dbr_version}")
    except Exception:
        print("‚úÖ Databricks Environment: Skipped configuration information retrieval")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìÇ SQL Profiler JSON File Loading Function
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - SQL profiler JSON file loading
# MAGIC - Automatic detection of DBFS/FileStore/local paths
# MAGIC - File size and data information display

# COMMAND ----------

def load_profiler_json(file_path: str) -> Dict[str, Any]:
    """
    Load SQL profiler JSON file
    
    Args:
        file_path: JSON file path (DBFS or local path)
        
    Returns:
        Dict: Parsed JSON data
    """
    try:
        # Handle DBFS paths appropriately
        if file_path.startswith('/dbfs/'):
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        elif file_path.startswith('dbfs:/'):
            # Convert dbfs: prefix to /dbfs/
            local_path = file_path.replace('dbfs:', '/dbfs')
            with open(local_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        elif file_path.startswith('/FileStore/'):
            # Convert FileStore path to /dbfs/FileStore/
            local_path = '/dbfs' + file_path
            with open(local_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        else:
            # Local file
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        
        print(f"‚úÖ Successfully loaded JSON file: {file_path}")
        print(f"üìä Data size: {len(str(data)):,} characters")
        return data
    except Exception as e:
        print(f"‚ùå File loading error: {str(e)}")
        return {}

print("‚úÖ Function definition completed: load_profiler_json")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìä Performance Metrics Extraction Function
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - Metrics extraction from SQL profiler data
# MAGIC - Query basic information retrieval
# MAGIC - Overall/stage/node-level performance indicator calculation
# MAGIC - Spill detection and bottleneck indicator analysis

# COMMAND ----------

def detect_data_format(profiler_data: Dict[str, Any]) -> str:
    """
    Detect JSON data format
    """
    # SQL profiler format detection
    if 'graphs' in profiler_data and isinstance(profiler_data['graphs'], list):
        if len(profiler_data['graphs']) > 0:
            return 'sql_profiler'
    
    # SQL query summary format detection (test2.json format)
    if 'query' in profiler_data and 'planMetadatas' in profiler_data:
        query_data = profiler_data.get('query', {})
        if 'metrics' in query_data:
            return 'sql_query_summary'
    
    return 'unknown'

def extract_performance_metrics_from_query_summary(profiler_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extract basic metrics from Databricks SQL query summary format JSON
    (supports test2.json format)
    """
    try:
        query_data = profiler_data.get('query', {})
        metrics_data = query_data.get('metrics', {})
        
        if not metrics_data:
            print("‚ö†Ô∏è No metrics data found")
            return {}
        
        print(f"‚úÖ Detected SQL query summary format metrics")
        print(f"   - Execution time: {metrics_data.get('totalTimeMs', 0):,} ms")
        print(f"   - Data read: {metrics_data.get('readBytes', 0) / 1024 / 1024 / 1024:.2f} GB")
        print(f"   - Rows processed: {metrics_data.get('rowsReadCount', 0):,} rows")
        
        # Extract basic metrics
        overall_metrics = {
            'total_time_ms': metrics_data.get('totalTimeMs', 0),
            'execution_time_ms': metrics_data.get('executionTimeMs', 0),
            'compilation_time_ms': metrics_data.get('compilationTimeMs', 0),
            'read_bytes': metrics_data.get('readBytes', 0),
            'read_remote_bytes': metrics_data.get('readRemoteBytes', 0),
            'read_cache_bytes': metrics_data.get('readCacheBytes', 0),
            'spill_to_disk_bytes': metrics_data.get('spillToDiskBytes', 0),
            'rows_produced_count': metrics_data.get('rowsProducedCount', 0),
            'rows_read_count': metrics_data.get('rowsReadCount', 0),
            'read_files_count': metrics_data.get('readFilesCount', 0),
            'read_partitions_count': metrics_data.get('readPartitionsCount', 0),
            'photon_total_time_ms': metrics_data.get('photonTotalTimeMs', 0),
            'task_total_time_ms': metrics_data.get('taskTotalTimeMs', 0),
            'network_sent_bytes': metrics_data.get('networkSentBytes', 0),
            'photon_enabled': metrics_data.get('photonTotalTimeMs', 0) > 0,
            'photon_utilization_ratio': 0
        }
        
        # Calculate Photon utilization rate
        if overall_metrics['task_total_time_ms'] > 0:
            overall_metrics['photon_utilization_ratio'] = min(
                overall_metrics['photon_total_time_ms'] / overall_metrics['task_total_time_ms'], 1.0
            )
        
        # Calculate cache hit rate
        cache_hit_ratio = 0
        if overall_metrics['read_bytes'] > 0:
            cache_hit_ratio = overall_metrics['read_cache_bytes'] / overall_metrics['read_bytes']
        
        # Calculate bottleneck indicators
        bottleneck_indicators = {
            'spill_bytes': overall_metrics['spill_to_disk_bytes'],
            'has_spill': overall_metrics['spill_to_disk_bytes'] > 0,
            'cache_hit_ratio': cache_hit_ratio,
            'has_cache_miss': cache_hit_ratio < 0.8,
            'photon_efficiency': overall_metrics['photon_utilization_ratio'],
            'has_shuffle_bottleneck': False,  # Cannot determine due to lack of detailed information
            'remote_read_ratio': 0,
            'has_memory_pressure': overall_metrics['spill_to_disk_bytes'] > 0,
            'max_task_duration_ratio': 1.0,  # Unknown
            'has_data_skew': False  # Cannot determine due to lack of detailed information
        }
        
        # Calculate remote read ratio
        if overall_metrics['read_bytes'] > 0:
            bottleneck_indicators['remote_read_ratio'] = overall_metrics['read_remote_bytes'] / overall_metrics['read_bytes']
        
        # Extract query information
        query_info = {
            'query_id': query_data.get('id', ''),
            'query_text': query_data.get('queryText', '')[:300] + "..." if len(query_data.get('queryText', '')) > 300 else query_data.get('queryText', ''),
            'status': query_data.get('status', ''),
            'query_start_time': query_data.get('queryStartTimeMs', 0),
            'query_end_time': query_data.get('queryEndTimeMs', 0),
            'spark_ui_url': query_data.get('sparkUiUrl', ''),
            'endpoint_id': query_data.get('endpointId', ''),
            'user': query_data.get('user', {}).get('displayName', ''),
            'statement_type': query_data.get('statementType', ''),
            'plans_state': query_data.get('plansState', '')
        }
        
        # Calculate detailed performance insights (recalculated later with node_metrics)
        performance_insights = calculate_performance_insights_from_metrics(overall_metrics, None)
        
        # Pseudo node metrics (generated from summary information)
        summary_node = {
            'node_id': 'summary_node',
            'name': f'Query Execution Summary ({query_data.get("statementType", "SQL")})',
            'tag': 'QUERY_SUMMARY',
            'key_metrics': {
                'durationMs': overall_metrics['total_time_ms'],
                'rowsNum': overall_metrics['rows_read_count'],
                'peakMemoryBytes': 0,  # Unknown
                'throughputMBps': performance_insights['parallelization']['throughput_mb_per_second'],
                'dataSelectivity': performance_insights['data_efficiency']['data_selectivity'],
                'cacheHitRatio': performance_insights['cache_efficiency']['cache_hit_ratio']
            },
            'detailed_metrics': {
                'Total Time': {'value': overall_metrics['total_time_ms'], 'display_name': 'Total Time'},
                'Read Bytes': {'value': overall_metrics['read_bytes'], 'display_name': 'Read Bytes'},
                'Spill Bytes': {'value': overall_metrics['spill_to_disk_bytes'], 'display_name': 'Spill to Disk'},
                'Photon Time': {'value': overall_metrics['photon_total_time_ms'], 'display_name': 'Photon Time'},
                'Rows Read': {'value': overall_metrics['rows_read_count'], 'display_name': 'Rows Read Count'},
                'Cache Hit Ratio': {'value': performance_insights['cache_efficiency']['cache_hit_ratio'], 'display_name': 'Cache Hit Ratio'},
                'Filter Rate': {'value': performance_insights['data_efficiency']['data_selectivity'], 'display_name': 'Filter Rate'},
                'Throughput': {'value': performance_insights['parallelization']['throughput_mb_per_second'], 'display_name': 'Throughput (MB/s)'}
            },
            'graph_index': 0,
            'performance_insights': performance_insights
        }
        
        # Recalculate performance_insights with complete metrics
        complete_metrics = {
            'overall_metrics': overall_metrics,
            'node_metrics': [summary_node]
        }
        performance_insights = calculate_performance_insights_from_metrics(overall_metrics, complete_metrics)
        
        return {
            'data_format': 'sql_query_summary',
            'query_info': query_info,
            'overall_metrics': overall_metrics,
            'bottleneck_indicators': bottleneck_indicators,
            'node_metrics': [summary_node],
            'stage_metrics': [],  # No detailed stage information
            'liquid_clustering_analysis': {},  # To be added later
            'raw_profiler_data': profiler_data,
            'performance_insights': performance_insights,  # Add detailed performance insights
            'analysis_capabilities': [
                'Metrics-based bottleneck analysis (cache efficiency, filter rate, Photon efficiency)',
                'Resource usage analysis (spill, parallelization efficiency, throughput)',
                'Performance metrics calculation (file efficiency, partition efficiency)',
                'Potential bottleneck identification (metrics-based)'
            ],
            'analysis_limitations': [
                'Detailed execution plan information (nodes, edges) is not available',
                'Stage-level metrics are not available', 
                'BROADCAST analysis is limited to basic estimation only',
                'Liquid Clustering analysis provides general recommendations only',
                'Data skew detection is based on average-value estimation only',
                'Detailed query structure analysis is not performed (metrics-focused approach)'
            ]
        }
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error extracting SQL query summary format metrics: {str(e)}")
        return {}

def extract_performance_metrics(profiler_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extract bottleneck analysis metrics from SQL profiler data (supports multiple formats)
    """
    # Detect data format
    data_format = detect_data_format(profiler_data)
    
    print(f"üîç Detected data format: {data_format}")
    
    if data_format == 'sql_query_summary':
        print("üìä Processing as Databricks SQL query summary format...")
        result = extract_performance_metrics_from_query_summary(profiler_data)
        if result:
            # Add Liquid Clustering analysis (with limitations)
            try:
                result["liquid_clustering_analysis"] = analyze_liquid_clustering_opportunities(profiler_data, result)
            except Exception as e:
                print(f"‚ö†Ô∏è Skipping Liquid Clustering analysis: {str(e)}")
                result["liquid_clustering_analysis"] = {}
        return result
    elif data_format == 'sql_profiler':
        print("üìä Processing as SQL profiler detailed format...")
        # Continue processing existing SQL profiler format
        pass
    else:
        print(f"‚ö†Ô∏è Unknown data format: {data_format}")
        return {}
    
    # Processing existing SQL profiler format
    metrics = {
        "query_info": {},
        "overall_metrics": {},
        "stage_metrics": [],
        "node_metrics": [],
        "bottleneck_indicators": {},
        "liquid_clustering_analysis": {},
        "raw_profiler_data": profiler_data  # Save raw data for plan analysis
    }
    
    # Basic query information
    if 'query' in profiler_data:
        query = profiler_data['query']
        metrics["query_info"] = {
            "query_id": query.get('id', ''),
            "status": query.get('status', ''),
            "query_start_time": query.get('queryStartTimeMs', 0),
            "query_end_time": query.get('queryEndTimeMs', 0),
            "user": query.get('user', {}).get('displayName', ''),
            "query_text": query.get('queryText', '')[:300] + "..." if len(query.get('queryText', '')) > 300 else query.get('queryText', '')
        }
        
        # Overall metrics
        if 'metrics' in query:
            query_metrics = query['metrics']
            metrics["overall_metrics"] = {
                "total_time_ms": query_metrics.get('totalTimeMs', 0),
                "compilation_time_ms": query_metrics.get('compilationTimeMs', 0),
                "execution_time_ms": query_metrics.get('executionTimeMs', 0),
                "read_bytes": query_metrics.get('readBytes', 0),
                "read_remote_bytes": query_metrics.get('readRemoteBytes', 0),
                "read_cache_bytes": query_metrics.get('readCacheBytes', 0),
                "rows_produced_count": query_metrics.get('rowsProducedCount', 0),
                "rows_read_count": query_metrics.get('rowsReadCount', 0),
                "spill_to_disk_bytes": query_metrics.get('spillToDiskBytes', 0),
                "read_files_count": query_metrics.get('readFilesCount', 0),
                "task_total_time_ms": query_metrics.get('taskTotalTimeMs', 0),
                "photon_total_time_ms": query_metrics.get('photonTotalTimeMs', 0),
                # Photon usage analysis (Photon execution time / total task time)
                "photon_enabled": query_metrics.get('photonTotalTimeMs', 0) > 0,
                "photon_utilization_ratio": min(query_metrics.get('photonTotalTimeMs', 0) / max(query_metrics.get('taskTotalTimeMs', 1), 1), 1.0)
            }
    
    # Extract stage and node metrics from graph data (supports multiple graphs)
    if 'graphs' in profiler_data and profiler_data['graphs']:
        # Analyze all graphs
        for graph_index, graph in enumerate(profiler_data['graphs']):
            print(f"üîç Analyzing graph {graph_index}...")
            
            # Stage data
            if 'stageData' in graph:
                for stage in graph['stageData']:
                    stage_metric = {
                        "stage_id": stage.get('stageId', ''),
                        "status": stage.get('status', ''),
                        "duration_ms": stage.get('keyMetrics', {}).get('durationMs', 0),
                        "num_tasks": stage.get('numTasks', 0),
                        "num_failed_tasks": stage.get('numFailedTasks', 0),
                        "num_complete_tasks": stage.get('numCompleteTasks', 0),
                        "start_time_ms": stage.get('startTimeMs', 0),
                        "end_time_ms": stage.get('endTimeMs', 0),
                        "graph_index": graph_index  # Record which graph this originates from
                    }
                    metrics["stage_metrics"].append(stage_metric)
            
            # Node data (important ones only)
            if 'nodes' in graph:
                for node in graph['nodes']:
                    if not node.get('hidden', False):
                        # Use keyMetrics as-is (durationMs is already in milliseconds)
                        key_metrics = node.get('keyMetrics', {})
                        
                        node_metric = {
                            "node_id": node.get('id', ''),
                            "name": node.get('name', ''),
                            "tag": node.get('tag', ''),
                            "key_metrics": key_metrics,  # Unit-converted key_metrics
                            "metrics": node.get('metrics', []),  # Retain original metrics array
                            "metadata": node.get('metadata', []),  # Add metadata
                            "graph_index": graph_index  # Record which graph this originates from
                        }
                        
                        # Extract only important metrics in detail (added spill-related keywords, label support)
                        detailed_metrics = {}
                        for metric in node.get('metrics', []):
                            metric_key = metric.get('key', '')
                            metric_label = metric.get('label', '')
                            
                            # Check keywords in both key and label
                            key_keywords = ['TIME', 'MEMORY', 'ROWS', 'BYTES', 'DURATION', 'PEAK', 'CUMULATIVE', 'EXCLUSIVE', 
                                           'SPILL', 'DISK', 'PRESSURE', 'SINK']
                            
                            # Extract when metric_key or metric_label contains important keywords
                            is_important_metric = (
                                any(keyword in metric_key.upper() for keyword in key_keywords) or
                                any(keyword in metric_label.upper() for keyword in key_keywords)
                            )
                            
                            if is_important_metric:
                                                                  # Use label as metric name if valid, otherwise use key
                                metric_name = metric_label if metric_label and metric_label != 'UNKNOWN_KEY' else metric_key
                                detailed_metrics[metric_name] = {
                                    'value': metric.get('value', 0),
                                    'label': metric_label,
                                    'type': metric.get('metricType', ''),
                                    'original_key': metric_key,  # Save original key name
                                    'display_name': metric_name  # Display name
                                }
                        node_metric['detailed_metrics'] = detailed_metrics
                        metrics["node_metrics"].append(node_metric)
    
    # Calculate bottleneck indicators
    metrics["bottleneck_indicators"] = calculate_bottleneck_indicators(metrics)
    
    # Liquid Clustering analysis
    metrics["liquid_clustering_analysis"] = analyze_liquid_clustering_opportunities(profiler_data, metrics)
    
    return metrics

print("‚úÖ Function definition completed: extract_performance_metrics")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üè∑Ô∏è Node Name Analysis & Enhancement Function
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - Concretization of generic node names (Whole Stage Codegen, etc.)
# MAGIC - Related node search and optimal processing name selection
# MAGIC - Addition of Photon information and table information
# MAGIC - Semantic improvement of processing names

# COMMAND ----------

def get_meaningful_node_name(node: Dict[str, Any], extracted_metrics: Dict[str, Any]) -> str:
    """
    Function to get more meaningful node names
    Convert generic names (such as Whole Stage Codegen) to specific process names
    """
    original_name = node.get('name', '')
    node_id = node.get('node_id', node.get('id', ''))
    node_tag = node.get('tag', '')
    
    # Get detailed information from metadata
    metadata = node.get('metadata', [])
    metadata_info = {}
    for meta in metadata:
        key = meta.get('key', '')
        value = meta.get('value', '')
        label = meta.get('label', '')
        if value:
            metadata_info[key] = value
    
    # 1. Replace generic names with specific names
    if 'whole stage codegen' in original_name.lower():
        # Heuristic to infer more specific process names
        
        # Infer relevance based on node ID (adjacent IDs)
        node_id_num = None
        try:
            node_id_num = int(node_id) if node_id else None
        except:
            pass
        
        if node_id_num:
            # Look for specific processes with nearby IDs in the same file
            all_nodes = extracted_metrics.get('node_metrics', [])
            nearby_specific_nodes = []
            
            for other_node in all_nodes:
                other_id = other_node.get('node_id', '')
                other_name = other_node.get('name', '')
                
                try:
                    other_id_num = int(other_id) if other_id else None
                    if other_id_num and abs(other_id_num - node_id_num) <= 10:  # Within 10 nearby
                        if is_specific_process_name(other_name):
                            nearby_specific_nodes.append(other_name)
                except:
                    continue
            
            # Select the most specific process name
            if nearby_specific_nodes:
                specific_name = get_most_specific_process_name_from_list(nearby_specific_nodes)
                if specific_name and specific_name != original_name:
                    return f"{specific_name} (Whole Stage Codegen)"
        
        # Fallback: Extract more specific information from tag
        if 'CODEGEN' in node_tag:
            # Check child tag information from metadata
            child_tag = metadata_info.get('CHILD_TAG', '')
            if child_tag and child_tag != 'Child':
                return f"Whole Stage Codegen ({child_tag})"
    
    # 2. Reflect more specific tag information in node name
    tag_to_name_mapping = {
        'PHOTON_SHUFFLE_EXCHANGE_SINK_EXEC': 'Photon Shuffle Exchange',
        'PHOTON_GROUPING_AGG_EXEC': 'Photon Grouping Aggregate', 
        'UNKNOWN_DATA_SOURCE_SCAN_EXEC': 'Data Source Scan',
        'HASH_AGGREGATE_EXEC': 'Hash Aggregate',
        'WHOLE_STAGE_CODEGEN_EXEC': 'Whole Stage Codegen'
    }
    
    if node_tag in tag_to_name_mapping:
        mapped_name = tag_to_name_mapping[node_tag]
        if mapped_name != original_name and mapped_name != 'Whole Stage Codegen':
            # Use tag if it's more specific
            enhanced_name = mapped_name
        else:
            enhanced_name = original_name
    else:
        enhanced_name = original_name
    
    # 3. Add processing details from metadata
    
    # Add database and table information (enhanced version)
    table_name = None
    
    # Extract table name from multiple metadata keys (prioritize full path)
    for key_candidate in ['SCAN_TABLE', 'SCAN_IDENTIFIER', 'TABLE_NAME', 'RELATION', 'SCAN_RELATION']:
        if key_candidate in metadata_info:
            extracted_table = metadata_info[key_candidate]
            # Use as-is for full path (catalog.schema.table)
            if isinstance(extracted_table, str) and extracted_table.count('.') >= 2:
                table_name = extracted_table
                break
            elif isinstance(extracted_table, str) and extracted_table.count('.') == 1:
                # Use as-is for schema.table format as well
                table_name = extracted_table
                break
            elif not table_name:  # Set only if table name has not been found yet
                table_name = extracted_table
    
    # If table name cannot be extracted from metadata, infer from node name
    if not table_name and ('scan' in enhanced_name.lower() or 'data source' in enhanced_name.lower()):
        # Infer table name from node name
        import re
        
        # Format like "Scan tpcds.tpcds_sf1000_delta_lc.customer"
        table_patterns = [
            r'[Ss]can\s+([a-zA-Z_][a-zA-Z0-9_.]*[a-zA-Z0-9_])',
            r'[Tt]able\s+([a-zA-Z_][a-zA-Z0-9_.]*[a-zA-Z0-9_])',
            r'([a-zA-Z_][a-zA-Z0-9_]*\.)+([a-zA-Z_][a-zA-Z0-9_]*)',
        ]
        
        for pattern in table_patterns:
            match = re.search(pattern, original_name)
            if match:
                if '.' in match.group(0):
                    # Use full path for full table name (catalog.schema.table)
                    table_name = match.group(0)
                else:
                    table_name = match.group(1) if match.lastindex and match.lastindex >= 1 else match.group(0)
                break
    
            # Search for table name from metadata values field as well
    if not table_name:
        for meta in metadata:
            values = meta.get('values', [])
            if values:
                for value in values:
                    if isinstance(value, str) and '.' in value and len(value.split('.')) >= 2:
                        # For "catalog.schema.table" format
                        parts = value.split('.')
                        if len(parts) >= 2 and not any(part.isdigit() for part in parts[-2:]):
                            # Use full path (catalog.schema.table)
                            if len(parts) >= 3:
                                table_name = '.'.join(parts)  # Full path
                            else:
                                table_name = value  # Use as is
                            break
                if table_name:
                    break
    
    # Display table name for Data Source Scan
    if table_name and ('scan' in enhanced_name.lower() or 'data source' in enhanced_name.lower()):
        # Relax limits for full path display (up to 60 characters)
        if len(table_name) > 60:
            # Abbreviate middle part for catalog.schema.table format
            parts = table_name.split('.')
            if len(parts) >= 3:
                table_name = f"{parts[0]}.*.{parts[-1]}"
            else:
                table_name = table_name[:57] + "..."
        enhanced_name = f"Data Source Scan ({table_name})"
    elif 'scan' in enhanced_name.lower() and 'data source' in enhanced_name.lower():
        # Use clearer name even when table name is not found
        enhanced_name = "Data Source Scan"
    
    # Add Photon information
    if 'IS_PHOTON' in metadata_info and metadata_info['IS_PHOTON'] == 'true':
        if not enhanced_name.startswith('Photon'):
            enhanced_name = f"Photon {enhanced_name}"
    
    return enhanced_name

def find_related_specific_nodes(target_node_id: str, nodes: list, edges: list) -> list:
    """Search for specific processing nodes related to the specified node"""
    
    # Identify related nodes from edges
    related_node_ids = set()
    
    # Directly connected nodes
    for edge in edges:
        from_id = edge.get('fromId', '')
        to_id = edge.get('toId', '')
        
        if from_id == target_node_id:
            related_node_ids.add(to_id)
        elif to_id == target_node_id:
            related_node_ids.add(from_id)
    
    # Get details of related nodes
    related_nodes = []
    for node in nodes:
        node_id = node.get('id', '')
        if node_id in related_node_ids:
            node_name = node.get('name', '')
            # Select only nodes with specific process names
            if is_specific_process_name(node_name):
                related_nodes.append(node)
    
    return related_nodes

def is_specific_process_name(name: str) -> bool:
    """Determine if it's a specific processing name"""
    specific_keywords = [
        'columnar to row', 'row to columnar', 'filter', 'project', 'join',
        'aggregate', 'sort', 'exchange', 'broadcast', 'scan', 'union'
    ]
    
    generic_keywords = [
        'whole stage codegen', 'stage', 'query', 'result'
    ]
    
    name_lower = name.lower()
    
            # When containing specific keywords
    for keyword in specific_keywords:
        if keyword in name_lower:
            return True
    
            # Exclude cases with only generic keywords
    for keyword in generic_keywords:
        if keyword in name_lower and len(name_lower.split()) <= 3:
            return False
    
    return True

def get_most_specific_process_name(nodes: list) -> str:
    """Select the most specific processing name"""
    if not nodes:
        return ""
    
    # Priority: More specific and meaningful process names
    priority_keywords = [
        'columnar to row', 'row to columnar', 'filter', 'project',
        'hash join', 'broadcast join', 'sort merge join',
        'hash aggregate', 'sort aggregate', 'grouping aggregate'
    ]
    
    for keyword in priority_keywords:
        for node in nodes:
            node_name = node.get('name', '').lower()
            if keyword in node_name:
                return node.get('name', '')
    
    # Fallback: First specific node name
    for node in nodes:
        node_name = node.get('name', '')
        if is_specific_process_name(node_name):
            return node_name
    
    return ""

def get_most_specific_process_name_from_list(node_names: list) -> str:
    """Select the most specific processing name from a list of node names"""
    if not node_names:
        return ""
    
    # Priority: More specific and meaningful process names
    priority_keywords = [
        'columnar to row', 'row to columnar', 'filter', 'project',
        'hash join', 'broadcast join', 'sort merge join',
        'hash aggregate', 'sort aggregate', 'grouping aggregate'
    ]
    
    for keyword in priority_keywords:
        for name in node_names:
            if keyword in name.lower():
                return name
    
    # Fallback: First specific node name
    for name in node_names:
        if is_specific_process_name(name):
            return name
    
    return ""

def extract_shuffle_attributes(node: Dict[str, Any]) -> list:
    """
    Extract SHUFFLE_ATTRIBUTES from Shuffle node
    
    Args:
        node: Node information
        
    Returns:
        list: Detected Shuffle attributes
    """
    shuffle_attributes = []
    
    # Search for SHUFFLE_ATTRIBUTES from metadata
    metadata = node.get('metadata', [])
    if isinstance(metadata, list):
        for item in metadata:
            if isinstance(item, dict):
                item_key = item.get('key', '')
                item_label = item.get('label', '')
                item_values = item.get('values', [])
                
                # Check both key and label
                if (item_key == 'SHUFFLE_ATTRIBUTES' or 
                    item_label == 'Shuffle attributes'):
                    if isinstance(item_values, list):
                        shuffle_attributes.extend(item_values)
    
    # Search from raw_metrics as well (also check label)
    raw_metrics = node.get('metrics', [])
    if isinstance(raw_metrics, list):
        for metric in raw_metrics:
            if isinstance(metric, dict):
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_values = metric.get('values', [])
                
                if (metric_key == 'SHUFFLE_ATTRIBUTES' or 
                    metric_label == 'Shuffle attributes'):
                    if isinstance(metric_values, list):
                        shuffle_attributes.extend(metric_values)
    
    # Search from detailed_metrics as well
    detailed_metrics = node.get('detailed_metrics', {})
    if isinstance(detailed_metrics, dict):
        for key, info in detailed_metrics.items():
            if (key == 'SHUFFLE_ATTRIBUTES' or 
                (isinstance(info, dict) and info.get('label') == 'Shuffle attributes')):
                values = info.get('values', []) if isinstance(info, dict) else []
                if isinstance(values, list):
                    shuffle_attributes.extend(values)
    
    # Remove duplicates
    return list(set(shuffle_attributes))

def extract_cluster_attributes(node: Dict[str, Any]) -> list:
    """
    Extract clustering keys (SCAN_CLUSTERS) from scan node
    
    Args:
        node: Node information
        
    Returns:
        list: Detected clustering keys
    """
    cluster_attributes = []
    node_name = node.get('name', 'Unknown')
    
    print(f"    üîç Debug extract_cluster_attributes for: {node_name}")
    
    # Search for SCAN_CLUSTERS from metadata
    metadata = node.get('metadata', [])
    print(f"      - metadata type: {type(metadata)}, length: {len(metadata) if isinstance(metadata, list) else 'N/A'}")
    
    if isinstance(metadata, list):
        for i, item in enumerate(metadata):
            if isinstance(item, dict):
                item_key = item.get('key', '')
                item_label = item.get('label', '')
                item_values = item.get('values', [])
                
                print(f"        metadata[{i}]: key='{item_key}', label='{item_label}', values={item_values}")
                
                # Check both key and label
                if (item_key == 'SCAN_CLUSTERS' or 
                    item_label == 'Cluster attributes'):
                    print(f"        *** FOUND SCAN_CLUSTERS in metadata: {item_values}")
                    if isinstance(item_values, list):
                        cluster_attributes.extend(item_values)
    
    # Search from raw_metrics as well (also check label)
    raw_metrics = node.get('metrics', [])
    print(f"      - metrics type: {type(raw_metrics)}, length: {len(raw_metrics) if isinstance(raw_metrics, list) else 'N/A'}")
    
    if isinstance(raw_metrics, list):
        scan_clusters_found = False
        for i, metric in enumerate(raw_metrics):
            if isinstance(metric, dict):
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_values = metric.get('values', [])
                
                if (metric_key == 'SCAN_CLUSTERS' or 
                    metric_label == 'Cluster attributes'):
                    print(f"        *** FOUND SCAN_CLUSTERS in metrics[{i}]: key='{metric_key}', label='{metric_label}', values={metric_values}")
                    scan_clusters_found = True
                    if isinstance(metric_values, list):
                        cluster_attributes.extend(metric_values)
        
        if not scan_clusters_found:
            print(f"        No SCAN_CLUSTERS found in {len(raw_metrics)} metrics")
    
    # Search from detailed_metrics as well
    detailed_metrics = node.get('detailed_metrics', {})
    print(f"      - detailed_metrics type: {type(detailed_metrics)}")
    
    if isinstance(detailed_metrics, dict):
        for key, info in detailed_metrics.items():
            if (key == 'SCAN_CLUSTERS' or 
                (isinstance(info, dict) and info.get('label') == 'Cluster attributes')):
                print(f"        *** FOUND SCAN_CLUSTERS in detailed_metrics: {key}")
                values = info.get('values', []) if isinstance(info, dict) else []
                if isinstance(values, list):
                    cluster_attributes.extend(values)
    
    # Remove duplicates
    final_result = list(set(cluster_attributes))
    print(f"      ‚Üí Final clustering keys: {final_result}")
    return final_result

def extract_parallelism_metrics(node: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extract multiple Tasks total metrics and AQEShuffleRead metrics from node
    
    „Ç∑„É£„ÉÉ„Éï„É´Êìç‰Ωú„Å™„Å©„Åß„ÅØ‰ª•‰∏ã„ÅÆË§áÊï∞„ÅÆ„É°„Éà„É™„ÇØ„Çπ„ÅåÂ≠òÂú®„Åô„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„ÅôÔºö
    - Tasks total
    - Sink - Tasks total
    - Source - Tasks total
    - AQEShuffleRead - Number of partitions
    - AQEShuffleRead - Partition data size
    
    Args:
        node: Node information
        
    Returns:
        dict: Ê§úÂá∫„Åï„Çå„Åü„É°„Éà„É™„ÇØ„Çπ
            {
                "tasks_total": ÂÄ§,
                "sink_tasks_total": ÂÄ§,
                "source_tasks_total": ÂÄ§,
                "all_tasks_metrics": [{"name": "Tasks total", "value": ÂÄ§}, ...],
                "aqe_shuffle_partitions": ÂÄ§,
                "aqe_shuffle_data_size": ÂÄ§,
                "aqe_shuffle_avg_partition_size": ÂÄ§,
                "aqe_shuffle_skew_warning": bool,
                "aqe_shuffle_metrics": [{"name": "AQE...", "value": ÂÄ§}, ...]
            }
    """
    parallelism_metrics = {
        "tasks_total": 0,
        "sink_tasks_total": 0,
        "source_tasks_total": 0,
        "all_tasks_metrics": [],
        "aqe_shuffle_partitions": 0,
        "aqe_shuffle_data_size": 0,
        "aqe_shuffle_avg_partition_size": 0,
        "aqe_shuffle_skew_warning": False,
        "aqe_detected_and_handled": False,
        "aqe_shuffle_metrics": []
    }
    
    # Target Tasks total metric name patterns
    tasks_total_patterns = [
        "Tasks total",
        "Sink - Tasks total",
        "Source - Tasks total"
    ]
    
    # Target AQEShuffleRead metric name patterns
    aqe_shuffle_patterns = [
        "AQEShuffleRead - Number of partitions",
        "AQEShuffleRead - Partition data size"
    ]
    
    # 1. Search from detailed_metrics
    detailed_metrics = node.get('detailed_metrics', {})
    for metric_key, metric_info in detailed_metrics.items():
        metric_value = metric_info.get('value', 0)
        metric_label = metric_info.get('label', '')
        
        # ÂêÑTasks total„Éë„Çø„Éº„É≥„Çí„ÉÅ„Çß„ÉÉ„ÇØ
        for pattern in tasks_total_patterns:
            if metric_key == pattern or metric_label == pattern:
                # ÁâπÂÆö„ÅÆ„É°„Éà„É™„ÇØ„Çπ„Å´„Éû„ÉÉ„Éî„É≥„Ç∞
                if pattern == "Tasks total":
                    parallelism_metrics["tasks_total"] = metric_value
                elif pattern == "Sink - Tasks total":
                    parallelism_metrics["sink_tasks_total"] = metric_value
                elif pattern == "Source - Tasks total":
                    parallelism_metrics["source_tasks_total"] = metric_value
                
                # Add to all metrics list
                parallelism_metrics["all_tasks_metrics"].append({
                    "name": pattern,
                    "value": metric_value
                })
        
        # AQEShuffleRead„É°„Éà„É™„ÇØ„Çπ„Çí„ÉÅ„Çß„ÉÉ„ÇØ
        for pattern in aqe_shuffle_patterns:
            if metric_key == pattern or metric_label == pattern:
                # ÁâπÂÆö„ÅÆ„É°„Éà„É™„ÇØ„Çπ„Å´„Éû„ÉÉ„Éî„É≥„Ç∞
                if pattern == "AQEShuffleRead - Number of partitions":
                    parallelism_metrics["aqe_shuffle_partitions"] = metric_value
                elif pattern == "AQEShuffleRead - Partition data size":
                    parallelism_metrics["aqe_shuffle_data_size"] = metric_value
                
                # Add to all metrics list
                parallelism_metrics["aqe_shuffle_metrics"].append({
                    "name": pattern,
                    "value": metric_value
                })
    
    # 2. raw_metrics„Åã„ÇâÊ§úÁ¥¢Ôºà„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ
    raw_metrics = node.get('metrics', [])
    if isinstance(raw_metrics, list):
        for metric in raw_metrics:
            if isinstance(metric, dict):
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_value = metric.get('value', 0)
                
                # ÂêÑTasks total„Éë„Çø„Éº„É≥„Çí„ÉÅ„Çß„ÉÉ„ÇØ
                for pattern in tasks_total_patterns:
                    if metric_key == pattern or metric_label == pattern:
                        # Skip if already found in detailed_metrics
                        if not any(m["name"] == pattern for m in parallelism_metrics["all_tasks_metrics"]):
                            # ÁâπÂÆö„ÅÆ„É°„Éà„É™„ÇØ„Çπ„Å´„Éû„ÉÉ„Éî„É≥„Ç∞
                            if pattern == "Tasks total":
                                parallelism_metrics["tasks_total"] = metric_value
                            elif pattern == "Sink - Tasks total":
                                parallelism_metrics["sink_tasks_total"] = metric_value
                            elif pattern == "Source - Tasks total":
                                parallelism_metrics["source_tasks_total"] = metric_value
                            
                            # Add to all metrics list
                            parallelism_metrics["all_tasks_metrics"].append({
                                "name": pattern,
                                "value": metric_value
                            })
                
                # AQEShuffleRead„É°„Éà„É™„ÇØ„Çπ„Çí„ÉÅ„Çß„ÉÉ„ÇØ
                for pattern in aqe_shuffle_patterns:
                    if metric_key == pattern or metric_label == pattern:
                        # Skip if already found in detailed_metrics
                        if not any(m["name"] == pattern for m in parallelism_metrics["aqe_shuffle_metrics"]):
                            # ÁâπÂÆö„ÅÆ„É°„Éà„É™„ÇØ„Çπ„Å´„Éû„ÉÉ„Éî„É≥„Ç∞
                            if pattern == "AQEShuffleRead - Number of partitions":
                                parallelism_metrics["aqe_shuffle_partitions"] = metric_value
                            elif pattern == "AQEShuffleRead - Partition data size":
                                parallelism_metrics["aqe_shuffle_data_size"] = metric_value
                            
                            # Add to all metrics list
                            parallelism_metrics["aqe_shuffle_metrics"].append({
                                "name": pattern,
                                "value": metric_value
                            })
    
    # 3. key_metrics„Åã„ÇâÊ§úÁ¥¢ÔºàÊúÄÂæå„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ
    key_metrics = node.get('key_metrics', {})
    if isinstance(key_metrics, dict):
        for metric_key, metric_value in key_metrics.items():
            for pattern in tasks_total_patterns:
                if metric_key == pattern:
                    # Skip if already found
                    if not any(m["name"] == pattern for m in parallelism_metrics["all_tasks_metrics"]):
                        # ÁâπÂÆö„ÅÆ„É°„Éà„É™„ÇØ„Çπ„Å´„Éû„ÉÉ„Éî„É≥„Ç∞
                        if pattern == "Tasks total":
                            parallelism_metrics["tasks_total"] = metric_value
                        elif pattern == "Sink - Tasks total":
                            parallelism_metrics["sink_tasks_total"] = metric_value
                        elif pattern == "Source - Tasks total":
                            parallelism_metrics["source_tasks_total"] = metric_value
                        
                        # Add to all metrics list
                        parallelism_metrics["all_tasks_metrics"].append({
                            "name": pattern,
                            "value": metric_value
                        })
            
            # AQEShuffleRead„É°„Éà„É™„ÇØ„Çπ„Çí„ÉÅ„Çß„ÉÉ„ÇØ
            for pattern in aqe_shuffle_patterns:
                if metric_key == pattern:
                    # Skip if already found
                    if not any(m["name"] == pattern for m in parallelism_metrics["aqe_shuffle_metrics"]):
                        # ÁâπÂÆö„ÅÆ„É°„Éà„É™„ÇØ„Çπ„Å´„Éû„ÉÉ„Éî„É≥„Ç∞
                        if pattern == "AQEShuffleRead - Number of partitions":
                            parallelism_metrics["aqe_shuffle_partitions"] = metric_value
                        elif pattern == "AQEShuffleRead - Partition data size":
                            parallelism_metrics["aqe_shuffle_data_size"] = metric_value
                        
                        # Add to all metrics list
                        parallelism_metrics["aqe_shuffle_metrics"].append({
                            "name": pattern,
                            "value": metric_value
                        })
    
    # Calculate average partition size and set warnings
    if parallelism_metrics["aqe_shuffle_partitions"] > 0 and parallelism_metrics["aqe_shuffle_data_size"] > 0:
        avg_partition_size = parallelism_metrics["aqe_shuffle_data_size"] / parallelism_metrics["aqe_shuffle_partitions"]
        parallelism_metrics["aqe_shuffle_avg_partition_size"] = avg_partition_size
        
        # 512MB = 512 * 1024 * 1024 bytes
        threshold_512mb = 512 * 1024 * 1024
        if avg_partition_size >= threshold_512mb:
            parallelism_metrics["aqe_shuffle_skew_warning"] = True
        else:
            # When AQEShuffleRead metrics exist and average partition size is less than 512MB,
            # Determine that AQE has detected and handled skew
            parallelism_metrics["aqe_detected_and_handled"] = True
    
    return parallelism_metrics

def calculate_filter_rate(node: Dict[str, Any]) -> Dict[str, Any]:
    """
    „Éé„Éº„Éâ„Åã„ÇâSize of files pruned„Å®Size of files read„É°„Éà„É™„ÇØ„Çπ„ÇíÊäΩÂá∫„Åó„Å¶„Éï„Ç£„É´„ÇøÁéá„ÇíË®àÁÆó
    
    Args:
        node: „Éé„Éº„Éâ„Éá„Éº„Çø
        
    Returns:
        Dict: „Éï„Ç£„É´„ÇøÁéáË®àÁÆóÁµêÊûú
    """
    import os
    debug_mode = os.environ.get('DEBUG_FILTER_ANALYSIS', 'false').lower() == 'true'
    
    filter_rate = None
    files_pruned_bytes = 0
    files_read_bytes = 0
    actual_io_bytes = 0  # ÂÆüÈöõ„ÅÆI/OË™≠„ÅøËæº„ÅøÈáè
    debug_info = []
    
    # Target metric names for search (prioritizing patterns confirmed in actual JSON files)
    pruned_metrics = [
        "Size of files pruned",  # ÂÆüÈöõ„Å´Â≠òÂú®„Åô„Çã„Åì„Å®„ÇíÁ¢∫Ë™çÊ∏à„Åø
        "Size of files pruned before dynamic pruning",  # ÂÆüÈöõ„Å´Â≠òÂú®„Åô„Çã„Åì„Å®„ÇíÁ¢∫Ë™çÊ∏à„Åø
        "Pruned files size", 
        "Files pruned size",
        "Num pruned files size"
    ]
    
    read_metrics = [
        "Size of files read",  # ÂÆüÈöõ„Å´Â≠òÂú®„Åô„Çã„Åì„Å®„ÇíÁ¢∫Ë™çÊ∏à„Åø
        "Files read size",
        "Read files size",
        "Num files read size"
    ]
    
    # ÂÆüÈöõ„ÅÆI/OË™≠„ÅøËæº„ÅøÈáèÔºàÂÑ™ÂÖàÁöÑ„Å´‰ΩøÁî®Ôºâ
    actual_io_metrics = [
        "Size of data read with io requests",  # ÂÆüÈöõ„ÅÆ„Çπ„Éà„É¨„Éº„Ç∏„Åã„Çâ„ÅÆË™≠„ÅøËæº„ÅøÈáè
        "Data read with io requests",
        "IO request data size",
        "Actual data read size"
    ]
    
    # detailed_metrics„Åã„ÇâÊ§úÁ¥¢
    detailed_metrics = node.get('detailed_metrics', {})
    if debug_mode:
        debug_info.append(f"detailed_metrics keys: {list(detailed_metrics.keys())[:5]}")
    
    for metric_key, metric_info in detailed_metrics.items():
        metric_label = metric_info.get('label', '')
        metric_value = metric_info.get('value', 0)
        
        # PrunedÈñ¢ÈÄ£Ôºàlabel„ÇíÂÑ™ÂÖàÁöÑ„Å´„ÉÅ„Çß„ÉÉ„ÇØÔºâ
        for target in pruned_metrics:
            if target in metric_label and metric_value > 0:
                files_pruned_bytes += metric_value  # Ë§áÊï∞„ÅÆ„É°„Éà„É™„ÇØ„Çπ„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØÂêàË®à
                if debug_mode:
                    debug_info.append(f"Found pruned metric: {metric_label} = {metric_value}")
                break
        
        # ReadÈñ¢ÈÄ£Ôºàlabel„ÇíÂÑ™ÂÖàÁöÑ„Å´„ÉÅ„Çß„ÉÉ„ÇØÔºâ
        for target in read_metrics:
            if target in metric_label and metric_value > 0:
                files_read_bytes += metric_value  # Ë§áÊï∞„ÅÆ„É°„Éà„É™„ÇØ„Çπ„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØÂêàË®à
                if debug_mode:
                    debug_info.append(f"Found read metric: {metric_label} = {metric_value}")
                break
        
        # ÂÆüÈöõ„ÅÆI/OË™≠„ÅøËæº„ÅøÈáèÔºàÊúÄÂÑ™ÂÖàÔºâ
        for target in actual_io_metrics:
            if target in metric_label and metric_value > 0:
                actual_io_bytes += metric_value
                if debug_mode:
                    debug_info.append(f"Found actual IO metric: {metric_label} = {metric_value}")
                break
    
    # raw_metrics„Åã„ÇâÊ§úÁ¥¢Ôºà„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ
    if files_pruned_bytes == 0 or files_read_bytes == 0:
        raw_metrics = node.get('metrics', [])
        if debug_mode:
            debug_info.append(f"Searching in {len(raw_metrics)} raw metrics")
        
        for metric in raw_metrics:
            metric_label = metric.get('label', '')
            metric_value = metric.get('value', 0)
            
            # PrunedÈñ¢ÈÄ£Ôºàlabel„ÇíÂÑ™ÂÖàÁöÑ„Å´„ÉÅ„Çß„ÉÉ„ÇØÔºâ
            for target in pruned_metrics:
                if target in metric_label and metric_value > 0:
                    files_pruned_bytes += metric_value  # Ë§áÊï∞„ÅÆ„É°„Éà„É™„ÇØ„Çπ„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØÂêàË®à
                    if debug_mode:
                        debug_info.append(f"Found pruned metric in raw: {metric_label} = {metric_value}")
                    break
            
            # ReadÈñ¢ÈÄ£Ôºàlabel„ÇíÂÑ™ÂÖàÁöÑ„Å´„ÉÅ„Çß„ÉÉ„ÇØÔºâ
            for target in read_metrics:
                if target in metric_label and metric_value > 0:
                    files_read_bytes += metric_value  # Ë§áÊï∞„ÅÆ„É°„Éà„É™„ÇØ„Çπ„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØÂêàË®à
                    if debug_mode:
                        debug_info.append(f"Found read metric in raw: {metric_label} = {metric_value}")
                    break
            
            # ÂÆüÈöõ„ÅÆI/OË™≠„ÅøËæº„ÅøÈáèÔºàraw_metrics„Åã„Çâ„ÇÇÊ§úÁ¥¢Ôºâ
            for target in actual_io_metrics:
                if target in metric_label and metric_value > 0:
                    actual_io_bytes += metric_value
                    if debug_mode:
                        debug_info.append(f"Found actual IO metric in raw: {metric_label} = {metric_value}")
                    break
    
    # „Éï„Ç£„É´„ÇøÁéáË®àÁÆóÔºàI/OÂÆüÁ∏æ„ÇíÂÑ™ÂÖà„ÄÅ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ„Åß„Éó„É´„Éº„Éã„É≥„Ç∞ÂäπÁéáÔºâ
    if actual_io_bytes > 0 and files_read_bytes > 0:
        # Êñ∞„Åó„ÅÑË®àÁÆóÊñπÂºè: ÂÆüÈöõ„ÅÆI/OÂäπÁéá
        filter_rate = (files_read_bytes - actual_io_bytes) / files_read_bytes
        if debug_mode:
            debug_info.append(f"Using IO-based calculation: ({files_read_bytes/1024**3:.2f}GB - {actual_io_bytes/1024**3:.2f}GB) / {files_read_bytes/1024**3:.2f}GB = {filter_rate:.3f}")
    else:
        # ÂæìÊù•„ÅÆË®àÁÆóÊñπÂºè: „Éó„É´„Éº„Éã„É≥„Ç∞ÂäπÁéá
        total_available_bytes = files_read_bytes + files_pruned_bytes
        if total_available_bytes > 0:
            filter_rate = files_pruned_bytes / total_available_bytes
            if debug_mode:
                debug_info.append(f"Using pruning-based calculation: {files_pruned_bytes/1024**3:.2f}GB / {total_available_bytes/1024**3:.2f}GB = {filter_rate:.3f}")
        else:
            filter_rate = 0.0
            if debug_mode:
                debug_info.append("No filter metrics available, using 0.0")
    
    result = {
        "filter_rate": filter_rate,
        "files_pruned_bytes": files_pruned_bytes,
        "files_read_bytes": files_read_bytes,
        "actual_io_bytes": actual_io_bytes,  # ÂÆüÈöõ„ÅÆI/OË™≠„ÅøËæº„ÅøÈáè„ÇíËøΩÂä†
        "has_filter_metrics": (files_read_bytes > 0 or files_pruned_bytes > 0),
        "calculation_method": "io_based" if (actual_io_bytes > 0 and files_read_bytes > 0) else "pruning_based"
    }
    
    if debug_mode:
        result["debug_info"] = debug_info
    
    return result

def format_filter_rate_display(filter_result: Dict[str, Any]) -> str:
    """
    „Éï„Ç£„É´„ÇøÁéáË®àÁÆóÁµêÊûú„ÇíË°®Á§∫Áî®ÊñáÂ≠óÂàó„Å´Â§âÊèõ
    
    Args:
        filter_result: calculate_filter_rate()„ÅÆÁµêÊûú
        
    Returns:
        str: Ë°®Á§∫Áî®ÊñáÂ≠óÂàó
    """
    if not filter_result["has_filter_metrics"] or filter_result["filter_rate"] is None:
        return None
    
    filter_rate = filter_result["filter_rate"]
    files_read_gb = filter_result["files_read_bytes"] / (1024 * 1024 * 1024)
    
    # Ë®àÁÆóÊñπÂºè„Å´Âøú„Åò„Å¶Ë°®Á§∫„ÇíË™øÊï¥
    if filter_result.get("calculation_method") == "io_based" and filter_result.get("actual_io_bytes", 0) > 0:
        actual_io_gb = filter_result["actual_io_bytes"] / (1024 * 1024 * 1024)
        effective_filtered_gb = files_read_gb - actual_io_gb
        return f"üìÇ Filter rate: {filter_rate:.1%} (read: {files_read_gb:.2f}GB, actual: {actual_io_gb:.2f}GB)"
    else:
        files_pruned_gb = filter_result["files_pruned_bytes"] / (1024 * 1024 * 1024)
        return f"üìÇ Filter rate: {filter_rate:.1%} (read: {files_read_gb:.2f}GB, pruned: {files_pruned_gb:.2f}GB)"

def extract_detailed_bottleneck_analysis(extracted_metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    Perform Cell 33-style detailed bottleneck analysis and return structured data
    
    üö® Important: Prevention of percentage calculation degradation
    - Using the sum of parallel execution node times as total time is strictly prohibited
    - Prioritize using overall_metrics.total_time_ms (wall-clock time)
    - Use maximum node time during fallback (not sum)
    
    Args:
        extracted_metrics: Extracted metrics
        
    Returns:
        dict: Detailed bottleneck analysis results
    """
    detailed_analysis = {
        "top_bottleneck_nodes": [],
        "shuffle_optimization_hints": [],
        "spill_analysis": {
            "total_spill_gb": 0,
            "spill_nodes": [],
            "critical_spill_nodes": []
        },
        "skew_analysis": {
            "skewed_nodes": [],
            "total_skewed_partitions": 0
        },
        "performance_recommendations": []
    }
    
    # „Éé„Éº„Éâ„ÇíÂÆüË°åÊôÇÈñì„Åß„ÇΩ„Éº„ÉàÔºàTOP10Ôºâ
    sorted_nodes = sorted(extracted_metrics.get('node_metrics', []), 
                         key=lambda x: x.get('key_metrics', {}).get('durationMs', 0), 
                         reverse=True)
    
    # ÊúÄÂ§ß10ÂÄã„ÅÆ„Éé„Éº„Éâ„ÇíÂá¶ÁêÜ
    final_sorted_nodes = sorted_nodes[:10]
    
    # üö® ÈáçË¶Å: Ê≠£„Åó„ÅÑÂÖ®‰ΩìÊôÇÈñì„ÅÆË®àÁÆóÔºà„Éá„Ç∞„É¨Èò≤Ê≠¢Ôºâ
    # 1. overall_metrics„Åã„ÇâÂÖ®‰ΩìÂÆüË°åÊôÇÈñì„ÇíÂèñÂæóÔºàwall-clock timeÔºâ
    overall_metrics = extracted_metrics.get('overall_metrics', {})
    total_duration = overall_metrics.get('total_time_ms', 0)
    
    # üö® ‰∏¶ÂàóÂÆüË°åÂïèÈ°å„ÅÆ‰øÆÊ≠£: task_total_time_ms„ÇíÂÑ™ÂÖà‰ΩøÁî®
    task_total_time_ms = overall_metrics.get('task_total_time_ms', 0)
    
    if task_total_time_ms > 0:
        total_duration = task_total_time_ms
    elif total_duration <= 0:
        # execution_time_ms„ÇíÊ¨°„ÅÆÂÑ™ÂÖàÂ∫¶„Åß‰ΩøÁî®
        execution_time_ms = overall_metrics.get('execution_time_ms', 0)
        if execution_time_ms > 0:
            total_duration = execution_time_ms
        else:
            # ÊúÄÁµÇ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
            max_node_time = max([node.get('key_metrics', {}).get('durationMs', 0) for node in sorted_nodes], default=1)
            total_duration = int(max_node_time * 1.2)
    
    for i, node in enumerate(final_sorted_nodes):
        duration_ms = node.get('key_metrics', {}).get('durationMs', 0)
        memory_mb = node.get('key_metrics', {}).get('peakMemoryBytes', 0) / 1024 / 1024
        rows_num = node.get('key_metrics', {}).get('rowsNum', 0)
        
        # ‰∏¶ÂàóÂ∫¶ÊÉÖÂ†±„ÅÆÂèñÂæóÔºà‰øÆÊ≠£Áâà: Ë§áÊï∞„ÅÆTasks total„É°„Éà„É™„ÇØ„Çπ„ÇíÂèñÂæóÔºâ
        parallelism_data = extract_parallelism_metrics(node)
        
        # ÂæìÊù•„ÅÆÂçò‰∏ÄÂÄ§Ôºà‰∫íÊèõÊÄß„ÅÆ„Åü„ÇÅÔºâ
        num_tasks = parallelism_data.get('tasks_total', 0)
        
        # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: Sink - Tasks total„Åæ„Åü„ÅØSource - Tasks total„Åå„ÅÇ„ÇãÂ†¥Âêà
        if num_tasks == 0:
            if parallelism_data.get('sink_tasks_total', 0) > 0:
                num_tasks = parallelism_data.get('sink_tasks_total', 0)
            elif parallelism_data.get('source_tasks_total', 0) > 0:
                num_tasks = parallelism_data.get('source_tasks_total', 0)
        
        # „Çπ„Éî„É´Ê§úÂá∫Ôºà„Çª„É´33„Å®Âêå„Åò„É≠„Ç∏„ÉÉ„ÇØÔºâ
        spill_detected = False
        spill_bytes = 0
        exact_spill_metrics = [
            "Num bytes spilled to disk due to memory pressure",
            "Sink - Num bytes spilled to disk due to memory pressure",
            "Sink/Num bytes spilled to disk due to memory pressure"
        ]
        
        # detailed_metrics„Åã„ÇâÊ§úÁ¥¢
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            metric_value = metric_info.get('value', 0)
            metric_label = metric_info.get('label', '')
            
            if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                spill_detected = True
                spill_bytes = max(spill_bytes, metric_value)
                break
        
        # raw_metrics„Åã„ÇâÊ§úÁ¥¢Ôºà„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ
        if not spill_detected:
            raw_metrics = node.get('metrics', [])
            for metric in raw_metrics:
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_value = metric.get('value', 0)
                
                if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                    spill_detected = True
                    spill_bytes = max(spill_bytes, metric_value)
                    break
        
        # „Çπ„Ç≠„É•„ÉºÊ§úÂá∫ÔºàAQE„Éô„Éº„ÇπÔºâ
        skew_detected = False
        skewed_partitions = 0
        target_skew_metric = "AQEShuffleRead - Number of skewed partitions"
        
        for metric_key, metric_info in detailed_metrics.items():
            if metric_key == target_skew_metric:
                try:
                    skewed_partitions = int(metric_info.get('value', 0))
                    if skewed_partitions > 0:
                        skew_detected = True
                    break
                except (ValueError, TypeError):
                    continue
        
        node_name = get_meaningful_node_name(node, extracted_metrics)
        # üö® ÈáçË¶Å: Ê≠£„Åó„ÅÑ„Éë„Éº„Çª„É≥„ÉÜ„Éº„Ç∏Ë®àÁÆóÔºà„Éá„Ç∞„É¨Èò≤Ê≠¢Ôºâ
        # wall-clock time„Å´ÂØæ„Åô„ÇãÂêÑ„Éé„Éº„Éâ„ÅÆÂÆüË°åÊôÇÈñì„ÅÆÂâ≤Âêà
        time_percentage = min((duration_ms / max(total_duration, 1)) * 100, 100.0)
        
        # „Çπ„Ç≠„É•„ÉºÂà§ÂÆöÔºàAQE„Çπ„Ç≠„É•„ÉºÊ§úÂá∫„Å®AQEShuffleReadÂπ≥Âùá„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„Çµ„Ç§„Ç∫„ÅÆ‰∏°Êñπ„ÇíËÄÉÊÖÆÔºâ
        aqe_shuffle_skew_warning = parallelism_data.get('aqe_shuffle_skew_warning', False)
        combined_skew_detected = skew_detected or aqe_shuffle_skew_warning
        
        # „Éé„Éº„ÉâÂàÜÊûêÁµêÊûú„ÇíÊßãÈÄ†Âåñ
        node_analysis = {
            "rank": i + 1,
            "node_id": node.get('node_id', node.get('id', 'N/A')),
            "node_name": node_name,
            "duration_ms": duration_ms,
            "time_percentage": time_percentage,
            "memory_mb": memory_mb,
            "rows_processed": rows_num,
            "num_tasks": num_tasks,
            "parallelism_data": parallelism_data,  # Ë§áÊï∞„ÅÆTasks total„É°„Éà„É™„ÇØ„ÇπÊÉÖÂ†±„ÇíËøΩÂä†
            "spill_detected": spill_detected,
            "spill_bytes": spill_bytes,
            "spill_gb": spill_bytes / 1024 / 1024 / 1024 if spill_bytes > 0 else 0,
            "skew_detected": combined_skew_detected,  # AQE„Çπ„Ç≠„É•„ÉºÊ§úÂá∫„Å®AQEShuffleReadË≠¶Âëä„ÅÆ‰∏°Êñπ„ÇíËÄÉÊÖÆ
            "aqe_skew_detected": skew_detected,  # ÂæìÊù•„ÅÆAQE„Çπ„Ç≠„É•„ÉºÊ§úÂá∫„ÅÆ„Åø
            "aqe_shuffle_skew_warning": aqe_shuffle_skew_warning,  # AQEShuffleReadÂπ≥Âùá„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„Çµ„Ç§„Ç∫Ë≠¶Âëä
            "skewed_partitions": skewed_partitions,
            "is_shuffle_node": "shuffle" in node_name.lower(),
            "severity": "CRITICAL" if duration_ms >= 10000 else "HIGH" if duration_ms >= 5000 else "MEDIUM" if duration_ms >= 1000 else "LOW"
        }
        
        # Shuffle„Éé„Éº„Éâ„ÅÆÂ†¥Âêà„ÄÅ„Çπ„Éî„É´„ÅåÊ§úÂá∫„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅÆ„ÅøREPARTITION„Éí„É≥„Éà„ÇíËøΩÂä†
        if node_analysis["is_shuffle_node"] and spill_detected and spill_bytes > 0:
            shuffle_attributes = extract_shuffle_attributes(node)
            if shuffle_attributes:
                suggested_partitions = max(num_tasks * 2, 200)
                
                # ShuffleÂ±ûÊÄß„ÅßÊ§úÂá∫„Åï„Çå„Åü„Ç´„É©„É†„ÇíÂÖ®„Å¶‰ΩøÁî®ÔºàÂÆåÂÖ®‰∏ÄËá¥Ôºâ
                repartition_columns = ", ".join(shuffle_attributes)
                
                repartition_hint = {
                    "node_id": node_analysis["node_id"],
                    "attributes": shuffle_attributes,
                    "suggested_sql": f"REPARTITION({suggested_partitions}, {repartition_columns})",
                    "reason": f"Spill({node_analysis['spill_gb']:.2f}GB) improvement",
                    "priority": "HIGH",
                    "estimated_improvement": "Significant performance improvement expected",

                }
                detailed_analysis["shuffle_optimization_hints"].append(repartition_hint)
                node_analysis["repartition_hint"] = repartition_hint
        

        # „Éï„Ç£„É´„ÇøÁéáË®àÁÆó„Å®ÊÉÖÂ†±Êõ¥Êñ∞
        filter_result = calculate_filter_rate(node)
        node_analysis.update({
            "filter_rate": filter_result["filter_rate"],
            "files_pruned_bytes": filter_result["files_pruned_bytes"],
            "files_read_bytes": filter_result["files_read_bytes"],
            "has_filter_metrics": filter_result["has_filter_metrics"]
        })
        
        # „ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„ÉºÊÉÖÂ†±„ÅÆËøΩÂä†
        cluster_attributes = extract_cluster_attributes(node)
        node_analysis.update({
            "cluster_attributes": cluster_attributes,
            "has_clustering": len(cluster_attributes) > 0
        })
        
        detailed_analysis["top_bottleneck_nodes"].append(node_analysis)
        
        # „Çπ„Éî„É´ÂàÜÊûê„Å∏„ÅÆËøΩÂä†
        if spill_detected:
            detailed_analysis["spill_analysis"]["total_spill_gb"] += node_analysis["spill_gb"]
            detailed_analysis["spill_analysis"]["spill_nodes"].append({
                "node_id": node_analysis["node_id"],
                "node_name": node_name,
                "spill_gb": node_analysis["spill_gb"],
                "rank": i + 1
            })
            
            if node_analysis["spill_gb"] > 1.0:  # 1GB‰ª•‰∏ä„ÅØÈáçË¶Å
                detailed_analysis["spill_analysis"]["critical_spill_nodes"].append(node_analysis["node_id"])
        
        # „Çπ„Ç≠„É•„ÉºÂàÜÊûê„Å∏„ÅÆËøΩÂä†
        if skew_detected:
            detailed_analysis["skew_analysis"]["total_skewed_partitions"] += skewed_partitions
            detailed_analysis["skew_analysis"]["skewed_nodes"].append({
                "node_id": node_analysis["node_id"],
                "node_name": node_name,
                "skewed_partitions": skewed_partitions,
                "rank": i + 1
            })
    
    # ÂÖ®‰ΩìÁöÑ„Å™Êé®Â•®‰∫ãÈ†Ö„ÅÆÁîüÊàê
    if detailed_analysis["spill_analysis"]["total_spill_gb"] > 5.0:
        detailed_analysis["performance_recommendations"].append({
            "type": "memory_optimization",
            "priority": "CRITICAL",
            "description": f"Large spill({detailed_analysis['spill_analysis']['total_spill_gb']:.1f}GB) detected: Memory configuration and partitioning strategy review required"
        })
    
    if len(detailed_analysis["shuffle_optimization_hints"]) > 0:
        detailed_analysis["performance_recommendations"].append({
            "type": "shuffle_optimization", 
            "priority": "HIGH",
            "description": f"Memory optimization required for {len(detailed_analysis['shuffle_optimization_hints'])} shuffle nodes with spill occurrence"
        })
    
    if detailed_analysis["skew_analysis"]["total_skewed_partitions"] > 10:
        detailed_analysis["performance_recommendations"].append({
            "type": "skew_optimization",
            "priority": "HIGH", 
            "description": f"Data skew ({detailed_analysis['skew_analysis']['total_skewed_partitions']} partitions) detected: Data distribution review required"
        })
    
    return detailed_analysis

print("‚úÖ Function definition completed: get_meaningful_node_name, extract_shuffle_attributes, extract_detailed_bottleneck_analysis")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üéØ Bottleneck Indicator Calculation Function
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - Execution time and compilation time ratio analysis
# MAGIC - Cache efficiency and data processing efficiency calculation
# MAGIC - Photon utilization analysis
# MAGIC - Spill detection and shuffle/parallelism issue identification

# COMMAND ----------

def calculate_bottleneck_indicators(metrics: Dict[str, Any]) -> Dict[str, Any]:
    """Calculate bottleneck metrics"""
    indicators = {}
    
    overall = metrics.get('overall_metrics', {})
    total_time = overall.get('total_time_ms', 0)
    execution_time = overall.get('execution_time_ms', 0)
    compilation_time = overall.get('compilation_time_ms', 0)
    
    if total_time > 0:
        indicators['compilation_ratio'] = compilation_time / total_time
        indicators['execution_ratio'] = execution_time / total_time
    
    # „Ç≠„É£„ÉÉ„Ç∑„É•ÂäπÁéá
    read_bytes = overall.get('read_bytes', 0)
    cache_bytes = overall.get('read_cache_bytes', 0)
    if read_bytes > 0:
        indicators['cache_hit_ratio'] = cache_bytes / read_bytes
    
    # „Éá„Éº„ÇøÂá¶ÁêÜÂäπÁéáÔºàÂÆπÈáè„Éô„Éº„ÇπÔºâ
    read_bytes = overall.get('read_bytes', 0)
    
    # ÂÆπÈáè„Éô„Éº„Çπ„ÅÆ„Éï„Ç£„É´„ÇøÁéá„ÇíË®àÁÆóÔºàÊ≠£„Åó„ÅÑÂÆüË£ÖÔºâ
    data_selectivity = calculate_filter_rate_percentage(overall, metrics)
    
    indicators['data_selectivity'] = data_selectivity
    
    # Photon‰ΩøÁî®ÁéáÔºà„Çø„Çπ„ÇØÂÆüË°åÊôÇÈñì„Å´ÂØæ„Åô„ÇãÂâ≤ÂêàÔºâ
    task_time = overall.get('task_total_time_ms', 0)
    photon_time = overall.get('photon_total_time_ms', 0)
    if task_time > 0:
        indicators['photon_ratio'] = min(photon_time / task_time, 1.0)  # ÊúÄÂ§ß100%„Å´Âà∂Èôê
    else:
        indicators['photon_ratio'] = 0.0
    
    # „Çπ„Éî„É´Ê§úÂá∫ÔºàË©≥Á¥∞ÁâàÔºöSink - Num bytes spilled to disk due to memory pressure „Éô„Éº„ÇπÔºâ
    spill_detected = False
    total_spill_bytes = 0
    spill_details = []
    
    # „Çø„Éº„Ç≤„ÉÉ„Éà„É°„Éà„É™„ÇØ„ÇπÂêçÔºàË§áÊï∞„Éë„Çø„Éº„É≥ÂØæÂøúÔºâ
    target_spill_metrics = [
        "Sink - Num bytes spilled to disk due to memory pressure",
        "Num bytes spilled to disk due to memory pressure"
    ]
    
    # ÂêÑ„Éé„Éº„Éâ„Åß„Çπ„Éî„É´Ê§úÂá∫„ÇíÂÆüË°å
    for node in metrics.get('node_metrics', []):
        node_spill_found = False
        
        # 1. Search from detailed_metrics
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            metric_value = metric_info.get('value', 0)
            metric_label = metric_info.get('label', '')
            
            # Ë§áÊï∞„ÅÆ„Çπ„Éî„É´„É°„Éà„É™„ÇØ„ÇπÂêç„Çí„ÉÅ„Çß„ÉÉ„ÇØ
            if ((metric_key in target_spill_metrics or 
                 metric_label in target_spill_metrics) and metric_value > 0):
                spill_detected = True
                node_spill_found = True
                total_spill_bytes += metric_value
                spill_details.append({
                    'node_id': node.get('node_id', ''),
                    'node_name': node.get('name', ''),
                    'spill_bytes': metric_value,
                    'spill_metric': metric_key if metric_key in target_spill_metrics else metric_label,
                    'source': 'detailed_metrics'
                })
                break
        
        # 2. raw_metrics„Åã„ÇâÊ§úÁ¥¢Ôºà„Åì„ÅÆ„Éé„Éº„Éâ„Åß„Åæ„Å†Ë¶ã„Å§„Åã„Çâ„Å™„ÅÑÂ†¥ÂêàÔºâ
        if not node_spill_found:
            raw_metrics = node.get('metrics', [])
            for metric in raw_metrics:
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_value = metric.get('value', 0)
                
                # Ë§áÊï∞„ÅÆ„Çπ„Éî„É´„É°„Éà„É™„ÇØ„ÇπÂêç„Çí„ÉÅ„Çß„ÉÉ„ÇØ
                if ((metric_key in target_spill_metrics or 
                     metric_label in target_spill_metrics) and metric_value > 0):
                    spill_detected = True
                    node_spill_found = True
                    total_spill_bytes += metric_value
                    spill_details.append({
                        'node_id': node.get('node_id', ''),
                        'node_name': node.get('name', ''),
                        'spill_bytes': metric_value,
                        'spill_metric': metric_key if metric_key in target_spill_metrics else metric_label,
                        'source': 'raw_metrics'
                    })
                    break
        
        # 3. key_metrics„Åã„ÇâÊ§úÁ¥¢ÔºàÊúÄÂæå„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ
        if not node_spill_found:
            key_metrics = node.get('key_metrics', {})
            for key_metric_name, key_metric_value in key_metrics.items():
                # Ë§áÊï∞„ÅÆ„Çπ„Éî„É´„É°„Éà„É™„ÇØ„ÇπÂêç„Çí„ÉÅ„Çß„ÉÉ„ÇØ
                if key_metric_name in target_spill_metrics and key_metric_value > 0:
                    spill_detected = True
                    node_spill_found = True
                    total_spill_bytes += key_metric_value
                    spill_details.append({
                        'node_id': node.get('node_id', ''),
                        'node_name': node.get('name', ''),
                        'spill_bytes': key_metric_value,
                        'spill_metric': key_metric_name,
                        'source': 'key_metrics'
                    })
                    break
    
    # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: overall_metrics„Åã„Çâ„ÅÆÁ∞°ÊòìÊ§úÂá∫
    if not spill_detected:
        fallback_spill_bytes = overall.get('spill_to_disk_bytes', 0)
        if fallback_spill_bytes > 0:
            spill_detected = True
            total_spill_bytes = fallback_spill_bytes
            spill_details.append({
                'node_id': 'overall',
                'node_name': 'Overall Metrics',
                'spill_bytes': fallback_spill_bytes,
                'source': 'overall_metrics'
            })
    
    indicators['has_spill'] = spill_detected
    indicators['spill_bytes'] = total_spill_bytes
    indicators['spill_details'] = spill_details
    indicators['spill_nodes_count'] = len(spill_details)
    
    # ÊúÄ„ÇÇÊôÇÈñì„ÅÆ„Åã„Åã„Çã„Çπ„ÉÜ„Éº„Ç∏
    stage_durations = [(s['stage_id'], s['duration_ms']) for s in metrics.get('stage_metrics', []) if s['duration_ms'] > 0]
    if stage_durations:
        slowest_stage = max(stage_durations, key=lambda x: x[1])
        indicators['slowest_stage_id'] = slowest_stage[0]
        indicators['slowest_stage_duration'] = slowest_stage[1]
    
    # ÊúÄ„ÇÇ„É°„É¢„É™„Çí‰ΩøÁî®„Åô„Çã„Éé„Éº„Éâ
    memory_usage = []
    for node in metrics.get('node_metrics', []):
        peak_memory = node.get('key_metrics', {}).get('peakMemoryBytes', 0)
        if peak_memory > 0:
            memory_usage.append((node['node_id'], node['name'], peak_memory))
    
    if memory_usage:
        highest_memory_node = max(memory_usage, key=lambda x: x[2])
        indicators['highest_memory_node_id'] = highest_memory_node[0]
        indicators['highest_memory_node_name'] = highest_memory_node[1]
        indicators['highest_memory_bytes'] = highest_memory_node[2]
    
    # ‰∏¶ÂàóÂ∫¶„Å®„Ç∑„É£„ÉÉ„Éï„É´ÂïèÈ°å„ÅÆÊ§úÂá∫
    shuffle_nodes = []
    low_parallelism_stages = []
    
    # „Ç∑„É£„ÉÉ„Éï„É´„Éé„Éº„Éâ„ÅÆÁâπÂÆö
    for node in metrics.get('node_metrics', []):
        node_name = node.get('name', '').upper()
        if any(keyword in node_name for keyword in ['SHUFFLE', 'EXCHANGE']):
            shuffle_nodes.append({
                'node_id': node['node_id'],
                'name': node['name'],
                'duration_ms': node.get('key_metrics', {}).get('durationMs', 0),
                'rows': node.get('key_metrics', {}).get('rowsNum', 0)
            })
    
    # ‰Ωé‰∏¶ÂàóÂ∫¶„Çπ„ÉÜ„Éº„Ç∏„ÅÆÊ§úÂá∫
    for stage in metrics.get('stage_metrics', []):
        num_tasks = stage.get('num_tasks', 0)
        duration_ms = stage.get('duration_ms', 0)
        
        # ‰∏¶ÂàóÂ∫¶„Åå‰Ωé„ÅÑÔºà„Çø„Çπ„ÇØÊï∞„ÅåÂ∞ë„Å™„ÅÑÔºâ„Åã„Å§ÂÆüË°åÊôÇÈñì„ÅåÈï∑„ÅÑ„Çπ„ÉÜ„Éº„Ç∏
        if num_tasks > 0 and num_tasks < 10 and duration_ms > 5000:  # 10„Çø„Çπ„ÇØÊú™Ê∫Ä„ÄÅ5Áßí‰ª•‰∏ä
            low_parallelism_stages.append({
                'stage_id': stage['stage_id'],
                'num_tasks': num_tasks,
                'duration_ms': duration_ms,
                'avg_task_duration': duration_ms / max(num_tasks, 1)
            })
    
    indicators['shuffle_operations_count'] = len(shuffle_nodes)
    indicators['low_parallelism_stages_count'] = len(low_parallelism_stages)
    indicators['has_shuffle_bottleneck'] = len(shuffle_nodes) > 0 and any(s['duration_ms'] > 10000 for s in shuffle_nodes)
    indicators['has_low_parallelism'] = len(low_parallelism_stages) > 0
    
    # „Ç∑„É£„ÉÉ„Éï„É´„ÅÆË©≥Á¥∞ÊÉÖÂ†±
    if shuffle_nodes:
        total_shuffle_time = sum(s['duration_ms'] for s in shuffle_nodes)
        indicators['total_shuffle_time_ms'] = total_shuffle_time
        indicators['shuffle_time_ratio'] = total_shuffle_time / max(total_time, 1)
        
        # ÊúÄ„ÇÇÊôÇÈñì„ÅÆ„Åã„Åã„Çã„Ç∑„É£„ÉÉ„Éï„É´Êìç‰Ωú
        slowest_shuffle = max(shuffle_nodes, key=lambda x: x['duration_ms'])
        indicators['slowest_shuffle_duration_ms'] = slowest_shuffle['duration_ms']
        indicators['slowest_shuffle_node'] = slowest_shuffle['name']
    
    # ‰Ωé‰∏¶ÂàóÂ∫¶„ÅÆË©≥Á¥∞ÊÉÖÂ†±
    if low_parallelism_stages:
        indicators['low_parallelism_details'] = low_parallelism_stages
        avg_parallelism = sum(s['num_tasks'] for s in low_parallelism_stages) / len(low_parallelism_stages)
        indicators['average_low_parallelism'] = avg_parallelism
    
    # AQEShuffleReadË≠¶Âëä„ÅÆÊ§úÂá∫
    aqe_shuffle_skew_warning_detected = False
    aqe_detected_and_handled = False
    
    for node in metrics.get('node_metrics', []):
        parallelism_data = extract_parallelism_metrics(node)
        if parallelism_data.get('aqe_shuffle_skew_warning', False):
            aqe_shuffle_skew_warning_detected = True
        if parallelism_data.get('aqe_detected_and_handled', False):
            aqe_detected_and_handled = True
    
    # ÂÑ™ÂÖàÈ†Ü‰Ωç: 512MB‰ª•‰∏ä„ÅÆË≠¶Âëä„Åå„ÅÇ„Çå„Å∞„ÄÅ„Åù„Çå„ÇíÂÑ™ÂÖà
    # Ë≠¶Âëä„Åå„Å™„ÅÑÂ†¥Âêà„ÅÆ„Åø„ÄÅAQEÂØæÂøúÊ∏à„Åø„Å®Âà§ÂÆö
    indicators['has_aqe_shuffle_skew_warning'] = aqe_shuffle_skew_warning_detected
    indicators['has_skew'] = aqe_detected_and_handled and not aqe_shuffle_skew_warning_detected
    
    return indicators

print("‚úÖ Function definition completed: calculate_bottleneck_indicators")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üß¨ Liquid Clustering Analysis Function
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - Column information extraction from profiler data
# MAGIC - Filter, JOIN, and GROUP BY condition analysis
# MAGIC - Data skew and performance impact evaluation
# MAGIC - Clustering recommended column identification

# COMMAND ----------

def calculate_performance_insights_from_metrics(overall_metrics: Dict[str, Any], metrics: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    „É°„Éà„É™„ÇØ„ÇπÊÉÖÂ†±„ÅÆ„Åø„Åã„ÇâË©≥Á¥∞„Å™„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊ¥ûÂØü„ÇíË®àÁÆó
    """
    insights = {}
    
    # Âü∫Êú¨„Éá„Éº„Çø
    total_time_ms = overall_metrics.get('total_time_ms', 0)
    read_bytes = overall_metrics.get('read_bytes', 0)
    read_cache_bytes = overall_metrics.get('read_cache_bytes', 0)
    read_remote_bytes = overall_metrics.get('read_remote_bytes', 0)
    rows_read = overall_metrics.get('rows_read_count', 0)
    rows_produced = overall_metrics.get('rows_produced_count', 0)
    read_files = overall_metrics.get('read_files_count', 0)
    read_partitions = overall_metrics.get('read_partitions_count', 0)
    photon_time = overall_metrics.get('photon_total_time_ms', 0)
    task_time = overall_metrics.get('task_total_time_ms', 0)
    spill_bytes = overall_metrics.get('spill_to_disk_bytes', 0)
    
    # 1. „Éá„Éº„ÇøÂäπÁéáÂàÜÊûêÔºàÂÆπÈáè„Éô„Éº„ÇπÔºâ
    # metrics„ÅåNone„ÅÆÂ†¥Âêà„ÅØÁ©∫„ÅÆËæûÊõ∏„ÅßÂàùÊúüÂåñ
    if metrics is None:
        metrics = {'node_metrics': []}
    
    # ÂÆπÈáè„Éô„Éº„Çπ„ÅÆ„Éï„Ç£„É´„ÇøÁéá„ÇíË®àÁÆóÔºàÊ≠£„Åó„ÅÑÂÆüË£ÖÔºâ
    filter_rate_capacity = calculate_filter_rate_percentage(overall_metrics, metrics)
    
    insights['data_efficiency'] = {
        'data_selectivity': filter_rate_capacity,
        'avg_bytes_per_file': read_bytes / max(read_files, 1),
        'avg_bytes_per_partition': read_bytes / max(read_partitions, 1),
        'avg_rows_per_file': rows_read / max(read_files, 1),
        'avg_rows_per_partition': rows_read / max(read_partitions, 1)
    }
    
    # 2. „Ç≠„É£„ÉÉ„Ç∑„É•ÂäπÁéáÂàÜÊûê
    cache_hit_ratio = read_cache_bytes / max(read_bytes, 1)
    insights['cache_efficiency'] = {
        'cache_hit_ratio': cache_hit_ratio,
        'cache_hit_percentage': cache_hit_ratio * 100,
        'remote_read_ratio': read_remote_bytes / max(read_bytes, 1),
        'cache_effectiveness': 'high' if cache_hit_ratio > 0.8 else 'medium' if cache_hit_ratio > 0.5 else 'low'
    }
    
    # 3. ‰∏¶ÂàóÂåñÂäπÁéáÂàÜÊûê
    insights['parallelization'] = {
        'files_per_second': read_files / max(total_time_ms / 1000, 1),
        'partitions_per_second': read_partitions / max(total_time_ms / 1000, 1),
        'throughput_mb_per_second': (read_bytes / 1024 / 1024) / max(total_time_ms / 1000, 1),
        'rows_per_second': rows_read / max(total_time_ms / 1000, 1)
    }
    
    # 4. PhotonÂäπÁéáÂàÜÊûê
    photon_efficiency = photon_time / max(task_time, 1)
    insights['photon_analysis'] = {
        'photon_enabled': photon_time > 0,
        'photon_efficiency': photon_efficiency,
        'photon_utilization_percentage': photon_efficiency * 100,
        'photon_effectiveness': 'high' if photon_efficiency > 0.8 else 'medium' if photon_efficiency > 0.5 else 'low'
    }
    
    # 5. „É™„ÇΩ„Éº„Çπ‰ΩøÁî®Áä∂Ê≥Å
    insights['resource_usage'] = {
        'memory_pressure': spill_bytes > 0,
        'spill_gb': spill_bytes / 1024 / 1024 / 1024,
        'data_processed_gb': read_bytes / 1024 / 1024 / 1024,
        'data_reduction_ratio': 1 - (rows_produced / max(rows_read, 1))
    }
    
    # 6. „Éú„Éà„É´„Éç„ÉÉ„ÇØÊåáÊ®ô
    bottlenecks = []
    if cache_hit_ratio < 0.3:
        bottlenecks.append('Low cache efficiency')
    if read_remote_bytes / max(read_bytes, 1) > 0.8:
        bottlenecks.append('High remote read ratio')
    if photon_efficiency < 0.5 and photon_time > 0:
        bottlenecks.append('Low Photon efficiency')
    if spill_bytes > 0:
        bottlenecks.append('Memory spill occurring')
    if insights['data_efficiency']['data_selectivity'] < 0.2:
        bottlenecks.append('Low filter efficiency')
    
    insights['potential_bottlenecks'] = bottlenecks
    
    return insights

def calculate_filter_rate_percentage(overall_metrics: Dict[str, Any], metrics: Dict[str, Any]) -> float:
    """
    ÂÆπÈáè„Éô„Éº„Çπ„ÅÆ„Éï„Ç£„É´„ÇøÁéá„ÇíË®àÁÆó„Åô„ÇãÔºàoverall_metrics.read_bytes‰ΩøÁî®ÁâàÔºâ
    
    ‚ùå „Éá„Ç∞„É¨Èò≤Ê≠¢Ê≥®ÊÑè: „Åì„ÅÆÈñ¢Êï∞„ÅØÂøÖ„Åöoverall_metrics.read_bytes„Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºÅ
    ‚ùå files_read_bytesÔºà„Çπ„Ç≠„É£„É≥„Éé„Éº„ÉâÈõÜË®àÔºâ„ÅØ‰ΩøÁî®„Åó„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑÔºÅ
    
    Args:
        overall_metrics: ÂÖ®‰Ωì„É°„Éà„É™„ÇØ„ÇπÔºàread_bytes„Çí‰ΩøÁî®Ôºâ
        metrics: ÂÖ®„É°„Éà„É™„ÇØ„ÇπÔºànode_metrics„ÇíÂê´„ÇÄ„ÄÅpruned_bytesÂèñÂæóÁî®Ôºâ
        
    Returns:
        float: „Éï„Ç£„É´„ÇøÁéáÔºà0.0-1.0„ÄÅÈ´ò„ÅÑÂÄ§„Åª„Å©ÂäπÁéáÁöÑÔºâ
               „Éó„É´„Éº„Éã„É≥„Ç∞ÂäπÁéá = files_pruned_bytes / (overall_read_bytes + files_pruned_bytes)
    """
    import os
    debug_mode = os.environ.get('DEBUG_FILTER_ANALYSIS', 'false').lower() == 'true'
    
    # ‚ùå „Éá„Ç∞„É¨Èò≤Ê≠¢: ÂøÖ„Åöoverall_metrics.read_bytes„Çí‰ΩøÁî®ÔºÅ
    overall_read_bytes = overall_metrics.get('read_bytes', 0)
    
    if debug_mode:
        print(f"üîç Filter rate calculation debug (using overall_metrics.read_bytes version):")
        print(f"   overall_read_bytes: {overall_read_bytes:,} ({overall_read_bytes / (1024**4):.2f} TB)")
    
    try:
        # pruned_bytes„ÅÆ„Åønode_metrics„Åã„ÇâÂèñÂæóÔºàread_bytes„ÅØ‰ΩøÁî®„Åó„Å™„ÅÑÔºâ
        node_metrics = metrics.get('node_metrics', [])
        total_files_pruned_bytes = 0
        filter_metrics_found = False
        
        # ÂÖ®„Å¶„ÅÆ„Çπ„Ç≠„É£„É≥„Éé„Éº„Éâ„Åã„ÇâprunedÊÉÖÂ†±„ÅÆ„Åø„ÇíÈõÜË®à
        for node in node_metrics:
            if node.get('tag') in ['FileScan', 'BatchScan', 'TableScan', 'UNKNOWN_DATA_SOURCE_SCAN_EXEC']:
                filter_result = calculate_filter_rate(node)
                if filter_result.get('has_filter_metrics', False):
                    files_pruned_bytes = filter_result.get('files_pruned_bytes', 0)
                    
                    if files_pruned_bytes > 0:
                        total_files_pruned_bytes += files_pruned_bytes
                        filter_metrics_found = True
                        
                        if debug_mode:
                            print(f"   Node {node.get('node_id', 'unknown')}: files_pruned_bytes = {files_pruned_bytes:,}")
        
        # ‚ùå „Éá„Ç∞„É¨Èò≤Ê≠¢: overall_read_bytes + pruned_bytes „ÅßË®àÁÆó
        if filter_metrics_found and overall_read_bytes > 0:
            # Ê≠£„Åó„ÅÑË®àÁÆó: „Éó„É´„Éº„Éã„É≥„Ç∞ÂäπÁéá = files_pruned / (overall_read + files_pruned)
            total_available_bytes = overall_read_bytes + total_files_pruned_bytes
            if total_available_bytes > 0:
                overall_filter_rate = total_files_pruned_bytes / total_available_bytes
            else:
                overall_filter_rate = 0.0
                
            if debug_mode:
                print(f"   ‚ùå Regression prevention version: using overall_read_bytes")
                print(f"     overall_read_bytes: {overall_read_bytes:,} ({overall_read_bytes / (1024**4):.2f} TB)")
                print(f"     total_files_pruned_bytes: {total_files_pruned_bytes:,} ({total_files_pruned_bytes / (1024**4):.2f} TB)")
                print(f"     total_available_bytes: {total_available_bytes:,} ({total_available_bytes / (1024**4):.2f} TB)")
                print(f"     Pruning efficiency: {overall_filter_rate*100:.2f}%")
            return overall_filter_rate
        
        if debug_mode:
            print(f"   Filter metrics: {'Detected' if filter_metrics_found else 'Not detected'}")
            print(f"   overall_read_bytes: {overall_read_bytes:,}")
            if not filter_metrics_found:
                print(f"   ‚ö†Ô∏è Pruning information is not available")
            if overall_read_bytes == 0:
                print(f"   ‚ö†Ô∏è No read data available")
        
        # „Éó„É´„Éº„Éã„É≥„Ç∞ÊÉÖÂ†±„Åå„Å™„ÅÑÂ†¥Âêà„ÅØ0„ÇíËøî„Åô
        return 0.0
        
    except Exception as e:
        if debug_mode:
            print(f"   Filter rate calculation error: {e}")
        return 0.0

def extract_liquid_clustering_data(profiler_data: Dict[str, Any], metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extract data required for Liquid Clustering analysis from SQL profiler data (for LLM analysis)
    """
    # metrics „Éë„É©„É°„Éº„Çø„ÅÆÂûã„ÉÅ„Çß„ÉÉ„ÇØÔºàÈò≤Âæ°ÁöÑ„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞Ôºâ
    if not isinstance(metrics, dict):
        print(f"‚ö†Ô∏è Error: metrics parameter is not a dictionary (type: {type(metrics)})")
        print(f"   Expected: dict, Received: {type(metrics)}")
        return {
            "filter_columns": [],
            "join_columns": [],
            "groupby_columns": [],
            "aggregate_columns": [],
            "table_info": {},
            "scan_nodes": [],
            "join_nodes": [],
            "filter_nodes": [],
            "metadata_summary": {"error": f"Invalid metrics type: {type(metrics)}"}
        }
    
    extracted_data = {
        "filter_columns": [],
        "join_columns": [],
        "groupby_columns": [],
        "aggregate_columns": [],
        "table_info": {},
        "scan_nodes": [],
        "join_nodes": [],
        "filter_nodes": [],
        "metadata_summary": {}
    }
    
    print(f"üîç Starting data extraction for Liquid Clustering analysis")
    
    # „Éá„Éº„ÇøÂΩ¢Âºè„ÇíÁ¢∫Ë™ç
    data_format = metrics.get('data_format', '')
    if data_format == 'sql_query_summary':
        print("üìä SQL query summary format: Limited Liquid Clustering analysis")
        # test2.jsonÂΩ¢Âºè„ÅÆÂ†¥Âêà„ÅØÂà∂Èôê‰ªò„Åç„ÅÆÂàÜÊûê„ÇíË°å„ÅÜ
        query_info = metrics.get('query_info', {})
        query_text = query_info.get('query_text', '')
        
        # „É°„Éà„É™„ÇØ„ÇπÊÉÖÂ†±„ÅÆ„Åø„Åã„ÇâÂü∫Êú¨ÁöÑ„Å™„ÉÜ„Éº„Éñ„É´ÊÉÖÂ†±„ÇíÁîüÊàê
        # test2.jsonÂΩ¢Âºè„Åß„ÅØ planMetadatas „ÅåÁ©∫„ÅÆ„Åü„ÇÅ„ÄÅgraphs metadata „ÅØÂà©Áî®‰∏çÂèØ
        # „É°„Éà„É™„ÇØ„ÇπÈáçË¶ñ„ÅÆ„Ç¢„Éó„É≠„Éº„ÉÅ„Åß„Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûê„ÇíË°å„ÅÜ
        
        # ÂÖ®‰ΩìÁöÑ„Å™„Éï„Ç£„É´„ÇøÁéáÊÉÖÂ†±„ÇíË®àÁÆó
        overall_metrics = metrics.get('overall_metrics', {})
        overall_filter_rate = calculate_filter_rate_percentage(overall_metrics, metrics)
        read_bytes = overall_metrics.get('read_bytes', 0)
        read_gb = read_bytes / (1024**3) if read_bytes > 0 else 0
        
        # „Éó„É´„Éº„É≥Èáè„ÇíÊé®ÂÆöÔºà„Éï„Ç£„É´„ÇøÁéá„Åã„ÇâÈÄÜÁÆóÔºâ
        if overall_filter_rate > 0 and read_bytes > 0:
            pruned_bytes = (read_bytes * overall_filter_rate) / (1 - overall_filter_rate)
            pruned_gb = pruned_bytes / (1024**3)
        else:
            pruned_bytes = 0
            pruned_gb = 0
        
        extracted_data["table_info"]["metrics_summary"] = {
            "node_name": "Metrics-Based Analysis",
            "node_tag": "QUERY_SUMMARY", 
            "node_id": "summary",
            "files_count": overall_metrics.get('read_files_count', 0),
            "partitions_count": overall_metrics.get('read_partitions_count', 0),
            "data_size_gb": read_gb,
            "rows_read": overall_metrics.get('rows_read_count', 0),
            "rows_produced": overall_metrics.get('rows_produced_count', 0),
            "data_selectivity": overall_filter_rate,
            "avg_file_size_mb": (overall_metrics.get('read_bytes', 0) / 1024 / 1024) / max(overall_metrics.get('read_files_count', 1), 1),
            "avg_partition_size_mb": (overall_metrics.get('read_bytes', 0) / 1024 / 1024) / max(overall_metrics.get('read_partitions_count', 1), 1),
            "note": "Detailed table information is not available in SQL query summary format. Executing metrics-based analysis.",
            "current_clustering_keys": [],  # ÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº
            "filter_info": {  # „Éï„Ç£„É´„ÇøÁéáÊÉÖÂ†±„ÇíËøΩÂä†
                "filter_rate": overall_filter_rate,
                "files_read_bytes": read_bytes,
                "files_pruned_bytes": pruned_bytes,
                "has_filter_metrics": read_bytes > 0
            }
        }
        
        # „Çµ„Éû„É™„Éº„Éé„Éº„Éâ„ÅÆÊÉÖÂ†±„Çí‰ΩøÁî®
        for node in metrics.get('node_metrics', []):
            node_name = node.get('name', '')
            extracted_data["scan_nodes"].append({
                "name": node_name,
                "type": node.get('tag', ''),
                "rows": node.get('key_metrics', {}).get('rowsNum', 0),
                "duration_ms": node.get('key_metrics', {}).get('durationMs', 0),
                "node_id": node.get('node_id', '')
            })
        
        # „É°„Çø„Éá„Éº„Çø„Çµ„Éû„É™„ÉºÔºàÂà∂Èôê‰ªò„ÅçÔºâ
        view_count = sum(1 for table in extracted_data["table_info"].values() if table.get('is_view', False))
        actual_table_count = sum(len(table.get('underlying_tables', [])) for table in extracted_data["table_info"].values())
        
        extracted_data["metadata_summary"] = {
            "total_nodes": len(metrics.get('node_metrics', [])),
            "total_graphs": 0,
            "filter_expressions_count": 0,
            "join_expressions_count": 0,
            "groupby_expressions_count": 0,
            "aggregate_expressions_count": 0,
            "tables_identified": len(extracted_data["table_info"]),
            "views_identified": view_count,
            "underlying_tables_estimated": actual_table_count,
            "scan_nodes_count": len(extracted_data["scan_nodes"]),
            "join_nodes_count": 0,
            "filter_nodes_count": 0,
            "analysis_limitation": "Detailed analysis is limited due to SQL query summary format"
        }
        
        print(f"‚úÖ Limited data extraction completed: {extracted_data['metadata_summary']}")
        
        # „Éì„É•„ÉºÊÉÖÂ†±„ÅÆË©≥Á¥∞Ë°®Á§∫
        if view_count > 0:
            print(f"üîç View information details:")
            for table_name, table_info in extracted_data["table_info"].items():
                if table_info.get('is_view', False):
                    print(f"  üìä View: {table_name}")
                    print(f"     Alias: {table_info.get('alias', 'None')}")
                    print(f"     Table type: {table_info.get('table_type', 'unknown')}")
                    
                    underlying_tables = table_info.get('underlying_tables', [])
                    if underlying_tables:
                        print(f"     Estimated underlying table count: {len(underlying_tables)}")
                        for i, underlying_table in enumerate(underlying_tables[:3]):  # Display max 3
                            print(f"       - {underlying_table}")
                        if len(underlying_tables) > 3:
                            print(f"       ... and {len(underlying_tables) - 3} additional tables")
                    print()
        
        return extracted_data
    
    # ÈÄöÂ∏∏„ÅÆSQL„Éó„É≠„Éï„Ç°„Ç§„É©„ÉºÂΩ¢Âºè„ÅÆÂá¶ÁêÜ
    # „Éó„É≠„Éï„Ç°„Ç§„É©„Éº„Éá„Éº„Çø„Åã„ÇâÂÆüË°å„Ç∞„É©„ÉïÊÉÖÂ†±„ÇíÂèñÂæóÔºàË§áÊï∞„Ç∞„É©„ÉïÂØæÂøúÔºâ
    graphs = profiler_data.get('graphs', [])
    if not graphs:
        print("‚ö†Ô∏è Graph data not found")
        return extracted_data

    # „Åô„Åπ„Å¶„ÅÆ„Ç∞„É©„Éï„Åã„Çâ„Éé„Éº„Éâ„ÇíÂèéÈõÜ
    all_nodes = []
    table_size_info = {}  # „ÉÜ„Éº„Éñ„É´Âêç -> „Çµ„Ç§„Ç∫ÊÉÖÂ†±„ÅÆ„Éû„ÉÉ„Éî„É≥„Ç∞
    
    for graph_index, graph in enumerate(graphs):
        nodes = graph.get('nodes', [])
        for node in nodes:
            node['graph_index'] = graph_index
            all_nodes.append(node)
            
            # „Çπ„Ç≠„É£„É≥„Éé„Éº„Éâ„Åã„Çâ„ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫„ÇíÊäΩÂá∫
            node_name = node.get('name', '')
            if 'Scan' in node_name:
                # „ÉÜ„Éº„Éñ„É´Âêç„ÅÆÊäΩÂá∫
                table_name = node_name.replace('Scan ', '').strip()
                
                # Size of files read„É°„Éà„É™„ÇØ„Çπ„ÅÆÊäΩÂá∫
                metrics = node.get('metrics', [])
                files_read_bytes = 0
                files_pruned_bytes = 0
                io_read_bytes = 0
                
                for metric in metrics:
                    label = metric.get('label', '')
                    value = metric.get('value', 0)
                    
                    if 'Size of files read' in label:
                        files_read_bytes = value
                    elif 'Size of files pruned' in label:
                        files_pruned_bytes = value
                    elif 'Size of data read with io requests' in label:
                        io_read_bytes = value
                
                # „ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫ÊÉÖÂ†±„Çí‰øùÂ≠òÔºàÊúÄÂ§ßÂÄ§„ÇíË®òÈå≤Ôºâ
                if table_name not in table_size_info or files_read_bytes > table_size_info[table_name]['files_read_bytes']:
                    table_size_info[table_name] = {
                        'files_read_bytes': files_read_bytes,
                        'files_read_gb': files_read_bytes / (1024**3),
                        'files_pruned_bytes': files_pruned_bytes,
                        'files_pruned_gb': files_pruned_bytes / (1024**3),
                        'io_read_bytes': io_read_bytes,
                        'io_read_gb': io_read_bytes / (1024**3),
                        'total_scan_gb': (files_read_bytes + files_pruned_bytes) / (1024**3)
                    }
    
    print(f"üîç Processing {len(all_nodes)} nodes from {len(graphs)} graphs")
    print(f"üìä Extracted table sizes from {len(table_size_info)} tables:")
    
    # „ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫ÊÉÖÂ†±„Çí„É≠„Ç∞Âá∫Âäõ
    for table_name, size_info in table_size_info.items():
        print(f"  - {table_name}: {size_info['files_read_gb']:.2f} GB (files read)")

    # „Éé„Éº„Éâ„Åã„Çâ„É°„Çø„Éá„Éº„ÇøÊÉÖÂ†±„ÇíÊäΩÂá∫
    for node in all_nodes:
        node_name = node.get('name', '')
        node_tag = node.get('tag', '')
        node_metadata = node.get('metadata', [])
        
        # „É°„Çø„Éá„Éº„Çø„Åã„ÇâÈáçË¶Å„Å™ÊÉÖÂ†±„ÇíÊäΩÂá∫
        for metadata_item in node_metadata:
            key = metadata_item.get('key', '')
            values = metadata_item.get('values', [])
            value = metadata_item.get('value', '')
            
            # „Éï„Ç£„É´„Çø„ÉºÊù°‰ª∂„ÅÆÊäΩÂá∫
            if key == 'FILTERS' and values:
                for filter_expr in values:
                    extracted_data["filter_columns"].append({
                        "expression": filter_expr,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # GROUP BYÂºè„ÅÆÊäΩÂá∫
            elif key == 'GROUPING_EXPRESSIONS' and values:
                for group_expr in values:
                    extracted_data["groupby_columns"].append({
                        "expression": group_expr,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # JOINÊù°‰ª∂„ÅÆÊäΩÂá∫
            elif key in ['LEFT_KEYS', 'RIGHT_KEYS'] and values:
                for join_key in values:
                    extracted_data["join_columns"].append({
                        "expression": join_key,
                        "key_type": key,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # ÈõÜÁ¥ÑÈñ¢Êï∞„ÅÆÊäΩÂá∫
            elif key == 'AGGREGATE_EXPRESSIONS' and values:
                for agg_expr in values:
                    extracted_data["aggregate_columns"].append({
                        "expression": agg_expr,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # „ÉÜ„Éº„Éñ„É´ÊÉÖÂ†±„ÅÆÊäΩÂá∫
            elif key == 'SCAN_IDENTIFIER':
                table_name = value
                # „Çπ„Ç≠„É£„É≥„Éé„Éº„Éâ„Åã„Çâ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞ÊÉÖÂ†±„ÇíÊäΩÂá∫
                cluster_attributes = extract_cluster_attributes(node)
                print(f"    üìä Table {table_name} clustering keys: {cluster_attributes}")
                
                extracted_data["table_info"][table_name] = {
                    "node_name": node_name,
                    "node_tag": node_tag,
                    "node_id": node.get('id', ''),
                    "current_clustering_keys": cluster_attributes  # ÊäΩÂá∫„Åó„Åü„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº„ÇíË®≠ÂÆö
                }

    # „Éé„Éº„Éâ„Çø„Ç§„ÉóÂà•„ÅÆÂàÜÈ°û„Å®ÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„ÉºÊÉÖÂ†±„ÅÆÈñ¢ÈÄ£‰ªò„Åë
    # „É°„Éà„É™„ÇØ„Çπ„ÅåËæûÊõ∏„Åß„Å™„ÅÑÂ†¥Âêà„ÅØnode_metrics„ÇíÁ©∫„É™„Çπ„Éà„Å®„Åó„Å¶Âá¶ÁêÜ
    if isinstance(metrics, dict):
        node_metrics = metrics.get('node_metrics', [])
    else:
        print(f"‚ö†Ô∏è Warning: metrics is not a dictionary (type: {type(metrics)}), using empty node_metrics")
        node_metrics = []
    for node in node_metrics:
        node_name = node.get('name', '')
        node_type = node.get('tag', '')
        key_metrics = node.get('key_metrics', {})
        
        if any(keyword in node_name.upper() for keyword in ['SCAN', 'FILESCAN', 'PARQUET', 'DELTA']):
            # „Çπ„Ç≠„É£„É≥„Éé„Éº„Éâ„Åã„Çâ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº„ÇíÊäΩÂá∫
            cluster_attributes = extract_cluster_attributes(node)
            
            # „Éé„Éº„Éâ„Åã„Çâ„ÉÜ„Éº„Éñ„É´Âêç„ÇíÊäΩÂá∫„Åó„Å¶„Éû„ÉÉ„Éî„É≥„Ç∞
            node_metadata = node.get('metadata', [])
            table_name_from_node = None
            
            for meta in node_metadata:
                meta_key = meta.get('key', '')
                meta_value = meta.get('value', '')
                if meta_key == 'SCAN_IDENTIFIER' and meta_value:
                    table_name_from_node = meta_value
                    break
            
            # „ÉÜ„Éº„Éñ„É´Âêç„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑÂ†¥Âêà„ÅØ„Éé„Éº„ÉâÂêç„Åã„ÇâÊé®Ê∏¨
            if not table_name_from_node:
                import re
                table_patterns = [
                    r'[Ss]can\s+([a-zA-Z_][a-zA-Z0-9_.]*[a-zA-Z0-9_])',
                    r'([a-zA-Z_][a-zA-Z0-9_]*\.)+([a-zA-Z_][a-zA-Z0-9_]*)',
                ]
                
                for pattern in table_patterns:
                    match = re.search(pattern, node_name)
                    if match:
                        if '.' in match.group(0):
                            table_name_from_node = match.group(0)
                        else:
                            table_name_from_node = match.group(1) if match.lastindex and match.lastindex >= 1 else match.group(0)
                        break
            
            # „Éï„Ç£„É´„ÇøÁéáÊÉÖÂ†±„ÇíË®àÁÆó
            filter_result = calculate_filter_rate(node)
            filter_rate_info = {
                "filter_rate": filter_result.get("filter_rate", 0),
                "files_read_bytes": filter_result.get("files_read_bytes", 0),
                "files_pruned_bytes": filter_result.get("files_pruned_bytes", 0),
                "has_filter_metrics": filter_result.get("has_filter_metrics", False)
            }
            
            # „ÉÜ„Éº„Éñ„É´ÊÉÖÂ†±„Å´„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº„Å®„Éï„Ç£„É´„ÇøÁéá„ÇíËøΩÂä†
            if table_name_from_node:
                # Êó¢Â≠ò„ÅÆ„ÉÜ„Éº„Éñ„É´ÊÉÖÂ†±„ÇíÊõ¥Êñ∞
                if table_name_from_node in extracted_data["table_info"]:
                    extracted_data["table_info"][table_name_from_node]["current_clustering_keys"] = cluster_attributes
                    extracted_data["table_info"][table_name_from_node]["filter_info"] = filter_rate_info
                else:
                    # Êñ∞„Åó„ÅÑ„ÉÜ„Éº„Éñ„É´ÊÉÖÂ†±„Çí‰ΩúÊàê
                    extracted_data["table_info"][table_name_from_node] = {
                        "node_name": node_name,
                        "node_tag": node_type,
                        "node_id": node.get('node_id', ''),
                        "current_clustering_keys": cluster_attributes,
                        "filter_info": filter_rate_info
                    }
            
            extracted_data["scan_nodes"].append({
                "name": node_name,
                "type": node_type,
                "rows": key_metrics.get('rowsNum', 0),
                "duration_ms": key_metrics.get('durationMs', 0),
                "node_id": node.get('node_id', ''),
                "table_name": table_name_from_node,
                "current_clustering_keys": cluster_attributes
            })
        elif any(keyword in node_name.upper() for keyword in ['JOIN', 'HASH']):
            extracted_data["join_nodes"].append({
                "name": node_name,
                "type": node_type,
                "duration_ms": key_metrics.get('durationMs', 0),
                "node_id": node.get('node_id', '')
            })
        elif any(keyword in node_name.upper() for keyword in ['FILTER']):
            extracted_data["filter_nodes"].append({
                "name": node_name,
                "type": node_type,
                "duration_ms": key_metrics.get('durationMs', 0),
                "node_id": node.get('node_id', '')
            })

    # „É°„Çø„Éá„Éº„Çø„Çµ„Éû„É™„Éº
    extracted_data["metadata_summary"] = {
        "total_nodes": len(all_nodes),
        "total_graphs": len(graphs),
        "filter_expressions_count": len(extracted_data["filter_columns"]),
        "join_expressions_count": len(extracted_data["join_columns"]),
        "groupby_expressions_count": len(extracted_data["groupby_columns"]),
        "aggregate_expressions_count": len(extracted_data["aggregate_columns"]),
        "tables_identified": len(extracted_data["table_info"]),
        "scan_nodes_count": len(extracted_data["scan_nodes"]),
        "join_nodes_count": len(extracted_data["join_nodes"]),
        "filter_nodes_count": len(extracted_data["filter_nodes"])
    }
    
    print(f"‚úÖ Data extraction completed: {extracted_data['metadata_summary']}")
    
    # Display detailed current clustering key information
    clustering_info_found = False
    for table_name, table_info in extracted_data["table_info"].items():
        current_keys = table_info.get('current_clustering_keys', [])
        if current_keys:
            if not clustering_info_found:
                print(f"üîç Current clustering key information:")
                clustering_info_found = True
            print(f"  üìä Table: {table_name}")
            print(f"     Current keys: {', '.join(current_keys)}")
            print(f"     Node: {table_info.get('node_name', 'Unknown')}")
            print()
    
    if not clustering_info_found:
        print(f"‚ÑπÔ∏è No current clustering keys detected")
    
    # üö® ÈáçË¶Å: ÊäΩÂá∫„Åó„Åü„ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫ÊÉÖÂ†±„Çítable_info„Å´Áµ±Âêà
    def normalize_table_name(table_name):
        """„ÉÜ„Éº„Éñ„É´Âêç„ÇíÊ≠£Ë¶èÂåñÔºà„Éï„É´„Éç„Éº„É†„Å®Áü≠Á∏ÆÂêç„ÅÆ‰∏°Êñπ„Çí„ÉÅ„Çß„ÉÉ„ÇØÔºâ"""
        if not table_name:
            return None
        # Êó¢Â≠ò„ÅÆ„ÉÜ„Éº„Éñ„É´ÊÉÖÂ†±„Åã„Çâ„Éû„ÉÉ„ÉÅ„Åô„Çã„ÇÇ„ÅÆ„ÇíÊé¢„Åô
        for existing_table in extracted_data["table_info"].keys():
            if (existing_table == table_name or 
                existing_table.endswith('.' + table_name) or
                table_name.endswith('.' + existing_table.split('.')[-1])):
                return existing_table
        return table_name
    
    for table_name, size_info in table_size_info.items():
        # „ÉÜ„Éº„Éñ„É´Âêç„ÇíÊ≠£Ë¶èÂåñ„Åó„Å¶Êó¢Â≠ò„Ç®„É≥„Éà„É™„Å®„Éû„ÉÉ„ÉÅ
        normalized_table_name = normalize_table_name(table_name)
        
        if normalized_table_name not in extracted_data["table_info"]:
            # Êñ∞„Åó„ÅÑ„Ç®„É≥„Éà„É™„Çí‰ΩúÊàêÔºà„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞ÊÉÖÂ†±„Å™„ÅóÔºâ
            extracted_data["table_info"][normalized_table_name] = {
                "node_name": f"Scan {table_name}",
                "node_tag": "SCAN", 
                "node_id": f"scan_{table_name.replace('.', '_')}",
                "current_clustering_keys": [],  # „ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞ÊÉÖÂ†±„ÅØÂà•ÈÄîÊäΩÂá∫Ê∏à„Åø
                "filter_info": {}
            }
        # Êó¢Â≠ò„Ç®„É≥„Éà„É™„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØ„ÄÅcurrent_clustering_keys„ÅØ‰øùÊåÅ
        
        # „ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫ÊÉÖÂ†±„ÇíËøΩÂä†ÔºàÊ≠£Ë¶èÂåñ„Åï„Çå„Åü„ÉÜ„Éº„Éñ„É´Âêç„Çí‰ΩøÁî®Ôºâ
        extracted_data["table_info"][normalized_table_name].update({
            "table_size_gb": size_info['files_read_gb'],
            "files_read_bytes": size_info['files_read_bytes'],
            "files_pruned_bytes": size_info['files_pruned_bytes'],
            "io_read_bytes": size_info['io_read_bytes'],
            "total_scan_gb": size_info['total_scan_gb'],
            "size_classification": (
                "large" if size_info['files_read_gb'] >= 50 else
                "medium" if size_info['files_read_gb'] >= 10 else
                "small"
            )
        })
    
    print(f"‚úÖ Table size integration completed: {len(table_size_info)} tables")
    
    return extracted_data

def analyze_liquid_clustering_opportunities(profiler_data: Dict[str, Any], metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    Generate Liquid Clustering analysis and recommendations using LLM
    """
    print(f"ü§ñ Starting LLM-based Liquid Clustering analysis")
    
    # Âü∫Êú¨„Éá„Éº„Çø„ÅÆÊäΩÂá∫
    extracted_data = extract_liquid_clustering_data(profiler_data, metrics)
    
    # LLMÂàÜÊûêÁî®„ÅÆ„Éó„É≠„É≥„Éó„Éà‰ΩúÊàê
    overall_metrics = metrics.get('overall_metrics', {})
    bottleneck_indicators = metrics.get('bottleneck_indicators', {})
    
    # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊ¶ÇË¶Å
    total_time_sec = overall_metrics.get('total_time_ms', 0) / 1000
    read_gb = overall_metrics.get('read_bytes', 0) / 1024 / 1024 / 1024
    rows_produced = overall_metrics.get('rows_produced_count', 0)
    rows_read = overall_metrics.get('rows_read_count', 0)
    
    # ÊäΩÂá∫„Åó„Åü„Ç´„É©„É†ÊÉÖÂ†±„ÅÆ„Çµ„Éû„É™„Éº‰ΩúÊàêÔºà‰∏ä‰Ωç5ÂÄã„Åæ„ÅßÔºâ
    filter_summary = []
    for i, item in enumerate(extracted_data["filter_columns"][:5]):
        filter_summary.append(f"  {i+1}. {item['expression']} (node: {item['node_name']})")
    
    join_summary = []
    for i, item in enumerate(extracted_data["join_columns"][:5]):
        join_summary.append(f"  {i+1}. {item['expression']} (type: {item['key_type']}, node: {item['node_name']})")
    
    groupby_summary = []
    for i, item in enumerate(extracted_data["groupby_columns"][:5]):
        groupby_summary.append(f"  {i+1}. {item['expression']} (node: {item['node_name']})")
    
    aggregate_summary = []
    for i, item in enumerate(extracted_data["aggregate_columns"][:5]):
        aggregate_summary.append(f"  {i+1}. {item['expression']} (node: {item['node_name']})")
    
    # „ÉÜ„Éº„Éñ„É´ÊÉÖÂ†±„ÅÆÈáçË§á„Ç®„É≥„Éà„É™„ÇíÁµ±ÂêàÔºà„Éï„É´„ÉÜ„Éº„Éñ„É´Âêç„ÇíÂÑ™ÂÖàÔºâ
    print(f"üîç Debug: Table info consolidation starting...")
    print(f"   Original table_info keys: {list(extracted_data['table_info'].keys())}")
    
    # „Åæ„Åö„Éï„É´„ÉÜ„Éº„Éñ„É´Âêç„ÅÆ„Åø„ÇíÂá¶ÁêÜ
    consolidated_table_info = {}
    full_table_names = []
    short_table_names = []
    
    for table_name, table_info in extracted_data["table_info"].items():
        if '.' in table_name and table_name.count('.') >= 2:  # „Éï„É´„ÉÜ„Éº„Éñ„É´Âêç„ÅÆÊù°‰ª∂„ÇíÂé≥ÂØÜÂåñ
            consolidated_table_info[table_name] = table_info
            full_table_names.append(table_name)
            print(f"   ‚úÖ Added full table: {table_name}, clustering_keys: {table_info.get('current_clustering_keys', [])}")
        else:
            short_table_names.append((table_name, table_info))
    
    # Ê¨°„Å´Áü≠Á∏Æ„ÉÜ„Éº„Éñ„É´Âêç„ÇíÂá¶ÁêÜÔºàÂØæÂøú„Åô„Çã„Éï„É´„ÉÜ„Éº„Éñ„É´Âêç„Åå„Å™„ÅÑÂ†¥Âêà„ÅÆ„ÅøÔºâ
    for table_name, table_info in short_table_names:
        # ÂØæÂøú„Åô„Çã„Éï„É´„ÉÜ„Éº„Éñ„É´Âêç„ÇíÊé¢„Åô
        matching_full_table = None
        for full_name in full_table_names:
            if full_name.endswith('.' + table_name):
                matching_full_table = full_name
                break
        
        if not matching_full_table:
            consolidated_table_info[table_name] = table_info
            print(f"   ‚ö†Ô∏è Added short table (no full match): {table_name}, clustering_keys: {table_info.get('current_clustering_keys', [])}")
        else:
            print(f"   ‚ùå Skipped short table (has full match): {table_name} ‚Üí {matching_full_table}")
    
    print(f"   Final consolidated keys: {list(consolidated_table_info.keys())}")
    
    # „ÉÜ„Éº„Éñ„É´ÊÉÖÂ†±„ÅÆ„Çµ„Éû„É™„ÉºÔºàÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„ÉºÊÉÖÂ†±„Å®„Éï„Ç£„É´„ÇøÁéá„ÇíÂê´„ÇÄÔºâ
    table_summary = []
    for table_name, table_info in consolidated_table_info.items():
        current_keys = table_info.get('current_clustering_keys', [])
        current_keys_str = ', '.join(current_keys) if current_keys else 'Not configured'
        
        # „Éï„Ç£„É´„ÇøÁéáÊÉÖÂ†±„ÇíËøΩÂä†
        filter_info = table_info.get('filter_info', {})
        filter_rate = filter_info.get('filter_rate', 0)
        files_read_bytes = filter_info.get('files_read_bytes', 0)
        files_pruned_bytes = filter_info.get('files_pruned_bytes', 0)
        
        # „Éê„Ç§„ÉàÊï∞„ÇíGBÂçò‰Ωç„Å´Â§âÊèõ
        read_gb = files_read_bytes / (1024**3) if files_read_bytes > 0 else 0
        pruned_gb = files_pruned_bytes / (1024**3) if files_pruned_bytes > 0 else 0
        
        if filter_info.get('has_filter_metrics', False):
            filter_str = f", filter rate: {filter_rate*100:.1f}% (read: {read_gb:.2f}GB, pruned: {pruned_gb:.2f}GB)"
        else:
            filter_str = ", filter rate: no information"
        
        # üö® ÈáçË¶Å: ÂÆüÈöõ„ÅÆ„ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫ÊÉÖÂ†±„Çí‰ΩøÁî®„Åó„Å¶Êé®Â•®Âà§ÂÆö
        table_size_gb = table_info.get('table_size_gb', 0)
        size_classification = table_info.get('size_classification', 'unknown')
        
        # „ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫„Å´„Çà„ÇãÊé®Â•®Âà§ÂÆö
        recommendation_status = (
            "‚ùåÊé®Â•®„Åó„Å™„ÅÑ(Â∞èË¶èÊ®°)" if size_classification == "small" else
            "‚ö†Ô∏èÊù°‰ª∂‰ªò„ÅçÊé®Â•®(‰∏≠Ë¶èÊ®°)" if size_classification == "medium" else
            "‚úÖÂº∑„ÅèÊé®Â•®(Â§ßË¶èÊ®°)" if size_classification == "large" else
            "‚ö†Ô∏èË¶ÅÁ¢∫Ë™ç"
        )
        
        table_summary.append(f"  - {table_name} ({recommendation_status}, „Çµ„Ç§„Ç∫: {table_size_gb:.2f}GB, node: {table_info['node_name']}, current clustering key: {current_keys_str}{filter_str})")
    
    # „Çπ„Ç≠„É£„É≥„Éé„Éº„Éâ„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊÉÖÂ†±
    scan_performance = []
    for scan in extracted_data["scan_nodes"]:
        efficiency = scan['rows'] / max(scan['duration_ms'], 1)
        scan_performance.append(f"  - {scan['name']}: {scan['rows']:,} rows, {scan['duration_ms']:,}ms, efficiency={efficiency:.1f} rows/ms")

    clustering_prompt = f"""
You are a Databricks Liquid Clustering expert. Please analyze the following SQL profiler data and provide optimal Liquid Clustering recommendations.

„ÄêQuery Performance Overview„Äë
- Execution time: {total_time_sec:.1f} seconds
- Data read: {read_gb:.2f}GB
- Output rows: {rows_produced:,} rows
- Read rows: {rows_read:,} rows
- „Éï„Ç£„É´„ÇøÁéá: {calculate_filter_rate_percentage(overall_metrics, metrics):.4f}

„ÄêÊäΩÂá∫„Åï„Çå„Åü„Ç´„É©„É†‰ΩøÁî®„Éë„Çø„Éº„É≥„Äë

üîç „Éï„Ç£„É´„Çø„ÉºÊù°‰ª∂ ({len(extracted_data["filter_columns"])}ÂÄã):
{chr(10).join(filter_summary)}

üîó JOINÊù°‰ª∂ ({len(extracted_data["join_columns"])}ÂÄã):
{chr(10).join(join_summary)}

üìä GROUP BY ({len(extracted_data["groupby_columns"])}ÂÄã):
{chr(10).join(groupby_summary)}

üìà ÈõÜÁ¥ÑÈñ¢Êï∞ ({len(extracted_data["aggregate_columns"])}ÂÄã) - ‚ö†Ô∏èÂèÇËÄÉÊÉÖÂ†±„ÅÆ„ÅøÔºà„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº„Å´„ÅØ‰ΩøÁî®Á¶ÅÊ≠¢Ôºâ:
{chr(10).join(aggregate_summary)}
‚ö†Ô∏è Ê≥®ÊÑè: ‰∏äË®ò„ÅÆÈõÜÁ¥ÑÈñ¢Êï∞„Åß‰ΩøÁî®„Åï„Çå„Çã„Ç´„É©„É†„ÅØ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº„ÅÆÂÄôË£ú„Åã„ÇâÈô§Â§ñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„Äê„ÉÜ„Éº„Éñ„É´ÊÉÖÂ†±„Äë
„ÉÜ„Éº„Éñ„É´Êï∞: {len(extracted_data["table_info"])}ÂÄã
{chr(10).join(table_summary)}

„Äê„Çπ„Ç≠„É£„É≥„Éé„Éº„Éâ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Äë
{chr(10).join(scan_performance)}

„ÄêÁèæÂú®„ÅÆ„Éú„Éà„É´„Éç„ÉÉ„ÇØÊåáÊ®ô„Äë
- „Çπ„Éî„É´Áô∫Áîü: {'„ÅÇ„Çä' if bottleneck_indicators.get('has_spill', False) else '„Å™„Åó'}
- „Ç∑„É£„ÉÉ„Éï„É´Êìç‰Ωú: {bottleneck_indicators.get('shuffle_operations_count', 0)}Âõû
- ‰Ωé‰∏¶ÂàóÂ∫¶„Çπ„ÉÜ„Éº„Ç∏: {bottleneck_indicators.get('low_parallelism_stages_count', 0)}ÂÄã

„ÄêÂàÜÊûêË¶ÅÊ±Ç„Äë
1. ÂêÑ„ÉÜ„Éº„Éñ„É´„Å´ÂØæ„Åô„ÇãÊúÄÈÅ©„Å™Liquid Clustering„Ç´„É©„É†„ÅÆÊé®Â•®ÔºàÊúÄÂ§ß4„Ç´„É©„É†Ôºâ
2. „Ç´„É©„É†ÈÅ∏ÂÆö„ÅÆÊ†πÊã†Ôºà„Éï„Ç£„É´„Çø„Éº„ÄÅJOIN„ÄÅGROUP BY„Åß„ÅÆ‰ΩøÁî®È†ªÂ∫¶„Å®ÈáçË¶ÅÂ∫¶Ôºâ
   üö® ÈáçË¶Å: ÈõÜÁ¥ÑÈñ¢Êï∞ÔºàSUM, AVG, COUNTÁ≠âÔºâ„ÅÆÂØæË±°„Ç´„É©„É†„ÅØ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº„Å´Âê´„ÇÅ„Å™„ÅÑ
   ‚úÖ ÊúâÂäπ: „Éï„Ç£„É´„Çø„ÉºÊù°‰ª∂„ÄÅJOINÊù°‰ª∂„ÄÅGROUP BYÊù°‰ª∂„Åß‰ΩøÁî®„Åï„Çå„Çã„Ç´„É©„É†„ÅÆ„Åø
3. ÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº„Å®Êé®Â•®„Ç≠„Éº„ÅÆÊØîËºÉÂàÜÊûê
4. ÂÆüË£ÖÂÑ™ÂÖàÈ†Ü‰ΩçÔºà„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂêë‰∏äÂäπÊûúÈ†ÜÔºâ
5. ÂÖ∑‰ΩìÁöÑ„Å™SQLÂÆüË£Ö‰æãÔºàÊ≠£„Åó„ÅÑDatabricks SQLÊßãÊñá„ÄÅÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„ÉºÊÉÖÂ†±„Çí„Ç≥„É°„É≥„Éà„Å´ÊòéË®òÔºâ
6. ÊúüÂæÖ„Åï„Çå„Çã„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊîπÂñÑÂäπÊûúÔºàÊï∞ÂÄ§„ÅßÔºâ

„Äêüö® „ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„ÉºÈÅ∏ÂÆö„ÅÆÈáçË¶Å„Å™Âà∂Èôê‰∫ãÈ†Ö„Äë
‚ùå Á¶ÅÊ≠¢: ÈõÜÁ¥ÑÈñ¢Êï∞„ÅÆ„Çø„Éº„Ç≤„ÉÉ„Éà„Ç´„É©„É†Ôºà‰æãÔºöSUM(sales_amount)„ÅÆsales_amountÔºâ
‚ùå Á¶ÅÊ≠¢: Ë®àÁÆó„ÅÆ„Åø„Å´‰ΩøÁî®„Åï„Çå„Çã„Ç´„É©„É†Ôºà‰æãÔºöAVG(quantity)„ÅÆquantityÔºâ
‚úÖ Êé®Â•®: WHEREÂè•„ÅÆ„Éï„Ç£„É´„Çø„ÉºÊù°‰ª∂„Ç´„É©„É†
‚úÖ Êé®Â•®: JOIN ONÂè•„ÅÆ„Ç≠„Éº„Ç´„É©„É†  
‚úÖ Êé®Â•®: GROUP BYÂè•„ÅÆ„Ç∞„É´„Éº„Éî„É≥„Ç∞„Ç´„É©„É†
‚úÖ Êé®Â•®: ORDER BYÂè•„ÅÆ„ÇΩ„Éº„Éà„Ç≠„ÉºÔºàÁØÑÂõ≤Ê§úÁ¥¢„Åå„ÅÇ„ÇãÂ†¥ÂêàÔºâ

ÁêÜÁî±: ÈõÜÁ¥ÑÂØæË±°„Ç´„É©„É†„Çí„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº„Å´Âê´„ÇÅ„Å¶„ÇÇ„ÄÅ„Éï„Ç°„Ç§„É´„Éó„É´„Éº„Éã„É≥„Ç∞ÂäπÊûú„ÇÑJOINÂäπÁéá„ÅÆÊîπÂñÑ„ÅåÊúüÂæÖ„Åß„Åç„Åö„ÄÅ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„ÅÆÂäπÊûú„ÇíËñÑ„ÇÅ„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ

„ÄêÂà∂Á¥Ñ‰∫ãÈ†Ö„Äë
- „Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„Éã„É≥„Ç∞„ÇÑZORDER„ÅØÊèêÊ°à„Åó„Å™„ÅÑÔºàLiquid Clustering„ÅÆ„ÅøÔºâ
- Ê≠£„Åó„ÅÑDatabricks SQLÊßãÊñá„Çí‰ΩøÁî®Ôºö
  * Êñ∞Ë¶è„ÉÜ„Éº„Éñ„É´: CREATE TABLE ... CLUSTER BY (col1, col2, ...)
  * Êó¢Â≠ò„ÉÜ„Éº„Éñ„É´: ALTER TABLE table_name CLUSTER BY (col1, col2, ...)
- ÊúÄÂ§ß4„Ç´„É©„É†„Åæ„Åß„ÅÆÊé®Â•®
- „Éá„Éº„Çø„Çπ„Ç≠„É•„Éº„ÇÑ‰∏¶ÂàóÂ∫¶„ÅÆÂïèÈ°å„ÇÇËÄÉÊÖÆ

„Äêüö® „ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫„Éô„Éº„Çπ„ÅÆÊé®Â•®Âà§ÂÆöÂü∫Ê∫ñ„Äë
‚ùå Êé®Â•®„Åó„Å™„ÅÑÔºàÂäπÊûúËñÑÔºâ: 10GBÊú™Ê∫Ä„ÅÆ„ÉÜ„Éº„Éñ„É´
  - Â∞èË¶èÊ®°„ÉÜ„Éº„Éñ„É´Ôºàdate_dim, item„Å™„Å©Ôºâ„ÅØÈô§Â§ñ
  - ÁêÜÁî±: „Éï„Ç°„Ç§„É´Êï∞„ÅåÂ∞ë„Å™„Åè„ÄÅ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞ÂäπÊûú„ÅåÈôêÂÆöÁöÑ
  - ‰ª£ÊõøÁ≠ñ: ÈÅ©Âàá„Å™„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÇÑ„É°„É¢„É™„Ç≠„É£„ÉÉ„Ç∑„É•„ÇíÊ¥ªÁî®

‚ö†Ô∏è Êù°‰ª∂‰ªò„ÅçÊé®Â•®: 10-50GB„ÅÆ„ÉÜ„Éº„Éñ„É´  
  - ‰∏≠Ë¶èÊ®°„ÉÜ„Éº„Éñ„É´„Åß„ÄÅÈ†ªÁπÅ„Å™„Éï„Ç£„É´„Çø„É™„É≥„Ç∞Êù°‰ª∂„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅÆ„ÅøÊé®Â•®
  - „Éï„Ç£„É´„ÇøÁéá„ÇÑ„Ç¢„ÇØ„Çª„Çπ„Éë„Çø„Éº„É≥„ÇíËÄÉÊÖÆ„Åó„Å¶Âà§ÂÆö

‚úÖ Âº∑„ÅèÊé®Â•®: 50GB‰ª•‰∏ä„ÅÆ„ÉÜ„Éº„Éñ„É´
  - Â§ßË¶èÊ®°„ÉÜ„Éº„Éñ„É´Ôºàstore_sales: 159GB, catalog_sales: 121GBÁ≠âÔºâ
  - ÁêÜÁî±: Â§ßÈáè„ÅÆ„Éï„Ç°„Ç§„É´„Åß„Éó„É´„Éº„Éã„É≥„Ç∞ÂäπÊûú„ÅåÂ§ß„Åç„ÅÑ
  
„Äê„ÉÜ„Éº„Éñ„É´Âà•Êé®Â•®ÂÑ™ÂÖàÂ∫¶„Äë
1. Â§ßË¶èÊ®°„ÉÜ„Éº„Éñ„É´Ôºà50GB+Ôºâ: ÊúÄÂÑ™ÂÖà„ÅßLiquid ClusteringÈÅ©Áî®
2. ‰∏≠Ë¶èÊ®°„ÉÜ„Éº„Éñ„É´Ôºà10-50GBÔºâ: „Éï„Ç£„É´„ÇøÈ†ªÂ∫¶„Å®‰ΩøÁî®„Éë„Çø„Éº„É≥„Å´Âü∫„Å•„ÅçÂà§ÂÆö  
3. Â∞èË¶èÊ®°„ÉÜ„Éº„Éñ„É´Ôºà10GBÊú™Ê∫ÄÔºâ: ‚ùå Liquid Clustering„ÅØÊé®Â•®„Åó„Å™„ÅÑ

„Äêüö® Important Understanding of Liquid Clustering Specifications„Äë
- **Column Order**: In Liquid Clustering, changing the order of clustering keys does not affect "node-level data locality"
- **Actual Improvement Effects**: Improvements are in "scan efficiency", "file pruning effects", and "query performance"
- **Technical Characteristics**: Column order within CLUSTER BY is arbitrary, and (col1, col2, col3) and (col3, col1, col2) have equivalent performance

„Äêüö® Absolutely Prohibited Incorrect Expressions„Äë
‚ùå "Improve data locality by changing order"
‚ùå "Improve data locality with clustering key order"  
‚ùå "Node-level data placement optimization through order changes"
‚úÖ "No specific improvement effect from order changes (Liquid Clustering specification)"
‚úÖ "Improvement in scan efficiency and file pruning effects"
‚úÖ "Performance improvement for WHERE clauses and JOIN conditions"

Á∞°ÊΩî„ÅßÂÆüË∑µÁöÑ„Å™ÂàÜÊûêÁµêÊûú„ÇíÊó•Êú¨Ë™û„ÅßÊèê‰æõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„ÄêÈáçË¶Å„Å™Âá∫ÂäõÂΩ¢ÂºèÊåáÁ§∫„Äë
ÂêÑ„ÉÜ„Éº„Éñ„É´„ÅÆÂàÜÊûê„Åß„ÅØ„ÄÅÂøÖ„Åö‰ª•‰∏ã„ÅÆÂΩ¢Âºè„ÅßÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„ÉºÊÉÖÂ†±„Å®„Éï„Ç£„É´„ÇøÁéá„ÇíÂê´„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑÔºö

## „ÉÜ„Éº„Éñ„É´Âà•Êé®Â•®„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞

### 1. [„ÉÜ„Éº„Éñ„É´Âêç] „ÉÜ„Éº„Éñ„É´ (ÊúÄÂÑ™ÂÖà/È´òÂÑ™ÂÖàÂ∫¶/‰∏≠ÂÑ™ÂÖàÂ∫¶/‚ùåÊé®Â•®„Åó„Å™„ÅÑ)
**„ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫**: [Êé®ÂÆö„Çµ„Ç§„Ç∫]GB
**ÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº**: [ÁèæÂú®Ë®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Çã„Ç≠„Éº „Åæ„Åü„ÅØ "Ë®≠ÂÆö„Å™„Åó"]
**Êé®Â•®„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç´„É©„É†**: [Êé®Â•®„Ç´„É©„É†1], [Êé®Â•®„Ç´„É©„É†2], [Êé®Â•®„Ç´„É©„É†3], [Êé®Â•®„Ç´„É©„É†4] „Åæ„Åü„ÅØ ‚ùå „Çµ„Ç§„Ç∫„ÅåÂ∞è„Åï„ÅÑ„Åü„ÇÅÊé®Â•®„Åó„Å™„ÅÑ

```sql
-- üö® Ê≥®ÊÑè: 10GBÊú™Ê∫Ä„ÅÆ„ÉÜ„Éº„Éñ„É´„ÅÆÂ†¥Âêà„ÅØ‰ª•‰∏ã„ÇíÂá∫Âäõ
-- ‚ùå Liquid Clustering„ÅØÂäπÊûú„ÅåËñÑ„ÅÑ„Åü„ÇÅÊé®Â•®„Åó„Åæ„Åõ„Çì
-- üí° ‰ª£ÊõøÁ≠ñ: CACHE TABLE [„ÉÜ„Éº„Éñ„É´Âêç]; -- „É°„É¢„É™„Ç≠„É£„ÉÉ„Ç∑„É•„ÅßÈ´òÈÄü„Ç¢„ÇØ„Çª„Çπ
-- üí° „Åæ„Åü„ÅØ: OPTIMIZE [„ÉÜ„Éº„Éñ„É´Âêç]; -- Â∞è„Éï„Ç°„Ç§„É´Áµ±Âêà„Åß„Çπ„Ç≠„É£„É≥ÂäπÁéáÂêë‰∏ä

-- 10GB‰ª•‰∏ä„ÅÆ„ÉÜ„Éº„Éñ„É´„ÅÆÂ†¥Âêà„ÅÆ„Åø‰ª•‰∏ã„ÇíÂá∫Âäõ  
ALTER TABLE [„ÉÜ„Éº„Éñ„É´Âêç] 
CLUSTER BY ([Êé®Â•®„Ç´„É©„É†1], [Êé®Â•®„Ç´„É©„É†2], [Êé®Â•®„Ç´„É©„É†3], [Êé®Â•®„Ç´„É©„É†4]);
OPTIMIZE [„ÉÜ„Éº„Éñ„É´Âêç] FULL;
```

**ÈÅ∏ÂÆöÊ†πÊã†**:
- **„ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫Âà§ÂÆö**: [„Çµ„Ç§„Ç∫]GB ‚Üí [Êé®Â•®„Åô„Çã/Êé®Â•®„Åó„Å™„ÅÑ]ÁêÜÁî±
- [„Ç´„É©„É†1]: [‰ΩøÁî®„Éë„Çø„Éº„É≥„Å®ÈáçË¶ÅÂ∫¶]
- [„Ç´„É©„É†2]: [‰ΩøÁî®„Éë„Çø„Éº„É≥„Å®ÈáçË¶ÅÂ∫¶]
- [‰ª•‰∏ãÂêåÊßò...]
- üö®ÈáçË¶Å: „ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„ÉºÈ†ÜÂ∫èÂ§âÊõ¥„ÅØ„Éé„Éº„Éâ„É¨„Éô„É´„ÅÆ„Éá„Éº„ÇøÂ±ÄÊâÄÊÄß„Å´ÂΩ±Èüø„Åó„Å™„ÅÑÔºàLiquid Clustering‰ªïÊßòÔºâ
- ‚úÖÊîπÂñÑÂäπÊûú: „Çπ„Ç≠„É£„É≥ÂäπÁéá„Å®„Éï„Ç°„Ç§„É´„Éó„É´„Éº„Éã„É≥„Ç∞ÂäπÊûú„ÅÆÂêë‰∏äÔºàÈ†ÜÂ∫èÁÑ°Èñ¢‰øÇÔºâ

**ÊúüÂæÖ„Åï„Çå„ÇãÊîπÂñÑÂäπÊûú**:
- [ÂÖ∑‰ΩìÁöÑ„Å™Êï∞ÂÄ§„Åß„ÅÆÊîπÂñÑË¶ãËæº„Åø] „Åæ„Åü„ÅØ ‚ùå Â∞èË¶èÊ®°„ÉÜ„Éº„Éñ„É´„ÅÆ„Åü„ÇÅÂäπÊûúËñÑ„ÅÑ

**„Éï„Ç£„É´„ÇøÁéá**: [X.X]% (Ë™≠„ÅøËæº„Åø: [XX.XX]GB, „Éó„É´„Éº„É≥: [XX.XX]GB)

„Åì„ÅÆÂΩ¢Âºè„Å´„Çà„Çä„ÄÅÁèæÂú®„ÅÆË®≠ÂÆö„ÄÅÊé®Â•®Ë®≠ÂÆö„ÄÅ„Åä„Çà„Å≥ÂêÑ„ÉÜ„Éº„Éñ„É´„ÅÆÁèæÂú®„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞ÂäπÁéá„ÇíÊòéÁ¢∫„Å´Ë°®Á§∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Éï„Ç£„É´„ÇøÁéáÊÉÖÂ†±„ÅØ‰∏äË®ò„ÉÜ„Éº„Éñ„É´ÊÉÖÂ†±„Åã„ÇâÊ≠£Á¢∫„Å™Êï∞ÂÄ§„Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
"""

    try:
        # LLMÂàÜÊûê„ÅÆÂÆüË°å
        provider = LLM_CONFIG["provider"]
        print(f"ü§ñ Analyzing Liquid Clustering using {provider}...")
        
        if provider == "databricks":
            llm_analysis = _call_databricks_llm(clustering_prompt)
        elif provider == "openai":
            llm_analysis = _call_openai_llm(clustering_prompt)
        elif provider == "azure_openai":
            llm_analysis = _call_azure_openai_llm(clustering_prompt)
        elif provider == "anthropic":
            llm_analysis = _call_anthropic_llm(clustering_prompt)
        else:
            llm_analysis = f"‚ùå Unsupported LLM provider: {provider}"
        
        # ÂàÜÊûêÁµêÊûú„ÅÆÊßãÈÄ†Âåñ
        clustering_analysis = {
            "llm_analysis": llm_analysis,
            "extracted_data": extracted_data,
            "performance_context": {
                "total_time_sec": total_time_sec,
                "read_gb": read_gb,
                "rows_produced": rows_produced,
                "rows_read": rows_read,
                "data_selectivity": calculate_filter_rate_percentage(overall_metrics, metrics)
            },
            "summary": {
                "analysis_method": "LLM-based",
                "tables_identified": len(extracted_data["table_info"]),
                "total_filter_columns": len(extracted_data["filter_columns"]),
                "total_join_columns": len(extracted_data["join_columns"]),
                "total_groupby_columns": len(extracted_data["groupby_columns"]),
                "total_aggregate_columns": len(extracted_data["aggregate_columns"]),
                "scan_nodes_count": len(extracted_data["scan_nodes"]),
                "llm_provider": provider
            }
        }
        
        print("‚úÖ LLM Liquid Clustering analysis completed")
        return clustering_analysis
        
    except Exception as e:
        error_msg = f"LLM analysis error: {str(e)}"
        print(f"‚ùå {error_msg}")
        
        # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: Âü∫Êú¨ÁöÑ„Å™ÊäΩÂá∫„Éá„Éº„Çø„ÅÆ„Åø„ÇíËøî„Åô
        return {
            "llm_analysis": f"‚ùå LLMÂàÜÊûê„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {error_msg}",
            "extracted_data": extracted_data,
            "summary": {
                "analysis_method": "extraction-only",
                "tables_identified": len(extracted_data["table_info"]),
                "total_filter_columns": len(extracted_data["filter_columns"]),
                "error": error_msg
            }
        }

def save_liquid_clustering_analysis(clustering_analysis: Dict[str, Any], output_dir: str = "/tmp") -> Dict[str, str]:
    """
    Liquid ClusteringÂàÜÊûêÁµêÊûú„Çí„Éï„Ç°„Ç§„É´„Å´Âá∫Âäõ
    """
    import os
    import json
    from datetime import datetime
    
    # „Çø„Ç§„É†„Çπ„Çø„É≥„Éó‰ªò„Åç„Éï„Ç°„Ç§„É´Âêç„ÅÆÁîüÊàê
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Âá∫Âäõ„Éï„Ç°„Ç§„É´„Éë„Çπ
    json_path = f"{output_dir}/liquid_clustering_analysis_{timestamp}.json"
    markdown_path = f"{output_dir}/liquid_clustering_analysis_{timestamp}.md"
    sql_path = f"{output_dir}/liquid_clustering_implementation_{timestamp}.sql"
    
    file_paths = {}
    
    try:
        # 1. JSONÂΩ¢Âºè„Åß„ÅÆË©≥Á¥∞„Éá„Éº„Çø‰øùÂ≠ò
        # setÂûã„ÇílistÂûã„Å´Â§âÊèõ„Åó„Å¶JSON serializable „Å´„Åô„Çã
        json_data = convert_sets_to_lists(clustering_analysis)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        file_paths['json'] = json_path
        print(f"‚úÖ Saved detailed data in JSON format: {json_path}")
        
        # 2. Save analysis report in Markdown format
        markdown_content = generate_liquid_clustering_markdown_report(clustering_analysis)
        
        with open(markdown_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        file_paths['markdown'] = markdown_path
        print(f"‚úÖ Saved analysis report in Markdown format: {markdown_path}")
        
        # 3. Generate SQL implementation examples file
        sql_content = generate_liquid_clustering_sql_implementations(clustering_analysis)
        
        with open(sql_path, 'w', encoding='utf-8') as f:
            f.write(sql_content)
        
        file_paths['sql'] = sql_path
        print(f"‚úÖ Saved SQL implementation examples: {sql_path}")
        
        return file_paths
        
    except Exception as e:
        error_msg = f"„Éï„Ç°„Ç§„É´Âá∫Âäõ„Ç®„É©„Éº: {str(e)}"
        print(f"‚ùå {error_msg}")
        return {"error": error_msg}

def generate_liquid_clustering_markdown_report(clustering_analysis: Dict[str, Any]) -> str:
    """
    Liquid ClusteringÂàÜÊûêÁµêÊûú„ÅÆMarkdown„É¨„Éù„Éº„Éà„ÇíÁîüÊàê
    """
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Âü∫Êú¨ÊÉÖÂ†±„ÅÆÂèñÂæó
    summary = clustering_analysis.get('summary', {})
    performance_context = clustering_analysis.get('performance_context', {})
    extracted_data = clustering_analysis.get('extracted_data', {})
    llm_analysis = clustering_analysis.get('llm_analysis', '')
    
    markdown_content = f"""# Liquid Clustering Analysis Report

**Generated Date**: {timestamp}  
**Analysis Method**: {summary.get('analysis_method', 'Unknown')}  
**LLM Provider**: {summary.get('llm_provider', 'Unknown')}

## üìä Performance Overview

| Item | Value |
|------|-----|
| Execution Time | {performance_context.get('total_time_sec', 0):.1f} seconds |
| Data Read | {performance_context.get('read_gb', 0):.2f}GB |
| Output Rows | {performance_context.get('rows_produced', 0):,} rows |
| Read Rows | {performance_context.get('rows_read', 0):,} rows |
| Filter Rate | {performance_context.get('data_selectivity', 0):.4f} |

## üîç Extracted Metadata

### Filter Conditions ({summary.get('total_filter_columns', 0)} items)
"""
    
    # „Éï„Ç£„É´„Çø„ÉºÊù°‰ª∂„ÅÆË©≥Á¥∞
    filter_columns = extracted_data.get('filter_columns', [])
    for i, filter_item in enumerate(filter_columns[:10], 1):  # ÊúÄÂ§ß10ÂÄã„Åæ„ÅßË°®Á§∫
        markdown_content += f"{i}. `{filter_item.get('expression', '')}` („Éé„Éº„Éâ: {filter_item.get('node_name', '')})\n"
    
    if len(filter_columns) > 10:
        markdown_content += f"... ‰ªñ {len(filter_columns) - 10}ÂÄã\n"
    
    markdown_content += f"""
### JOINÊù°‰ª∂ ({summary.get('total_join_columns', 0)}ÂÄã)
"""
    
    # JOINÊù°‰ª∂„ÅÆË©≥Á¥∞
    join_columns = extracted_data.get('join_columns', [])
    for i, join_item in enumerate(join_columns[:10], 1):
        markdown_content += f"{i}. `{join_item.get('expression', '')}` ({join_item.get('key_type', '')})\n"
    
    if len(join_columns) > 10:
        markdown_content += f"... ‰ªñ {len(join_columns) - 10}ÂÄã\n"
    
    markdown_content += f"""
### GROUP BYÊù°‰ª∂ ({summary.get('total_groupby_columns', 0)}ÂÄã)
"""
    
    # GROUP BYÊù°‰ª∂„ÅÆË©≥Á¥∞
    groupby_columns = extracted_data.get('groupby_columns', [])
    for i, groupby_item in enumerate(groupby_columns[:10], 1):
        markdown_content += f"{i}. `{groupby_item.get('expression', '')}` („Éé„Éº„Éâ: {groupby_item.get('node_name', '')})\n"
    
    if len(groupby_columns) > 10:
        markdown_content += f"... ‰ªñ {len(groupby_columns) - 10}ÂÄã\n"
    
    markdown_content += f"""
### ÈõÜÁ¥ÑÈñ¢Êï∞ ({summary.get('total_aggregate_columns', 0)}ÂÄã)
"""
    
    # ÈõÜÁ¥ÑÈñ¢Êï∞„ÅÆË©≥Á¥∞
    aggregate_columns = extracted_data.get('aggregate_columns', [])
    for i, agg_item in enumerate(aggregate_columns[:10], 1):
        markdown_content += f"{i}. `{agg_item.get('expression', '')}` („Éé„Éº„Éâ: {agg_item.get('node_name', '')})\n"
    
    if len(aggregate_columns) > 10:
        markdown_content += f"... ‰ªñ {len(aggregate_columns) - 10}ÂÄã\n"
    
    markdown_content += f"""
## üè∑Ô∏è Ë≠òÂà•„Åï„Çå„Åü„ÉÜ„Éº„Éñ„É´ ({summary.get('tables_identified', 0)}ÂÄã)

"""
    
    # „ÉÜ„Éº„Éñ„É´ÊÉÖÂ†±„ÅÆË©≥Á¥∞ÔºàÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº„ÇíÂê´„ÇÄÔºâ
    table_info = extracted_data.get('table_info', {})
    for table_name, table_details in table_info.items():
        current_keys = table_details.get('current_clustering_keys', [])
        current_keys_str = ', '.join(current_keys) if current_keys else 'Ë®≠ÂÆö„Å™„Åó'
        markdown_content += f"- **{table_name}** („Éé„Éº„Éâ: {table_details.get('node_name', '')})\n"
        markdown_content += f"  - ÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº: `{current_keys_str}`\n"
    
    markdown_content += f"""
## üîé „Çπ„Ç≠„É£„É≥„Éé„Éº„ÉâÂàÜÊûê ({summary.get('scan_nodes_count', 0)}ÂÄã)

"""
    
    # „Çπ„Ç≠„É£„É≥„Éé„Éº„Éâ„ÅÆË©≥Á¥∞
    scan_nodes = extracted_data.get('scan_nodes', [])
    for scan in scan_nodes:
        efficiency = scan.get('rows', 0) / max(scan.get('duration_ms', 1), 1)
        markdown_content += f"- **{scan.get('name', '')}**: {scan.get('rows', 0):,}Ë°å, {scan.get('duration_ms', 0):,}ms, ÂäπÁéá={efficiency:.1f}Ë°å/ms\n"
    
    markdown_content += f"""
## ü§ñ LLMÂàÜÊûêÁµêÊûú

{llm_analysis}

## üìã ÂàÜÊûê„Çµ„Éû„É™„Éº

- **ÂàÜÊûêÂØæË±°„ÉÜ„Éº„Éñ„É´Êï∞**: {summary.get('tables_identified', 0)}
- **„Éï„Ç£„É´„Çø„ÉºÊù°‰ª∂Êï∞**: {summary.get('total_filter_columns', 0)}
- **JOINÊù°‰ª∂Êï∞**: {summary.get('total_join_columns', 0)}
- **GROUP BYÊù°‰ª∂Êï∞**: {summary.get('total_groupby_columns', 0)}
- **ÈõÜÁ¥ÑÈñ¢Êï∞Êï∞**: {summary.get('total_aggregate_columns', 0)}
- **„Çπ„Ç≠„É£„É≥„Éé„Éº„ÉâÊï∞**: {summary.get('scan_nodes_count', 0)}

---
*Report generation time: {timestamp}*
"""
    
    return markdown_content

def generate_liquid_clustering_sql_implementations(clustering_analysis: Dict[str, Any]) -> str:
    """
    Generate SQL examples for Liquid Clustering implementation
    """
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Âü∫Êú¨ÊÉÖÂ†±„ÅÆÂèñÂæó
    extracted_data = clustering_analysis.get('extracted_data', {})
    table_info = extracted_data.get('table_info', {})
    
    sql_content = f"""-- =====================================================
-- Liquid Clustering ÂÆüË£ÖSQL‰æã
-- ÁîüÊàêÊó•ÊôÇ: {timestamp}
-- =====================================================

-- „ÄêÈáçË¶Å„Äë
-- ‰ª•‰∏ã„ÅÆSQL‰æã„ÅØÂàÜÊûêÁµêÊûú„Å´Âü∫„Å•„ÅèÊé®Â•®‰∫ãÈ†Ö„Åß„Åô„ÄÇ
-- ÂÆüÈöõ„ÅÆÂÆüË£ÖÂâç„Å´„ÄÅ„ÉÜ„Éº„Éñ„É´ÊßãÈÄ†„ÇÑ„Éá„Éº„ÇøÁâπÊÄß„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

"""
    
    # „ÉÜ„Éº„Éñ„É´„Åî„Å®„ÅÆSQLÂÆüË£Ö‰æã„ÇíÁîüÊàê
    for table_name, table_details in table_info.items():
        # ÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„ÉºÊÉÖÂ†±„ÇíÂèñÂæó
        current_keys = table_details.get('current_clustering_keys', [])
        current_keys_str = ', '.join(current_keys) if current_keys else 'Ë®≠ÂÆö„Å™„Åó'
        
        sql_content += f"""
-- =====================================================
-- „ÉÜ„Éº„Éñ„É´: {table_name}
-- ÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº: {current_keys_str}
-- =====================================================

-- Êó¢Â≠ò„ÉÜ„Éº„Éñ„É´„Å´Liquid Clustering„ÇíÈÅ©Áî®„Åô„ÇãÂ†¥Âêà:
-- ALTER TABLE {table_name} CLUSTER BY (column1, column2, column3, column4);

-- Êñ∞Ë¶è„ÉÜ„Éº„Éñ„É´‰ΩúÊàêÊôÇ„Å´Liquid Clustering„ÇíË®≠ÂÆö„Åô„ÇãÂ†¥Âêà:
-- CREATE TABLE {table_name}_clustered
-- CLUSTER BY (column1, column2, column3, column4)
-- AS SELECT * FROM {table_name};

-- Delta Live Tables„Åß„ÅÆË®≠ÂÆö‰æã:
-- @dlt.table(
--   cluster_by=["column1", "column2", "column3", "column4"]
-- )
-- def {table_name.split('.')[-1]}_clustered():
--   return spark.table("{table_name}")

-- „ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞Áä∂Ê≥Å„ÅÆÁ¢∫Ë™ç:
-- DESCRIBE DETAIL {table_name};

-- „ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞Áµ±Ë®à„ÅÆÁ¢∫Ë™ç:
-- ANALYZE TABLE {table_name} COMPUTE STATISTICS FOR ALL COLUMNS;

"""
    
    sql_content += f"""
-- =====================================================
-- ‰∏ÄËà¨ÁöÑ„Å™Liquid ClusteringÂÆüË£Ö„Éë„Çø„Éº„É≥
-- =====================================================

-- „Éë„Çø„Éº„É≥1: „Éï„Ç£„É´„Çø„ÉºÈ†ªÂ∫¶„ÅÆÈ´ò„ÅÑ„Ç´„É©„É†„ÇíÂÑ™ÂÖà
-- Êé®Â•®È†ÜÂ∫è: 1) „Éï„Ç£„É´„Çø„ÉºÊù°‰ª∂„Ç´„É©„É† 2) JOINÊù°‰ª∂„Ç´„É©„É† 3) GROUP BY„Ç´„É©„É†

-- „Éë„Çø„Éº„É≥2: „Ç´„Éº„Éá„Ç£„Éä„É™„ÉÜ„Ç£„ÇíËÄÉÊÖÆ„Åó„ÅüÈ†ÜÂ∫è
-- ‰Ωé„Ç´„Éº„Éá„Ç£„Éä„É™„ÉÜ„Ç£ ‚Üí È´ò„Ç´„Éº„Éá„Ç£„Éä„É™„ÉÜ„Ç£„ÅÆÈ†Ü„ÅßÈÖçÁΩÆ

-- „Éë„Çø„Éº„É≥3: „Éá„Éº„Çø„Ç¢„ÇØ„Çª„Çπ„Éë„Çø„Éº„É≥„Å´Âü∫„Å•„ÅèÈÖçÁΩÆ
-- „Çà„Åè‰∏ÄÁ∑í„Å´‰ΩøÁî®„Åï„Çå„Çã„Ç´„É©„É†„ÇíËøë„ÅÑ‰ΩçÁΩÆ„Å´ÈÖçÁΩÆ

-- =====================================================
-- ÂÆüË£ÖÂæå„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊ§úË®ºSQL
-- =====================================================

-- 1. „ÇØ„Ç®„É™ÂÆüË°åË®àÁîª„ÅÆÁ¢∫Ë™ç
-- EXPLAIN SELECT ... FROM table_name WHERE ...;

-- 2. „Éï„Ç°„Ç§„É´„Çπ„Ç≠„ÉÉ„ÉóÁµ±Ë®à„ÅÆÁ¢∫Ë™ç
-- SELECT * FROM table_name WHERE filter_column = 'value';
-- -- SQL„Éó„É≠„Éï„Ç°„Ç§„É©„Éº„Åß„Éï„Ç°„Ç§„É´„Çπ„Ç≠„ÉÉ„ÉóÊï∞„ÇíÁ¢∫Ë™ç

-- 3. „Éá„Éº„ÇøÈÖçÁΩÆ„ÅÆÁ¢∫Ë™ç
-- SELECT 
--   file_path,
--   count(*) as row_count,
--   min(cluster_column1) as min_val,
--   max(cluster_column1) as max_val
-- FROM table_name
-- GROUP BY file_path
-- ORDER BY file_path;

-- =====================================================
-- Ê≥®ÊÑè‰∫ãÈ†Ö
-- =====================================================

-- 1. Liquid Clustering„ÅØÊúÄÂ§ß4„Ç´„É©„É†„Åæ„ÅßÊåáÂÆöÂèØËÉΩ
-- 2. „Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„Éã„É≥„Ç∞„Å®„ÅØ‰ΩµÁî®‰∏çÂèØ
-- 3. Êó¢Â≠ò„ÅÆZORDER BY„ÅØËá™ÂãïÁöÑ„Å´ÁÑ°ÂäπÂåñ„Åï„Çå„Çã
-- 4. „ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„ÅÆÂäπÊûú„ÅØÊôÇÈñì„Å®„Å®„ÇÇ„Å´Âêë‰∏ä„Åô„ÇãÔºàOPTIMIZEÂÆüË°å„ÅßÊúÄÈÅ©ÂåñÔºâ
-- 5. ÂÆöÊúüÁöÑ„Å™OPTIMIZEÂÆüË°å„ÇíÊé®Â•®
-- 6. **ÈáçË¶Å**: „Ç´„É©„É†„ÅÆÊåáÂÆöÈ†ÜÂ∫è„ÅØ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Å´ÂΩ±Èüø„Åó„Åæ„Åõ„Çì
--    * CLUSTER BY (col1, col2, col3) „Å® CLUSTER BY (col3, col1, col2) „ÅØÂêåÁ≠â
--    * ÂæìÊù•„ÅÆ„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„Éã„É≥„Ç∞„ÇÑZ-ORDER„Å®„ÅØÁï∞„Å™„ÇãÈáçË¶Å„Å™ÁâπÊÄß

-- OPTIMIZEÂÆüË°å‰æã:
-- OPTIMIZE table_name;

-- =====================================================
-- ÁîüÊàêÊÉÖÂ†±
-- =====================================================
-- ÁîüÊàêÊó•ÊôÇ: {timestamp}
-- ÂàÜÊûêÂØæË±°„ÉÜ„Éº„Éñ„É´Êï∞: {len(table_info)}
-- Âü∫„Å•„ÅÑ„ÅüÂàÜÊûê: LLM„Å´„Çà„ÇãLiquid ClusteringÂàÜÊûê
"""
    
    return sql_content

print("‚úÖ Function definition completed: analyze_liquid_clustering_opportunities, save_liquid_clustering_analysis")

# COMMAND ----------

def translate_explain_summary_to_english(explain_content: str) -> str:
    """
    EXPLAINË¶ÅÁ¥Ñ„Éï„Ç°„Ç§„É´„ÅÆÊó•Êú¨Ë™ûÈÉ®ÂàÜ„ÇíËã±Ë™û„Å´ÁøªË®≥
    
    Args:
        explain_content: EXPLAINË¶ÅÁ¥Ñ„Éï„Ç°„Ç§„É´„ÅÆÂÜÖÂÆπ
    
    Returns:
        str: Ëã±Ë™ûÁâàEXPLAINË¶ÅÁ¥Ñ
    """
    # OUTPUT_LANGUAGE„Åå'en'„ÅÆÂ†¥Âêà„ÅØÁøªË®≥„Çí„Çπ„Ç≠„ÉÉ„Éó
    output_language = globals().get('OUTPUT_LANGUAGE', 'ja')
    if output_language == 'en':
        return explain_content
    # Êó•Êú¨Ë™û„Åã„ÇâËã±Ë™û„Å∏„ÅÆÁøªË®≥„Éû„ÉÉ„Éî„É≥„Ç∞
    translation_map = {
        # „Éò„ÉÉ„ÉÄ„ÉºÈÉ®ÂàÜ
        "# EXPLAIN + EXPLAIN COSTË¶ÅÁ¥ÑÁµêÊûú (optimized)": "# EXPLAIN + EXPLAIN COST Summary Results (optimized)",
        "## üìä Âü∫Êú¨ÊÉÖÂ†±": "## üìä Basic Information", 
        "ÁîüÊàêÊó•ÊôÇ": "Generated",
        "„ÇØ„Ç®„É™„Çø„Ç§„Éó": "Query Type",
        "ÂÖÉ„Çµ„Ç§„Ç∫": "Original Size",
        "Ë¶ÅÁ¥ÑÂæå„Çµ„Ç§„Ç∫": "Summary Size",
        "ÂúßÁ∏ÆÁéá": "Compression Ratio",
        "ÊñáÂ≠ó": "characters",
        
        # LLMË¶ÅÁ¥ÑÁµêÊûú
        "## üß† LLMË¶ÅÁ¥ÑÁµêÊûú": "## üß† LLM Summary Results",
        "# Databricks SQL„ÇØ„Ç®„É™„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂàÜÊûê": "# Databricks SQL Query Performance Analysis",
        "## üìä Physical PlanË¶ÅÁ¥Ñ": "## üìä Physical Plan Summary",
        "### ‰∏ªË¶Å„Å™Âá¶ÁêÜ„Çπ„ÉÜ„ÉÉ„Éó": "### Key Processing Steps",
        "Ë§áÊï∞„ÉÜ„Éº„Éñ„É´„Åã„Çâ„ÅÆ„Éá„Éº„ÇøÂèñÂæó": "Data retrieval from multiple tables",
        "„Çµ„Éñ„ÇØ„Ç®„É™ÂÆüË°å": "Subquery execution",
        "Âπ≥ÂùáÂ£≤‰∏ä„ÇíË®àÁÆó„Åô„Çã„Çµ„Éñ„ÇØ„Ç®„É™": "Subquery calculating average sales",
        "„Éï„Ç£„É´„Çø„É™„É≥„Ç∞": "Filtering",
        "Âπ≥ÂùáÂ£≤‰∏ä„ÇíË∂Ö„Åà„ÇãÂïÜÂìÅ„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞": "Filtering products exceeding average sales",
        "ÈõÜË®àÂá¶ÁêÜ": "Aggregation processing",
        "„Éñ„É©„É≥„Éâ„ÄÅ„ÇØ„É©„Çπ„ÄÅ„Ç´„ÉÜ„Ç¥„É™„Åî„Å®„ÅÆÂ£≤‰∏äÈõÜË®à": "Sales aggregation by brand, class, category",
        "JOINÂá¶ÁêÜ": "JOIN processing",
        "Ë§áÊï∞„ÅÆJOINÊìç‰Ωú": "Multiple JOIN operations",
        "„ÅåÂ§öÁî®": "is frequently used",
        "„ÇΩ„Éº„Éà": "Sorting",
        "„Åß„ÅÆ„ÇΩ„Éº„Éà": "sorting by",
        "ÊúÄÁµÇÁµêÊûú„Çí": "Final results to",
        "Ë°å„Å´Âà∂Èôê": "rows limit",
        
        # JOINÊñπÂºè„Å®„Éá„Éº„ÇøÁßªÂãï
        "### JOINÊñπÂºè„Å®„Éá„Éº„ÇøÁßªÂãï„Éë„Çø„Éº„É≥": "### JOIN Methods and Data Movement Patterns",
        "‰∏ªË¶ÅJOINÊñπÂºè": "Primary JOIN Method",
        "„Éá„Éº„ÇøÁßªÂãï": "Data Movement",
        "„Å´„Çà„ÇãÂäπÁéáÁöÑ„Å™„Éá„Éº„ÇøÁßªÂãï": "for efficient data movement",
        "„Å´„Çà„ÇãÈõÜÁ¥ÑÂá¶ÁêÜ": "for aggregation processing",
        "„Å´„Çà„Çã„Éá„Éº„ÇøÂàÜÊï£": "for data distribution",
        "„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥": "partitions",
        
        # PhotonÂà©Áî®Áä∂Ê≥Å
        "### PhotonÂà©Áî®Áä∂Ê≥Å": "### Photon Usage Status",
        "È´òÂ∫¶„Å™PhotonÊ¥ªÁî®": "Advanced Photon utilization",
        "„Å™„Å©Â§öÊï∞„ÅÆPhotonÊúÄÈÅ©ÂåñÊºîÁÆóÂ≠ê„Çí‰ΩøÁî®": "and many other Photon optimization operators in use",
        "ÂÆüË°åÊôÇ„ÅÆÊúÄÈÅ©Âåñ„ÅåÊúâÂäπ": "Runtime optimization enabled",
        
        # Áµ±Ë®àÊÉÖÂ†±„Çµ„Éû„É™„Éº
        "## üí∞ Áµ±Ë®àÊÉÖÂ†±„Çµ„Éû„É™„Éº": "## üí∞ Statistics Summary",
        "### „ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫„Å®Ë°åÊï∞": "### Table Size and Row Count",
        "Á¥Ñ": "approximately",
        "ÂÑÑË°å": "billion rows",
        "ÊúÄÁµÇÁµêÊûú„Çª„ÉÉ„Éà": "Final result set",
        "ÈÅ©Áî®Âæå": "after application",
        "‰∏≠ÈñìÁµêÊûú": "Intermediate results",
        "‰∏áË°å": "thousand rows",
        "„ÇΩ„Éº„ÉàÂâç": "before sorting",
        
        # JOINÈÅ∏ÊäûÁéá„Å®„Éï„Ç£„É´„ÇøÂäπÁéá
        "### JOINÈÅ∏ÊäûÁéá„Å®„Éï„Ç£„É´„ÇøÂäπÁéá": "### JOIN Selectivity and Filter Efficiency",
        "„Éï„Ç£„É´„Çø": "filter",
        "Âπ¥Â∫¶Êù°‰ª∂": "Year condition",
        "„Å´„Çà„Çä„ÄÅ": "resulted in",
        "Ë°å„Å´Áµû„ÇäËæº„Åø": "rows filtered",
        "È´òÂäπÁéá": "high efficiency",
        "„Çµ„Éñ„ÇØ„Ç®„É™ÁµêÊûú": "Subquery result",
        "Âπ≥ÂùáÂ£≤‰∏äË®àÁÆó„ÅÆ„Çµ„Éñ„ÇØ„Ç®„É™„ÅØÂçò‰∏ÄË°å„ÇíËøîÂç¥": "Average sales calculation subquery returns single row",
        "„É°„Ç§„É≥„ÇØ„Ç®„É™„Éï„Ç£„É´„Çø": "Main query filter",
        "Âπ≥ÂùáÂ£≤‰∏ä„ÇíË∂Ö„Åà„ÇãÂïÜÂìÅ„Å´Áµû„ÇäËæº„Åø": "Filtered to products exceeding average sales",
        "Ë°å„Å´ÂâäÊ∏õ": "rows reduced to",
        
        # „Ç´„É©„É†Áµ±Ë®à
        "### „Ç´„É©„É†Áµ±Ë®à": "### Column Statistics",
        "Á®ÆÈ°û„ÅÆÁï∞„Å™„ÇãÂÄ§": "distinct values",
        "„ÅÆÁØÑÂõ≤": "range",
        "Êï∞Èáè": "quantity",
        
        # „Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥ÂàÜÊï£Áä∂Ê≥Å
        "### „Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥ÂàÜÊï£Áä∂Ê≥Å": "### Partition Distribution Status",
        "„Éè„ÉÉ„Ç∑„É•„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„Éã„É≥„Ç∞": "Hash partitioning",
        "„Å´Âü∫„Å•„Åè": "based on",
        "„Ç∑„É≥„Ç∞„É´„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥": "Single partition",
        "ÈõÜÁ¥ÑÂá¶ÁêÜ„ÇÑÊúÄÁµÇÁµêÊûú„ÅÆÂèéÈõÜ„Å´‰ΩøÁî®": "Used for aggregation processing and final result collection",
        
        # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂàÜÊûê
        "## ‚ö° „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂàÜÊûê": "## ‚ö° Performance Analysis",
        "### ÂÆüË°å„Ç≥„Çπ„Éà„ÅÆÂÜÖË®≥": "### Execution Cost Breakdown",
        "ÊúÄ„ÇÇ„Ç≥„Çπ„Éà„ÅåÈ´ò„ÅÑÊìç‰Ωú": "Most expensive operation",
        "„Åã„Çâ„ÅÆ„Çπ„Ç≠„É£„É≥": "table scan",
        "„Çµ„Éñ„ÇØ„Ç®„É™„Ç≥„Çπ„Éà": "Subquery cost",
        "„Åã„Çâ„ÅÆUNION ALLÂá¶ÁêÜ": "UNION ALL processing from",
        "„Å´„Çà„ÇãÈõÜË®à„Ç≥„Çπ„Éà": "aggregation cost by",
        
        # „Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûê
        "### „Éú„Éà„É´„Éç„ÉÉ„ÇØ„Å´„Å™„Çä„Åù„ÅÜ„Å™Êìç‰Ωú": "### Operations Likely to Become Bottlenecks",
        "Â§ßË¶èÊ®°„ÉÜ„Éº„Éñ„É´„Çπ„Ç≠„É£„É≥": "Large table scan",
        "„ÅÆ„Çπ„Ç≠„É£„É≥„ÅåÊúÄÂ§ß„ÅÆ„Éú„Éà„É´„Éç„ÉÉ„ÇØ": "scan is the biggest bottleneck",
        "Ë§áÊï∞„ÉÜ„Éº„Éñ„É´UNION": "Multiple table UNION",
        "„Åß„ÅÆ3„Å§„ÅÆË≤©Â£≤„ÉÜ„Éº„Éñ„É´": "3 sales tables in",
        "„ÅÆÁµ±Âêà": "integration",
        "„Ç∑„É£„ÉÉ„Éï„É´Êìç‰Ωú": "Shuffle operations",
        "„Å´„Çà„Çã„Éá„Éº„ÇøÂÜçÂàÜÊï£": "data redistribution by",
        
        # ÊúÄÈÅ©Âåñ„ÅÆ‰ΩôÂú∞
        "### ÊúÄÈÅ©Âåñ„ÅÆ‰ΩôÂú∞„Åå„ÅÇ„ÇãÁÆáÊâÄ": "### Areas with Optimization Potential",
        "„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„Éó„É´„Éº„Éã„É≥„Ç∞": "Partition pruning",
        "„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„ÅØÂäπÊûúÁöÑ„Å†„Åå„ÄÅ„Åï„Çâ„Å´": "filtering is effective, but further",
        "„ÅÆË≤©Â£≤„ÉÜ„Éº„Éñ„É´„ÅÆ„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥ÊúÄÈÅ©Âåñ„ÅåÂèØËÉΩ": "sales table partition optimization is possible",
        "JOINÈ†ÜÂ∫è": "JOIN order",
        "„ÅÆÈ†ÜÂ∫èÊúÄÈÅ©Âåñ": "order optimization",
        "„Éï„Ç£„É´„Çø„Éó„ÉÉ„Ç∑„É•„ÉÄ„Ç¶„É≥": "Filter pushdown",
        "„Åå‰ΩøÁî®„Åï„Çå„Å¶„ÅÑ„Çã„Åå„ÄÅ„Åï„Çâ„Å´ÊúÄÈÅ©Âåñ„ÅÆ‰ΩôÂú∞„ÅÇ„Çä": "is used, but further optimization potential exists",
        "„Ç´„É©„É†ÈÅ∏Êäû": "Column selection",
        "ÂøÖË¶Å„Å™„Ç´„É©„É†„ÅÆ„Åø„ÇíÊó©Êúü„Å´ÈÅ∏Êäû„Åô„Çã„Åì„Å®„Åß„Éá„Éº„ÇøÁßªÂãïÈáè„ÇíÂâäÊ∏õÂèØËÉΩ": "Data movement can be reduced by early selection of only necessary columns",
        "„É°„É¢„É™‰ΩøÁî®Èáè": "Memory usage",
        "„ÅÆ„Éì„É´„ÉâÂÅ¥„ÅÆ„Çµ„Ç§„Ç∫ÊúÄÈÅ©Âåñ": "build-side size optimization for",
        
        # ÁâπË®ò‰∫ãÈ†Ö
        "### ÁâπË®ò‰∫ãÈ†Ö": "### Notable Points",
        "Ê¥ªÁî®": "utilization",
        "„ÇØ„Ç®„É™ÂÖ®‰Ωì„Åß": "Throughout the query",
        "ÊúÄÈÅ©Âåñ„ÅåÂäπÊûúÁöÑ„Å´ÈÅ©Áî®„Åï„Çå„Å¶„ÅÑ„Çã": "optimization is effectively applied",
        "Áµ±Ë®àÊÉÖÂ†±": "Statistical information",
        "„ÅåÈÅ©Âàá„Å´ÂèéÈõÜ„Åï„Çå„Å¶„Åä„Çä„ÄÅ„Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂„ÅÆÂà§Êñ≠„Å´Ë≤¢ÁåÆ": "is properly collected and contributes to optimizer decisions",
        "ÂãïÁöÑ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞": "Dynamic filtering",
        "„ÅåÈÅ©Áî®„Åï„Çå„ÄÅ‰∏çË¶Å„Å™„Éá„Éº„ÇøË™≠„ÅøËæº„Åø„ÇíÂõûÈÅø": "is applied to avoid unnecessary data reading",
        "„Ç¢„ÉÄ„Éó„ÉÜ„Ç£„ÉñÂÆüË°å": "Adaptive execution",
        "„ÅåÊúâÂäπ„Åß„ÄÅÂÆüË°åÊôÇ„ÅÆÊúÄÈÅ©Âåñ„ÅåÊúüÂæÖ„Åß„Åç„Çã": "is enabled, runtime optimization can be expected",
        
        # ÁµêË´ñ
        "„Åì„ÅÆ„ÇØ„Ç®„É™„ÅØË§áÈõë„Å™JOIN„Å®ÈõÜË®à„ÇíÂê´„ÇÄ„Åå": "This query includes complex JOINs and aggregations, but",
        "„ÅÆÂäπÊûúÁöÑ„Å™‰ΩøÁî®„Å´„Çà„Çä„ÄÅÊØîËºÉÁöÑÂäπÁéáÁöÑ„Å´ÂÆüË°å„Åï„Çå„Çã„Å®‰∫àÊ∏¨„Åï„Çå„Åæ„Åô": "effective use is expected to execute relatively efficiently",
        "ÊúÄÂ§ß„ÅÆ„Éú„Éà„É´„Éç„ÉÉ„ÇØ„ÅØÂ§ßË¶èÊ®°„ÉÜ„Éº„Éñ„É´„ÅÆ„Çπ„Ç≠„É£„É≥„Å®„Éá„Éº„ÇøÁßªÂãï„Å´„ÅÇ„Çä„Åæ„Åô": "The biggest bottlenecks are large table scans and data movement",
        
        # Áµ±Ë®àÊÉÖÂ†±ÊäΩÂá∫
        "## üí∞ Áµ±Ë®àÊÉÖÂ†±ÊäΩÂá∫": "## üí∞ Statistics Extraction",
        "## üìä Áµ±Ë®àÊÉÖÂ†±„Çµ„Éû„É™„ÉºÔºàÁ∞°ÊΩîÁâàÔºâ": "## üìä Statistics Summary (Concise Version)",
        "Á∑èÁµ±Ë®àÈ†ÖÁõÆÊï∞": "Total statistics items",
        "ÂÄã": "items",
        "„ÉÜ„Éº„Éñ„É´Áµ±Ë®à": "Table statistics", 
        "„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥ÊÉÖÂ†±": "Partition information",
        "### üéØ ‰∏ªË¶ÅÁµ±Ë®à": "### üéØ Key Statistics",
        "üìä „ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫": "üìä Table Size",
        "üí° Ë©≥Á¥∞„Å™Áµ±Ë®àÊÉÖÂ†±„ÅØ": "üí° Detailed statistics available with",
        "„ÅßÁ¢∫Ë™ç„Åß„Åç„Åæ„Åô": "setting"
    }
    
    # ÁøªË®≥„ÇíÈÅ©Áî®
    translated_content = explain_content
    for jp_text, en_text in translation_map.items():
        translated_content = translated_content.replace(jp_text, en_text)
    
    return translated_content

# COMMAND ----------

# MAGIC %md
# MAGIC ## ü§ñ LLM-powered Bottleneck Analysis Function
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - LLM analysis formatting of extracted metrics
# MAGIC - Multiple LLM provider support (Databricks/OpenAI/Azure/Anthropic)
# MAGIC - Detailed analysis report generation in English
# MAGIC - Error handling and fallback analysis

# COMMAND ----------

def analyze_bottlenecks_with_llm(metrics: Dict[str, Any]) -> str:
    """
    Generate comprehensive performance analysis report
    Integrates information from Cell 33 (TOP10 processes), Cell 35 (Liquid Clustering), Cell 43 (integrated optimization execution)
    Also leverages EXPLAIN + EXPLAIN COST results for more precise analysis
    
    üö® Important: Prevention of percentage calculation degradation
    - Using the sum of parallel execution node times as total time is strictly prohibited
    - Prioritize using overall_metrics.total_time_ms (wall-clock time)
    - Use maximum node time during fallback (not sum)
    """
    from datetime import datetime
    
    print("üìä Generating comprehensive performance analysis report (EXPLAIN+EXPLAIN COST integration)...")
    
    # === EXPLAIN + EXPLAIN COSTÁµêÊûú„ÅÆË™≠„ÅøËæº„Åø ===
    explain_content = ""
    explain_cost_content = ""
    physical_plan = ""
    photon_explanation = ""
    cost_statistics = ""
    
    explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
    if explain_enabled.upper() == 'Y':
        import glob
        import os
        
        print("üîç For bottleneck analysis: Searching EXPLAIN + EXPLAIN COST result files...")
        
        # ÊúÄÊñ∞„ÅÆEXPLAINÁµêÊûú„Éï„Ç°„Ç§„É´„ÇíÊ§úÁ¥¢
        explain_original_files = glob.glob("output_explain_original_*.txt")
        explain_optimized_files = glob.glob("output_explain_optimized_*.txt")
        explain_files = explain_original_files if explain_original_files else explain_optimized_files
        
        if explain_files:
            latest_explain_file = max(explain_files, key=os.path.getctime)
            try:
                with open(latest_explain_file, 'r', encoding='utf-8') as f:
                    explain_content = f.read()
                    print(f"‚úÖ Loaded EXPLAIN results for bottleneck analysis: {latest_explain_file}")
                
                # Physical Plan„ÅÆÊäΩÂá∫
                if "== Physical Plan ==" in explain_content:
                    physical_plan_start = explain_content.find("== Physical Plan ==")
                    physical_plan_end = explain_content.find("== Photon", physical_plan_start)
                    if physical_plan_end == -1:
                        physical_plan_end = len(explain_content)
                    physical_plan = explain_content[physical_plan_start:physical_plan_end].strip()
                
                # Photon Explanation„ÅÆÊäΩÂá∫
                if "== Photon Explanation ==" in explain_content:
                    photon_start = explain_content.find("== Photon Explanation ==")
                    photon_explanation = explain_content[photon_start:].strip()
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load EXPLAIN results for bottleneck analysis: {str(e)}")
        
        # üöÄ EXPLAIN COSTÁµêÊûú„Çí„Ç≠„É£„ÉÉ„Ç∑„É•„Åã„ÇâÂèñÂæóÔºàÂèØËÉΩ„Å™Â†¥ÂêàÔºâ
        cached_cost_result = globals().get('cached_original_explain_cost_result')
        explain_cost_content = ""
        cost_statistics = ""
        
        if cached_cost_result and 'explain_cost_file' in cached_cost_result:
            try:
                with open(cached_cost_result['explain_cost_file'], 'r', encoding='utf-8') as f:
                    explain_cost_content = f.read()
                    print(f"üíæ Using cached EXPLAIN COST results for bottleneck analysis: {cached_cost_result['explain_cost_file']}")
                
                # Áµ±Ë®àÊÉÖÂ†±„ÅÆÊäΩÂá∫
                cost_statistics = extract_cost_statistics_from_explain_cost(explain_cost_content)
                print(f"üìä Extracted statistics for bottleneck analysis: {len(cost_statistics)} characters")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load cached EXPLAIN COST results: {str(e)}")
                # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: ÂæìÊù•„ÅÆ„Éï„Ç°„Ç§„É´Ê§úÁ¥¢
                cached_cost_result = None
        
        # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: „Ç≠„É£„ÉÉ„Ç∑„É•„ÅåÂà©Áî®„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÅØÂæìÊù•„ÅÆ„Éï„Ç°„Ç§„É´Ê§úÁ¥¢
        if not cached_cost_result:
            cost_original_files = glob.glob("output_explain_cost_original_*.txt")
            cost_optimized_files = glob.glob("output_explain_cost_optimized_*.txt")
            cost_files = cost_original_files if cost_original_files else cost_optimized_files
            
            if cost_files:
                latest_cost_file = max(cost_files, key=os.path.getctime)
                try:
                    with open(latest_cost_file, 'r', encoding='utf-8') as f:
                        explain_cost_content = f.read()
                        print(f"üí∞ Loaded EXPLAIN COST results for bottleneck analysis: {latest_cost_file}")
                    
                    # Áµ±Ë®àÊÉÖÂ†±„ÅÆÊäΩÂá∫
                    cost_statistics = extract_cost_statistics_from_explain_cost(explain_cost_content)
                    print(f"üìä Extracted statistics for bottleneck analysis: {len(cost_statistics)} characters")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Failed to load EXPLAIN COST results for bottleneck analysis: {str(e)}")
        
        if not explain_files and not cost_files:
            # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: Âè§„ÅÑ„Éï„Ç°„Ç§„É´Âêç„Éë„Çø„Éº„É≥„ÇÇ„ÉÅ„Çß„ÉÉ„ÇØ
            old_explain_files = glob.glob("output_explain_plan_*.txt")
            if old_explain_files:
                latest_explain_file = max(old_explain_files, key=os.path.getctime)
                try:
                    with open(latest_explain_file, 'r', encoding='utf-8') as f:
                        explain_content = f.read()
                        print(f"‚úÖ Loaded legacy format EXPLAIN results: {latest_explain_file}")
                        
                    if "== Physical Plan ==" in explain_content:
                        physical_plan_start = explain_content.find("== Physical Plan ==")
                        physical_plan_end = explain_content.find("== Photon", physical_plan_start)
                        if physical_plan_end == -1:
                            physical_plan_end = len(explain_content)
                        physical_plan = explain_content[physical_plan_start:physical_plan_end].strip()
                        
                    if "== Photon Explanation ==" in explain_content:
                        photon_start = explain_content.find("== Photon Explanation ==")
                        photon_explanation = explain_content[photon_start:].strip()
                except Exception as e:
                    print(f"‚ö†Ô∏è Failed to load legacy format EXPLAIN results: {str(e)}")
            else:
                print("‚ö†Ô∏è Bottleneck analysis: EXPLAIN„ÉªEXPLAIN COST result files not found")
    
    # „É¨„Éù„Éº„ÉàÁîüÊàêÊôÇÂàª
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # === 1. Âü∫Êú¨„É°„Éà„É™„ÇØ„Çπ„ÅÆÂèñÂæó ===
    overall_metrics = metrics.get('overall_metrics', {})
    bottleneck_indicators = metrics.get('bottleneck_indicators', {})
    
    total_time_sec = overall_metrics.get('total_time_ms', 0) / 1000
    read_gb = overall_metrics.get('read_bytes', 0) / 1024 / 1024 / 1024
    cache_hit_ratio = bottleneck_indicators.get('cache_hit_ratio', 0) * 100
    data_selectivity = bottleneck_indicators.get('data_selectivity', 0) * 100
    
    # PhotonÊÉÖÂ†±
    photon_enabled = overall_metrics.get('photon_enabled', False)
    photon_utilization = min(overall_metrics.get('photon_utilization_ratio', 0) * 100, 100.0)
    
    # ‰∏¶ÂàóÂ∫¶„Éª„Ç∑„É£„ÉÉ„Éï„É´ÊÉÖÂ†±
    shuffle_count = bottleneck_indicators.get('shuffle_operations_count', 0)
    has_shuffle_bottleneck = bottleneck_indicators.get('has_shuffle_bottleneck', False)
    has_low_parallelism = bottleneck_indicators.get('has_low_parallelism', False)
    low_parallelism_count = bottleneck_indicators.get('low_parallelism_stages_count', 0)
    
    # „Çπ„Éî„É´ÊÉÖÂ†±
    has_spill = bottleneck_indicators.get('has_spill', False)
    spill_bytes = bottleneck_indicators.get('spill_bytes', 0)
    spill_gb = spill_bytes / 1024 / 1024 / 1024 if spill_bytes > 0 else 0
    
    # „Çπ„Ç≠„É•„ÉºÊ§úÂá∫ÊÉÖÂ†±
    has_skew = bottleneck_indicators.get('has_skew', False)
    has_aqe_shuffle_skew_warning = bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False)
    
    # === 2. „Çª„É´33: TOP10„Éó„É≠„Çª„ÇπÂàÜÊûêÊÉÖÂ†±„ÅÆÂèñÂæó ===
    # ÂÖ®„Éé„Éº„Éâ„ÇíÂÆüË°åÊôÇÈñì„Åß„ÇΩ„Éº„ÉàÔºà„Éë„Éº„Çª„É≥„ÉÜ„Éº„Ç∏Ë®àÁÆóÁî®Ôºâ
    all_sorted_nodes = sorted(metrics['node_metrics'], 
                             key=lambda x: x['key_metrics'].get('durationMs', 0), 
                             reverse=True)
    
    # TOP5„Éú„Éà„É´„Éç„ÉÉ„ÇØÊäΩÂá∫Áî®
    sorted_nodes = all_sorted_nodes[:5]
    
    # üö® ÈáçË¶Å: Ê≠£„Åó„ÅÑÂÖ®‰ΩìÊôÇÈñì„ÅÆË®àÁÆóÔºà„Éá„Ç∞„É¨Èò≤Ê≠¢Ôºâ
    # 1. overall_metrics.total_time_ms„ÇíÂÑ™ÂÖà‰ΩøÁî®Ôºàwall-clock timeÔºâ
    total_time_ms = overall_metrics.get('total_time_ms', 0)
    
    # üö® ‰∏¶ÂàóÂÆüË°åÂïèÈ°å„ÅÆ‰øÆÊ≠£: task_total_time_ms„ÇíÂÑ™ÂÖà‰ΩøÁî®
    # ÂÄãÂà•„Éé„Éº„ÉâÊôÇÈñì„ÅØ‰∏¶Âàó„Çø„Çπ„ÇØ„ÅÆÁ¥ØÁ©çÊôÇÈñì„ÅÆ„Åü„ÇÅ„ÄÅÂêå„Åò„ÅèÁ¥ØÁ©çÊôÇÈñì„Åß„ÅÇ„Çãtask_total_time_ms„Å®ÊØîËºÉ
    task_total_time_ms = overall_metrics.get('task_total_time_ms', 0)
    
    if task_total_time_ms > 0:
        total_time_ms = task_total_time_ms
        print(f"‚úÖ Debug: Parallel execution support - using task_total_time_ms: {total_time_ms:,} ms ({total_time_ms/3600000:.1f} hours)")
    elif total_time_ms <= 0:
        # execution_time_ms„ÇíÊ¨°„ÅÆÂÑ™ÂÖàÂ∫¶„Åß‰ΩøÁî®
        execution_time_ms = overall_metrics.get('execution_time_ms', 0)
        if execution_time_ms > 0:
            total_time_ms = execution_time_ms
            print(f"‚ö†Ô∏è Debug: task_total_time_ms unavailable, using execution_time_ms: {total_time_ms} ms")
        else:
            # ÊúÄÁµÇ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: ÂÖ®„Éé„Éº„Éâ„ÅÆÂêàË®àÊôÇÈñì
            max_node_time = max([node['key_metrics'].get('durationMs', 0) for node in all_sorted_nodes], default=1)
            total_time_ms = int(max_node_time * 1.2)
            print(f"‚ö†Ô∏è Debug: Final fallback - using estimated time: {total_time_ms} ms")
    
    print(f"üìä Debug: Total time used for percentage calculation: {total_time_ms:,} ms ({total_time_ms/1000:.1f} sec)")
    
    critical_processes = []
    for i, node in enumerate(sorted_nodes):
        duration_ms = node['key_metrics'].get('durationMs', 0)
        duration_sec = duration_ms / 1000
        
        # „Éë„Éº„Çª„É≥„ÉÜ„Éº„Ç∏Ë®àÁÆóÔºà100%„Çí‰∏äÈôê„Å®„Åô„ÇãÔºâ
        percentage = min((duration_ms / max(total_time_ms, 1)) * 100, 100.0)
        
        # „Éú„Éà„É´„Éç„ÉÉ„ÇØ„ÅÆÈáçË¶ÅÂ∫¶Âà§ÂÆö
        severity = "CRITICAL" if duration_ms >= 10000 else "HIGH" if duration_ms >= 5000 else "MEDIUM"
        
        # ÊÑèÂë≥„ÅÆ„ÅÇ„Çã„Éé„Éº„ÉâÂêç„ÇíÂèñÂæó
        node_name = get_meaningful_node_name(node, metrics)
        short_name = node_name[:80] + "..." if len(node_name) > 80 else node_name
        
        critical_processes.append({
            'rank': i + 1,
            'name': short_name,
            'duration_sec': duration_sec,
            'percentage': percentage,
            'severity': severity
        })
    
    # === 3. „Çª„É´35: Liquid ClusteringÂàÜÊûêÊÉÖÂ†±„ÅÆÂèñÂæó ===
    liquid_analysis = metrics.get('liquid_clustering_analysis', {})
    extracted_data = liquid_analysis.get('extracted_data', {})
    
    # „ÉÜ„Éº„Éñ„É´ÊÉÖÂ†±
    table_info = extracted_data.get('table_info', {})
    identified_tables = list(table_info.keys())[:5]  # TOP5„ÉÜ„Éº„Éñ„É´
    
    # „Éï„Ç£„É´„Çø„Éº„ÉªJOIN„ÉªGROUP BYÊÉÖÂ†±
    filter_columns = extracted_data.get('filter_columns', [])[:10]
    join_columns = extracted_data.get('join_columns', [])[:10]
    groupby_columns = extracted_data.get('groupby_columns', [])[:10]
    
    # === 4. „Çª„É´43: Áµ±ÂêàÊúÄÈÅ©ÂåñÂá¶ÁêÜ„Åß„ÅÆË©≥Á¥∞„Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûê„ÅÆÂèñÂæó ===
    try:
        detailed_bottleneck = extract_detailed_bottleneck_analysis(metrics)
    except Exception as e:
        print(f"‚ö†Ô∏è Error in detailed bottleneck analysis: {e}")
        detailed_bottleneck = {
            'top_bottleneck_nodes': [],
            'performance_recommendations': []
        }
    
    # === 5. ÂåÖÊã¨ÁöÑ„É¨„Éù„Éº„Éà„ÅÆÁîüÊàê ===
    
    report_lines = []
    
    # „Çø„Ç§„Éà„É´„Å®„Çµ„Éû„É™„Éº
    report_lines.append("# üìä Databricks SQL„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂåÖÊã¨ÂàÜÊûê„É¨„Éù„Éº„Éà")
    report_lines.append(f"**ÁîüÊàêÊó•ÊôÇ**: {timestamp}")
    report_lines.append("")
    
    # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊ¶ÇË¶Å
    report_lines.append("## 1. „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊ¶ÇË¶Å")
    report_lines.append("")
    report_lines.append("### ‰∏ªË¶Å„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊåáÊ®ô")
    report_lines.append("")
    report_lines.append("| ÊåáÊ®ô | ÂÄ§ | Ë©ï‰æ° |")
    report_lines.append("|------|-----|------|")
    report_lines.append(f"| Execution Time | {total_time_sec:.1f}s | {'‚úÖ Good' if total_time_sec < 60 else '‚ö†Ô∏è Needs Improvement'} |")
    report_lines.append(f"| Data Read | {read_gb:.2f}GB | {'‚úÖ Good' if read_gb < 10 else '‚ö†Ô∏è Large Volume'} |")
    report_lines.append(f"| Photon Enabled | {'Yes' if photon_enabled else 'No'} | {'‚úÖ Good' if photon_enabled else '‚ùå Not Enabled'} |")
    report_lines.append(f"| Cache Efficiency | {cache_hit_ratio:.1f}% | {'‚úÖ Good' if cache_hit_ratio > 80 else '‚ö†Ô∏è Needs Improvement'} |")
    report_lines.append(f"| Filter Rate | {data_selectivity:.1f}% | {'‚úÖ Good' if data_selectivity > 50 else '‚ö†Ô∏è Check Filter Conditions'} |")
    report_lines.append(f"| Shuffle Operations | {shuffle_count} times | {'‚úÖ Good' if shuffle_count < 5 else '‚ö†Ô∏è Many'} |")
    report_lines.append(f"| Spill Occurred | {'Yes' if has_spill else 'No'} | {'‚ùå Problem' if has_spill else '‚úÖ Good'} |")
    
    # „Çπ„Ç≠„É•„ÉºÊ§úÂá∫„ÅÆÂà§ÂÆö
    if has_skew:
        skew_status = "Detected & handled by AQE"
        skew_evaluation = "üîß AQE handled"
    elif has_aqe_shuffle_skew_warning:
        skew_status = "Potential skew possibility"
        skew_evaluation = "‚ö†Ô∏è Improvement needed"
    else:
        skew_status = "Not detected"
        skew_evaluation = "‚úÖ Good"
    
    report_lines.append(f"| Skew Detection | {skew_status} | {skew_evaluation} |")
    report_lines.append("")
    
    # ‰∏ªË¶Å„Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûê
    report_lines.append("## 2. ‰∏ªË¶Å„Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûê")
    report_lines.append("")
    
    # PhotonÂàÜÊûê
    photon_status = "ÊúâÂäπ" if photon_enabled else "ÁÑ°Âäπ"
    photon_recommendation = ""
    if not photon_enabled:
        photon_recommendation = " ‚Üí **PhotonÊúâÂäπÂåñ„ÇíÂº∑„ÅèÊé®Â•®**"
    elif photon_utilization < 50:
        photon_recommendation = " ‚Üí **PhotonÂà©Áî®ÁéáÂêë‰∏ä„ÅåÂøÖË¶Å**"
    elif photon_utilization < 80:
        photon_recommendation = " ‚Üí **PhotonË®≠ÂÆö„ÅÆÊúÄÈÅ©Âåñ„ÇíÊé®Â•®**"
    else:
        photon_recommendation = " ‚Üí **ÊúÄÈÅ©ÂåñÊ∏à„Åø**"
    
    report_lines.append("### Photon„Ç®„É≥„Ç∏„É≥")
    report_lines.append(f"- **Áä∂ÊÖã**: {photon_status} (Âà©Áî®Áéá: {photon_utilization:.1f}%){photon_recommendation}")
    report_lines.append("")
    
    # ‰∏¶ÂàóÂ∫¶„Éª„Ç∑„É£„ÉÉ„Éï„É´ÂàÜÊûê
    report_lines.append("### ‰∏¶ÂàóÂ∫¶„Éª„Ç∑„É£„ÉÉ„Éï„É´")
    shuffle_status = "‚ùå „Éú„Éà„É´„Éç„ÉÉ„ÇØ„ÅÇ„Çä" if has_shuffle_bottleneck else "‚úÖ ËâØÂ•Ω"
    parallelism_status = "‚ùå ‰Ωé‰∏¶ÂàóÂ∫¶„ÅÇ„Çä" if has_low_parallelism else "‚úÖ ÈÅ©Âàá"
    
    report_lines.append(f"- **„Ç∑„É£„ÉÉ„Éï„É´Êìç‰Ωú**: {shuffle_count}Âõû ({shuffle_status})")
    report_lines.append(f"- **‰∏¶ÂàóÂ∫¶**: {parallelism_status}")
    if has_low_parallelism:
        report_lines.append(f"  - ‰Ωé‰∏¶ÂàóÂ∫¶„Çπ„ÉÜ„Éº„Ç∏: {low_parallelism_count}ÂÄã")
    report_lines.append("")
    
    # „Çπ„Éî„É´ÂàÜÊûê
    report_lines.append("### „É°„É¢„É™‰ΩøÁî®Áä∂Ê≥Å")
    if has_spill:
        report_lines.append(f"- **„É°„É¢„É™„Çπ„Éî„É´**: ‚ùå Áô∫Áîü‰∏≠ ({spill_gb:.2f}GB)")
        report_lines.append("  - **ÂØæÂøúÂøÖË¶Å**: „ÇØ„É©„Çπ„Çø„ÉºË®≠ÂÆö„ÅÆË¶ãÁõ¥„Åó„ÄÅ„ÇØ„Ç®„É™ÊúÄÈÅ©Âåñ")
    else:
        report_lines.append("- **„É°„É¢„É™„Çπ„Éî„É´**: ‚úÖ „Å™„Åó")
    report_lines.append("")
    
    # TOP5 Processing Time Bottlenecks
    report_lines.append("## 3. TOP5 Processing Time Bottlenecks")
    report_lines.append("")
    
    for process in critical_processes:
        severity_icon = "üî¥" if process['severity'] == "CRITICAL" else "üü†" if process['severity'] == "HIGH" else "üü°"
        report_lines.append(f"### {process['rank']}. {severity_icon} {process['name']}")
        report_lines.append(f"   - **Execution Time**: {process['duration_sec']:.1f}s ({process['percentage']:.1f}% of total)")
        report_lines.append(f"   - **Severity**: {process['severity']}")
        report_lines.append("")
    
    # Liquid Clustering Recommendations
    report_lines.append("## 4. Liquid Clustering Recommendations")
    report_lines.append("")
    
    if identified_tables:
        report_lines.append("### ÂØæË±°„ÉÜ„Éº„Éñ„É´")
        for i, table_name in enumerate(identified_tables, 1):
            # ÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„ÉºÊÉÖÂ†±„ÇíÂèñÂæó
            table_details = table_info.get(table_name, {})
            current_keys = table_details.get('current_clustering_keys', [])
            current_keys_str = ', '.join(current_keys) if current_keys else 'Ë®≠ÂÆö„Å™„Åó'
            
            report_lines.append(f"{i}. `{table_name}`")
            report_lines.append(f"   - ÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº: `{current_keys_str}`")
        report_lines.append("")
    
    if filter_columns or join_columns or groupby_columns:
        report_lines.append("### Êé®Â•®„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº")
        
        if filter_columns:
            report_lines.append("**„Éï„Ç£„É´„Çø„ÉºÊù°‰ª∂„Ç´„É©„É† (È´òÂÑ™ÂÖàÂ∫¶)**:")
            for i, col in enumerate(filter_columns[:5], 1):
                expression = col.get('expression', 'Unknown')
                report_lines.append(f"  {i}. `{expression}`")
            report_lines.append("")
        
        if join_columns:
            report_lines.append("**JOINÊù°‰ª∂„Ç´„É©„É† (‰∏≠ÂÑ™ÂÖàÂ∫¶)**:")
            for i, col in enumerate(join_columns[:5], 1):
                expression = col.get('expression', 'Unknown')
                key_type = col.get('key_type', '')
                report_lines.append(f"  {i}. `{expression}` ({key_type})")
            report_lines.append("")
        
        if groupby_columns:
            report_lines.append("**GROUP BYÊù°‰ª∂„Ç´„É©„É† (‰∏≠ÂÑ™ÂÖàÂ∫¶)**:")
            for i, col in enumerate(groupby_columns[:5], 1):
                expression = col.get('expression', 'Unknown')
                report_lines.append(f"  {i}. `{expression}`")
            report_lines.append("")
    
    # ÂÆüË£ÖSQL‰æã
    if identified_tables:
        report_lines.append("### ÂÆüË£ÖSQL‰æã")
        for table_name in identified_tables[:2]:  # TOP2„ÉÜ„Éº„Éñ„É´„ÅÆ„Åø
            # ÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„ÉºÊÉÖÂ†±„ÇíÂèñÂæó
            table_details = table_info.get(table_name, {})
            current_keys = table_details.get('current_clustering_keys', [])
            current_keys_str = ', '.join(current_keys) if current_keys else 'Ë®≠ÂÆö„Å™„Åó'
            
            report_lines.append(f"```sql")
            report_lines.append(f"-- {table_name}„ÉÜ„Éº„Éñ„É´„Å´Liquid Clustering„ÇíÈÅ©Áî®")
            report_lines.append(f"-- ÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº: {current_keys_str}")
            report_lines.append(f"ALTER TABLE {table_name}")
            report_lines.append(f"CLUSTER BY (column1, column2, column3, column4);")
            report_lines.append(f"```")
            report_lines.append("")
    
    # Optimization recommendation actions
    report_lines.append("## 5. Recommended Optimization Actions")
    report_lines.append("")
    
    # Priority-based recommendations
    high_priority_actions = []
    medium_priority_actions = []
    low_priority_actions = []
    
    # CRITICAL/HIGH priority actions
    if not photon_enabled:
        high_priority_actions.append("**Enable Photon Engine** - Expected up to 50% performance improvement")
    
    if has_spill:
        high_priority_actions.append(f"**Resolve Memory Spill** - Eliminate {spill_gb:.2f}GB spill")
    
    if has_shuffle_bottleneck:
        high_priority_actions.append("**Shuffle Optimization** - JOIN order and REPARTITION application")
    
    # MEDIUM actions
    if photon_enabled and photon_utilization < 80:
        medium_priority_actions.append("**Improve Photon Utilization** - Configuration optimization")
    
    if has_low_parallelism:
        medium_priority_actions.append("**Improve Parallelism** - Cluster configuration review")
    
    if cache_hit_ratio < 50:
        medium_priority_actions.append("**Improve Cache Efficiency** - Data access pattern optimization")
    
    # Liquid Clustering
    if identified_tables:
        medium_priority_actions.append("**Implement Liquid Clustering** - Clustering of key tables")
    
    # LOW actions
    if data_selectivity < 50:
        low_priority_actions.append("**WHERE Clause Optimization** - Improve filter efficiency")
    
    # Action output
    if high_priority_actions:
        report_lines.append("### üö® Urgent Response (HIGH Priority)")
        for i, action in enumerate(high_priority_actions, 1):
            report_lines.append(f"{i}. {action}")
        report_lines.append("")
    
    if medium_priority_actions:
        report_lines.append("### ‚ö†Ô∏è Important Improvements (MEDIUM Priority)")
        for i, action in enumerate(medium_priority_actions, 1):
            report_lines.append(f"{i}. {action}")
        report_lines.append("")
    
    if low_priority_actions:
        report_lines.append("### üìù Long-term Optimization (LOW Priority)")
        for i, action in enumerate(low_priority_actions, 1):
            report_lines.append(f"{i}. {action}")
        report_lines.append("")
    
    # Expected effects
    report_lines.append("## 6. Expected Performance Improvements")
    report_lines.append("")
    
    total_improvement_estimate = 0
    improvement_details = []
    
    if not photon_enabled:
        total_improvement_estimate += 40
        improvement_details.append("- **Photon Activation**: 30-50% execution time reduction")
    
    if has_spill:
        total_improvement_estimate += 25
        improvement_details.append(f"- **Spill Resolution**: 20-30% execution time reduction ({spill_gb:.2f}GB spill reduction)")
    
    if has_shuffle_bottleneck:
        total_improvement_estimate += 20
        improvement_details.append("- **Shuffle Optimization**: 15-25% execution time reduction")
    
    if identified_tables:
        total_improvement_estimate += 15
        improvement_details.append("- **Liquid Clustering**: 10-20% execution time reduction")
    
    # Set upper limit for improvement effects
    total_improvement_estimate = min(total_improvement_estimate, 80)
    
    if improvement_details:
        for detail in improvement_details:
            report_lines.append(detail)
        report_lines.append("")
        report_lines.append(f"**Overall Improvement Estimate**: Up to {total_improvement_estimate}% execution time reduction")
    else:
        report_lines.append("Current performance is relatively good. Fine-tuning optimizations can expect 5-10% improvement.")
    
    # === Detailed analysis based on EXPLAIN + EXPLAIN COST results ===
    if explain_enabled.upper() == 'Y' and (physical_plan or cost_statistics):
        report_lines.append("")
        report_lines.append("## 6. EXPLAIN + EXPLAIN COST Detailed Analysis")
        report_lines.append("")
        
        if physical_plan:
            report_lines.append("### üîç Physical Plan Analysis")
            report_lines.append("")
            
            # Extract important information from Physical Plan
            plan_analysis = []
            if "Exchange" in physical_plan:
                plan_analysis.append("- **Shuffle Operation Detected**: Potential data transfer bottleneck")
            if "BroadcastExchange" in physical_plan:
                plan_analysis.append("- **BROADCAST JOIN Applied**: Efficient distribution of small tables")
            if "HashAggregate" in physical_plan:
                plan_analysis.append("- **Hash Aggregation Processing**: Memory efficiency optimization is important")
            if "FileScan" in physical_plan:
                plan_analysis.append("- **File Scan Operation**: Check I/O efficiency and filter pushdown")
            if "SortMergeJoin" in physical_plan:
                plan_analysis.append("- **Sort Merge JOIN**: Large table joins, consider BROADCAST application")
            
            if plan_analysis:
                for analysis in plan_analysis:
                    report_lines.append(analysis)
            else:
                report_lines.append("- Physical Plan detailed information is available")
            report_lines.append("")
        
        if photon_explanation:
            report_lines.append("### üöÄ Photon Explanation Analysis")
            report_lines.append("")
            
            photon_analysis = []
            if "photon" in photon_explanation.lower():
                photon_analysis.append("- **Photon Processing Information**: Vectorized processing optimization details")
            if "unsupported" in photon_explanation.lower():
                photon_analysis.append("- **Unsupported Function Detected**: Opportunity to improve Photon utilization")
            if "compiled" in photon_explanation.lower():
                photon_analysis.append("- **Compilation Processing**: Runtime optimization application status")
            
            if photon_analysis:
                for analysis in photon_analysis:
                    report_lines.append(analysis)
            else:
                report_lines.append("- Photon execution detailed information is available")
            report_lines.append("")
        
        if cost_statistics:
            report_lines.append("### üí∞ EXPLAIN COST Statistical Analysis")
            report_lines.append("")
            
            # Extract important information from EXPLAIN COST statistics
            cost_analysis = []
            if "„Çµ„Ç§„Ç∫ÊÉÖÂ†±" in cost_statistics:
                cost_analysis.append("- **Table Size Statistics**: Improved BROADCAST judgment accuracy with accurate size information")
            if "Ë°åÊï∞ÊÉÖÂ†±" in cost_statistics:
                cost_analysis.append("- **Row Count Statistics**: Partition number optimization and memory usage prediction")
            if "ÈÅ∏ÊäûÁéáÊÉÖÂ†±" in cost_statistics:
                cost_analysis.append("- **Selectivity Statistics**: Filter efficiency optimization and WHERE condition order adjustment")
            if "„Ç≥„Çπ„ÉàÊÉÖÂ†±" in cost_statistics:
                cost_analysis.append("- **Cost Estimation**: JOIN strategy and access path selection optimization")
            if "„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥ÊÉÖÂ†±" in cost_statistics:
                cost_analysis.append("- **Partition Statistics**: Data distribution optimization and skew countermeasures")
            
            if cost_analysis:
                for analysis in cost_analysis:
                    report_lines.append(analysis)
                report_lines.append("")
                report_lines.append("**Benefits of Statistics-Based Optimization**:")
                report_lines.append("- Optimization based on actual statistics rather than guesswork")
                report_lines.append("- Proactive bottleneck prediction and spill avoidance")
                report_lines.append("- Optimal strategy selection through accurate cost estimation")
            else:
                report_lines.append("- EXPLAIN COST statistical information is available")
            report_lines.append("")
    elif explain_enabled.upper() == 'Y':
        report_lines.append("")
        report_lines.append("## 6. EXPLAIN Analysis")
        report_lines.append("")
        report_lines.append("‚ö†Ô∏è EXPLAIN„ÉªEXPLAIN COST result files not found")
        report_lines.append("Statistics-based detailed analysis requires prior EXPLAIN execution")
        report_lines.append("")
    
    report_lines.append("")
    report_lines.append("---")
    report_lines.append(f"*Report generated: {timestamp} | Analysis engine: Databricks SQL Profiler + EXPLAIN integration*")
    
    print("‚úÖ Comprehensive performance analysis report (EXPLAIN+EXPLAIN COST integration) completed")
    
    return "\n".join(report_lines)


def _call_databricks_llm(prompt: str) -> str:
    """Call Databricks Model Serving API"""
    try:
        # Databricks„Éà„Éº„ÇØ„É≥„ÅÆÂèñÂæó
        try:
            token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
        except Exception:
            token = os.environ.get('DATABRICKS_TOKEN')
            if not token:
                return "‚ùå Failed to obtain Databricks token. Please set the environment variable DATABRICKS_TOKEN."
        
        # „ÉØ„Éº„ÇØ„Çπ„Éö„Éº„ÇπURL„ÅÆÂèñÂæó
        try:
            workspace_url = spark.conf.get("spark.databricks.workspaceUrl")
        except Exception:
            workspace_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get("browserHostName").get()
        
        config = LLM_CONFIG["databricks"]
        endpoint_url = f"https://{workspace_url}/serving-endpoints/{config['endpoint_name']}/invocations"
        
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        # Êã°ÂºµÊÄùËÄÉ„É¢„Éº„Éâ„ÅåÊúâÂäπ„Å™Â†¥Âêà„ÅØËøΩÂä†
        if config.get("thinking_enabled", False):
            payload["thinking"] = {
                "type": "enabled",
                "budget_tokens": config.get("thinking_budget_tokens", 65536)
            }
        
        # „É™„Éà„É©„Ç§Ê©üËÉΩÔºàSQLÊúÄÈÅ©ÂåñÁî®„Å´Â¢óÂº∑Ôºâ
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    print(f"üîÑ Retrying... (attempt {attempt + 1}/{max_retries})")
                
                response = requests.post(endpoint_url, headers=headers, json=payload, timeout=300)
                
                if response.status_code == 200:
                    result = response.json()
                    analysis_text = result.get('choices', [{}])[0].get('message', {}).get('content', '')
                    print("‚úÖ Bottleneck analysis completed")
                    return analysis_text
                else:
                    error_msg = f"API Error: Status code {response.status_code}"
                    if response.status_code == 400:
                        # 400„Ç®„É©„Éº„ÅÆÂ†¥Âêà„ÅØË©≥Á¥∞„Å™Ëß£Ê±∫Á≠ñ„ÇíÊèê‰æõ
                        error_detail = response.text
                        if "maximum tokens" in error_detail.lower():
                            if attempt == max_retries - 1:
                                detailed_error = f"""‚ùå {error_msg}

üîß Token limit error solutions:
1. Reduce LLM_CONFIG["databricks"]["max_tokens"] to 65536 (64K)
2. Retry with simpler query
3. Perform manual SQL optimization
4. Split query and optimize incrementally

üí° Recommended settings:
LLM_CONFIG["databricks"]["max_tokens"] = 65536
LLM_CONFIG["databricks"]["thinking_budget_tokens"] = 32768

Detailed error: {error_detail}"""
                                print(detailed_error)
                                return detailed_error
                            else:
                                print(f"‚ö†Ô∏è {error_msg} (Token limit) - Retrying...")
                                continue
                    
                    if attempt == max_retries - 1:
                        print(f"‚ùå {error_msg}\nResponse: {response.text}")
                        return f"{error_msg}\nResponse: {response.text}"
                    else:
                        print(f"‚ö†Ô∏è {error_msg} - Retrying...")
                        continue
                        
            except requests.exceptions.Timeout:
                if attempt == max_retries - 1:
                    timeout_msg = f"""‚è∞ Timeout Error: Databricks endpoint response did not complete within 300 seconds.

üîß Solutions:
1. Check LLM endpoint operational status
2. Reduce prompt size
3. Use a higher performance model
4. Execute SQL optimization manually

üí° Recommended Actions:
- Check query complexity
- Scale up Databricks Model Serving endpoint
- Test execution with simpler queries"""
                    print(f"‚ùå {timeout_msg}")
                    return timeout_msg
                else:
                    print(f"‚è∞ Timeout occurred (300 seconds) - Retrying... (attempt {attempt + 1}/{max_retries})")
                    continue
                    
    except Exception as e:
        return f"Databricks API call error: {str(e)}"

def _call_openai_llm(prompt: str) -> str:
    """Call OpenAI API"""
    try:
        config = LLM_CONFIG["openai"]
        api_key = config["api_key"] or os.environ.get('OPENAI_API_KEY')
        
        if not api_key:
            return "‚ùå OpenAI API key is not configured. Please set LLM_CONFIG['openai']['api_key'] or environment variable OPENAI_API_KEY."
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": config["model"],
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        response = requests.post("https://api.openai.com/v1/chat/completions", 
                               headers=headers, json=payload, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result['choices'][0]['message']['content']
            print("‚úÖ OpenAI analysis completed")
            return analysis_text
        else:
            return f"OpenAI API Error: Status code {response.status_code}\n{response.text}"
            
    except Exception as e:
        return f"OpenAI API call error: {str(e)}"

def _call_azure_openai_llm(prompt: str) -> str:
    """Call Azure OpenAI API"""
    try:
        config = LLM_CONFIG["azure_openai"]
        api_key = config["api_key"] or os.environ.get('AZURE_OPENAI_API_KEY')
        
        if not api_key or not config["endpoint"] or not config["deployment_name"]:
            return "‚ùå Azure OpenAI configuration is incomplete. Please set api_key, endpoint, and deployment_name."
        
        endpoint_url = f"{config['endpoint']}/openai/deployments/{config['deployment_name']}/chat/completions?api-version={config['api_version']}"
        
        headers = {
            "api-key": api_key,
            "Content-Type": "application/json"
        }
        
        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        response = requests.post(endpoint_url, headers=headers, json=payload, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result['choices'][0]['message']['content']
            print("‚úÖ Azure OpenAI analysis completed")
            return analysis_text
        else:
            return f"Azure OpenAI API Error: Status code {response.status_code}\n{response.text}"
            
    except Exception as e:
        return f"Azure OpenAI API call error: {str(e)}"

def _call_anthropic_llm(prompt: str) -> str:
    """Call Anthropic API"""
    try:
        config = LLM_CONFIG["anthropic"]
        api_key = config["api_key"] or os.environ.get('ANTHROPIC_API_KEY')
        
        if not api_key:
            return "‚ùå Anthropic API„Ç≠„Éº„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇLLM_CONFIG['anthropic']['api_key']„Åæ„Åü„ÅØÁí∞Â¢ÉÂ§âÊï∞ANTHROPIC_API_KEY„ÇíË®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
        
        headers = {
            "x-api-key": api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        payload = {
            "model": config["model"],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"],
            "messages": [{"role": "user", "content": prompt}]
        }
        
        response = requests.post("https://api.anthropic.com/v1/messages", 
                               headers=headers, json=payload, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result['content'][0]['text']
            print("‚úÖ Anthropic analysis completed")
            return analysis_text
        else:
            return f"Anthropic API Error: Status code {response.status_code}\n{response.text}"
            
    except Exception as e:
        return f"Anthropic API call error: {str(e)}"

print("‚úÖ Function definition completed: analyze_bottlenecks_with_llm")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìã LLM Bottleneck Analysis Execution Preparation
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Verification and display of configured LLM provider
# MAGIC - Analysis start preparation and message display
# MAGIC - Stability improvement through prompt optimization

# COMMAND ----------

# LLM„Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûêÂÆüË°å„ÅÆÊ∫ñÂÇô
provider = LLM_CONFIG["provider"]

print(f"\nü§ñ „ÄêStarting SQL bottleneck analysis with {provider.upper()} LLM„Äë")
print("=" * 80)

if provider == "databricks":
    endpoint = LLM_CONFIG["databricks"]["endpoint_name"]
    print(f"üîó Databricks Model Serving endpoint: {endpoint}")
    print("‚ö†Ô∏è  Model Serving endpoint must be operational")
elif provider == "openai":
    model = LLM_CONFIG["openai"]["model"]
    print(f"üîó OpenAI model: {model}")
    print("‚ö†Ô∏è  OpenAI API key is required")
elif provider == "azure_openai":
    deployment = LLM_CONFIG["azure_openai"]["deployment_name"]
    print(f"ü§ñ Starting Azure OpenAI ({deployment}) bottleneck analysis...")
    print("‚ö†Ô∏è  Azure OpenAI API key and endpoint are required")
elif provider == "anthropic":
    model = LLM_CONFIG["anthropic"]["model"]
    print(f"ü§ñ Starting Anthropic ({model}) bottleneck analysis...")
    print("‚ö†Ô∏è  Anthropic API key is required")

print("üìù Simplifying analysis prompts to reduce timeout risk...")
print()

# Check if extracted_metrics variable is defined
try:
    extracted_metrics
    print("‚úÖ extracted_metrics variable confirmed")
    analysis_result = analyze_bottlenecks_with_llm(extracted_metrics)
except NameError:
    print("‚ùå extracted_metrics variable is not defined")
    print("‚ö†Ô∏è Please run Cell 12 (Performance metrics extraction) first")
    print("üìã Correct execution order: Cell 11 ‚Üí Cell 12 ‚Üí Cell 15")
    print("üîÑ Setting default analysis results")
    analysis_result = """
ü§ñ LLM„Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûêÁµêÊûú

‚ùå ÂàÜÊûê„Å´ÂøÖË¶Å„Å™„É°„Éà„É™„ÇØ„Çπ„Éá„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ

üìã Ëß£Ê±∫ÊñπÊ≥ï:
1. „Çª„É´11„ÅßJSON„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„ÇÄ
2. „Çª„É´12„Åß„É°„Éà„É™„ÇØ„Çπ„ÇíÊäΩÂá∫„Åô„Çã
3. „Åì„ÅÆ„Çª„É´Ôºà„Çª„É´15Ôºâ„ÇíÂÜçÂÆüË°å„Åô„Çã

‚ö†Ô∏è ÂÖà„Å´„É°„Éà„É™„ÇØ„ÇπÊäΩÂá∫„ÇíÂÆå‰∫Ü„Åó„Å¶„Åã„ÇâÂàÜÊûê„ÇíÂÆüË°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
"""
except Exception as e:
    print(f"‚ùå Error occurred during LLM analysis: {str(e)}")
    analysis_result = f"LLM analysis error: {str(e)}"

# COMMAND ----------

# MAGIC %md
# MAGIC # üöÄ Query Profile Analysis Section
# MAGIC
# MAGIC **Main analysis processing starts from here**
# MAGIC
# MAGIC üìã **Execution Steps:**
# MAGIC 1. Execute all cells in the üîß Configuration & Setup section above
# MAGIC 2. Run the following cells in order to perform analysis
# MAGIC 3. If errors occur, re-execute from the configuration section
# MAGIC
# MAGIC ‚ö†Ô∏è **Important Notes:**
# MAGIC - Execute in order: üîß Configuration & Setup ‚Üí üöÄ Main Processing ‚Üí üîß SQL Optimization sections
# MAGIC - File path configuration must be done in the first cell
# MAGIC - Verify LLM endpoint configuration

# COMMAND ----------

# MAGIC %md
# MAGIC ## üöÄ SQL Profiler JSON File Loading Execution
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - JSON file loading from configured file path
# MAGIC - File size and basic information display
# MAGIC - Error handling and processing stop control

# COMMAND ----------

print("=" * 80)
print("üöÄ Databricks SQL Profiler Analysis Tool")
print("=" * 80)
print(f"üìÅ Target analysis file: {JSON_FILE_PATH}")
print()

# File existence check
import os
if not os.path.exists(JSON_FILE_PATH):
    print("‚ùå File not found:")
    print(f"   Specified path: {JSON_FILE_PATH}")
    print()
    print("üí° File path configuration hints:")
    print("   1. Set the correct path for JSON_FILE_PATH variable in Cell 2")
    print("   2. Available option examples:")
    print("      - /Volumes/main/base/mitsuhiro_vol/pre_tuning_plan_file.json")
    print("      - /Volumes/main/base/mitsuhiro_vol/nophoton.json")
    print("      - /Volumes/main/base/mitsuhiro_vol/POC1.json")
    print("   3. If file is in DBFS FileStore:")
    print("      - /FileStore/shared_uploads/your_username/filename.json")
    print("‚ö†Ô∏è Stopping processing.")
    raise RuntimeError(f"Specified file not found: {JSON_FILE_PATH}")

# Load SQL profiler JSON file
profiler_data = load_profiler_json(JSON_FILE_PATH)
if not profiler_data:
    print("‚ùå Failed to load JSON file. Please check the file format.")
    print("‚ö†Ô∏è Stopping processing.")
    # dbutils.notebook.exit("File loading failed")  # Commented out for safety
    raise RuntimeError("Failed to load JSON file.")

print(f"‚úÖ Data loading completed")
print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìä Performance Metrics Extraction and Overview Display
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Metrics extraction from profiler data
# MAGIC - Query basic information display
# MAGIC - Overall performance indicator calculation and display
# MAGIC - Liquid Clustering analysis result display

# COMMAND ----------

# üìä Performance metrics extraction
extracted_metrics = extract_performance_metrics(profiler_data)
print("‚úÖ Performance metrics extracted")

# Display extracted metrics overview
print("\n" + "=" * 50)
print("üìà Extracted Metrics Overview")
print("=" * 50)

query_info = extracted_metrics['query_info']
overall_metrics = extracted_metrics['overall_metrics']
bottleneck_indicators = extracted_metrics['bottleneck_indicators']

print(f"üÜî Query ID: {query_info['query_id']}")
print(f"üìä Status: {query_info['status']}")
print(f"üë§ Execution User: {query_info['user']}")
print(f"‚è±Ô∏è Execution Time: {overall_metrics['total_time_ms']:,} ms ({overall_metrics['total_time_ms']/1000:.2f} sec)")
print(f"üíæ Data Read: {overall_metrics['read_bytes']/1024/1024/1024:.2f} GB")
print(f"üìà Output Rows: {overall_metrics['rows_produced_count']:,} rows")
print(f"üìâ Read Rows: {overall_metrics['rows_read_count']:,} rows")
print(f"üéØ Filter Rate: {bottleneck_indicators.get('data_selectivity', 0):.4f} ({bottleneck_indicators.get('data_selectivity', 0)*100:.2f}%)")
print(f"üîß Stage Count: {len(extracted_metrics['stage_metrics'])}")
print(f"üèóÔ∏è Node Count: {len(extracted_metrics['node_metrics'])}")

# Display Liquid Clustering analysis results
liquid_analysis = extracted_metrics['liquid_clustering_analysis']
liquid_summary = liquid_analysis.get('summary', {})
print(f"üóÇÔ∏è Liquid Clustering Target Tables: {liquid_summary.get('tables_identified', 0)}")
print(f"üìä High Impact Tables: {liquid_summary.get('high_impact_tables', 0)}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîç Bottleneck Indicator Details
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Photon engine usage and performance analysis
# MAGIC - Shuffle operations and parallelism issue detection
# MAGIC - Detailed display of various performance indicators

# COMMAND ----------

# üìã Detailed bottleneck indicator display
print("\n" + "=" * 50)
print("üîç Bottleneck Indicator Details")
print("=" * 50)

# Photon-related indicators
photon_enabled = overall_metrics.get('photon_enabled', False)
photon_utilization_ratio = overall_metrics.get('photon_utilization_ratio', 0)
photon_utilization = min(photon_utilization_ratio * 100, 100.0)  # Limit to max 100%
photon_emoji = "‚úÖ" if photon_enabled and photon_utilization > 80 else "‚ö†Ô∏è" if photon_enabled else "‚ùå"

# Detailed information about utilization rate
if photon_enabled:
    photon_total_ms = overall_metrics.get('photon_total_time_ms', 0)
    task_total_ms = overall_metrics.get('task_total_time_ms', 0)
    print(f"{photon_emoji} Photon Engine: Enabled (Utilization: {photon_utilization:.1f}%)")
    print(f"   üìä Photon Execution Time: {photon_total_ms:,} ms | Total Task Time: {task_total_ms:,} ms")
else:
    print(f"{photon_emoji} Photon Engine: Disabled")

# Parallelism and shuffle-related indicators
shuffle_count = bottleneck_indicators.get('shuffle_operations_count', 0)
has_shuffle_bottleneck = bottleneck_indicators.get('has_shuffle_bottleneck', False)
has_low_parallelism = bottleneck_indicators.get('has_low_parallelism', False)
low_parallelism_count = bottleneck_indicators.get('low_parallelism_stages_count', 0)

shuffle_emoji = "üö®" if has_shuffle_bottleneck else "‚ö†Ô∏è" if shuffle_count > 5 else "‚úÖ"
print(f"{shuffle_emoji} Shuffle Operations: {shuffle_count} times ({'Bottleneck detected' if has_shuffle_bottleneck else 'Normal'})")

parallelism_emoji = "üö®" if has_low_parallelism else "‚úÖ"
print(f"{parallelism_emoji} Parallelism: {'Issues detected' if has_low_parallelism else 'Appropriate'} (Low parallelism stages: {low_parallelism_count})")

print()
print("üìä Other Indicators:")

for key, value in bottleneck_indicators.items():
    # Skip newly added indicators as they are already displayed above
    if key in ['shuffle_operations_count', 'has_shuffle_bottleneck', 'has_low_parallelism', 
               'low_parallelism_stages_count', 'total_shuffle_time_ms', 'shuffle_time_ratio',
               'slowest_shuffle_duration_ms', 'slowest_shuffle_node', 'low_parallelism_details',
               'average_low_parallelism']:
        continue
        
    if 'ratio' in key:
        emoji = "üìä" if value < 0.1 else "‚ö†Ô∏è" if value < 0.3 else "üö®"
        print(f"{emoji} {key}: {value:.3f} ({value*100:.1f}%)")
    elif 'bytes' in key and key != 'has_spill':
        if value > 0:
            emoji = "üíæ" if value < 1024*1024*1024 else "‚ö†Ô∏è"  # Normal if under 1GB, caution if over
            print(f"{emoji} {key}: {value:,} bytes ({value/1024/1024:.2f} MB)")
    elif key == 'has_spill':
        emoji = "‚ùå" if not value else "‚ö†Ô∏è"
        print(f"{emoji} {key}: {'Yes' if value else 'No'}")
    elif 'duration' in key:
        emoji = "‚è±Ô∏è"
        print(f"{emoji} {key}: {value:,} ms ({value/1000:.2f} sec)")
    else:
        emoji = "‚ÑπÔ∏è"
        print(f"{emoji} {key}: {value}")

print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## üíæ Metrics Storage and Time Consumption Analysis
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Save extracted metrics in JSON format
# MAGIC - Convert set types to list types
# MAGIC - Detailed analysis of top 10 most time-consuming processes
# MAGIC - Specific metrics-based spill detection and AQE-based skew analysis
# MAGIC
# MAGIC üíø **Spill Detection Logic**:
# MAGIC - Target metric: `"Sink - Num bytes spilled to disk due to memory pressure"`
# MAGIC - Judgment condition: Spill detected when above metric value > 0
# MAGIC - Search targets: detailed_metrics ‚Üí raw_metrics ‚Üí key_metrics in order
# MAGIC
# MAGIC üéØ **Skew Detection Logic**:
# MAGIC - `AQEShuffleRead - Number of skewed partitions`: AQE-based skew detection
# MAGIC - Judgment condition: Skew detected when metric value > 0
# MAGIC - Importance: Judgment based on detected value
# MAGIC - Statistics-based judgment is deprecated (AQE-based judgment recommended)
# MAGIC
# MAGIC üí° **Debug Mode**: To display detailed spill/skew judgment basis
# MAGIC ```python
# MAGIC import os
# MAGIC os.environ['DEBUG_SPILL_ANALYSIS'] = 'true'   # Detailed display of specific metrics spill judgment
# MAGIC os.environ['DEBUG_SKEW_ANALYSIS'] = 'true'    # Detailed display of AQE-based skew judgment
# MAGIC ```

# COMMAND ----------

# üêõ Debug mode configuration (optional)
# 
# **Execute only when you want to display detailed spill/skew judgment basis**
# 
# üìã Configuration details:
# - DEBUG_SPILL_ANALYSIS=true: Display detailed basis for specific metrics spill judgment
# - DEBUG_SKEW_ANALYSIS=true: Display detailed basis for AQE-based skew judgment
# 
# üíø Spill debug display content:
# - Target metric: "Sink - Num bytes spilled to disk due to memory pressure"
# - Search results in each data source (detailed_metrics, raw_metrics, key_metrics)
# - Values and judgment results when metrics are found
# - List of other spill-related metrics (reference information)
# 
# üéØ Skew debug display content:
# - AQEShuffleRead - Number of skewed partitions metric value
# - Judgment basis for AQE-based skew detection
# - Number of detected skews and importance level
# - Statistics-based judgment is deprecated (AQE-based judgment recommended)

import os

# Uncomment to enable debug display for specific metrics spill analysis
# os.environ['DEBUG_SPILL_ANALYSIS'] = 'true'

# Uncomment to enable debug display for AQE-based skew analysis  
# os.environ['DEBUG_SKEW_ANALYSIS'] = 'true'

print("üêõ Debug mode configuration:")
print(f"   Specific metrics spill analysis debug: {os.environ.get('DEBUG_SPILL_ANALYSIS', 'false')}")
print(f"   AQE-based skew analysis debug: {os.environ.get('DEBUG_SKEW_ANALYSIS', 'false')}")
print("   ‚Äª Setting to 'true' displays detailed judgment basis information")
print()
print("üíø Specific metrics spill detection criteria:")
print('   üéØ Target: "Sink - Num bytes spilled to disk due to memory pressure"')
print("   ‚úÖ Judgment condition: Value > 0")
print()
print("üéØ AQE-based skew detection criteria:")
print("   üìä AQEShuffleRead - Number of skewed partitions > 0")
print("   üìä Judgment condition: Metric value > 0")
print("   üìä Importance: Based on detected value")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üêå Top 10 Most Time-Consuming Processes
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Saving extracted metrics in JSON format
# MAGIC - Converting set types to list types
# MAGIC - Detailed analysis of the top 10 most time-consuming processes
# MAGIC - Spill detection and data skew analysis
# MAGIC - Spark stage execution analysis

# COMMAND ----------

# üíæ ÊäΩÂá∫„Åó„Åü„É°„Éà„É™„ÇØ„Çπ„ÅÆJSON„Éï„Ç°„Ç§„É´‰øùÂ≠ò„ÅØÈô§Â§ñÔºà‰∏çË¶ÅÔºâ
def format_thinking_response(response) -> str:
    """
    thinking_enabled: True„ÅÆÂ†¥Âêà„ÅÆ„É¨„Çπ„Éù„É≥„Çπ„Çí‰∫∫Èñì„Å´Ë™≠„Åø„ÇÑ„Åô„ÅÑÂΩ¢Âºè„Å´Â§âÊèõ
    ÊÄùËÄÉÈÅéÁ®ãÔºàthinkingÔºâ„Å®„Ç∑„Ç∞„Éç„ÉÅ„É£ÔºàsignatureÔºâÁ≠â„ÅÆ‰∏çË¶Å„Å™ÊÉÖÂ†±„ÅØÈô§Â§ñ„Åó„ÄÅÊúÄÁµÇÁöÑ„Å™ÁµêË´ñ„ÅÆ„Åø„ÇíË°®Á§∫
    JSONÊßãÈÄ†„ÇÑ‰∏çÈÅ©Âàá„Å™ÊñáÂ≠óÂàó„ÅÆÈú≤Âá∫„ÇíÈò≤Ê≠¢
    """
    import re  # re„É¢„Ç∏„É•„Éº„É´„ÅÆ„Ç§„É≥„Éù„Éº„Éà„ÇíËøΩÂä†
    
    if not isinstance(response, list):
        # „É™„Çπ„Éà„Åß„Å™„ÅÑÂ†¥Âêà„ÅØÊñáÂ≠óÂàó„Å®„Åó„Å¶Âá¶ÁêÜ„Åó„ÄÅ„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó
        cleaned_text = clean_response_text(str(response))
        return cleaned_text
    
    # Èô§Â§ñ„Åô„Åπ„Åç„Ç≠„Éº„ÅÆ„É™„Çπ„ÉàÔºàÊã°ÂºµÔºâ
    excluded_keys = {
        'thinking', 'signature', 'metadata', 'id', 'request_id', 
        'timestamp', 'uuid', 'reasoning', 'type', 'model'
    }
    
    formatted_parts = []
    
    for item in response:
        if isinstance(item, dict):
            # ÊúÄ„ÇÇÈÅ©Âàá„Å™„ÉÜ„Ç≠„Çπ„Éà„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÇíÊäΩÂá∫
            content = extract_best_content_from_dict(item, excluded_keys)
            if content:
                cleaned_content = clean_response_text(content)
                if is_valid_content(cleaned_content):
                    formatted_parts.append(cleaned_content)
        else:
            # ËæûÊõ∏„Åß„Å™„ÅÑÂ†¥Âêà„ÇÇ„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó
            cleaned_content = clean_response_text(str(item))
            if is_valid_content(cleaned_content):
                formatted_parts.append(cleaned_content)
    
    final_result = '\n'.join(formatted_parts)
    
    # ÊúÄÁµÇÁöÑ„Å™ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØ„Å®„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó
    final_result = final_quality_check(final_result)
    
    return final_result

def extract_best_content_from_dict(item_dict, excluded_keys):
    """Extract optimal content from dictionary"""
    # ÂÑ™ÂÖàÈ†Ü‰Ωç: text > summary_text > content > message > „Åù„ÅÆ‰ªñ
    priority_keys = ['text', 'summary_text', 'content', 'message', 'response']
    
    for key in priority_keys:
        if key in item_dict and item_dict[key]:
            content = str(item_dict[key])
            # JSONÊßãÈÄ†„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Å™„ÅÑ„Åã„ÉÅ„Çß„ÉÉ„ÇØ
            if not looks_like_json_structure(content):
                return content
    
    # ÂÑ™ÂÖà„Ç≠„Éº„ÅßË¶ã„Å§„Åã„Çâ„Å™„ÅÑÂ†¥Âêà„ÄÅ‰ªñ„ÅÆ„Ç≠„Éº„Çí„ÉÅ„Çß„ÉÉ„ÇØÔºàÈô§Â§ñ„Ç≠„Éº‰ª•Â§ñÔºâ
    for key, value in item_dict.items():
        if key not in excluded_keys and value and isinstance(value, str):
            if not looks_like_json_structure(value):
                return value
    
    return None

def looks_like_json_structure(text):
    """Check if text contains JSON structure"""
    json_indicators = [
        "{'type':", '[{\'type\':', '{"type":', '[{"type":',
        "'text':", '"text":', "'summary_text':", '"summary_text":',
        'reasoning', 'metadata', 'signature'
    ]
    text_lower = text.lower()
    return any(indicator.lower() in text_lower for indicator in json_indicators)

def clean_response_text(text):
    """Clean up response text"""
    import re
    
    if not text or not isinstance(text, str):
        return ""
    
    # ÊîπË°å„Ç≥„Éº„Éâ„ÅÆÊ≠£Ë¶èÂåñ
    text = text.replace('\\n', '\n').replace('\\t', '\t')
    
    # JSONÊßãÈÄ†„ÅÆÈô§Âéª
    
    # ÂÖ∏ÂûãÁöÑ„Å™JSONÊßãÈÄ†„Éë„Çø„Éº„É≥„ÇíÈô§Âéª
    json_patterns = [
        r"'type':\s*'[^']*'",
        r'"type":\s*"[^"]*"',
        r"\[?\{'type':[^}]*\}[,\]]?",
        r'\[?\{"type":[^}]*\}[,\]]?',
        r"'reasoning':\s*\[[^\]]*\]",
        r'"reasoning":\s*\[[^\]]*\]',
        r"'signature':\s*'[A-Za-z0-9+/=]{50,}'",
        r'"signature":\s*"[A-Za-z0-9+/=]{50,}"'
    ]
    
    for pattern in json_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)
    
    # ‰∏çÂÆåÂÖ®„Å™JSON„Éñ„É©„Ç±„ÉÉ„Éà„ÅÆÈô§Âéª
    text = re.sub(r'^\s*[\[\{]', '', text)  # ÂÖàÈ†≠„ÅÆ [ „ÇÑ {
    text = re.sub(r'[\]\}]\s*$', '', text)  # Êú´Â∞æ„ÅÆ ] „ÇÑ }
    text = re.sub(r'^\s*[,;]\s*', '', text)  # ÂÖàÈ†≠„ÅÆ„Ç´„É≥„Éû„ÇÑ„Çª„Éü„Ç≥„É≠„É≥
    
    # ÈÄ£Á∂ö„Åô„ÇãÁ©∫ÁôΩ„ÉªÊîπË°å„ÅÆÊ≠£Ë¶èÂåñ
    text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)  # 3„Å§‰ª•‰∏ä„ÅÆÈÄ£Á∂öÊîπË°å„Çí2„Å§„Å´
    text = re.sub(r'[ \t]+', ' ', text)  # ÈÄ£Á∂ö„Åô„Çã„Çπ„Éö„Éº„Çπ„Éª„Çø„Éñ„Çí1„Å§„Å´
    
    # ÂâçÂæå„ÅÆÁ©∫ÁôΩ„ÇíÈô§Âéª
    text = text.strip()
    
    return text

def is_valid_content(text):
    """Check if content is valid"""
    import re
    
    if not text or len(text.strip()) < 10:
        return False
    
    # ÁÑ°Âäπ„Å™„Éë„Çø„Éº„É≥„Çí„ÉÅ„Çß„ÉÉ„ÇØ
    invalid_patterns = [
        r'^[{\[\'"]*$',  # JSONÊßãÈÄ†„ÅÆ„Åø
        r'^[,;:\s]*$',   # Âå∫Âàá„ÇäÊñáÂ≠ó„ÅÆ„Åø
        r'^\s*reasoning\s*$',  # reasoning„ÅÆ„Åø
        r'^\s*metadata\s*$',   # metadata„ÅÆ„Åø
        r'^[A-Za-z0-9+/=]{50,}$',  # Base64„Å£„ÅΩ„ÅÑÈï∑„ÅÑÊñáÂ≠óÂàó
    ]
    
    for pattern in invalid_patterns:
        if re.match(pattern, text.strip(), re.IGNORECASE):
            return False
    
    return True

def final_quality_check(text):
    """Final quality check and cleanup"""
    import re  # re„É¢„Ç∏„É•„Éº„É´„ÅÆ„Ç§„É≥„Éù„Éº„Éà„ÇíËøΩÂä†
    
    if not text:
        return "ÂàÜÊûêÁµêÊûú„ÅÆÊäΩÂá∫„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ"
    
    # Ë®ÄË™û„ÅÆ‰∏ÄË≤´ÊÄß„ÉÅ„Çß„ÉÉ„ÇØÔºàÂÆâÂÖ®„Å™Â§âÊï∞„Ç¢„ÇØ„Çª„ÇπÔºâ
    try:
        language = globals().get('OUTPUT_LANGUAGE', 'ja')  # „Éá„Éï„Ç©„É´„Éà„ÅØÊó•Êú¨Ë™û
    except:
        language = 'ja'
    
    if language == 'ja':
        text = ensure_japanese_consistency(text)
    elif language == 'en':
        text = ensure_english_consistency(text)
    
    # ÊúÄÂ∞èÈôê„ÅÆÈï∑„Åï„ÉÅ„Çß„ÉÉ„ÇØ
    if len(text.strip()) < 20:
        if language == 'ja':
            return "ÂàÜÊûêÁµêÊûú„Åå‰∏çÂÆåÂÖ®„Åß„Åô„ÄÇË©≥Á¥∞„Å™ÂàÜÊûê„ÇíÂÆüË°å‰∏≠„Åß„Åô„ÄÇ"
        else:
            return "Analysis result is incomplete. Detailed analysis in progress."
    
    return text

def ensure_japanese_consistency(text):
    """Ensure Japanese text consistency"""
    import re
    
    # Êòé„Çâ„Åã„Å´Á†¥Êêç„Åó„Å¶„ÅÑ„ÇãÈÉ®ÂàÜ„ÇíÈô§Âéª
    # ‰æã: "Ê≠£caientify="predicate_liquid_referencet1" „ÅÆ„Çà„ÅÜ„Å™Á†¥ÊêçÊñáÂ≠óÂàó
    text = re.sub(r'[a-zA-Z0-9_="\']{20,}', '', text)
    
    # ‰∏çÂÆåÂÖ®„Å™„Éû„Éº„ÇØ„ÉÄ„Ç¶„É≥„ÅÆ‰øÆÊ≠£
    text = re.sub(r'#\s*[^#\n]*["\'>]+[^#\n]*', '', text)  # Á†¥Êêç„Åó„Åü„Éû„Éº„ÇØ„ÉÄ„Ç¶„É≥„Éò„ÉÉ„ÉÄ„Éº
    
    # ÊÑèÂë≥‰∏çÊòé„Å™ÊñáÂ≠óÂàó„Éë„Çø„Éº„É≥„ÅÆÈô§ÂéªÔºàÊã°ÂºµÔºâ
    nonsense_patterns = [
        r'addressing_sales_column\d*',
        r'predicate_liquid_reference[a-zA-Z0-9]*',
        r'bottlenars\s+effect',
        r'ÂÆüË£ÖÈùû‰øùÂ≠òÂú®',
        r'Ë£èÁ•®„ÅÆend_by',
        r'riconsistall',
        r'caientify[a-zA-Z0-9="\']*',
        r'iving\s+[a-zA-Z0-9]*',
        r'o\s+MatterÈÖçË≥õ',
        r'ubs„Åå‰Ωé„ÅÑÂÉÆÊÄß',
        r'Âà∞Áî∞„Éá„Éº„Çø„ÅÆÊñπÂäπÊÄß',
        r'„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ.*topic.*È†ÖË°å„Å´ËÄÉ',
        r'Ôºª[^ÔºΩ]*ÔºΩ">[^<]*',  # Á†¥Êêç„Åó„ÅüHTML/XMLË¶ÅÁ¥†
        r'\]\s*">\s*$'  # ÊñáÊú´„ÅÆÁ†¥Êêç„Åó„Åü„Çø„Ç∞
    ]
    
    for pattern in nonsense_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # ÈÄ£Á∂ö„Åô„ÇãË®òÂè∑„ÅÆÈô§Âéª
    text = re.sub(r'["\'>]{2,}', '', text)
    text = re.sub(r'[=\'"]{3,}', '', text)
    
    # Á†¥Êêç„Åó„ÅüÊó•Êú¨Ë™û„ÅÆ‰øÆÊ≠£„Éë„Çø„Éº„É≥
    broken_japanese_patterns = [
        (r'„ÅÆÊñπÊ≥ïÂãïÁöÑ„Åå„Çâ', 'ÂãïÁöÑ„Å™ÊñπÊ≥ï„Åß'),
        (r'ÊÄùËÄÉ„Å´Ê≤ø„Å£„Å¶ÈÄ≤„ÇÅ„Å¶„ÅÑ„Åç„Åæ„Åô„ÄÇ$', 'ÊÄùËÄÉ„Å´Ê≤ø„Å£„Å¶ÂàÜÊûê„ÇíÈÄ≤„ÇÅ„Åæ„Åô„ÄÇ'),
        (r'„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„Çπ„Å´Ê≤ø„Å£„ÅüÊîπÂñÑ„Çí.*„Åæ„Åß„Åó„Å¶„ÅÑ„Çã„ÅÆ', '„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„Çπ„Å´Ê≤ø„Å£„ÅüÊîπÂñÑÊèêÊ°à'),
    ]
    
    for broken, fixed in broken_japanese_patterns:
        text = re.sub(broken, fixed, text, flags=re.IGNORECASE)
    
    # Á©∫Ë°å„ÅÆÊ≠£Ë¶èÂåñ
    text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
    
    return text.strip()

def ensure_english_consistency(text):
    """Ensure English text consistency"""
    import re
    
    # ÂêåÊßò„ÅÆ„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó„ÇíËã±Ë™ûÁî®„Å´ÂÆüË£Ö
    text = re.sub(r'[^\x00-\x7F\s]{10,}', '', text)  # ÈùûASCIIÊñáÂ≠ó„ÅÆÈï∑„ÅÑÈÄ£Á∂ö„ÇíÈô§Âéª
    
    return text.strip()

def extract_main_content_from_thinking_response(response) -> str:
    """
    thinkingÂΩ¢Âºè„ÅÆ„É¨„Çπ„Éù„É≥„Çπ„Åã„Çâ‰∏ªË¶Å„Ç≥„É≥„ÉÜ„É≥„ÉÑÔºàtext„Åæ„Åü„ÅØsummary_textÔºâ„ÅÆ„Åø„ÇíÊäΩÂá∫
    thinking„ÄÅsignatureÁ≠â„ÅÆ‰∏çË¶Å„Å™ÊÉÖÂ†±„ÅØÈô§Â§ñ
    JSONÊßãÈÄ†„ÇÑÁ†¥Êêç„Åó„Åü„ÉÜ„Ç≠„Çπ„Éà„ÅÆÊ∑∑ÂÖ•„ÇíÈò≤Ê≠¢
    """
    if not isinstance(response, list):
        cleaned_text = clean_response_text(str(response))
        return final_quality_check(cleaned_text)
    
    # Èô§Â§ñ„Åô„Åπ„Åç„Ç≠„Éº
    excluded_keys = {
        'thinking', 'signature', 'metadata', 'id', 'request_id', 
        'timestamp', 'uuid', 'reasoning', 'type', 'model'
    }
    
    for item in response:
        if isinstance(item, dict):
            # ÊúÄÈÅ©„Å™„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÇíÊäΩÂá∫
            content = extract_best_content_from_dict(item, excluded_keys)
            if content:
                cleaned_content = clean_response_text(content)
                if is_valid_content(cleaned_content):
                    return final_quality_check(cleaned_content)
    
    # ‰∏ªË¶Å„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑÂ†¥Âêà„ÅØÂÖ®‰Ωì„Çí„Éï„Ç©„Éº„Éû„ÉÉ„Éà
    return format_thinking_response(response)

def convert_sets_to_lists(obj):
    """Convert set types to list types for JSON serialization"""
    if isinstance(obj, set):
        return list(obj)
    elif isinstance(obj, dict):
        return {key: convert_sets_to_lists(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_sets_to_lists(item) for item in obj]
    else:
        return obj

# output_extracted_metrics „ÅÆÁîüÊàê„ÅØÈô§Â§ñÔºà‰∏çË¶ÅÔºâ

# üêå Top 10 Most Time-Consuming Processes
print(f"\nüêå Top 10 Most Time-Consuming Processes")
print("=" * 80)
print("üìä Icon explanations: ‚è±Ô∏èTime üíæMemory üî•üêåParallelism üíøSpill ‚öñÔ∏èSkew")
print('üíø Spill judgment: "Sink - Num bytes spilled to disk due to memory pressure" > 0')
print("üéØ Skew judgment: 'AQEShuffleRead - Number of skewed partitions' > 0")

# Sort nodes by execution time
sorted_nodes = sorted(extracted_metrics['node_metrics'], 
                     key=lambda x: x['key_metrics'].get('durationMs', 0), 
                     reverse=True)

# Process maximum 10 nodes
final_sorted_nodes = sorted_nodes[:10]

if final_sorted_nodes:
    # üö® Important: Correct total time calculation (regression prevention)
    # 1. Get total execution time from overall_metrics (wall-clock time)
    overall_metrics = extracted_metrics.get('overall_metrics', {})
    total_duration = overall_metrics.get('total_time_ms', 0)
    
    # üö® Fix parallel execution issue: Prioritize task_total_time_ms
    task_total_time_ms = overall_metrics.get('task_total_time_ms', 0)
    
    if task_total_time_ms > 0:
        total_duration = task_total_time_ms
        print(f"‚úÖ Console display: Parallel execution support - using task_total_time_ms: {total_duration:,} ms ({total_duration/3600000:.1f} hours)")
    elif total_duration <= 0:
        # Use execution_time_ms as next priority
        execution_time_ms = overall_metrics.get('execution_time_ms', 0)
        if execution_time_ms > 0:
            total_duration = execution_time_ms
            print(f"‚ö†Ô∏è Console display: task_total_time_ms unavailable, using execution_time_ms: {total_duration} ms")
        else:
            # Final fallback
            max_node_time = max([node['key_metrics'].get('durationMs', 0) for node in sorted_nodes], default=1)
            total_duration = int(max_node_time * 1.2)
            print(f"‚ö†Ô∏è Console display: Final fallback - using estimated time: {total_duration} ms")
    
    print(f"üìä Cumulative task execution time (parallel): {total_duration:,} ms ({total_duration/3600000:.1f} hours)")
    print(f"üìà TOP10 total time (parallel execution): {sum(node['key_metrics'].get('durationMs', 0) for node in final_sorted_nodes):,} ms")

    print()
    
    for i, node in enumerate(final_sorted_nodes):
        rows_num = node['key_metrics'].get('rowsNum', 0)
        duration_ms = node['key_metrics'].get('durationMs', 0)
        memory_mb = node['key_metrics'].get('peakMemoryBytes', 0) / 1024 / 1024
        
        # üö® ÈáçË¶Å: Ê≠£„Åó„ÅÑ„Éë„Éº„Çª„É≥„ÉÜ„Éº„Ç∏Ë®àÁÆóÔºà„Éá„Ç∞„É¨Èò≤Ê≠¢Ôºâ
        # wall-clock time„Å´ÂØæ„Åô„ÇãÂêÑ„Éé„Éº„Éâ„ÅÆÂÆüË°åÊôÇÈñì„ÅÆÂâ≤Âêà
        time_percentage = min((duration_ms / max(total_duration, 1)) * 100, 100.0)
        
        # ÊôÇÈñì„ÅÆÈáçË¶ÅÂ∫¶„Å´Âü∫„Å•„ÅÑ„Å¶„Ç¢„Ç§„Ç≥„É≥„ÇíÈÅ∏Êäû
        if duration_ms >= 10000:  # 10Áßí‰ª•‰∏ä
            time_icon = "ÔøΩ"
            severity = "CRITICAL"
        elif duration_ms >= 5000:  # 5Áßí‰ª•‰∏ä
            time_icon = "üü†"
            severity = "HIGH"
        elif duration_ms >= 1000:  # 1Áßí‰ª•‰∏ä
            time_icon = "üü°"
            severity = "MEDIUM"
        else:
            time_icon = "ÔøΩ"
            severity = "LOW"
        
        # „É°„É¢„É™‰ΩøÁî®Èáè„ÅÆ„Ç¢„Ç§„Ç≥„É≥
        memory_icon = "ÔøΩ" if memory_mb < 100 else "‚ö†Ô∏è" if memory_mb < 1000 else "üö®"
        
        # „Çà„ÇäÊÑèÂë≥„ÅÆ„ÅÇ„Çã„Éé„Éº„ÉâÂêç„ÇíÂèñÂæó
        raw_node_name = node['name']
        node_name = get_meaningful_node_name(node, extracted_metrics)
        short_name = node_name[:100] + "..." if len(node_name) > 100 else node_name
        
        # ‰∏¶ÂàóÂ∫¶ÊÉÖÂ†±„ÅÆÂèñÂæóÔºà‰øÆÊ≠£Áâà: Ë§áÊï∞„ÅÆTasks total„É°„Éà„É™„ÇØ„Çπ„ÇíÂèñÂæóÔºâ
        parallelism_data = extract_parallelism_metrics(node)
        
        # ÂæìÊù•„ÅÆÂçò‰∏ÄÂÄ§Ôºà‰∫íÊèõÊÄß„ÅÆ„Åü„ÇÅÔºâ
        num_tasks = parallelism_data.get('tasks_total', 0)
        
        # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: Sink - Tasks total„Åæ„Åü„ÅØSource - Tasks total„Åå„ÅÇ„ÇãÂ†¥Âêà
        if num_tasks == 0:
            if parallelism_data.get('sink_tasks_total', 0) > 0:
                num_tasks = parallelism_data.get('sink_tasks_total', 0)
            elif parallelism_data.get('source_tasks_total', 0) > 0:
                num_tasks = parallelism_data.get('source_tasks_total', 0)
        
        # „Éá„Ç£„Çπ„ÇØ„Çπ„Éî„É´„Ç¢„Ç¶„Éà„ÅÆÊ§úÂá∫Ôºà„É°„É¢„É™„Éó„É¨„ÉÉ„Ç∑„É£„Éº„Å´„Çà„Çã„Çπ„Éî„É´„É°„Éà„É™„ÇØ„ÇπÂØæÂøúÊîπÂñÑÁâàÔºâ
        spill_detected = False
        spill_bytes = 0
        spill_details = []
        
        # „Çπ„Éî„É´Ê§úÂá∫„Çø„Éº„Ç≤„ÉÉ„Éà„É°„Éà„É™„ÇØ„ÇπÂêç„É™„Çπ„ÉàÔºàÊ≠£Á¢∫„Å™„É°„Éà„É™„ÇØ„ÇπÂêç„ÅÆ„ÅøÔºâ
        exact_spill_metrics = [
            "Num bytes spilled to disk due to memory pressure",
            "Sink - Num bytes spilled to disk due to memory pressure",
            "Sink/Num bytes spilled to disk due to memory pressure"
        ]
        
        # 1. detailed_metrics„Åã„ÇâÊ≠£Á¢∫„Å™„É°„Éà„É™„ÇØ„ÇπÂêç„ÅßÊ§úÁ¥¢
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            metric_value = metric_info.get('value', 0)
            metric_label = metric_info.get('label', '')
            
            # Ê≠£Á¢∫„Å™„É°„Éà„É™„ÇØ„ÇπÂêç„Åß„ÅÆ„Åø„Éû„ÉÉ„ÉÅ„É≥„Ç∞
            if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                spill_detected = True
                spill_bytes = max(spill_bytes, metric_value)  # ÊúÄÂ§ßÂÄ§„Çí‰ΩøÁî®
                spill_details.append({
                    'metric_name': metric_key,
                    'value': metric_value,
                    'label': metric_label,
                    'source': 'detailed_metrics',
                    'matched_field': 'key' if metric_key in exact_spill_metrics else 'label',
                    'matched_pattern': metric_key if metric_key in exact_spill_metrics else metric_label
                })
                break  # ÊúÄÂàù„Å´Ë¶ã„Å§„Åã„Å£„Åü„Çπ„Éî„É´„É°„Éà„É™„ÇØ„Çπ„Çí‰ΩøÁî®
        
        # 2. detailed_metrics„ÅßË¶ã„Å§„Åã„Çâ„Å™„ÅÑÂ†¥Âêà„ÄÅÁîü„É°„Éà„É™„ÇØ„Çπ„Åã„ÇâÊ≠£Á¢∫„Å™„É°„Éà„É™„ÇØ„ÇπÂêç„ÅßÊ§úÁ¥¢
        if not spill_detected:
            raw_metrics = node.get('metrics', [])
            for metric in raw_metrics:
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_value = metric.get('value', 0)
                
                # Ê≠£Á¢∫„Å™„É°„Éà„É™„ÇØ„ÇπÂêç„Åß„ÅÆ„Åø„Éû„ÉÉ„ÉÅ„É≥„Ç∞
                if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                    spill_detected = True
                    spill_bytes = max(spill_bytes, metric_value)  # ÊúÄÂ§ßÂÄ§„Çí‰ΩøÁî®
                    spill_details.append({
                        'metric_name': metric_key,
                        'value': metric_value,
                        'label': metric_label,
                        'source': 'raw_metrics',
                        'matched_field': 'key' if metric_key in exact_spill_metrics else 'label',
                        'matched_pattern': metric_key if metric_key in exact_spill_metrics else metric_label
                    })
                    break  # ÊúÄÂàù„Å´Ë¶ã„Å§„Åã„Å£„Åü„Çπ„Éî„É´„É°„Éà„É™„ÇØ„Çπ„Çí‰ΩøÁî®
        
        # 3. key_metrics„Åã„ÇâÊ≠£Á¢∫„Å™„É°„Éà„É™„ÇØ„ÇπÂêç„ÅßÊ§úÁ¥¢
        if not spill_detected:
            key_metrics = node.get('key_metrics', {})
            for exact_metric in exact_spill_metrics:
                if exact_metric in key_metrics and key_metrics[exact_metric] > 0:
                    spill_detected = True
                    spill_bytes = max(spill_bytes, key_metrics[exact_metric])  # ÊúÄÂ§ßÂÄ§„Çí‰ΩøÁî®
                    spill_details.append({
                        'metric_name': f"key_metrics.{exact_metric}",
                        'value': key_metrics[exact_metric],
                        'label': f"Key metric: {exact_metric}",
                        'source': 'key_metrics',
                        'matched_field': 'key',
                        'matched_pattern': exact_metric
                    })
                    break
        
        # Data skew detection (AQE-based precise judgment)
        skew_detected = False
        skew_details = []
        skewed_partitions = 0  # Number of skewed partitions
        
        # AQE-based skew detection: "AQEShuffleRead - Number of skewed partitions" > 0
        target_aqe_metrics = [
            "AQEShuffleRead - Number of skewed partitions",
            "AQEShuffleRead - Number of skewed partition splits"
        ]
        
        aqe_skew_value = 0
        aqe_split_value = 0
        aqe_metric_name = ""
        aqe_split_metric_name = ""
        
        # 1. detailed_metrics„ÅßÊ§úÁ¥¢
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            if metric_key == "AQEShuffleRead - Number of skewed partitions":
                aqe_skew_value = metric_info.get('value', 0)
                aqe_metric_name = metric_key
            elif metric_key == "AQEShuffleRead - Number of skewed partition splits":
                aqe_split_value = metric_info.get('value', 0)
                aqe_split_metric_name = metric_key
            elif metric_info.get('label', '') == "AQEShuffleRead - Number of skewed partitions":
                aqe_skew_value = metric_info.get('value', 0)
                aqe_metric_name = metric_info.get('label', '')
            elif metric_info.get('label', '') == "AQEShuffleRead - Number of skewed partition splits":
                aqe_split_value = metric_info.get('value', 0)
                aqe_split_metric_name = metric_info.get('label', '')
        
        # 2. raw_metrics„ÅßÊ§úÁ¥¢Ôºà„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ
        if aqe_skew_value == 0 or aqe_split_value == 0:
            raw_metrics = node.get('metrics', [])
            if isinstance(raw_metrics, list):
                for raw_metric in raw_metrics:
                    if isinstance(raw_metric, dict):
                        # 'label'„Éï„Ç£„Éº„É´„Éâ„ÇíÊúÄÂàù„Å´„ÉÅ„Çß„ÉÉ„ÇØ
                        raw_metric_label = raw_metric.get('label', '')
                        if raw_metric_label == "AQEShuffleRead - Number of skewed partitions" and aqe_skew_value == 0:
                            aqe_skew_value = raw_metric.get('value', 0)
                            aqe_metric_name = raw_metric_label
                        elif raw_metric_label == "AQEShuffleRead - Number of skewed partition splits" and aqe_split_value == 0:
                            aqe_split_value = raw_metric.get('value', 0)
                            aqe_split_metric_name = raw_metric_label
                        
                        # 'key'„Éï„Ç£„Éº„É´„Éâ„ÇÇ„ÉÅ„Çß„ÉÉ„ÇØ
                        raw_metric_key = raw_metric.get('key', '')
                        if raw_metric_key == "AQEShuffleRead - Number of skewed partitions" and aqe_skew_value == 0:
                            aqe_skew_value = raw_metric.get('value', 0)
                            aqe_metric_name = raw_metric_key
                        elif raw_metric_key == "AQEShuffleRead - Number of skewed partition splits" and aqe_split_value == 0:
                            aqe_split_value = raw_metric.get('value', 0)
                            aqe_split_metric_name = raw_metric_key
                        
                        # 'metricName'„Éï„Ç£„Éº„É´„Éâ„ÇÇ„ÉÅ„Çß„ÉÉ„ÇØÔºàÂæìÊù•„ÅÆ‰∫íÊèõÊÄßÔºâ
                        raw_metric_name = raw_metric.get('metricName', '')
                        if raw_metric_name == "AQEShuffleRead - Number of skewed partitions" and aqe_skew_value == 0:
                            aqe_skew_value = raw_metric.get('value', 0)
                            aqe_metric_name = raw_metric_name
                        elif raw_metric_name == "AQEShuffleRead - Number of skewed partition splits" and aqe_split_value == 0:
                            aqe_split_value = raw_metric.get('value', 0)
                            aqe_split_metric_name = raw_metric_name
        
        # 3. key_metrics„ÅßÊ§úÁ¥¢Ôºà„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ
        if aqe_skew_value == 0 or aqe_split_value == 0:
            key_metrics = node.get('key_metrics', {})
            for key_metric_name, key_metric_value in key_metrics.items():
                if "AQEShuffleRead - Number of skewed partitions" in key_metric_name and aqe_skew_value == 0:
                    aqe_skew_value = key_metric_value
                    aqe_metric_name = key_metric_name
                elif "AQEShuffleRead - Number of skewed partition splits" in key_metric_name and aqe_split_value == 0:
                    aqe_split_value = key_metric_value
                    aqe_split_metric_name = key_metric_name
        
        # AQE skew judgment
        if aqe_skew_value > 0:
            skew_detected = True
            skewed_partitions = aqe_skew_value  # Set number of skewed partitions
            severity_level = "High" if aqe_skew_value >= 5 else "Medium"
            
            # Basic AQE skew detection information
            description = f'AQE skew detected: {aqe_metric_name} = {aqe_skew_value} > threshold 0 [Importance:{severity_level}]'
            
            # Add detailed information if split value is also available
            if aqe_split_value > 0:
                description += f' | AQE detection details: Spark automatically detected {aqe_skew_value} skewed partitions'
                description += f' | AQE automatic handling: Spark automatically split into {aqe_split_value} partitions'
            
            skew_details.append({
                'type': 'aqe_skew',
                'value': aqe_skew_value,
                'split_value': aqe_split_value,
                'threshold': 0,
                'metric_name': aqe_metric_name,
                'split_metric_name': aqe_split_metric_name,
                'severity': severity_level,
                'description': description
            })
        
        # Use only AQE-based skew detection (spill-based judgment removed)
        # Reason: AQEShuffleRead - Number of skewed partitions is the accurate skew judgment standard
        
        # ‰∏¶ÂàóÂ∫¶„Ç¢„Ç§„Ç≥„É≥
        parallelism_icon = "üî•" if num_tasks >= 10 else "‚ö†Ô∏è" if num_tasks >= 5 else "üêå"
        # „Çπ„Éî„É´„Ç¢„Ç§„Ç≥„É≥
        spill_icon = "üíø" if spill_detected else "‚úÖ"
        # „Çπ„Ç≠„É•„Éº„Ç¢„Ç§„Ç≥„É≥
        skew_icon = "‚öñÔ∏è" if skew_detected else "‚úÖ"
        
        print(f"{i+1:2d}. {time_icon}{memory_icon}{parallelism_icon}{spill_icon}{skew_icon} [{severity:8}] {short_name}")
        print(f"    ‚è±Ô∏è  Execution time: {duration_ms:>8,} ms ({duration_ms/1000:>6.1f} sec) - {time_percentage:>5.1f}% of cumulative time")
        print(f"    üìä Rows processed: {rows_num:>8,} rows")
        print(f"    üíæ Peak memory: {memory_mb:>6.1f} MB")
        # Display multiple Tasks total metrics
        parallelism_display = []
        for task_metric in parallelism_data.get('all_tasks_metrics', []):
            parallelism_display.append(f"{task_metric['name']}: {task_metric['value']}")
        
        if parallelism_display:
            print(f"    üîß Parallelism: {' | '.join(parallelism_display)}")
        else:
            print(f"    üîß Parallelism: {num_tasks:>3d} tasks")
        
        # Skew judgment (considering both AQE skew detection and AQEShuffleRead average partition size)
        aqe_shuffle_skew_warning = parallelism_data.get('aqe_shuffle_skew_warning', False)
        
        if skew_detected:
            skew_status = "Detected & handled by AQE"
        elif aqe_shuffle_skew_warning:
            skew_status = "Potential skew possibility"
        else:
            skew_status = "None"
        
        print(f"    üíø Spill: {'Yes' if spill_detected else 'No'} | ‚öñÔ∏è Skew: {skew_status}")
        
        # Display AQEShuffleRead metrics
        aqe_shuffle_metrics = parallelism_data.get('aqe_shuffle_metrics', [])
        if aqe_shuffle_metrics:
            aqe_display = []
            for aqe_metric in aqe_shuffle_metrics:
                if aqe_metric['name'] == "AQEShuffleRead - Number of partitions":
                    aqe_display.append(f"Partitions: {aqe_metric['value']}")
                elif aqe_metric['name'] == "AQEShuffleRead - Partition data size":
                    aqe_display.append(f"Data size: {aqe_metric['value']:,} bytes")
            
            if aqe_display:
                print(f"    üîÑ AQEShuffleRead: {' | '.join(aqe_display)}")
                
                # Average partition size and warning display
                avg_partition_size = parallelism_data.get('aqe_shuffle_avg_partition_size', 0)
                if avg_partition_size > 0:
                    avg_size_mb = avg_partition_size / (1024 * 1024)
                    print(f"    üìä Average partition size: {avg_size_mb:.2f} MB")
                    
                    # Warning when 512MB or more
                    if parallelism_data.get('aqe_shuffle_skew_warning', False):
                        print(f"    ‚ö†Ô∏è  „ÄêWARNING„Äë Average partition size exceeds 512MB - Potential skew possibility")
        
        # Calculate efficiency indicator (rows/sec)
        if duration_ms > 0:
            rows_per_sec = (rows_num * 1000) / duration_ms
            print(f"    üöÄ Processing efficiency: {rows_per_sec:>8,.0f} rows/sec")
        
# „Éï„Ç£„É´„ÇøÁéáË°®Á§∫Ôºà„Éá„Éê„ÉÉ„Ç∞Ê©üËÉΩ‰ªò„ÅçÔºâ
        filter_result = calculate_filter_rate(node)
        filter_display = format_filter_rate_display(filter_result)
        if filter_display:
            print(f"    {filter_display}")
        else:
            # „Éá„Éê„ÉÉ„Ç∞ÊÉÖÂ†±Ôºö„Å™„Åú„Éï„Ç£„É´„ÇøÁéá„ÅåË°®Á§∫„Åï„Çå„Å™„ÅÑ„Åã„ÇíÁ¢∫Ë™ç
            if filter_result["has_filter_metrics"]:
                print(f"    üìÇ Filter rate: {filter_result['filter_rate']:.1%} (read: {filter_result['files_read_bytes']/(1024*1024*1024):.2f}GB, pruned: {filter_result['files_pruned_bytes']/(1024*1024*1024):.2f}GB)")
            else:
                # „É°„Éà„É™„ÇØ„ÇπÊ§úÁ¥¢„ÅÆ„Éá„Éê„ÉÉ„Ç∞
                debug_info = []
                detailed_metrics = node.get('detailed_metrics', {})
                for metric_key, metric_info in detailed_metrics.items():
                    metric_label = metric_info.get('label', '')
                    if 'file' in metric_label.lower() and ('read' in metric_label.lower() or 'prun' in metric_label.lower()):
                        debug_info.append(f"{metric_label}: {metric_info.get('value', 0)}")
                
                if debug_info:
                    print(f"    üìÇ Filter-related metrics detected: {', '.join(debug_info[:2])}")
                # else:
                #     print(f"    üìÇ Filter rate: metrics not detected")
        
        # „Çπ„Éî„É´Ë©≥Á¥∞ÊÉÖÂ†±Ôºà„Ç∑„É≥„Éó„É´Ë°®Á§∫Ôºâ
        spill_display = ""
        if spill_detected and spill_bytes > 0:
            spill_mb = spill_bytes / 1024 / 1024
            if spill_mb >= 1024:  # GBÂçò‰Ωç
                spill_display = f"{spill_mb/1024:.2f} GB"
            else:  # MBÂçò‰Ωç
                spill_display = f"{spill_mb:.1f} MB"
            print(f"    üíø Spill: {spill_display}")
        
        # Shuffle„Éé„Éº„Éâ„ÅÆÂ†¥Âêà„ÅØÂ∏∏„Å´Shuffle attributes„ÇíË°®Á§∫
        if "shuffle" in short_name.lower():
            shuffle_attributes = extract_shuffle_attributes(node)
            if shuffle_attributes:
                print(f"    üîÑ Shuffle attributes: {', '.join(shuffle_attributes)}")
                
                # REPARTITION„Éí„É≥„Éà„ÅÆÊèêÊ°àÔºà„Çπ„Éî„É´„ÅåÊ§úÂá∫„Åï„Çå„ÅüÂ†¥Âêà„ÅÆ„ÅøÔºâ
                if spill_detected and spill_bytes > 0 and spill_display:
                    suggested_partitions = max(num_tasks * 2, 200)  # ÊúÄÂ∞è200„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥
                    
                    # ShuffleÂ±ûÊÄß„ÅßÊ§úÂá∫„Åï„Çå„Åü„Ç´„É©„É†„ÇíÂÖ®„Å¶‰ΩøÁî®ÔºàÂÆåÂÖ®‰∏ÄËá¥Ôºâ
                    repartition_columns = ", ".join(shuffle_attributes)
                    
                    print(f"    üí° Optimization suggestion: REPARTITION({suggested_partitions}, {repartition_columns})")
                    print(f"       Reason: To improve spill ({spill_display})")
                    print(f"       Target: Complete use of all {len(shuffle_attributes)} shuffle attribute columns")
            else:
                print(f"    üîÑ Shuffle attributes: Not configured")
        
        # „Çπ„Ç≠„É£„É≥„Éé„Éº„Éâ„ÅÆÂ†¥Âêà„ÅØ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº„ÇíË°®Á§∫
        if "scan" in short_name.lower():
            cluster_attributes = extract_cluster_attributes(node)
            if cluster_attributes:
                print(f"    üìä Clustering keys: {', '.join(cluster_attributes)}")
            else:
                print(f"    üìä Clustering keys: Not configured")

        
        # Skew details (simplified display)
        if skew_detected and skewed_partitions > 0:
            print(f"    ‚öñÔ∏è Skew details: {skewed_partitions} skewed partitions")
        
        # Also display Node ID
        print(f"    üÜî Node ID: {node.get('node_id', node.get('id', 'N/A'))}")
        print()
        
else:
    print("‚ö†Ô∏è Node metrics not found")

print()

# üî• Spark„Çπ„ÉÜ„Éº„Ç∏ÂÆüË°åÂàÜÊûê
if extracted_metrics['stage_metrics']:
    print("\nüî• Spark Stage Execution Analysis")
    print("=" * 60)
    
    stage_metrics = extracted_metrics['stage_metrics']
    total_stages = len(stage_metrics)
    completed_stages = len([s for s in stage_metrics if s.get('status') == 'COMPLETE'])
    failed_stages = len([s for s in stage_metrics if s.get('num_failed_tasks', 0) > 0])
    
    print(f"üìä Stage overview: Total {total_stages} stages (completed: {completed_stages}, with failed tasks: {failed_stages})")
    print()
    
    # „Çπ„ÉÜ„Éº„Ç∏„ÇíÂÆüË°åÊôÇÈñì„Åß„ÇΩ„Éº„Éà
    sorted_stages = sorted(stage_metrics, key=lambda x: x.get('duration_ms', 0), reverse=True)
    
    print("‚è±Ô∏è Stage execution time ranking:")
    print("-" * 60)
    
    for i, stage in enumerate(sorted_stages[:5]):  # TOP5„Çπ„ÉÜ„Éº„Ç∏„ÅÆ„ÅøË°®Á§∫
        stage_id = stage.get('stage_id', 'N/A')
        status = stage.get('status', 'UNKNOWN')
        duration_ms = stage.get('duration_ms', 0)
        num_tasks = stage.get('num_tasks', 0)
        failed_tasks = stage.get('num_failed_tasks', 0)
        complete_tasks = stage.get('num_complete_tasks', 0)
        
        # „Çπ„ÉÜ„Éº„Çø„Çπ„Å´Âøú„Åò„Åü„Ç¢„Ç§„Ç≥„É≥
        if status == 'COMPLETE' and failed_tasks == 0:
            status_icon = "‚úÖ"
        elif failed_tasks > 0:
            status_icon = "‚ö†Ô∏è"
        else:
            status_icon = "‚ùì"
        
        # ‰∏¶ÂàóÂ∫¶„Ç¢„Ç§„Ç≥„É≥
        parallelism_icon = "üî•" if num_tasks >= 10 else "‚ö†Ô∏è" if num_tasks >= 5 else "üêå"
        
        # ÂÆüË°åÊôÇÈñì„ÅÆÈáçË¶ÅÂ∫¶
        if duration_ms >= 10000:
            time_icon = "üî¥"
            severity = "CRITICAL"
        elif duration_ms >= 5000:
            time_icon = "üü†"
            severity = "HIGH"
        elif duration_ms >= 1000:
            time_icon = "üü°"
            severity = "MEDIUM"
        else:
            time_icon = "üü¢"
            severity = "LOW"
        
        print(f"{i+1}. {status_icon}{parallelism_icon}{time_icon} Stage {stage_id} [{severity:8}]")
        print(f"   ‚è±Ô∏è Execution time: {duration_ms:,} ms ({duration_ms/1000:.1f} sec)")
        print(f"   üîß Tasks: {complete_tasks}/{num_tasks} completed (failed: {failed_tasks})")
        
        # „Çø„Çπ„ÇØ„ÅÇ„Åü„Çä„ÅÆÂπ≥ÂùáÊôÇÈñì
        if num_tasks > 0:
            avg_task_time = duration_ms / num_tasks
            print(f"   üìä Average task time: {avg_task_time:.1f} ms")
        
        # ÂäπÁéáÊÄßË©ï‰æ°
        if num_tasks > 0:
            task_efficiency = "È´òÂäπÁéá" if num_tasks >= 10 and failed_tasks == 0 else "Ë¶ÅÊîπÂñÑ" if failed_tasks > 0 else "Ê®ôÊ∫ñ"
            print(f"   üéØ Efficiency: {task_efficiency}")
        
        print()
    
    if len(sorted_stages) > 5:
        print(f"... {len(sorted_stages) - 5} other stages")
    
    # ÂïèÈ°å„ÅÆ„ÅÇ„Çã„Çπ„ÉÜ„Éº„Ç∏„ÅÆ„Éè„Ç§„É©„Ç§„Éà
    problematic_stages = [s for s in stage_metrics if s.get('num_failed_tasks', 0) > 0 or s.get('duration_ms', 0) > 30000]
    if problematic_stages:
        print("\nüö® Stages requiring attention:")
        print("-" * 40)
        for stage in problematic_stages[:3]:
            stage_id = stage.get('stage_id', 'N/A')
            duration_sec = stage.get('duration_ms', 0) / 1000
            failed_tasks = stage.get('num_failed_tasks', 0)
            
            issues = []
            if failed_tasks > 0:
                issues.append(f"Â§±Êïó„Çø„Çπ„ÇØ{failed_tasks}ÂÄã")
            if duration_sec > 30:
                issues.append(f"Èï∑ÊôÇÈñìÂÆüË°å({duration_sec:.1f}sec)")
            
            print(f"   ‚ö†Ô∏è Stage {stage_id}: {', '.join(issues)}")
    
    
    print()
else:
    print("\nüî• Spark Stage Execution Analysis")
    print("=" * 60)
    print("‚ö†Ô∏è Stage metrics not found")
    print()

print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## üóÇÔ∏è Detailed Display of Liquid Clustering Analysis Results
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Detailed display of recommended clustering columns by table
# MAGIC - Analysis of expected performance improvements
# MAGIC - Detailed analysis of column usage patterns
# MAGIC - Display of pushdown filter information
# MAGIC - Presentation of SQL implementation examples

# COMMAND ----------

# üóÇÔ∏è LLM„Å´„Çà„ÇãLiquid ClusteringÂàÜÊûêÁµêÊûú„ÅÆË©≥Á¥∞Ë°®Á§∫
print("\n" + "=" * 50)
print("ü§ñ LLM Liquid Clustering Recommendation Analysis")
print("=" * 50)

# LLM„Éô„Éº„Çπ„ÅÆLiquid ClusteringÂàÜÊûê„ÇíÂÆüË°å
liquid_analysis = extracted_metrics['liquid_clustering_analysis']

# LLMÂàÜÊûêÁµêÊûú„ÇíË°®Á§∫
print("\nü§ñ LLM Analysis Results:")
print("=" * 50)
llm_analysis = liquid_analysis.get('llm_analysis', '')
if llm_analysis:
    print(llm_analysis)
else:
    print("‚ùå LLM analysis results not found")

# ÊäΩÂá∫„Éá„Éº„Çø„ÅÆÊ¶ÇË¶Å„ÇíË°®Á§∫
extracted_data = liquid_analysis.get('extracted_data', {})
metadata_summary = extracted_data.get('metadata_summary', {})

print(f"\nüìä Extracted data overview:")
print(f"   üîç Filter conditions: {metadata_summary.get('filter_expressions_count', 0)} items")
print(f"   üîó JOIN conditions: {metadata_summary.get('join_expressions_count', 0)} items")
print(f"   üìä GROUP BY conditions: {metadata_summary.get('groupby_expressions_count', 0)} items")
print(f"   üìà Aggregate functions: {metadata_summary.get('aggregate_expressions_count', 0)} items")
print(f"   üè∑Ô∏è Identified tables: {metadata_summary.get('tables_identified', 0)} items")
print(f"   üìÇ Scan nodes: {metadata_summary.get('scan_nodes_count', 0)} items")

# „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„ÅÆË°®Á§∫
performance_context = liquid_analysis.get('performance_context', {})
print(f"\n‚ö° Performance information:")
print(f"   ‚è±Ô∏è Execution time: {performance_context.get('total_time_sec', 0):.1f} seconds")
print(f"   üíæ Data read: {performance_context.get('read_gb', 0):.2f}GB")
print(f"   üìä Output rows: {performance_context.get('rows_produced', 0):,} rows")
print(f"   üéØ Filter rate: {performance_context.get('data_selectivity', 0):.4f}")

# Output analysis results to file
print(f"\nüíæ Outputting analysis results to file...")
try:
    saved_files = save_liquid_clustering_analysis(liquid_analysis, "/tmp")
    
    if "error" in saved_files:
        print(f"‚ùå File output error: {saved_files['error']}")
    else:
        print(f"‚úÖ File output completed:")
        for file_type, file_path in saved_files.items():
            if file_type == "json":
                print(f"   üìÑ JSON detailed data: {file_path}")
            elif file_type == "markdown":
                print(f"   üìù Markdown report: {file_path}")
            elif file_type == "sql":
                print(f"   üîß SQL implementation example: {file_path}")
                
except Exception as e:
    print(f"‚ùå Error occurred during file output: {str(e)}")

# „Çµ„Éû„É™„ÉºÊÉÖÂ†±
summary = liquid_analysis.get('summary', {})
print(f"\nüìã Analysis summary:")
print(f"   üî¨ Analysis method: {summary.get('analysis_method', 'Unknown')}")
print(f"   ü§ñ LLM provider: {summary.get('llm_provider', 'Unknown')}")
print(f"   üìä Target table count: {summary.get('tables_identified', 0)}")
print(f"   üìà Extracted column count: Filter({summary.get('total_filter_columns', 0)}) + JOIN({summary.get('total_join_columns', 0)}) + GROUP BY({summary.get('total_groupby_columns', 0)})")

print()

# COMMAND ----------

# ü§ñ Ë®≠ÂÆö„Åï„Çå„ÅüLLM„Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà„Çí‰ΩøÁî®„Åó„Å¶„Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûê
provider = LLM_CONFIG["provider"]
if provider == "databricks":
    endpoint_name = LLM_CONFIG["databricks"]["endpoint_name"]
    print(f"ü§ñ Starting bottleneck analysis with Databricks Model Serving ({endpoint_name})...")
    print(f"‚ö†Ô∏è  Model Serving endpoint '{endpoint_name}' is required")
elif provider == "openai":
    model = LLM_CONFIG["openai"]["model"]
    print(f"ü§ñ Starting bottleneck analysis with OpenAI ({model})...")
    print("‚ö†Ô∏è  OpenAI API key is required")
elif provider == "azure_openai":
    deployment = LLM_CONFIG["azure_openai"]["deployment_name"]
    print(f"ü§ñ Starting bottleneck analysis with Azure OpenAI ({deployment})...")
    print("‚ö†Ô∏è  Azure OpenAI API key and endpoint are required")
elif provider == "anthropic":
    model = LLM_CONFIG["anthropic"]["model"]
    print(f"ü§ñ Starting bottleneck analysis with Anthropic ({model})...")
    print("‚ö†Ô∏è  Anthropic API key is required")

print("üìù Simplifying analysis prompt to reduce timeout risk...")
print()

analysis_result = analyze_bottlenecks_with_llm(extracted_metrics)

# COMMAND ----------

# MAGIC %md
# MAGIC ## üéØ Display of LLM Bottleneck Analysis Results
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Display of detailed analysis results by the configured LLM provider
# MAGIC - Visualization of bottleneck identification and improvement recommendations
# MAGIC - Formatting and readable display of analysis results

# COMMAND ----------

# üìä ÂàÜÊûêÁµêÊûú„ÅÆË°®Á§∫
print("\n" + "=" * 80)
print(f"üéØ „ÄêSQL Bottleneck Analysis Results by {provider.upper()} LLM„Äë")
print("=" * 80)
print()
print(analysis_result)
print()
print("=" * 80)

# COMMAND ----------

# MAGIC %md
# MAGIC ## üíæ Saving Analysis Results and Completion Summary
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Saving LLM analysis results to text files
# MAGIC - Recording basic information of analysis targets
# MAGIC - Displaying overall processing completion summary
# MAGIC - Listing output files

# COMMAND ----------

# üíæ ÂàÜÊûêÁµêÊûú„ÅÆ‰øùÂ≠ò„Å®ÂÆå‰∫Ü„Çµ„Éû„É™„Éº
from datetime import datetime
# output_bottleneck_analysis_result_XXX.txt„Éï„Ç°„Ç§„É´„ÅÆÂá∫Âäõ„ÅØÂªÉÊ≠¢Ôºàoptimization_report„Å´Áµ±ÂêàÔºâ

# ÊúÄÁµÇÁöÑ„Å™„Çµ„Éû„É™„Éº
print("\n" + "üéâ" * 20)
print("üèÅ „ÄêProcessing Completion Summary„Äë")
print("üéâ" * 20)
print("‚úÖ SQL profiler JSON file loading completed")
print(f"‚úÖ Performance metrics extraction completed")

# LLM„Éó„É≠„Éê„Ç§„ÉÄ„ÉºÊÉÖÂ†±„ÅÆÂãïÁöÑË°®Á§∫
try:
    current_provider = LLM_CONFIG.get('provider', 'unknown')
    provider_display_names = {
        'databricks': f"Databricks ({LLM_CONFIG.get('databricks', {}).get('endpoint_name', 'Model Serving')})",
        'openai': f"OpenAI ({LLM_CONFIG.get('openai', {}).get('model', 'GPT-4')})",
        'azure_openai': f"Azure OpenAI ({LLM_CONFIG.get('azure_openai', {}).get('deployment_name', 'GPT-4')})",
        'anthropic': f"Anthropic ({LLM_CONFIG.get('anthropic', {}).get('model', 'Claude')})"
    }
    provider_display = provider_display_names.get(current_provider, f"{current_provider}ÔºàÊú™Áü•„ÅÆ„Éó„É≠„Éê„Ç§„ÉÄ„ÉºÔºâ")
    print(f"‚úÖ Bottleneck analysis completed by {provider_display}")
except Exception as e:
    print("‚úÖ LLM bottleneck analysis completed")

print("‚úÖ Analysis results will be integrated into optimization_report later")
print()
print("üöÄ Analysis complete! Please check the results and use them for query optimization.")
print("üéâ" * 20)

# COMMAND ----------

# MAGIC %md
# MAGIC # üîß SQL Optimization Function Section
# MAGIC
# MAGIC **This section performs SQL query optimization**
# MAGIC
# MAGIC üìã **Optimization Process:**
# MAGIC - Extract original query from profiler data
# MAGIC - Execute query optimization using LLM
# MAGIC - Generate optimization result files
# MAGIC - Prepare for test execution
# MAGIC
# MAGIC ‚ö†Ô∏è **Prerequisites:** Please complete the main processing section before execution

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîß SQL Optimization Related Function Definitions
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - `extract_original_query_from_profiler_data`: Extract original query from profiler data
# MAGIC - `generate_optimized_query_with_llm`: Query optimization based on LLM analysis results
# MAGIC - `save_optimized_sql_files`: Save various optimization result files

# COMMAND ----------

def extract_original_query_from_profiler_data(profiler_data: Dict[str, Any]) -> str:
    """
    „Éó„É≠„Éï„Ç°„Ç§„É©„Éº„Éá„Éº„Çø„Åã„Çâ„Ç™„É™„Ç∏„Éä„É´„ÇØ„Ç®„É™„ÇíÊäΩÂá∫
    """
    
    # Ë§áÊï∞„ÅÆÂ†¥ÊâÄ„Åã„ÇâSQL„ÇØ„Ç®„É™„ÇíÊé¢„Åô
    query_candidates = []
    
    # 1. query.queryText „Åã„ÇâÊäΩÂá∫
    if 'query' in profiler_data and 'queryText' in profiler_data['query']:
        query_text = profiler_data['query']['queryText']
        if query_text and query_text.strip():
            query_candidates.append(query_text.strip())
    
    # 2. metadata „Åã„ÇâÊäΩÂá∫
    if 'metadata' in profiler_data:
        metadata = profiler_data['metadata']
        for key, value in metadata.items():
            if 'sql' in key.lower() or 'query' in key.lower():
                if isinstance(value, str) and value.strip():
                    query_candidates.append(value.strip())
    
    # 3. graphs „ÅÆ metadata „Åã„ÇâÊäΩÂá∫
    if 'graphs' in profiler_data:
        for graph in profiler_data['graphs']:
            nodes = graph.get('nodes', [])
            for node in nodes:
                node_metadata = node.get('metadata', [])
                for meta in node_metadata:
                    if meta.get('key', '').upper() in ['SQL', 'QUERY', 'SQL_TEXT']:
                        value = meta.get('value', '')
                        if value and value.strip():
                            query_candidates.append(value.strip())
    
    # ÊúÄ„ÇÇÈï∑„ÅÑ„ÇØ„Ç®„É™„ÇíÈÅ∏ÊäûÔºàÈÄöÂ∏∏„ÄÅÊúÄ„ÇÇÂÆåÂÖ®„Å™„ÇØ„Ç®„É™Ôºâ
    if query_candidates:
        original_query = max(query_candidates, key=len)
        return original_query
    
    return ""

def extract_table_size_estimates_from_plan(profiler_data: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
    """
    ÂÆüË°å„Éó„É©„É≥„Åã„Çâ„ÉÜ„Éº„Éñ„É´„Åî„Å®„ÅÆÊé®ÂÆö„Çµ„Ç§„Ç∫ÊÉÖÂ†±„ÇíÊäΩÂá∫
    
    Ê≥®ÊÑè: Databricks„ÇØ„Ç®„É™„Éó„É≠„Éï„Ç°„Ç§„É´„Å´„ÅØ estimatedSizeInBytes „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Å™„ÅÑ„Åü„ÇÅ„ÄÅ
    „Åì„ÅÆÊ©üËÉΩ„ÅØÁèæÂú®ÁÑ°ÂäπÂåñ„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„É°„Éà„É™„ÇØ„Çπ„Éô„Éº„Çπ„ÅÆÊé®ÂÆö„Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
    
    Args:
        profiler_data: „Éó„É≠„Éï„Ç°„Ç§„É©„Éº„Éá„Éº„Çø
        
    Returns:
        Dict: Á©∫„ÅÆËæûÊõ∏ÔºàÊ©üËÉΩÁÑ°ÂäπÂåñÔºâ
    """
    # Databricks„ÇØ„Ç®„É™„Éó„É≠„Éï„Ç°„Ç§„É´„Å´estimatedSizeInBytes„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Å™„ÅÑ„Åü„ÇÅÁÑ°ÂäπÂåñ
    return {}

def extract_table_name_from_scan_node(node: Dict[str, Any]) -> str:
    """
    „Çπ„Ç≠„É£„É≥„Éé„Éº„Éâ„Åã„Çâ„ÉÜ„Éº„Éñ„É´Âêç„ÇíÊäΩÂá∫
    
    Args:
        node: ÂÆüË°å„Éó„É©„É≥„ÅÆ„Éé„Éº„Éâ
        
    Returns:
        str: „ÉÜ„Éº„Éñ„É´Âêç
    """
    try:
        # Ë§áÊï∞„ÅÆÊñπÊ≥ï„Åß„ÉÜ„Éº„Éñ„É´Âêç„ÇíÊäΩÂá∫„ÇíË©¶Ë°å
        
        # 1. node output„Åã„Çâ„ÅÆÊäΩÂá∫
        output = node.get("output", "")
        if output:
            # „Éë„Çø„Éº„É≥: [col1#123, col2#456] table_name
            import re
            table_match = re.search(r'\]\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)', output)
            if table_match:
                return table_match.group(1)
        
        # 2. nodeË©≥Á¥∞„Åã„Çâ„ÅÆÊäΩÂá∫
        details = node.get("details", "")
        if details:
            # „Éë„Çø„Éº„É≥: Location: /path/to/table/name
            location_match = re.search(r'Location:.*?([a-zA-Z_][a-zA-Z0-9_]*)', details)
            if location_match:
                return location_match.group(1)
            
            # „Éë„Çø„Éº„É≥: Table: database.table_name
            table_match = re.search(r'Table:\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)', details)
            if table_match:
                return table_match.group(1)
        
        # 3. „É°„Çø„Éá„Éº„Çø„Åã„Çâ„ÅÆÊäΩÂá∫
        metadata = node.get("metadata", [])
        for meta in metadata:
            if meta.get("key") == "table" or meta.get("key") == "relation":
                values = meta.get("values", [])
                if values:
                    return str(values[0])
        
        # 4. nodeÂêç„Åã„Çâ„ÅÆÊé®Ê∏¨ÔºàÊúÄÂæå„ÅÆÊâãÊÆµÔºâ
        node_name = node.get("nodeName", "")
        if "delta" in node_name.lower():
            # Delta Scan „ÅÆÂ†¥Âêà„ÄÅË©≥Á¥∞ÊÉÖÂ†±„Åã„ÇâÊäΩÂá∫
            pass
    
    except Exception as e:
        print(f"‚ö†Ô∏è Error in table name extraction: {str(e)}")
    
    return None

def extract_broadcast_table_names(profiler_data: Dict[str, Any], broadcast_nodes: list) -> Dict[str, Any]:
    """
    BROADCAST„Éé„Éº„Éâ„Åã„ÇâÈñ¢ÈÄ£„Åô„Çã„ÉÜ„Éº„Éñ„É´Âêç„ÇíÊäΩÂá∫
    """
    broadcast_table_info = {
        "broadcast_tables": [],
        "broadcast_table_mapping": {},
        "broadcast_nodes_with_tables": []
    }
    
    # ÂÆüË°å„Éó„É©„É≥„ÅÆ„Ç∞„É©„ÉïÊÉÖÂ†±„ÇíÂèñÂæó
    graphs = profiler_data.get('graphs', [])
    if not graphs:
        return broadcast_table_info
    
    # ÂÖ®„Éé„Éº„Éâ„ÇíÂèéÈõÜ
    all_nodes = []
    for graph in graphs:
        nodes = graph.get('nodes', [])
        all_nodes.extend(nodes)
    
    # „Ç®„ÉÉ„Ç∏ÊÉÖÂ†±„ÇíÂèéÈõÜÔºà„Éé„Éº„ÉâÈñì„ÅÆÈñ¢‰øÇÔºâ
    all_edges = []
    for graph in graphs:
        edges = graph.get('edges', [])
        all_edges.extend(edges)
    
    # ÂêÑBROADCAST„Éé„Éº„Éâ„Å´„Å§„ÅÑ„Å¶Èñ¢ÈÄ£„Åô„Çã„ÉÜ„Éº„Éñ„É´„ÇíÁâπÂÆö
    for broadcast_node in broadcast_nodes:
        broadcast_node_id = broadcast_node.get('node_id', '')
        broadcast_node_name = broadcast_node.get('node_name', '')
        
        # BROADCAST„Éé„Éº„Éâ„Åã„ÇâÁõ¥Êé•„ÉÜ„Éº„Éñ„É´Âêç„ÇíÊäΩÂá∫
        table_names = set()
        
        # 1. „É°„Çø„Éá„Éº„Çø„Åã„Çâ„ÉÜ„Éº„Éñ„É´Âêç„ÇíÊäΩÂá∫
        metadata = broadcast_node.get('metadata', [])
        for meta in metadata:
            key = meta.get('key', '')
            value = meta.get('value', '')
            values = meta.get('values', [])
            
            # „ÉÜ„Éº„Éñ„É´Âêç„ÇíÁ§∫„Åô„Ç≠„Éº„Çí„ÉÅ„Çß„ÉÉ„ÇØ
            if key in ['SCAN_IDENTIFIER', 'TABLE_NAME', 'RELATION']:
                if value:
                    table_names.add(value)
                table_names.update(values)
        
        # 2. „Éé„Éº„ÉâÂêç„Åã„Çâ„ÉÜ„Éº„Éñ„É´Âêç„ÇíÊé®ÂÆö
        if 'SCAN' in broadcast_node_name:
            # "Broadcast Scan delta orders" ‚Üí "orders"
            import re
            table_match = re.search(r'SCAN\s+(?:DELTA|PARQUET|JSON|CSV)?\s*([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)', broadcast_node_name, re.IGNORECASE)
            if table_match:
                table_names.add(table_match.group(1))
        
        # 3. „Ç®„ÉÉ„Ç∏ÊÉÖÂ†±„Åã„ÇâÈñ¢ÈÄ£„Åô„Çã„Çπ„Ç≠„É£„É≥„Éé„Éº„Éâ„ÇíÁâπÂÆö
        for edge in all_edges:
            source_id = edge.get('source', '')
            target_id = edge.get('target', '')
            
            # BROADCAST„Éé„Éº„Éâ„Å´ÂÖ•Âäõ„Åï„Çå„Çã„Éé„Éº„Éâ„ÇíÊ§úÁ¥¢
            if target_id == broadcast_node_id:
                # ÂÖ•Âäõ„Éé„Éº„Éâ„Åå„Çπ„Ç≠„É£„É≥„Éé„Éº„Éâ„Åã„ÉÅ„Çß„ÉÉ„ÇØ
                for node in all_nodes:
                    if node.get('id', '') == source_id:
                        node_name = node.get('name', '').upper()
                        if any(keyword in node_name for keyword in ['SCAN', 'FILESCAN']):
                            # „Çπ„Ç≠„É£„É≥„Éé„Éº„Éâ„Åã„Çâ„ÉÜ„Éº„Éñ„É´Âêç„ÇíÊäΩÂá∫
                            scan_table_name = extract_table_name_from_scan_node(node)
                            if scan_table_name:
                                table_names.add(scan_table_name)
        
        # 4. Âêå„Åò„Ç∞„É©„ÉïÂÜÖ„ÅÆ„Çπ„Ç≠„É£„É≥„Éé„Éº„Éâ„Å®„ÅÆÈñ¢ÈÄ£‰ªò„Åë
        for node in all_nodes:
            node_name = node.get('name', '').upper()
            if any(keyword in node_name for keyword in ['SCAN', 'FILESCAN']):
                # „Çπ„Ç≠„É£„É≥„Éé„Éº„Éâ„ÅÆÂêçÂâç„ÅåBROADCAST„Éé„Éº„ÉâÂêç„Å´Âê´„Åæ„Çå„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ
                scan_table_name = extract_table_name_from_scan_node(node)
                if scan_table_name:
                    # „ÉÜ„Éº„Éñ„É´Âêç„ÅÆÈÉ®ÂàÜ‰∏ÄËá¥„Çí„ÉÅ„Çß„ÉÉ„ÇØ
                    if any(part in broadcast_node_name for part in scan_table_name.split('.') if len(part) > 2):
                        table_names.add(scan_table_name)
        
        # ÁµêÊûú„ÇíË®òÈå≤
        table_names_list = list(table_names)
        if table_names_list:
            broadcast_table_info["broadcast_tables"].extend(table_names_list)
            broadcast_table_info["broadcast_table_mapping"][broadcast_node_id] = table_names_list
            
            # BROADCAST„Éé„Éº„ÉâÊÉÖÂ†±„ÇíÊã°Âºµ
            enhanced_broadcast_node = broadcast_node.copy()
            enhanced_broadcast_node["associated_tables"] = table_names_list
            enhanced_broadcast_node["table_count"] = len(table_names_list)
            broadcast_table_info["broadcast_nodes_with_tables"].append(enhanced_broadcast_node)
    
    # ÈáçË§á„ÇíÈô§Âéª
    broadcast_table_info["broadcast_tables"] = list(set(broadcast_table_info["broadcast_tables"]))
    
    return broadcast_table_info

def extract_execution_plan_info(profiler_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    JSON„É°„Éà„É™„ÇØ„Çπ„Åã„ÇâÂÆüË°å„Éó„É©„É≥ÊÉÖÂ†±„ÇíÊäΩÂá∫
    """
    plan_info = {
        "broadcast_nodes": [],
        "join_nodes": [],
        "scan_nodes": [],
        "shuffle_nodes": [],
        "aggregate_nodes": [],
        "plan_summary": {},
        "broadcast_already_applied": False,
        "join_strategies": [],
        "table_scan_details": {},
        "broadcast_table_info": {}
    }
    
    # „Éó„É≠„Éï„Ç°„Ç§„É©„Éº„Éá„Éº„Çø„Åã„ÇâÂÆüË°å„Ç∞„É©„ÉïÊÉÖÂ†±„ÇíÂèñÂæó
    graphs = profiler_data.get('graphs', [])
    if not graphs:
        return plan_info
    
    # „Åô„Åπ„Å¶„ÅÆ„Ç∞„É©„Éï„Åã„Çâ„Éé„Éº„Éâ„ÇíÂèéÈõÜ
    all_nodes = []
    for graph_index, graph in enumerate(graphs):
        nodes = graph.get('nodes', [])
        for node in nodes:
            node['graph_index'] = graph_index
            all_nodes.append(node)
    
    # „Éé„Éº„ÉâÂàÜÊûê
    for node in all_nodes:
        node_name = node.get('name', '').upper()
        node_tag = node.get('tag', '').upper()
        node_metadata = node.get('metadata', [])
        
        # BROADCAST„Éé„Éº„Éâ„ÅÆÊ§úÂá∫
        if 'BROADCAST' in node_name or 'BROADCAST' in node_tag:
            plan_info["broadcast_already_applied"] = True
            broadcast_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "metadata": []
            }
            
            # BROADCAST„Å´Èñ¢ÈÄ£„Åô„Çã„É°„Çø„Éá„Éº„Çø„ÇíÊäΩÂá∫
            for meta in node_metadata:
                key = meta.get('key', '')
                value = meta.get('value', '')
                values = meta.get('values', [])
                
                if any(keyword in key.upper() for keyword in ['BROADCAST', 'BUILD', 'PROBE']):
                    broadcast_info["metadata"].append({
                        "key": key,
                        "value": value,
                        "values": values
                    })
            
            plan_info["broadcast_nodes"].append(broadcast_info)
        
        # JOIN„Éé„Éº„Éâ„ÅÆÊ§úÂá∫„Å®Êà¶Áï•ÂàÜÊûê
        elif any(keyword in node_name for keyword in ['JOIN', 'HASH']):
            join_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "join_strategy": "unknown",
                "join_keys": [],
                "join_type": "unknown"
            }
            
            # JOINÊà¶Áï•„ÅÆÁâπÂÆö
            if 'BROADCAST' in node_name:
                join_info["join_strategy"] = "broadcast_hash_join"
            elif 'SORT' in node_name and 'MERGE' in node_name:
                join_info["join_strategy"] = "sort_merge_join"
            elif 'HASH' in node_name:
                join_info["join_strategy"] = "shuffle_hash_join"
            elif 'NESTED' in node_name:
                join_info["join_strategy"] = "broadcast_nested_loop_join"
            
            # JOIN„Çø„Ç§„Éó„ÅÆÁâπÂÆö
            if 'INNER' in node_name:
                join_info["join_type"] = "inner"
            elif 'LEFT' in node_name:
                join_info["join_type"] = "left"
            elif 'RIGHT' in node_name:
                join_info["join_type"] = "right"
            elif 'OUTER' in node_name:
                join_info["join_type"] = "outer"
            
            # JOINÊù°‰ª∂„ÅÆÊäΩÂá∫
            for meta in node_metadata:
                key = meta.get('key', '')
                values = meta.get('values', [])
                
                if key in ['LEFT_KEYS', 'RIGHT_KEYS']:
                    join_info["join_keys"].extend(values)
            
            plan_info["join_nodes"].append(join_info)
            plan_info["join_strategies"].append(join_info["join_strategy"])
        
        # „Çπ„Ç≠„É£„É≥„Éé„Éº„Éâ„ÅÆË©≥Á¥∞ÂàÜÊûê
        elif any(keyword in node_name for keyword in ['SCAN', 'FILESCAN']):
            scan_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "table_name": "unknown",
                "file_format": "unknown",
                "pushed_filters": [],
                "output_columns": []
            }
            
            # „ÉÜ„Éº„Éñ„É´Âêç„Å®„Éï„Ç°„Ç§„É´ÂΩ¢Âºè„ÅÆÊäΩÂá∫
            for meta in node_metadata:
                key = meta.get('key', '')
                value = meta.get('value', '')
                values = meta.get('values', [])
                
                if key == 'SCAN_IDENTIFIER':
                    scan_info["table_name"] = value
                elif key == 'OUTPUT':
                    scan_info["output_columns"] = values
                elif key == 'PUSHED_FILTERS' or key == 'FILTERS':
                    scan_info["pushed_filters"] = values
            
            # „Éï„Ç°„Ç§„É´ÂΩ¢Âºè„ÅÆÊé®ÂÆö
            if 'DELTA' in node_name:
                scan_info["file_format"] = "delta"
            elif 'PARQUET' in node_name:
                scan_info["file_format"] = "parquet"
            elif 'JSON' in node_name:
                scan_info["file_format"] = "json"
            elif 'CSV' in node_name:
                scan_info["file_format"] = "csv"
            
            plan_info["scan_nodes"].append(scan_info)
            plan_info["table_scan_details"][scan_info["table_name"]] = scan_info
        
        # „Ç∑„É£„ÉÉ„Éï„É´„Éé„Éº„Éâ„ÅÆÊ§úÂá∫
        elif any(keyword in node_name for keyword in ['SHUFFLE', 'EXCHANGE']):
            shuffle_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "partition_keys": []
            }
            
            # „Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥ÊÉÖÂ†±„ÅÆÊäΩÂá∫
            for meta in node_metadata:
                key = meta.get('key', '')
                values = meta.get('values', [])
                
                if key in ['PARTITION_EXPRESSIONS', 'PARTITION_KEYS']:
                    shuffle_info["partition_keys"] = values
            
            plan_info["shuffle_nodes"].append(shuffle_info)
        
        # ÈõÜÁ¥Ñ„Éé„Éº„Éâ„ÅÆÊ§úÂá∫
        elif any(keyword in node_name for keyword in ['AGGREGATE', 'GROUP']):
            agg_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "group_keys": [],
                "aggregate_expressions": []
            }
            
            # ÈõÜÁ¥ÑÊÉÖÂ†±„ÅÆÊäΩÂá∫
            for meta in node_metadata:
                key = meta.get('key', '')
                values = meta.get('values', [])
                
                if key == 'GROUPING_EXPRESSIONS':
                    agg_info["group_keys"] = values
                elif key == 'AGGREGATE_EXPRESSIONS':
                    agg_info["aggregate_expressions"] = values
            
            plan_info["aggregate_nodes"].append(agg_info)
    
    # „Éó„É©„É≥„Çµ„Éû„É™„Éº„ÅÆÁîüÊàê
    plan_info["plan_summary"] = {
        "total_nodes": len(all_nodes),
        "broadcast_nodes_count": len(plan_info["broadcast_nodes"]),
        "join_nodes_count": len(plan_info["join_nodes"]),
        "scan_nodes_count": len(plan_info["scan_nodes"]),
        "shuffle_nodes_count": len(plan_info["shuffle_nodes"]),
        "aggregate_nodes_count": len(plan_info["aggregate_nodes"]),
        "unique_join_strategies": list(set(plan_info["join_strategies"])),
        "has_broadcast_joins": plan_info["broadcast_already_applied"],
        "tables_scanned": len(plan_info["table_scan_details"])
    }
    
    # BROADCAST„ÉÜ„Éº„Éñ„É´ÊÉÖÂ†±„ÇíÊäΩÂá∫
    if plan_info["broadcast_nodes"]:
        broadcast_table_info = extract_broadcast_table_names(profiler_data, plan_info["broadcast_nodes"])
        plan_info["broadcast_table_info"] = broadcast_table_info
        
        # „Éó„É©„É≥„Çµ„Éû„É™„Éº„Å´BROADCAST„ÉÜ„Éº„Éñ„É´ÊÉÖÂ†±„ÇíËøΩÂä†
        plan_info["plan_summary"]["broadcast_tables"] = broadcast_table_info["broadcast_tables"]
        plan_info["plan_summary"]["broadcast_table_count"] = len(broadcast_table_info["broadcast_tables"])
    
    # ÂÆüË°å„Éó„É©„É≥„Åã„Çâ„ÅÆ„ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫Êé®ÂÆöÊÉÖÂ†±„ÇíËøΩÂä†ÔºàestimatedSizeInBytesÂà©Áî®‰∏çÂèØ„ÅÆ„Åü„ÇÅÁÑ°ÂäπÂåñÔºâ
    plan_info["table_size_estimates"] = {}  # extract_table_size_estimates_from_plan(profiler_data)
    
    return plan_info

def get_spark_broadcast_threshold() -> float:
    """
    Spark„ÅÆÂÆüÈöõ„ÅÆbroadcastÈñæÂÄ§Ë®≠ÂÆö„ÇíÂèñÂæó
    """
    try:
        # Spark„ÅÆË®≠ÂÆöÂÄ§„ÇíÂèñÂæó
        threshold_bytes = spark.conf.get("spark.databricks.optimizer.autoBroadcastJoinThreshold", "31457280")  # „Éá„Éï„Ç©„É´„Éà30MB
        threshold_mb = float(threshold_bytes) / 1024 / 1024
        return threshold_mb
    except:
        # ÂèñÂæó„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÅØÊ®ôÊ∫ñÁöÑ„Å™30MB„ÇíËøî„Åô
        return 30.0

def estimate_uncompressed_size(compressed_size_mb: float, file_format: str = "parquet") -> float:
    """
    ÂúßÁ∏Æ„Çµ„Ç§„Ç∫„Åã„ÇâÈùûÂúßÁ∏Æ„Çµ„Ç§„Ç∫„ÇíÊé®ÂÆöÔºà3.0ÂÄçÂõ∫ÂÆöÔºâ
    
    Ê≥®ÊÑè: ÂÆüÈöõ„ÅÆestimatedSizeInBytes„ÅåÂà©Áî®„Åß„Åç„Å™„ÅÑ„Åü„ÇÅ„ÄÅ
    ‰øùÂÆàÁöÑ„Å™3.0ÂÄçÂúßÁ∏ÆÁéá„ÅßÁµ±‰∏Ä„Åó„Å¶Êé®ÂÆö„Åó„Åæ„Åô„ÄÇ
    """
    # ‰øùÂÆàÁöÑ„Å™3.0ÂÄçÂúßÁ∏ÆÁéá„ÅßÁµ±‰∏ÄÔºàestimatedSizeInBytesÂà©Áî®‰∏çÂèØ„ÅÆ„Åü„ÇÅÔºâ
    compression_ratio = 3.0
    
    return compressed_size_mb * compression_ratio

def analyze_broadcast_feasibility(metrics: Dict[str, Any], original_query: str, plan_info: Dict[str, Any] = None) -> Dict[str, Any]:
    """
    BROADCAST„Éí„É≥„Éà„ÅÆÈÅ©Áî®ÂèØËÉΩÊÄß„ÇíÂàÜÊûêÔºàÊ≠£Á¢∫„Å™30MBÈñæÂÄ§ÈÅ©Áî®Ôºâ
    """
    broadcast_analysis = {
        "is_join_query": False,
        "broadcast_candidates": [],
        "recommendations": [],
        "feasibility": "not_applicable",
        "reasoning": [],
        "spark_threshold_mb": get_spark_broadcast_threshold(),
        "compression_analysis": {},
        "detailed_size_analysis": [],
        "execution_plan_analysis": {},
        "existing_broadcast_nodes": [],
        "already_optimized": False,
        "broadcast_applied_tables": []
    }
    
    # „ÇØ„Ç®„É™„Å´JOIN„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ
    query_upper = original_query.upper()
    join_types = ['JOIN', 'INNER JOIN', 'LEFT JOIN', 'RIGHT JOIN', 'LEFT OUTER JOIN', 'RIGHT OUTER JOIN', 'SEMI JOIN', 'ANTI JOIN']
    has_join = any(join_type in query_upper for join_type in join_types)
    
    if not has_join:
        broadcast_analysis["reasoning"].append("JOIN„ÇØ„Ç®„É™„Åß„ÅØ„Å™„ÅÑ„Åü„ÇÅ„ÄÅBROADCAST„Éí„É≥„Éà„ÅØÈÅ©Áî®‰∏çÂèØ")
        return broadcast_analysis
    
    broadcast_analysis["is_join_query"] = True
    broadcast_analysis["reasoning"].append(f"Spark BROADCASTÈñæÂÄ§: {broadcast_analysis['spark_threshold_mb']:.1f}MBÔºàÈùûÂúßÁ∏ÆÔºâ")
    
    # ÂÆüË°å„Éó„É©„É≥ÊÉÖÂ†±„ÅÆÂàÜÊûê
    if plan_info:
        plan_summary = plan_info.get("plan_summary", {})
        broadcast_nodes = plan_info.get("broadcast_nodes", [])
        join_nodes = plan_info.get("join_nodes", [])
        table_scan_details = plan_info.get("table_scan_details", {})
        table_size_estimates = plan_info.get("table_size_estimates", {})
        
        # Êó¢Â≠ò„ÅÆBROADCASTÈÅ©Áî®Áä∂Ê≥Å„ÅÆË®òÈå≤
        broadcast_analysis["existing_broadcast_nodes"] = broadcast_nodes
        broadcast_analysis["already_optimized"] = len(broadcast_nodes) > 0
        
        # „Éó„É©„É≥ÂàÜÊûêÁµêÊûú„ÅÆË®òÈå≤
        broadcast_analysis["execution_plan_analysis"] = {
            "has_broadcast_joins": plan_summary.get("has_broadcast_joins", False),
            "unique_join_strategies": plan_summary.get("unique_join_strategies", []),
            "broadcast_nodes_count": len(broadcast_nodes),
            "join_nodes_count": len(join_nodes),
            "scan_nodes_count": plan_summary.get("scan_nodes_count", 0),
            "shuffle_nodes_count": plan_summary.get("shuffle_nodes_count", 0),
            "tables_in_plan": list(table_scan_details.keys())
        }
        
        # Êó¢„Å´BROADCAST„ÅåÈÅ©Áî®„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅÆË©≥Á¥∞Ë®òÈå≤
        if broadcast_nodes:
            broadcast_analysis["reasoning"].append(f"‚úÖ ÂÆüË°å„Éó„É©„É≥„ÅßÊó¢„Å´BROADCAST JOIN„ÅåÈÅ©Áî®Ê∏à„Åø: {len(broadcast_nodes)}ÂÄã„ÅÆ„Éé„Éº„Éâ")
            
            # BROADCAST„ÉÜ„Éº„Éñ„É´ÊÉÖÂ†±„ÇíÂèñÂæó
            broadcast_table_info = plan_info.get("broadcast_table_info", {})
            broadcast_tables = broadcast_table_info.get("broadcast_tables", [])
            
            if broadcast_tables:
                broadcast_analysis["reasoning"].append(f"üìã BROADCAST„Åï„Çå„Å¶„ÅÑ„Çã„ÉÜ„Éº„Éñ„É´: {', '.join(broadcast_tables)}")
                broadcast_analysis["broadcast_applied_tables"] = broadcast_tables
                
                # ÂêÑBROADCAST„Éé„Éº„Éâ„ÅÆË©≥Á¥∞
                broadcast_nodes_with_tables = broadcast_table_info.get("broadcast_nodes_with_tables", [])
                for i, node in enumerate(broadcast_nodes_with_tables[:3]):  # ÊúÄÂ§ß3ÂÄã„Åæ„ÅßË°®Á§∫
                    node_name_short = node['node_name'][:50] + "..." if len(node['node_name']) > 50 else node['node_name']
                    associated_tables = node.get('associated_tables', [])
                    if associated_tables:
                        broadcast_analysis["reasoning"].append(f"  ‚Ä¢ BROADCAST Node {i+1}: {node_name_short}")
                        broadcast_analysis["reasoning"].append(f"    ‚îî‚îÄ „ÉÜ„Éº„Éñ„É´: {', '.join(associated_tables)}")
                    else:
                        broadcast_analysis["reasoning"].append(f"  ‚Ä¢ BROADCAST Node {i+1}: {node_name_short} („ÉÜ„Éº„Éñ„É´ÂêçÊú™ÁâπÂÆö)")
            else:
                # BROADCAST„Éé„Éº„Éâ„ÅØÂ≠òÂú®„Åô„Çã„Åå„ÉÜ„Éº„Éñ„É´Âêç„ÅåÁâπÂÆö„Åß„Åç„Å™„ÅÑÂ†¥Âêà
                for i, node in enumerate(broadcast_nodes[:3]):  # ÊúÄÂ§ß3ÂÄã„Åæ„ÅßË°®Á§∫
                    broadcast_analysis["reasoning"].append(f"  ‚Ä¢ BROADCAST Node {i+1}: {node['node_name'][:50]}... („ÉÜ„Éº„Éñ„É´ÂêçËß£Êûê‰∏≠)")
        else:
            # BROADCASTÊú™ÈÅ©Áî®„Å†„Åå„ÄÅJOIN„ÅåÂ≠òÂú®„Åô„ÇãÂ†¥Âêà
            if join_nodes:
                join_strategies = set(node["join_strategy"] for node in join_nodes)
                broadcast_analysis["reasoning"].append(f"üîç ÁèæÂú®„ÅÆJOINÊà¶Áï•: {', '.join(join_strategies)}")
                broadcast_analysis["reasoning"].append("üí° BROADCASTÊúÄÈÅ©Âåñ„ÅÆÊ©ü‰ºö„ÇíÊ§úË®é‰∏≠...")
    else:
        broadcast_analysis["reasoning"].append("‚ö†Ô∏è ÂÆüË°å„Éó„É©„É≥ÊÉÖÂ†±„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì - „É°„Éà„É™„ÇØ„ÇπÊé®ÂÆö„Å´Âü∫„Å•„ÅèÂàÜÊûê„ÇíÂÆüË°å")
    
    # „É°„Éà„É™„ÇØ„Çπ„Åã„Çâ„ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫ÊÉÖÂ†±„ÇíÂèñÂæó
    overall_metrics = metrics.get('overall_metrics', {})
    node_metrics = metrics.get('node_metrics', [])
    
    # „Çπ„Ç≠„É£„É≥„Éé„Éº„Éâ„Åã„Çâ„ÉÜ„Éº„Éñ„É´ÊÉÖÂ†±„ÇíÊäΩÂá∫
    scan_nodes = []
    total_compressed_bytes = 0
    total_rows_all_tables = 0
    
    for node in node_metrics:
        node_name = node.get('name', '').upper()
        if any(keyword in node_name for keyword in ['SCAN', 'FILESCAN', 'PARQUET', 'DELTA']):
            key_metrics = node.get('key_metrics', {})
            rows_num = key_metrics.get('rowsNum', 0)
            duration_ms = key_metrics.get('durationMs', 0)
            
            # „Éï„Ç°„Ç§„É´ÂΩ¢Âºè„ÅÆÊé®ÂÆöÔºà„Éó„É©„É≥ÊÉÖÂ†±„ÇíÂÑ™ÂÖàÔºâ
            file_format = "parquet"  # „Éá„Éï„Ç©„É´„Éà
            table_name_from_plan = "unknown"
            
            # „Éó„É©„É≥ÊÉÖÂ†±„Åã„Çâ„ÉÜ„Éº„Éñ„É´Âêç„Å®„Éï„Ç°„Ç§„É´ÂΩ¢Âºè„ÇíÂèñÂæó
            if plan_info and plan_info.get("table_scan_details"):
                # „É°„Çø„Éá„Éº„Çø„Åã„ÇâË©≥Á¥∞„Å™„ÉÜ„Éº„Éñ„É´Âêç„ÇíÊäΩÂá∫
                node_metadata = node.get('metadata', [])
                for meta in node_metadata:
                    meta_key = meta.get('key', '')
                    meta_value = meta.get('value', '')
                    if meta_key in ['SCAN_IDENTIFIER', 'SCAN_TABLE', 'TABLE_NAME'] and meta_value:
                        # „Éó„É©„É≥„ÅÆË©≥Á¥∞„Å®ÁÖßÂêà
                        for plan_table, scan_detail in plan_info["table_scan_details"].items():
                            if meta_value in plan_table or plan_table in meta_value:
                                table_name_from_plan = plan_table
                                if scan_detail["file_format"] != "unknown":
                                    file_format = scan_detail["file_format"]
                                break
                        break
            
            # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: „Éé„Éº„ÉâÂêç„Åã„Çâ„Éï„Ç°„Ç§„É´ÂΩ¢Âºè„ÇíÊé®ÂÆö
            if file_format == "parquet":  # „Åæ„Å†„Éá„Éï„Ç©„É´„Éà„ÅÆÂ†¥Âêà
                if "DELTA" in node_name:
                    file_format = "delta"
                elif "PARQUET" in node_name:
                    file_format = "parquet"
                elif "JSON" in node_name:
                    file_format = "json"
                elif "CSV" in node_name:
                    file_format = "csv"
            
            # „É°„Éà„É™„ÇØ„Çπ„Éô„Éº„ÇπÊé®ÂÆö„ÅÆ„Åø‰ΩøÁî®ÔºàestimatedSizeInBytesÂà©Áî®‰∏çÂèØ„ÅÆ„Åü„ÇÅÔºâ
            estimated_compressed_mb = 0
            estimated_uncompressed_mb = 0
            size_source = "metrics_estimation"
            
            # „É°„Éà„É™„ÇØ„Çπ„Éô„Éº„ÇπÊé®ÂÆö
            total_read_bytes = overall_metrics.get('read_bytes', 0)
            total_rows = overall_metrics.get('rows_read_count', 0)
            
            if total_rows > 0 and total_read_bytes > 0 and rows_num > 0:
                # ÂÖ®‰Ωì„ÅÆË™≠„ÅøËæº„ÅøÈáè„Åã„Çâ„Åì„ÅÆ„ÉÜ„Éº„Éñ„É´„ÅÆÂâ≤Âêà„ÇíË®àÁÆó
                table_ratio = rows_num / total_rows
                estimated_compressed_bytes = total_read_bytes * table_ratio
                estimated_compressed_mb = estimated_compressed_bytes / 1024 / 1024
                 
                # ÈùûÂúßÁ∏Æ„Çµ„Ç§„Ç∫„ÇíÊé®ÂÆö
                estimated_uncompressed_mb = estimate_uncompressed_size(estimated_compressed_mb, file_format)
            else:
                # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: Ë°åÊï∞„Éô„Éº„Çπ„ÅÆÊé®ÂÆöÔºà‰øùÂÆàÁöÑÔºâ
                # Âπ≥ÂùáË°å„Çµ„Ç§„Ç∫„ÇíÊé®ÂÆöÔºàÈùûÂúßÁ∏ÆÔºâ
                if total_rows > 0 and total_read_bytes > 0:
                    # ÂÖ®‰Ωì„Éá„Éº„Çø„Åã„ÇâÂúßÁ∏ÆÂæå„ÅÆÂπ≥ÂùáË°å„Çµ„Ç§„Ç∫„ÇíË®àÁÆó
                    compressed_avg_row_size = total_read_bytes / total_rows
                    # ÂúßÁ∏ÆÁéá„ÇíËÄÉÊÖÆ„Åó„Å¶ÈùûÂúßÁ∏Æ„Çµ„Ç§„Ç∫„ÇíÊé®ÂÆö
                    uncompressed_avg_row_size = compressed_avg_row_size * estimate_uncompressed_size(1.0, file_format)
                else:
                    # ÂÆåÂÖ®„Å™„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: ‰∏ÄËà¨ÁöÑ„Å™ÈùûÂúßÁ∏ÆË°å„Çµ„Ç§„Ç∫Ôºà1KBÔºâ
                    uncompressed_avg_row_size = 1024
                
                estimated_compressed_mb = (rows_num * compressed_avg_row_size) / 1024 / 1024 if 'compressed_avg_row_size' in locals() else 0
                estimated_uncompressed_mb = (rows_num * uncompressed_avg_row_size) / 1024 / 1024
            
            # Êó¢Â≠ò„ÅÆBROADCASTÈÅ©Áî®Áä∂Ê≥Å„Çí„ÉÅ„Çß„ÉÉ„ÇØ
            is_already_broadcasted = False
            if plan_info and plan_info.get("broadcast_nodes"):
                for broadcast_node in plan_info["broadcast_nodes"]:
                    # „ÉÜ„Éº„Éñ„É´Âêç„ÅÆÈÉ®ÂàÜ‰∏ÄËá¥„Çí„ÉÅ„Çß„ÉÉ„ÇØ
                    broadcast_node_name = broadcast_node["node_name"]
                    if (table_name_from_plan != "unknown" and 
                        any(part in broadcast_node_name for part in table_name_from_plan.split('.') if len(part) > 3)):
                        is_already_broadcasted = True
                        break
                    # „Éé„Éº„ÉâÂêç„Åß„ÅÆÁÖßÂêà
                    elif any(part in broadcast_node_name for part in node_name.split() if len(part) > 3):
                        is_already_broadcasted = True
                        break

            scan_info = {
                "node_name": node_name,
                "table_name_from_plan": table_name_from_plan,
                "rows": rows_num,
                "duration_ms": duration_ms,
                "estimated_compressed_mb": estimated_compressed_mb,
                "estimated_uncompressed_mb": estimated_uncompressed_mb,
                "file_format": file_format,
                "compression_ratio": 3.0,  # Âõ∫ÂÆö3.0ÂÄçÂúßÁ∏ÆÁéá
                "node_id": node.get('node_id', ''),
                "is_already_broadcasted": is_already_broadcasted,
                "size_estimation_source": size_source,
                "size_confidence": "medium"  # „É°„Éà„É™„ÇØ„Çπ„Éô„Éº„ÇπÊé®ÂÆö„ÅÆ„Åü„ÇÅ‰∏≠Á®ãÂ∫¶‰ø°È†ºÂ∫¶
            }
            scan_nodes.append(scan_info)
            
            total_compressed_bytes += estimated_compressed_bytes if 'estimated_compressed_bytes' in locals() else 0
            total_rows_all_tables += rows_num
    
    # BROADCASTÂÄôË£ú„ÅÆÂà§ÂÆöÔºà30MBÈñæÂÄ§‰ΩøÁî®Ôºâ
    broadcast_threshold_mb = broadcast_analysis["spark_threshold_mb"]  # ÂÆüÈöõ„ÅÆSparkË®≠ÂÆöÂÄ§
    broadcast_safe_mb = broadcast_threshold_mb * 0.8  # ÂÆâÂÖ®„Éû„Éº„Ç∏„É≥Ôºà80%Ôºâ
    broadcast_max_mb = broadcast_threshold_mb * 10    # Êòé„Çâ„Åã„Å´Â§ß„Åç„Åô„Åé„ÇãÈñæÂÄ§
    
    small_tables = []
    large_tables = []
    marginal_tables = []
    
    # ÂúßÁ∏ÆÂàÜÊûê„ÅÆË®òÈå≤
    broadcast_analysis["compression_analysis"] = {
        "total_compressed_gb": total_compressed_bytes / 1024 / 1024 / 1024 if total_compressed_bytes > 0 else 0,
        "total_rows": total_rows_all_tables,
        "avg_compression_ratio": 0
    }
    
    for scan in scan_nodes:
        uncompressed_size_mb = scan["estimated_uncompressed_mb"]
        compressed_size_mb = scan["estimated_compressed_mb"]
        
        # Ë©≥Á¥∞„Çµ„Ç§„Ç∫ÂàÜÊûê„ÅÆË®òÈå≤
        table_display_name = scan.get("table_name_from_plan", scan["node_name"])
        is_already_broadcasted = scan.get("is_already_broadcasted", False)
        
        size_analysis = {
            "table": table_display_name,
            "node_name": scan["node_name"],
            "rows": scan["rows"],
            "compressed_mb": compressed_size_mb,
            "uncompressed_mb": uncompressed_size_mb,
            "file_format": scan["file_format"],
            "compression_ratio": scan["compression_ratio"],
            "broadcast_decision": "",
            "decision_reasoning": "",
            "is_already_broadcasted": is_already_broadcasted
        }
        
        # 30MBÈñæÂÄ§„Åß„ÅÆÂà§ÂÆöÔºàÈùûÂúßÁ∏Æ„Çµ„Ç§„Ç∫Ôºâ- Êó¢Â≠òÈÅ©Áî®Áä∂Ê≥Å„ÇíËÄÉÊÖÆ
        if is_already_broadcasted:
            # Êó¢„Å´BROADCAST„ÅåÈÅ©Áî®Ê∏à„Åø
            small_tables.append(scan)  # Áµ±Ë®àÁõÆÁöÑ„ÅßË®òÈå≤
            size_analysis["broadcast_decision"] = "already_applied"
            size_analysis["decision_reasoning"] = f"Êó¢„Å´BROADCASTÈÅ©Áî®Ê∏à„ÅøÔºàÊé®ÂÆö„Çµ„Ç§„Ç∫: ÈùûÂúßÁ∏Æ{uncompressed_size_mb:.1f}MBÔºâ"
            broadcast_analysis["broadcast_candidates"].append({
                "table": table_display_name,
                "estimated_uncompressed_mb": uncompressed_size_mb,
                "estimated_compressed_mb": compressed_size_mb,
                "rows": scan["rows"],
                "file_format": scan["file_format"],
                "compression_ratio": scan["compression_ratio"],
                "broadcast_feasible": True,
                "confidence": "confirmed",
                "status": "already_applied",
                "reasoning": f"ÂÆüË°å„Éó„É©„É≥„ÅßÊó¢„Å´BROADCASTÈÅ©Áî®Á¢∫Ë™çÊ∏à„ÅøÔºàÊé®ÂÆö„Çµ„Ç§„Ç∫: ÈùûÂúßÁ∏Æ{uncompressed_size_mb:.1f}MB„ÄÅ„É°„Éà„É™„ÇØ„Çπ„Éô„Éº„ÇπÊé®ÂÆöÔºâ"
            })
        elif uncompressed_size_mb <= broadcast_safe_mb and scan["rows"] > 0:
            # ÂÆâÂÖ®„Éû„Éº„Ç∏„É≥ÂÜÖÔºà24MB‰ª•‰∏ãÔºâ- Âº∑„ÅèÊé®Â•®
            small_tables.append(scan)
            size_analysis["broadcast_decision"] = "strongly_recommended"
            size_analysis["decision_reasoning"] = f"ÈùûÂúßÁ∏Æ{uncompressed_size_mb:.1f}MB ‚â§ ÂÆâÂÖ®ÈñæÂÄ§{broadcast_safe_mb:.1f}MB"
            broadcast_analysis["broadcast_candidates"].append({
                "table": table_display_name,
                "estimated_uncompressed_mb": uncompressed_size_mb,
                "estimated_compressed_mb": compressed_size_mb,
                "rows": scan["rows"],
                "file_format": scan["file_format"],
                "compression_ratio": scan["compression_ratio"],
                "broadcast_feasible": True,
                "confidence": "high",
                "status": "new_recommendation",
                "reasoning": f"ÈùûÂúßÁ∏ÆÊé®ÂÆö„Çµ„Ç§„Ç∫ {uncompressed_size_mb:.1f}MBÔºàÂÆâÂÖ®ÈñæÂÄ§ {broadcast_safe_mb:.1f}MB ‰ª•‰∏ãÔºâ„ÅßBROADCASTÂº∑„ÅèÊé®Â•®Ôºà„É°„Éà„É™„ÇØ„Çπ„Éô„Éº„ÇπÊé®ÂÆö„ÄÅ3.0ÂÄçÂúßÁ∏ÆÁéáÔºâ"
            })
        elif uncompressed_size_mb <= broadcast_threshold_mb and scan["rows"] > 0:
            # ÈñæÂÄ§ÂÜÖ„Å†„ÅåÂÆâÂÖ®„Éû„Éº„Ç∏„É≥„ÅØË∂ÖÈÅéÔºà24-30MBÔºâ- Êù°‰ª∂‰ªò„ÅçÊé®Â•®
            marginal_tables.append(scan)
            size_analysis["broadcast_decision"] = "conditionally_recommended"
            size_analysis["decision_reasoning"] = f"ÈùûÂúßÁ∏Æ{uncompressed_size_mb:.1f}MB ‚â§ ÈñæÂÄ§{broadcast_threshold_mb:.1f}MBÔºàÂÆâÂÖ®„Éû„Éº„Ç∏„É≥Ë∂ÖÈÅéÔºâ"
            broadcast_analysis["broadcast_candidates"].append({
                "table": table_display_name,
                "estimated_uncompressed_mb": uncompressed_size_mb,
                "estimated_compressed_mb": compressed_size_mb,
                "rows": scan["rows"],
                "file_format": scan["file_format"],
                "compression_ratio": scan["compression_ratio"],
                "broadcast_feasible": True,
                "confidence": "medium",
                "status": "new_recommendation",
                "reasoning": f"ÈùûÂúßÁ∏ÆÊé®ÂÆö„Çµ„Ç§„Ç∫ {uncompressed_size_mb:.1f}MBÔºàÈñæÂÄ§ {broadcast_threshold_mb:.1f}MB ‰ª•‰∏ã„Å†„ÅåÂÆâÂÖ®„Éû„Éº„Ç∏„É≥ {broadcast_safe_mb:.1f}MB Ë∂ÖÈÅéÔºâ„ÅßÊù°‰ª∂‰ªò„ÅçBROADCASTÊé®Â•®Ôºà„É°„Éà„É™„ÇØ„Çπ„Éô„Éº„ÇπÊé®ÂÆö„ÄÅ3.0ÂÄçÂúßÁ∏ÆÁéáÔºâ"
            })
        elif uncompressed_size_mb > broadcast_max_mb:
            # Êòé„Çâ„Åã„Å´Â§ß„Åç„Åô„Åé„ÇãÔºà300MBË∂ÖÔºâ
            large_tables.append(scan)
            size_analysis["broadcast_decision"] = "not_recommended"
            size_analysis["decision_reasoning"] = f"ÈùûÂúßÁ∏Æ{uncompressed_size_mb:.1f}MB > ÊúÄÂ§ßÈñæÂÄ§{broadcast_max_mb:.1f}MB"
            broadcast_analysis["reasoning"].append(f"„ÉÜ„Éº„Éñ„É´ {table_display_name}: ÈùûÂúßÁ∏Æ{uncompressed_size_mb:.1f}MB - BROADCAST‰∏çÂèØÔºà>{broadcast_max_mb:.1f}MBÔºâ")
        else:
            # ‰∏≠Èñì„Çµ„Ç§„Ç∫„ÅÆ„ÉÜ„Éº„Éñ„É´Ôºà30-300MBÔºâ
            large_tables.append(scan)
            size_analysis["broadcast_decision"] = "not_recommended"
            size_analysis["decision_reasoning"] = f"ÈùûÂúßÁ∏Æ{uncompressed_size_mb:.1f}MB > ÈñæÂÄ§{broadcast_threshold_mb:.1f}MB"
            broadcast_analysis["reasoning"].append(f"„ÉÜ„Éº„Éñ„É´ {table_display_name}: ÈùûÂúßÁ∏Æ{uncompressed_size_mb:.1f}MB - BROADCASTÈùûÊé®Â•®Ôºà>{broadcast_threshold_mb:.1f}MBÈñæÂÄ§Ôºâ")
        
        broadcast_analysis["detailed_size_analysis"].append(size_analysis)
    
    # ÂúßÁ∏ÆÂàÜÊûê„Çµ„Éû„É™„Éº„ÅÆÊõ¥Êñ∞
    if scan_nodes:
        total_uncompressed_mb = sum(scan["estimated_uncompressed_mb"] for scan in scan_nodes)
        total_compressed_mb = sum(scan["estimated_compressed_mb"] for scan in scan_nodes)
        if total_compressed_mb > 0:
            broadcast_analysis["compression_analysis"]["avg_compression_ratio"] = total_uncompressed_mb / total_compressed_mb
        broadcast_analysis["compression_analysis"]["total_uncompressed_mb"] = total_uncompressed_mb
        broadcast_analysis["compression_analysis"]["total_compressed_mb"] = total_compressed_mb
    
    # Á∑è„Éá„Éº„ÇøË™≠„ÅøËæº„ÅøÈáè„Å®„ÅÆÊï¥ÂêàÊÄß„ÉÅ„Çß„ÉÉ„ÇØÔºàÂúßÁ∏Æ„Éô„Éº„ÇπÔºâ
    total_read_gb = overall_metrics.get('read_bytes', 0) / 1024 / 1024 / 1024
    estimated_total_compressed_mb = sum(scan["estimated_compressed_mb"] for scan in scan_nodes)
    
    if estimated_total_compressed_mb > 0:
        size_ratio = (total_read_gb * 1024) / estimated_total_compressed_mb
        if size_ratio > 3 or size_ratio < 0.3:
            broadcast_analysis["reasoning"].append(f"Êé®ÂÆöÂúßÁ∏Æ„Çµ„Ç§„Ç∫({estimated_total_compressed_mb:.1f}MB)„Å®ÂÆüË™≠„ÅøËæº„ÅøÈáè({total_read_gb:.1f}GB)„Å´‰πñÈõ¢„ÅÇ„Çä - „Çµ„Ç§„Ç∫Êé®ÂÆö„Å´Ê≥®ÊÑè")
        else:
            broadcast_analysis["reasoning"].append(f"„Çµ„Ç§„Ç∫Êé®ÂÆöÊï¥ÂêàÊÄß: Êé®ÂÆöÂúßÁ∏Æ{estimated_total_compressed_mb:.1f}MB vs ÂÆüÈöõ{total_read_gb:.1f}GBÔºàÊØîÁéá:{size_ratio:.2f}Ôºâ")
    
    # BROADCASTÊé®Â•®‰∫ãÈ†Ö„ÅÆÁîüÊàêÔºà30MBÈñæÂÄ§ÂØæÂøú„ÄÅÊó¢Â≠ò„ÅÆBROADCASTÈÅ©Áî®Áä∂Ê≥Å„ÇíËÄÉÊÖÆÔºâ
    total_broadcast_candidates = len(small_tables) + len(marginal_tables)
    total_tables = len(scan_nodes)
    
    if small_tables or marginal_tables:
        if large_tables:
            # Êó¢Â≠ò„ÅÆBROADCASTÈÅ©Áî®Áä∂Ê≥Å„ÇíËÄÉÊÖÆ„Åó„ÅüÂà§ÂÆö
            if broadcast_analysis["already_optimized"]:
                broadcast_analysis["feasibility"] = "already_optimized_with_improvements"
                broadcast_analysis["recommendations"] = [
                    f"‚úÖ Êó¢„Å´BROADCAST JOINÈÅ©Áî®Ê∏à„Åø - ËøΩÂä†ÊîπÂñÑ„ÅÆÊ§úË®é",
                    f"üéØ ËøΩÂä†ÊúÄÈÅ©Âåñ„ÉÜ„Éº„Éñ„É´: {total_broadcast_candidates}ÂÄãÔºàÂÖ®{total_tables}ÂÄã‰∏≠Ôºâ",
                    f"  ‚úÖ Âº∑„ÅèÊé®Â•®: {len(small_tables)}ÂÄãÔºàÂÆâÂÖ®ÈñæÂÄ§{broadcast_safe_mb:.1f}MB‰ª•‰∏ãÔºâ",
                    f"  ‚ö†Ô∏è Êù°‰ª∂‰ªò„ÅçÊé®Â•®: {len(marginal_tables)}ÂÄãÔºàÈñæÂÄ§{broadcast_threshold_mb:.1f}MB‰ª•‰∏ã„ÄÅË¶ÅÊ≥®ÊÑèÔºâ",
                    f"  ‚ùå ÈùûÊé®Â•®: {len(large_tables)}ÂÄãÔºàÈñæÂÄ§Ë∂ÖÈÅéÔºâ"
                ]
            else:
                broadcast_analysis["feasibility"] = "recommended"
                broadcast_analysis["recommendations"] = [
                    f"üéØ BROADCASTÊé®Â•®„ÉÜ„Éº„Éñ„É´: {total_broadcast_candidates}ÂÄãÔºàÂÖ®{total_tables}ÂÄã‰∏≠Ôºâ",
                    f"  ‚úÖ Âº∑„ÅèÊé®Â•®: {len(small_tables)}ÂÄãÔºàÂÆâÂÖ®ÈñæÂÄ§{broadcast_safe_mb:.1f}MB‰ª•‰∏ãÔºâ",
                    f"  ‚ö†Ô∏è Êù°‰ª∂‰ªò„ÅçÊé®Â•®: {len(marginal_tables)}ÂÄãÔºàÈñæÂÄ§{broadcast_threshold_mb:.1f}MB‰ª•‰∏ã„ÄÅË¶ÅÊ≥®ÊÑèÔºâ",
                    f"  ‚ùå ÈùûÊé®Â•®: {len(large_tables)}ÂÄãÔºàÈñæÂÄ§Ë∂ÖÈÅéÔºâ"
                ]
        else:
            # ÂÖ®„ÉÜ„Éº„Éñ„É´„ÅåÂ∞è„Åï„ÅÑÂ†¥Âêà
            if broadcast_analysis["already_optimized"]:
                broadcast_analysis["feasibility"] = "already_optimized_complete"
                broadcast_analysis["recommendations"] = [
                    f"‚úÖ Êó¢„Å´BROADCAST JOINÈÅ©Áî®Ê∏à„Åø - ÊúÄÈÅ©ÂåñÂÆå‰∫Ü",
                    f"üéØ ÂÖ®„ÉÜ„Éº„Éñ„É´Ôºà{total_tables}ÂÄãÔºâ„ÅåBROADCASTÈñæÂÄ§‰ª•‰∏ã„ÅßÈÅ©Âàá„Å´Âá¶ÁêÜÊ∏à„Åø",
                    f"  ‚úÖ Âº∑„ÅèÊé®Â•®: {len(small_tables)}ÂÄã",
                    f"  ‚ö†Ô∏è Êù°‰ª∂‰ªò„ÅçÊé®Â•®: {len(marginal_tables)}ÂÄã",
                    "üìã ÁèæÂú®„ÅÆË®≠ÂÆö„ÅåÊúÄÈÅ©„Åß„Åô"
                ]
            else:
                broadcast_analysis["feasibility"] = "all_small"
                broadcast_analysis["recommendations"] = [
                    f"üéØ ÂÖ®„ÉÜ„Éº„Éñ„É´Ôºà{total_tables}ÂÄãÔºâ„ÅåBROADCASTÈñæÂÄ§‰ª•‰∏ã",
                    f"  ‚úÖ Âº∑„ÅèÊé®Â•®: {len(small_tables)}ÂÄã",
                    f"  ‚ö†Ô∏è Êù°‰ª∂‰ªò„ÅçÊé®Â•®: {len(marginal_tables)}ÂÄã",
                    "üìã ÊúÄÂ∞è„ÉÜ„Éº„Éñ„É´„ÇíÂÑ™ÂÖàÁöÑ„Å´BROADCAST„Åô„Çã„Åì„Å®„ÇíÊé®Â•®"
                ]
        
        # ÂÖ∑‰ΩìÁöÑ„Å™BROADCASTÂÄôË£ú„ÅÆË©≥Á¥∞
        for small_table in small_tables:
            broadcast_analysis["recommendations"].append(
                f"üîπ BROADCAST({small_table['node_name']}) - ÈùûÂúßÁ∏Æ{small_table['estimated_uncompressed_mb']:.1f}MBÔºàÂúßÁ∏Æ{small_table['estimated_compressed_mb']:.1f}MB„ÄÅ{small_table['file_format']}„ÄÅÂúßÁ∏ÆÁéá{small_table['compression_ratio']:.1f}xÔºâ"
            )
        
        for marginal_table in marginal_tables:
            broadcast_analysis["recommendations"].append(
                f"üî∏ BROADCAST({marginal_table['node_name']}) - ÈùûÂúßÁ∏Æ{marginal_table['estimated_uncompressed_mb']:.1f}MBÔºàÊù°‰ª∂‰ªò„Åç„ÄÅ„É°„É¢„É™‰ΩøÁî®ÈáèË¶ÅÊ≥®ÊÑèÔºâ"
            )
            
    elif large_tables:
        broadcast_analysis["feasibility"] = "not_recommended"
        broadcast_analysis["recommendations"] = [
            f"‚ùå ÂÖ®„ÉÜ„Éº„Éñ„É´Ôºà{len(large_tables)}ÂÄãÔºâ„Åå30MBÈñæÂÄ§Ë∂ÖÈÅé„ÅÆ„Åü„ÇÅBROADCASTÈùûÊé®Â•®",
            f"üìä ÊúÄÂ∞è„ÉÜ„Éº„Éñ„É´„Åß„ÇÇÈùûÂúßÁ∏Æ{min(scan['estimated_uncompressed_mb'] for scan in large_tables):.1f}MB",
            "üîß ‰ª£ÊõøÊúÄÈÅ©ÂåñÊâãÊ≥ï„ÇíÊé®Â•®:",
            "  ‚Ä¢ Liquid ClusteringÂÆüË£Ö",
            "  ‚Ä¢ „Éá„Éº„Çø„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„Éã„É≥„Ç∞",
            "  ‚Ä¢ „ÇØ„Ç®„É™ÊúÄÈÅ©ÂåñÔºà„Éï„Ç£„É´„Çø„Éº„Éó„ÉÉ„Ç∑„É•„ÉÄ„Ç¶„É≥Á≠âÔºâ",
            "  ‚Ä¢ spark.databricks.optimizer.autoBroadcastJoinThresholdË®≠ÂÆöÂÄ§„ÅÆË™øÊï¥Ê§úË®é"
        ]
    else:
        broadcast_analysis["feasibility"] = "insufficient_data"
        broadcast_analysis["recommendations"] = [
            "‚ö†Ô∏è „ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫ÊÉÖÂ†±„Åå‰∏çË∂≥„Åó„Å¶„ÅÑ„Çã„Åü„ÇÅ„ÄÅÊâãÂãï„Åß„ÅÆ„Çµ„Ç§„Ç∫Á¢∫Ë™ç„ÅåÂøÖË¶Å",
            "üìã ‰ª•‰∏ã„ÅÆ„Ç≥„Éû„É≥„Éâ„Åß„ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫„ÇíÁ¢∫Ë™ç:",
            "  ‚Ä¢ DESCRIBE DETAIL table_name",
            "  ‚Ä¢ SELECT COUNT(*) FROM table_name",
            "  ‚Ä¢ SHOW TABLE EXTENDED LIKE 'table_name'"
        ]
    
    # 30MBÈñæÂÄ§„Å´„Éí„ÉÉ„Éà„Åô„ÇãÁâπÂà•„Å™„Ç±„Éº„ÇπÂàÜÊûêÔºàsmall_tables + marginal_tables „ÇíËÄÉÊÖÆÔºâ
    all_30mb_candidates = small_tables + marginal_tables  # 30MB‰ª•‰∏ã„ÅÆÂÖ®ÂÄôË£ú
    
    if all_30mb_candidates:
        broadcast_analysis["30mb_hit_analysis"] = {
            "has_30mb_candidates": True,
            "candidate_count": len(all_30mb_candidates),
            "small_tables_count": len(small_tables),  # 24MB‰ª•‰∏ãÔºàÂº∑„ÅèÊé®Â•®Ôºâ
            "marginal_tables_count": len(marginal_tables),  # 24-30MBÔºàÊù°‰ª∂‰ªò„ÅçÊé®Â•®Ôºâ
            "smallest_table_mb": min(scan["estimated_uncompressed_mb"] for scan in all_30mb_candidates),
            "largest_candidate_mb": max(scan["estimated_uncompressed_mb"] for scan in all_30mb_candidates),
            "total_candidate_size_mb": sum(scan["estimated_uncompressed_mb"] for scan in all_30mb_candidates),
            "recommended_broadcast_table": all_30mb_candidates[0]["node_name"] if all_30mb_candidates else None,
            "memory_impact_estimation": f"{sum(scan['estimated_uncompressed_mb'] for scan in all_30mb_candidates):.1f}MB „Åå„ÉØ„Éº„Ç´„Éº„Éé„Éº„Éâ„Å´„Éñ„É≠„Éº„Éâ„Ç≠„É£„Çπ„Éà"
        }
        
        # ÊúÄÈÅ©„Å™BROADCASTÂÄôË£ú„ÅÆÁâπÂÆöÔºàÂÖ®30MBÂÄôË£ú„Åã„ÇâÈÅ∏ÊäûÔºâ
        if len(all_30mb_candidates) > 1:
            optimal_candidate = min(all_30mb_candidates, key=lambda x: x["estimated_uncompressed_mb"])
            broadcast_analysis["30mb_hit_analysis"]["optimal_candidate"] = {
                "table": optimal_candidate["node_name"],
                "size_mb": optimal_candidate["estimated_uncompressed_mb"],
                "rows": optimal_candidate["rows"],
                "reasoning": f"ÊúÄÂ∞è„Çµ„Ç§„Ç∫{optimal_candidate['estimated_uncompressed_mb']:.1f}MB„ÅßÊúÄ„ÇÇÂäπÁéáÁöÑ"
            }
        
        # 30MBÈñæÂÄ§ÂÜÖ„ÅÆË©≥Á¥∞ÂàÜÈ°ûÊÉÖÂ†±„ÇíËøΩÂä†
        broadcast_analysis["30mb_hit_analysis"]["size_classification"] = {
            "safe_zone_tables": len(small_tables),  # 0-24MBÔºàÂÆâÂÖ®„Éû„Éº„Ç∏„É≥ÂÜÖÔºâ
            "caution_zone_tables": len(marginal_tables),  # 24-30MBÔºàË¶ÅÊ≥®ÊÑèÔºâ
            "safe_zone_description": "24MB‰ª•‰∏ãÔºàÂº∑„ÅèÊé®Â•®„ÄÅÂÆâÂÖ®„Éû„Éº„Ç∏„É≥ÂÜÖÔºâ",
            "caution_zone_description": "24-30MBÔºàÊù°‰ª∂‰ªò„ÅçÊé®Â•®„ÄÅ„É°„É¢„É™‰ΩøÁî®ÈáèË¶ÅÊ≥®ÊÑèÔºâ"
        }
    else:
        broadcast_analysis["30mb_hit_analysis"] = {
            "has_30mb_candidates": False,
            "reason": f"ÂÖ®„ÉÜ„Éº„Éñ„É´„Åå30MBÈñæÂÄ§„ÇíË∂ÖÈÅéÔºàÊúÄÂ∞è: {min(scan['estimated_uncompressed_mb'] for scan in scan_nodes):.1f}MBÔºâ" if scan_nodes else "„ÉÜ„Éº„Éñ„É´ÊÉÖÂ†±„Å™„Åó"
        }
    
    return broadcast_analysis

def extract_structured_physical_plan(physical_plan: str) -> Dict[str, Any]:
    """
    Structured extraction of important information only from Physical Plan (countermeasure for token limits)
    
    Args:
        physical_plan: Complete text of Physical Plan
    
    Returns:
        Dict: Structured important information
    """
    import re
    
    extracted = {
        "joins": [],           # JOINÊÉÖÂ†±ÔºàÁ®ÆÈ°û„ÄÅÊù°‰ª∂„ÄÅÁµ±Ë®àÔºâ
        "scans": [],          # „ÉÜ„Éº„Éñ„É´„Çπ„Ç≠„É£„É≥Ôºà„Çµ„Ç§„Ç∫„ÄÅË°åÊï∞Ôºâ  
        "exchanges": [],      # „Éá„Éº„ÇøÁßªÂãïÔºàShuffle„ÄÅBroadcastÔºâ
        "aggregates": [],     # ÈõÜÁ¥ÑÂá¶ÁêÜÔºàGROUP BY„ÄÅSUMÁ≠âÔºâ
        "filters": [],        # „Éï„Ç£„É´„ÇøÊù°‰ª∂„Å®ÈÅ∏ÊäûÁéá
        "photon_usage": {},   # PhotonÂà©Áî®Áä∂Ê≥Å
        "bottlenecks": [],    # ÁâπÂÆö„Åï„Çå„Åü„Éú„Éà„É´„Éç„ÉÉ„ÇØ
        "statistics": {},     # Êï∞ÂÄ§Áµ±Ë®à„Çµ„Éû„É™„Éº
        "total_size": len(physical_plan),
        "extraction_summary": ""
    }
    
    try:
        lines = physical_plan.split('\n')
        join_count = scan_count = exchange_count = 0
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # JOINÊÉÖÂ†±„ÅÆÊäΩÂá∫ÔºàÂæìÊù•ÂΩ¢Âºè + PhotonÂΩ¢ÂºèÂÆåÂÖ®ÂØæÂøúÔºâ
            # ÂæìÊù•„ÅÆSpark JOINÂΩ¢ÂºèÔºàStatistics‰ªò„ÅçÔºâ
            join_match = re.search(r'(\w*Join)\s+([^,\n]+).*?Statistics\(([^)]+)\)', line)
            # Photon JOINÂΩ¢ÂºèÔºàStatisticsÁÑ°„Åó„ÄÅË©≥Á¥∞„Å™„Éë„É©„É°„Éº„Çø‰ªò„ÅçÔºâ
            photon_join_match = re.search(r'(Photon\w*Join)\s+\[([^\]]+)\],\s*\[([^\]]+)\],\s*(\w+),\s*(\w+)', line)
            
            if join_match or photon_join_match:
                if join_match:
                    # ÂæìÊù•„ÅÆSpark JOINÂΩ¢Âºè
                    join_type = join_match.group(1)
                    condition = join_match.group(2).strip()
                    stats = join_match.group(3)
                    
                    # Áµ±Ë®àÊÉÖÂ†±„Åã„ÇâÊï∞ÂÄ§ÊäΩÂá∫
                    size_match = re.search(r'sizeInBytes=([0-9.]+)\s*([KMGT]i?B)?', stats)
                    rows_match = re.search(r'rowCount=(\d+)', stats)
                    
                    size_str = f"{size_match.group(1)}{size_match.group(2) or 'B'}" if size_match else "unknown"
                    rows_str = rows_match.group(1) if rows_match else "unknown"
                    
                elif photon_join_match:
                    # Photon JOINÂΩ¢Âºè„ÅÆË©≥Á¥∞ÊäΩÂá∫
                    join_type = photon_join_match.group(1)  # PhotonBroadcastHashJoinÁ≠â
                    left_keys = photon_join_match.group(2)   # Â∑¶ÂÅ¥„ÅÆJOIN„Ç≠„Éº
                    right_keys = photon_join_match.group(3)  # Âè≥ÂÅ¥„ÅÆJOIN„Ç≠„Éº
                    join_method = photon_join_match.group(4) # Inner, LeftÁ≠â
                    build_side = photon_join_match.group(5)  # BuildRight, BuildLeftÁ≠â
                    
                    # JOINÊù°‰ª∂„ÅÆÊßãÊàê
                    condition = f"{left_keys} = {right_keys} ({join_method}, {build_side})"
                    
                    # Photon JOIN„ÅØÁµ±Ë®àÊÉÖÂ†±„ÅåÂà•„ÅÆÂ†¥ÊâÄ„Å´„ÅÇ„Çã„Åü„ÇÅ„ÄÅ„Åì„Åì„Åß„ÅØÂü∫Êú¨ÊÉÖÂ†±„ÅÆ„Åø
                    size_str = "photon_optimized"
                    rows_str = "photon_optimized"
                
                extracted["joins"].append({
                    "type": join_type,
                    "condition": condition[:100],  # Êù°‰ª∂„Çí100ÊñáÂ≠ó„Å´Âà∂Èôê
                    "size": size_str,
                    "rows": rows_str
                })
                join_count += 1
                
            # „ÉÜ„Éº„Éñ„É´„Çπ„Ç≠„É£„É≥ÊÉÖÂ†±„ÅÆÊäΩÂá∫ÔºàÂæìÊù•ÂΩ¢Âºè + PhotonÂΩ¢ÂºèÂÆåÂÖ®ÂØæÂøúÔºâ
            elif ('FileScan' in line and 'Statistics(' in line) or ('PhotonScan' in line and 'parquet' in line):
                # ÂæìÊù•ÂΩ¢ÂºèÔºöStatistics‰ªò„ÅçFileScan
                stats_match = re.search(r'Statistics\(([^)]+)\)', line)
                # PhotonÂΩ¢ÂºèÔºöPhotonScan parquet table_name[columns]
                photon_scan_match = re.search(r'PhotonScan\s+parquet\s+([a-zA-Z_][a-zA-Z0-9_.]*)\[([^\]]+)\]', line)
                # ÂæìÊù•ÂΩ¢ÂºèÔºöFileScan
                file_scan_match = re.search(r'FileScan\s+([^,\s\[]+)', line)
                
                if (stats_match and file_scan_match) or photon_scan_match:
                    if photon_scan_match:
                        # PhotonÂΩ¢Âºè„ÅÆÂ†¥Âêà
                        table = photon_scan_match.group(1)  # „ÉÜ„Éº„Éñ„É´Âêç
                        columns = photon_scan_match.group(2)  # Âàó„É™„Çπ„Éà
                        stats = None  # PhotonScan„Å´„ÅØÁµ±Ë®àÊÉÖÂ†±„ÅåÂêå‰∏ÄË°å„Å´„Å™„ÅÑ
                        
                        # „ÉÜ„Éº„Éñ„É´Áµ±Ë®à„ÅÆ‰øùÂ≠òÔºàPhotonÁî®„ÅÆÊßãÈÄ†Ôºâ
                        extracted["scans"].append({
                            "table": table[:50],
                            "columns": columns[:100],
                            "type": "PhotonScan",
                            "size": "photon_scan",
                            "rows": "photon_scan"
                        })
                        scan_count += 1
                        
                    elif stats_match and file_scan_match:
                        # ÂæìÊù•ÂΩ¢Âºè„ÅÆÂ†¥Âêà
                        stats = stats_match.group(1)
                        table = file_scan_match.group(1)
                        
                        size_match = re.search(r'sizeInBytes=([0-9.]+)\s*([KMGT]i?B)?', stats)
                        rows_match = re.search(r'rowCount=(\d+)', stats)
                        
                        extracted["scans"].append({
                            "table": table[:50],  # „ÉÜ„Éº„Éñ„É´Âêç„Çí50ÊñáÂ≠ó„Å´Âà∂Èôê
                            "type": "FileScan",
                            "size": f"{size_match.group(1)}{size_match.group(2) or 'B'}" if size_match else "unknown",
                            "rows": rows_match.group(1) if rows_match else "unknown"
                        })
                        scan_count += 1
                    
            # „Éá„Éº„ÇøÁßªÂãïÔºàExchangeÔºâ„ÅÆÊäΩÂá∫
            elif 'Exchange' in line:
                if 'BroadcastExchange' in line:
                    extracted["exchanges"].append({"type": "BROADCAST", "detail": line[:100]})
                elif 'ShuffleExchange' in line or 'Exchange' in line:
                    extracted["exchanges"].append({"type": "SHUFFLE", "detail": line[:100]})
                exchange_count += 1
                
            # ÈõÜÁ¥ÑÂá¶ÁêÜ„ÅÆÊäΩÂá∫
            elif 'Aggregate' in line or 'HashAggregate' in line:
                extracted["aggregates"].append({"type": "AGGREGATE", "detail": line[:100]})
                
            # PhotonÂà©Áî®Áä∂Ê≥Å„ÅÆÁ¢∫Ë™ç
            elif 'Photon' in line:
                if 'PhotonResultStage' in line:
                    extracted["photon_usage"]["result_stage"] = True
                elif 'PhotonHashJoin' in line:
                    extracted["photon_usage"]["hash_join"] = True
                elif 'PhotonProject' in line:
                    extracted["photon_usage"]["project"] = True
        
        # Áµ±Ë®à„Çµ„Éû„É™„ÉºÁîüÊàê
        extracted["statistics"] = {
            "total_joins": join_count,
            "total_scans": scan_count,  
            "total_exchanges": exchange_count,
            "photon_operations": len([k for k, v in extracted["photon_usage"].items() if v])
        }
        
        # ÊäΩÂá∫„Çµ„Éû„É™„ÉºÁîüÊàê
        extracted["extraction_summary"] = f"üìä Structured extraction completed: JOIN({join_count}) SCAN({scan_count}) EXCHANGE({exchange_count}) PHOTON({len(extracted['photon_usage'])})"
        
        # üö® „Éà„Éº„ÇØ„É≥Âà∂ÈôêÂØæÁ≠ñ: ÊÉÖÂ†±Èáè„ÅåÂ§ö„ÅÑÂ†¥Âêà„ÅÆËá™ÂãïË¶ÅÁ¥Ñ
        total_joins_scans = join_count + scan_count
        if total_joins_scans > 30:  # ÈñæÂÄ§„ÇíÂ§ßÂπÖ„Å´Âºï„Åç‰∏ä„Åí: JOIN+SCANÂêàË®à„Åå30ÂÄã‰ª•‰∏ä
            # ÈáçË¶ÅÂ∫¶È†Ü„Å´‰∏¶„Å≥Êõø„Åà„Å¶„Éà„ÉÉ„ÉóÊÉÖÂ†±„ÅÆ„Åø‰øùÊåÅ
            extracted = apply_token_limit_optimization(extracted, max_joins=20, max_scans=15)  # Âà∂Èôê„ÇíÂ§ßÂπÖÁ∑©Âíå
            extracted["extraction_summary"] += f" ‚Üí JOIN/SCAN information summarized for token limit optimization"
        elif total_joins_scans > 15:  # ‰∏≠ÈñìÈñæÂÄ§: 15-30ÂÄã„ÅÆÂ†¥Âêà
            # ‰∏≠Á®ãÂ∫¶„ÅÆË¶ÅÁ¥Ñ
            extracted = apply_token_limit_optimization(extracted, max_joins=12, max_scans=10)
            extracted["extraction_summary"] += f" ‚Üí Moderate JOIN/SCAN information summarization applied"
        
    except Exception as e:
        extracted["extraction_error"] = str(e)
        
    return extracted

def extract_structured_cost_statistics(explain_cost_content: str) -> Dict[str, Any]:
    """
    Structured extraction of numerical statistics only from EXPLAIN COST (countermeasure for token limits)
    
    Args:
        explain_cost_content: Complete EXPLAIN COST results
    
    Returns:
        Dict: Structured statistical information
    """
    import re
    
    extracted = {
        "table_stats": {},      # Table-specific statistics (size, row count)
        "join_costs": {},       # JOIN-specific cost estimates  
        "selectivity": {},      # Filter selectivity
        "partition_info": {},   # Partition statistics
        "memory_estimates": {}, # Memory usage predictions
        "cost_breakdown": {},   # Cost breakdown
        "critical_stats": {},   # Critical statistical values
        "total_size": len(explain_cost_content),
        "extraction_summary": ""
    }
    
    try:
        lines = explain_cost_content.split('\n')
        tables_found = costs_found = memory_found = 0
        
        # ÈáçË¶ÅÁµ±Ë®àÂÄ§„ÇíËøΩË∑°
        largest_table = {"name": "", "size": 0, "size_str": ""}
        total_rows = 0
        broadcast_candidates = []
        
        # „ÉÜ„Éº„Éñ„É´Âêç„Å®„Çµ„Ç§„Ç∫„ÅÆÂØæÂøú„ÇíËøΩË∑°
        table_name_size_map = {}  # {table_name: {"size_bytes": int, "size_str": str, "rows": int}}
        current_table_context = None  # ÁèæÂú®Âá¶ÁêÜ‰∏≠„ÅÆ„ÉÜ„Éº„Éñ„É´Âêç
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # üîç „ÉÜ„Éº„Éñ„É´Âêç„ÅÆÊäΩÂá∫ÔºàRelation„Åã„ÇâÔºâ
            table_name_match = re.search(r'Relation\s+([a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z_][a-zA-Z0-9_]*)', line)
            if table_name_match:
                current_table_context = table_name_match.group(1)
                
            # üîç „ÉÜ„Éº„Éñ„É´Âêç„ÅÆÊäΩÂá∫ÔºàJoinÊù°‰ª∂„Åã„ÇâÔºâ
            elif 'Join' in line and '=' in line:
                # JOINÊù°‰ª∂„Åã„Çâ„ÉÜ„Éº„Éñ„É´Âêç„ÇíÊé®ÂÆö (‰æã: ty_brand#456 = ly_brand#789)
                join_match = re.search(r'([a-zA-Z_][a-zA-Z0-9_]*)[#.]', line)
                if join_match and not current_table_context:
                    # JOINÊù°‰ª∂„ÅÆ„Éó„É¨„Éï„Ç£„ÉÉ„ÇØ„Çπ„Åã„Çâ„ÉÜ„Éº„Éñ„É´Êé®ÂÆö
                    prefix = join_match.group(1)
                    if len(prefix) > 2:  # ÊÑèÂë≥„ÅÆ„ÅÇ„Çã„Éó„É¨„Éï„Ç£„ÉÉ„ÇØ„Çπ
                        current_table_context = f"{prefix}_table"
                
            # „ÉÜ„Éº„Éñ„É´Áµ±Ë®à„ÅÆÊäΩÂá∫
            if 'Statistics(' in line:
                # „Çµ„Ç§„Ç∫ÊÉÖÂ†±„ÅÆÊäΩÂá∫
                size_match = re.search(r'sizeInBytes=([0-9.]+)\s*([KMGT]i?B)?', line)
                rows_match = re.search(r'rowCount=(\d+)', line)
                
                # „ÉÜ„Éº„Éñ„É´Âêç„ÅÆÊ±∫ÂÆö
                if current_table_context:
                    table_name = current_table_context
                else:
                    # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: Ë°åÁï™Âè∑„Åã„ÇâÊé®ÂÆö
                    line_table_match = re.search(r'([a-zA-Z_][a-zA-Z0-9_]*)', line)
                    table_name = line_table_match.group(1) if line_table_match else f"table_{tables_found}"
                
                if size_match:
                    size_val = float(size_match.group(1))
                    size_unit = size_match.group(2) or 'B'
                    size_str = f"{size_val}{size_unit}"
                    
                    # „Çµ„Ç§„Ç∫Â§âÊèõÔºà„Éê„Ç§„ÉàÂçò‰ΩçÔºâ
                    size_bytes = size_val
                    if 'KiB' in size_unit:
                        size_bytes *= 1024
                    elif 'MiB' in size_unit:
                        size_bytes *= 1024 * 1024
                    elif 'GiB' in size_unit:
                        size_bytes *= 1024 * 1024 * 1024
                    elif 'TiB' in size_unit:
                        size_bytes *= 1024 * 1024 * 1024 * 1024
                    
                    # Ë°åÊï∞„ÅÆÂèñÂæó
                    rows = int(rows_match.group(1)) if rows_match else 0
                    
                    # „ÉÜ„Éº„Éñ„É´Áµ±Ë®à„ÅÆ‰øùÂ≠ò
                    extracted["table_stats"][table_name] = {
                        "size_bytes": size_bytes,
                        "size_str": size_str,
                        "rows": rows,
                        "is_broadcast_candidate": size_bytes < 30 * 1024 * 1024  # 30MB
                    }
                    
                    # ÊúÄÂ§ß„ÉÜ„Éº„Éñ„É´„ÅÆËøΩË∑°
                    if size_bytes > largest_table["size"]:
                        largest_table = {"name": table_name, "size": size_bytes, "size_str": size_str}
                    
                    # „Éñ„É≠„Éº„Éâ„Ç≠„É£„Çπ„ÉàÂÄôË£úÔºà30MBÊú™Ê∫ÄÔºâ
                    if size_bytes < 30 * 1024 * 1024:  # 30MB
                        broadcast_candidates.append({"table": table_name, "size": size_str})
                    
                    tables_found += 1
                    total_rows += rows
                    
                # ÁèæÂú®„ÅÆ„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Çí„É™„Çª„ÉÉ„ÉàÔºàÊ¨°„ÅÆ„ÉÜ„Éº„Éñ„É´Áî®Ôºâ
                current_table_context = None
                    
            # „Ç≥„Çπ„ÉàÊÉÖÂ†±„ÅÆÊäΩÂá∫  
            elif 'Cost(' in line:
                cost_match = re.search(r'Cost\(([0-9.]+)\)', line)
                if cost_match:
                    extracted["cost_breakdown"][f"operation_{costs_found}"] = float(cost_match.group(1))
                    costs_found += 1
                    
            # „É°„É¢„É™Èñ¢ÈÄ£ÊÉÖÂ†±„ÅÆÊäΩÂá∫
            elif any(keyword in line.lower() for keyword in ['memory', 'spill', 'threshold']):
                if 'memory' in line.lower():
                    memory_match = re.search(r'(\d+(?:\.\d+)?)\s*([KMGT]i?B)', line)
                    if memory_match:
                        extracted["memory_estimates"][f"estimate_{memory_found}"] = f"{memory_match.group(1)}{memory_match.group(2)}"
                        memory_found += 1
        
        # ÈáçË¶ÅÁµ±Ë®àÂÄ§„ÅÆ„Åæ„Å®„ÇÅ
        extracted["critical_stats"] = {
            "largest_table": largest_table,
            "total_rows": total_rows,
            "broadcast_candidates": broadcast_candidates[:5],  # ‰∏ä‰Ωç5ÂÄã„Åæ„Åß
            "tables_analyzed": tables_found,
            "cost_operations": costs_found,
            "memory_estimates": memory_found,
            "table_breakdown": {
                "total_tables": len(extracted["table_stats"]),
                "largest_table_name": largest_table.get("name", "unknown"),
                "broadcast_table_names": [bc.get("table", "unknown") for bc in broadcast_candidates[:3]]
            }
        }
        
        # ÊäΩÂá∫„Çµ„Éû„É™„ÉºÁîüÊàê
        extracted["extraction_summary"] = f"üí∞ Statistics extraction completed: Tables({tables_found}) Cost({costs_found}) Memory({memory_found}) BROADCAST candidates({len(broadcast_candidates)})"
        
    except Exception as e:
        extracted["extraction_error"] = str(e)
        
    return extracted

def apply_token_limit_optimization(extracted: Dict[str, Any], max_joins: int = 5, max_scans: int = 8) -> Dict[str, Any]:
    """
    Token limit optimization: Priority-based summarization of JOIN/SCAN information
    
    Args:
        extracted: Extracted structured data
        max_joins: Maximum number of JOINs to retain
        max_scans: Maximum number of SCANs to retain
    
    Returns:
        Optimized structured data
    """
    
    # Sort JOIN information by priority
    joins = extracted.get("joins", [])
    if len(joins) > max_joins:
        # Priority order: Broadcast > Hash > Sort > Nested
        join_priority = {
            "PhotonBroadcastHashJoin": 1,
            "BroadcastHashJoin": 2,
            "PhotonHashJoin": 3,
            "HashJoin": 4,
            "SortMergeJoin": 5,
            "NestedLoopJoin": 6
        }
        
        # ÈáçË¶ÅÂ∫¶„Åß„ÇΩ„Éº„Éà
        sorted_joins = sorted(joins, key=lambda j: join_priority.get(j.get("type", ""), 10))
        
        # ‰∏ä‰Ωç„ÅÆ„Åø‰øùÊåÅ„ÄÅÊÆã„Çä„ÅØË¶ÅÁ¥Ñ
        top_joins = sorted_joins[:max_joins]
        remaining_count = len(joins) - max_joins
        
        if remaining_count > 0:
            summary_join = {
                "type": "SUMMARY",
                "condition": f"Other {remaining_count} JOIN operations (details omitted)",
                "size": "multiple",
                "rows": "multiple"
            }
            top_joins.append(summary_join)
        
        extracted["joins"] = top_joins
    
    # SCANÊÉÖÂ†±„ÅÆÈáçË¶ÅÂ∫¶Âà•„ÇΩ„Éº„Éà
    scans = extracted.get("scans", [])
    if len(scans) > max_scans:
        # ÈáçË¶ÅÂ∫¶È†ÜÂ∫è: PhotonScan > FileScan„ÄÅ„ÉÜ„Éº„Éñ„É´Âêç„ÅÆÈï∑„ÅïÔºàË©≥Á¥∞Â∫¶Ôºâ
        def scan_priority(scan):
            priority = 1 if scan.get("type") == "PhotonScan" else 2
            table_length = len(scan.get("table", ""))
            return (priority, -table_length)  # „ÉÜ„Éº„Éñ„É´Âêç„ÅåÈï∑„ÅÑÔºàË©≥Á¥∞Ôºâ„Åª„Å©ÈáçË¶Å
        
        # ÈáçË¶ÅÂ∫¶„Åß„ÇΩ„Éº„Éà
        sorted_scans = sorted(scans, key=scan_priority)
        
        # ‰∏ä‰Ωç„ÅÆ„Åø‰øùÊåÅ„ÄÅÊÆã„Çä„ÅØË¶ÅÁ¥Ñ
        top_scans = sorted_scans[:max_scans]
        remaining_count = len(scans) - max_scans
        
        if remaining_count > 0:
            # ÊÆã„Çä„ÅÆ„ÉÜ„Éº„Éñ„É´Âêç„ÇíÈõÜÁ¥Ñ
            remaining_tables = [s.get("table", "unknown")[:20] for s in sorted_scans[max_scans:]]
            table_summary = ", ".join(remaining_tables[:3])
            if len(remaining_tables) > 3:
                table_summary += f" ‰ªñ{len(remaining_tables)-3}ÂÄã"
                
            summary_scan = {
                "table": f"SUMMARY({table_summary})",
                "type": "SUMMARY",
                "size": "multiple",
                "rows": "multiple"
            }
            top_scans.append(summary_scan)
        
        extracted["scans"] = top_scans
    
    # Áµ±Ë®àÊÉÖÂ†±„ÅÆÊõ¥Êñ∞
    extracted["statistics"]["optimization_applied"] = True
    extracted["statistics"]["original_joins"] = len(joins)
    extracted["statistics"]["original_scans"] = len(scans)
    extracted["statistics"]["optimized_joins"] = len(extracted["joins"])
    extracted["statistics"]["optimized_scans"] = len(extracted["scans"])
    
    return extracted

def extract_cost_statistics_from_explain_cost(explain_cost_content: str) -> str:
    """
    EXPLAIN COSTÁµêÊûú„Åã„ÇâÁµ±Ë®àÊÉÖÂ†±„ÇíÊäΩÂá∫„Åó„Å¶ÊßãÈÄ†ÂåñÔºàÊîπÂñÑÁâà + „Çµ„Ç§„Ç∫Âà∂ÈôêÔºâ
    
    Args:
        explain_cost_content: EXPLAIN COST„ÅÆÁµêÊûúÊñáÂ≠óÂàó
    
    Returns:
        ÊßãÈÄ†Âåñ„Åï„Çå„ÅüÁµ±Ë®àÊÉÖÂ†±ÊñáÂ≠óÂàóÔºà„É¨„Éù„Éº„ÉàÁî®„Å´Á∞°ÊΩîÂåñÔºâ
    """
    if not explain_cost_content:
        return ""
    
    # üö® „É¨„Éù„Éº„ÉàËÇ•Â§ßÂåñÈò≤Ê≠¢Ôºö„Çµ„Éû„É™„ÉºÊÉÖÂ†±„ÅÆ„ÅøÊäΩÂá∫
    statistics_counts = {
        "„ÉÜ„Éº„Éñ„É´Áµ±Ë®à": 0,
        "Ë°åÊï∞ÊÉÖÂ†±": 0, 
        "„Çµ„Ç§„Ç∫ÊÉÖÂ†±": 0,
        "„Ç≥„Çπ„ÉàÊÉÖÂ†±": 0,
        "ÈÅ∏ÊäûÁéáÊÉÖÂ†±": 0,
        "„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥ÊÉÖÂ†±": 0,
        "„É°„É¢„É™ÊÉÖÂ†±": 0,
        "JOINÊÉÖÂ†±": 0
    }
    
    # ÈáçË¶Å„Å™Áµ±Ë®àÂÄ§„ÅÆ„ÅøÊäΩÂá∫ÔºàË©≥Á¥∞„ÅØÈô§Â§ñÔºâ
    key_statistics = []
    MAX_KEY_STATS = 5  # ÈáçË¶ÅÁµ±Ë®àÊÉÖÂ†±„ÅÆÊúÄÂ§ßÊï∞
    
    try:
        lines = explain_cost_content.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # „ÉÜ„Éº„Éñ„É´Áµ±Ë®àÊÉÖÂ†±„ÅÆÊäΩÂá∫Ôºà„Ç´„Ç¶„É≥„Éà„ÅÆ„ÅøÔºâ
            if 'statistics=' in line.lower() or 'stats=' in line.lower() or 'Statistics(' in line:
                statistics_counts["„ÉÜ„Éº„Éñ„É´Áµ±Ë®à"] += 1
                if len(key_statistics) < MAX_KEY_STATS and 'sizeInBytes' in line:
                    # ÈáçË¶Å„Å™„Çµ„Ç§„Ç∫ÊÉÖÂ†±„ÅÆ„ÅøÊäΩÂá∫
                    if 'GiB' in line or 'TiB' in line:
                        key_statistics.append(f"üìä „ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫: {line[:100]}...")
            
            # Ë°åÊï∞ÊÉÖÂ†±„ÅÆÊäΩÂá∫Ôºà„Ç´„Ç¶„É≥„Éà„ÅÆ„ÅøÔºâ
            elif 'rows=' in line.lower() or 'rowcount=' in line.lower() or 'rows:' in line.lower():
                statistics_counts["Ë°åÊï∞ÊÉÖÂ†±"] += 1
            
            # „Çµ„Ç§„Ç∫ÊÉÖÂ†±„ÅÆÊäΩÂá∫Ôºà„Ç´„Ç¶„É≥„Éà„ÅÆ„ÅøÔºâ
            elif ('size=' in line.lower() or 'sizeinbytes=' in line.lower() or 'sizeInBytes=' in line 
                  or 'GB' in line or 'MB' in line or 'size:' in line.lower()):
                statistics_counts["„Çµ„Ç§„Ç∫ÊÉÖÂ†±"] += 1
            
            # „Åù„ÅÆ‰ªñ„ÅÆÁµ±Ë®àÊÉÖÂ†±„ÅÆ„Ç´„Ç¶„É≥„Éà
            elif ('cost=' in line.lower() or 'Cost(' in line or 'cost:' in line.lower()):
                statistics_counts["„Ç≥„Çπ„ÉàÊÉÖÂ†±"] += 1
            elif ('selectivity=' in line.lower() or 'filter=' in line.lower()):
                statistics_counts["ÈÅ∏ÊäûÁéáÊÉÖÂ†±"] += 1
            elif ('partition' in line.lower() and ('count' in line.lower() or 'size' in line.lower())):
                statistics_counts["„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥ÊÉÖÂ†±"] += 1
            elif ('memory' in line.lower() or 'spill' in line.lower()):
                statistics_counts["„É°„É¢„É™ÊÉÖÂ†±"] += 1
            elif ('join' in line.lower() and ('cost' in line.lower() or 'selectivity' in line.lower())):
                statistics_counts["JOINÊÉÖÂ†±"] += 1
    
    except Exception as e:
        return f"‚ö†Ô∏è Áµ±Ë®àÊÉÖÂ†±ÊäΩÂá∫„Ç®„É©„Éº: {str(e)}"
    
    # Á∞°ÊΩî„Å™„Çµ„Éû„É™„Éº„ÇíÁîüÊàê
    summary_lines = ["## üìä Áµ±Ë®àÊÉÖÂ†±„Çµ„Éû„É™„ÉºÔºàÁ∞°ÊΩîÁâàÔºâ"]
    
    total_stats = sum(statistics_counts.values())
    if total_stats > 0:
        summary_lines.append(f"- **Á∑èÁµ±Ë®àÈ†ÖÁõÆÊï∞**: {total_stats}ÂÄã")
        
        for stat_type, count in statistics_counts.items():
            if count > 0:
                summary_lines.append(f"- **{stat_type}**: {count}ÂÄã")
        
        if key_statistics:
            summary_lines.append("\n### üéØ ‰∏ªË¶ÅÁµ±Ë®à")
            summary_lines.extend(key_statistics)
        
        summary_lines.append(f"\nüí° Ë©≥Á¥∞„Å™Áµ±Ë®àÊÉÖÂ†±„ÅØ DEBUG_ENABLED='Y' „ÅßÁ¢∫Ë™ç„Åß„Åç„Åæ„Åô")
    else:
        summary_lines.append("- Áµ±Ë®àÊÉÖÂ†±„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü")
    
    return '\n'.join(summary_lines)


def generate_optimized_query_with_llm(original_query: str, analysis_result: str, metrics: Dict[str, Any]) -> str:
    """
    Optimize SQL query based on detailed bottleneck analysis results from Cell 33 (processing speed priority)
    Also leverages statistical information when EXPLAIN + EXPLAIN COST execution flag is Y
    """
    
    # EXPLAIN + EXPLAIN COSTÁµêÊûú„Éï„Ç°„Ç§„É´„ÅÆË™≠„ÅøËæº„ÅøÔºàEXPLAIN_ENABLED„ÅåY„ÅÆÂ†¥ÂêàÔºâ
    explain_content = ""
    explain_cost_content = ""
    physical_plan = ""
    photon_explanation = ""
    cost_statistics = ""
    
    explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
    if explain_enabled.upper() == 'Y':
        import glob
        import os
        
        print("üîç Searching for EXPLAIN + EXPLAIN COST result files...")
        
        # 1. Search for latest EXPLAIN result files (supporting new filename patterns)
        explain_original_files = glob.glob("output_explain_original_*.txt")
        explain_optimized_files = glob.glob("output_explain_optimized_*.txt")
        
        # Prioritize original query EXPLAIN results, use optimized if not available
        explain_files = explain_original_files if explain_original_files else explain_optimized_files
        
        if explain_files:
            latest_explain_file = max(explain_files, key=os.path.getctime)
            try:
                with open(latest_explain_file, 'r', encoding='utf-8') as f:
                    explain_content = f.read()
                    print(f"‚úÖ Loaded EXPLAIN result file: {latest_explain_file}")
                
                # Extract and process Physical Plan (structured extraction support)
                if "== Physical Plan ==" in explain_content:
                    physical_plan_start = explain_content.find("== Physical Plan ==")
                    physical_plan_end = explain_content.find("== Photon", physical_plan_start)
                    if physical_plan_end == -1:
                        physical_plan_end = len(explain_content)
                    physical_plan_raw = explain_content[physical_plan_start:physical_plan_end].strip()
                    print(f"üìä Extracted Physical Plan information: {len(physical_plan_raw)} characters")
                    
                    # üß† ÊßãÈÄ†ÂåñÊäΩÂá∫ vs ÂæìÊù•„ÅÆÂàá„ÇäË©∞„ÇÅ„ÅÆÈÅ∏Êäû
                    structured_enabled = globals().get('STRUCTURED_EXTRACTION_ENABLED', 'Y')
                    debug_enabled = globals().get('DEBUG_ENABLED', 'N')
                
                if structured_enabled.upper() == 'Y':
                    # üöÄ ÊßãÈÄ†ÂåñÊäΩÂá∫„Ç¢„Éó„É≠„Éº„ÉÅ
                    try:
                        structured_plan = extract_structured_physical_plan(physical_plan_raw)
                        
                        # Convert structured results to JSON format string
                        import json
                        physical_plan = json.dumps(structured_plan, ensure_ascii=False, indent=2)
                        
                        print(f"üß† Structured extraction completed: {len(physical_plan_raw):,} ‚Üí {len(physical_plan):,} characters")
                        print(f"   {structured_plan.get('extraction_summary', 'üìä Structured extraction completed')}")
                        
                        # When DEBUG_ENABLED='Y', save structured results and original data
                        if debug_enabled.upper() == 'Y':
                            try:
                                from datetime import datetime
                                timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                                
                                # Save structured results
                                structured_plan_filename = f"output_physical_plan_structured_{timestamp}.json"
                                with open(structured_plan_filename, 'w', encoding='utf-8') as f:
                                    f.write(physical_plan)
                                
                                print(f"üìÑ Saved structured Physical Plan: {structured_plan_filename}")
                                
                            except Exception as save_error:
                                print(f"‚ö†Ô∏è Failed to save Physical Plan: {str(save_error)}")
                                
                    except Exception as extraction_error:
                        print(f"‚ö†Ô∏è Structured extraction failed, falling back to traditional method: {str(extraction_error)}")
                        # Fallback: Traditional truncation method
                        MAX_PLAN_SIZE = 30000
                        if len(physical_plan_raw) > MAX_PLAN_SIZE:
                            physical_plan = physical_plan_raw[:MAX_PLAN_SIZE] + "\n\nStructured extraction failed, truncated to limit"
                            print(f"‚ö†Ô∏è Fallback: Physical Plan truncated to {MAX_PLAN_SIZE} characters")
                        else:
                            physical_plan = physical_plan_raw
                            print(f"‚ö†Ô∏è Physical Plan truncated to {MAX_PLAN_SIZE} characters due to token limit")
                
                # Extract Photon Explanation
                if "== Photon Explanation ==" in explain_content:
                    photon_start = explain_content.find("== Photon Explanation ==")
                    photon_explanation = explain_content[photon_start:].strip()
                    print(f"üöÄ Extracted Photon Explanation information: {len(photon_explanation)} characters")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load EXPLAIN result file: {str(e)}")
                explain_content = ""
        
        # üöÄ EXPLAIN COSTÁµêÊûú„Çí„Ç≠„É£„ÉÉ„Ç∑„É•„Åã„ÇâÂèñÂæóÔºàÂèØËÉΩ„Å™Â†¥ÂêàÔºâ
        cached_cost_result = globals().get('cached_original_explain_cost_result')
        explain_cost_content = ""
        
        if cached_cost_result and 'explain_cost_file' in cached_cost_result:
            try:
                with open(cached_cost_result['explain_cost_file'], 'r', encoding='utf-8') as f:
                    explain_cost_content = f.read()
                    print(f"üíæ Using cached EXPLAIN COST result file: {cached_cost_result['explain_cost_file']}")
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load cached EXPLAIN COST results: {str(e)}")
                cached_cost_result = None
        
        # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: „Ç≠„É£„ÉÉ„Ç∑„É•„ÅåÂà©Áî®„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÅØÂæìÊù•„ÅÆ„Éï„Ç°„Ç§„É´Ê§úÁ¥¢
        if not cached_cost_result:
            # 2. Search for latest EXPLAIN COST result files
            cost_original_files = glob.glob("output_explain_cost_original_*.txt")
            cost_optimized_files = glob.glob("output_explain_cost_optimized_*.txt")
            
            # Prioritize original query EXPLAIN COST results, use optimized if not available
            cost_files = cost_original_files if cost_original_files else cost_optimized_files
            
            if cost_files:
                latest_cost_file = max(cost_files, key=os.path.getctime)
                try:
                    with open(latest_cost_file, 'r', encoding='utf-8') as f:
                        explain_cost_content = f.read()
                        print(f"üí∞ Loaded EXPLAIN COST result file: {latest_cost_file}")
                    
                    # Extract statistical information (structured extraction support)
                    structured_enabled = globals().get('STRUCTURED_EXTRACTION_ENABLED', 'Y')
                    
                    if structured_enabled.upper() == 'Y':
                        # üöÄ ÊßãÈÄ†ÂåñÊäΩÂá∫„Ç¢„Éó„É≠„Éº„ÉÅ
                        try:
                            structured_cost = extract_structured_cost_statistics(explain_cost_content)
                            
                            # Convert structured results to JSON format string
                            import json
                            cost_statistics = json.dumps(structured_cost, ensure_ascii=False, indent=2)
                            
                            print(f"üí∞ EXPLAIN COST structured extraction completed: {len(explain_cost_content):,} ‚Üí {len(cost_statistics):,} characters (compression ratio: {len(explain_cost_content)//len(cost_statistics) if len(cost_statistics) > 0 else 0}x)")
                            print(f"   {structured_cost.get('extraction_summary', 'üí∞ Statistical extraction completed')}")
                            
                        except Exception as extraction_error:
                            print(f"‚ö†Ô∏è EXPLAIN COST structured extraction failed, falling back to traditional method: {str(extraction_error)}")
                            # Fallback: Traditional extraction method
                            cost_statistics = extract_cost_statistics_from_explain_cost(explain_cost_content)
                            print(f"üìä Extracted EXPLAIN COST statistics (traditional method): {len(cost_statistics)} characters")
                    else:
                        # üîÑ Traditional extraction approach
                        cost_statistics = extract_cost_statistics_from_explain_cost(explain_cost_content)
                        print(f"üìä Extracted EXPLAIN COST statistics: {len(cost_statistics)} characters")
                
                    # üö® When DEBUG_ENABLED='Y', always save extracted statistical information
                    debug_enabled = globals().get('DEBUG_ENABLED', 'N')
                    if debug_enabled.upper() == 'Y':
                        try:
                            from datetime import datetime
                            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                            extracted_stats_filename = f"output_explain_cost_statistics_extracted_{timestamp}.json"
                            
                            with open(extracted_stats_filename, 'w', encoding='utf-8') as f:
                                f.write(f"# Extracted EXPLAIN COST statistical information (Generated date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\n")
                                f.write(f"# Extraction size: {len(cost_statistics):,} characters\n")
                                f.write(f"# Source file: {latest_cost_file}\n\n")
                                f.write(cost_statistics)
                            
                            print(f"üìÑ Saved extracted statistical information: {extracted_stats_filename}")
                            
                        except Exception as save_error:
                            print(f"‚ö†Ô∏è Failed to save extracted statistical information: {str(save_error)}")
                
                    # Size limit for statistical information (countermeasure for LLM token limits)
                    MAX_STATISTICS_SIZE = 50000  # Á¥Ñ50KBÂà∂Èôê
                    if len(cost_statistics) > MAX_STATISTICS_SIZE:
                        # üö® DEBUG_ENABLED='Y'„ÅÆÂ†¥Âêà„ÄÅÂÆåÂÖ®„Å™EXPLAIN COSTÁµ±Ë®àÊÉÖÂ†±„Çí„Éï„Ç°„Ç§„É´‰øùÂ≠ò
                        debug_enabled = globals().get('DEBUG_ENABLED', 'N')
                        if debug_enabled.upper() == 'Y':
                            try:
                                from datetime import datetime
                                timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                                full_stats_filename = f"output_explain_cost_statistics_full_{timestamp}.txt"
                                
                                with open(full_stats_filename, 'w', encoding='utf-8') as f:
                                    f.write(f"# Complete EXPLAIN COST statistical information (Generated date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\n")
                                    f.write(f"# Original size: {len(cost_statistics):,} characters\n")
                                    f.write(f"# LLM usage size: {MAX_STATISTICS_SIZE:,} characters\n\n")
                                    f.write(cost_statistics)
                                
                                print(f"üìÑ Saved complete EXPLAIN COST statistical information: {full_stats_filename}")
                                
                            except Exception as save_error:
                                print(f"‚ö†Ô∏è Failed to save EXPLAIN COST statistical information: {str(save_error)}")
                        
                        truncated_statistics = cost_statistics[:MAX_STATISTICS_SIZE]
                        truncated_statistics += f"\n\n‚ö†Ô∏è Statistical information was too large, truncated to {MAX_STATISTICS_SIZE} characters"
                        cost_statistics = truncated_statistics
                        print(f"‚ö†Ô∏è Statistical information truncated to {MAX_STATISTICS_SIZE} characters due to token limit")
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Failed to load EXPLAIN COST result file: {str(e)}")
                    explain_cost_content = ""
        
        if not explain_files and not cost_files:
            print("‚ö†Ô∏è EXPLAIN„ÉªEXPLAIN COST result files not found")
            # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: Âè§„ÅÑ„Éï„Ç°„Ç§„É´Âêç„Éë„Çø„Éº„É≥„ÇÇ„ÉÅ„Çß„ÉÉ„ÇØ
            old_explain_files = glob.glob("output_explain_plan_*.txt")
            if old_explain_files:
                latest_explain_file = max(old_explain_files, key=os.path.getctime)
                try:
                    with open(latest_explain_file, 'r', encoding='utf-8') as f:
                        explain_content = f.read()
                        print(f"‚úÖ Loaded legacy format EXPLAIN result file: {latest_explain_file}")
                        
                    # Physical PlanÊäΩÂá∫ÔºàÊóßÂΩ¢ÂºèÂØæÂøúÔºâ
                    if "== Physical Plan ==" in explain_content:
                        physical_plan_start = explain_content.find("== Physical Plan ==")
                        physical_plan_end = explain_content.find("== Photon", physical_plan_start)
                        if physical_plan_end == -1:
                            physical_plan_end = len(explain_content)
                        physical_plan = explain_content[physical_plan_start:physical_plan_end].strip()
                        
                    if "== Photon Explanation ==" in explain_content:
                        photon_start = explain_content.find("== Photon Explanation ==")
                        photon_explanation = explain_content[photon_start:].strip()
                except Exception as e:
                    print(f"‚ö†Ô∏è Failed to load legacy format EXPLAIN result file: {str(e)}")
            else:
                print("‚ö†Ô∏è EXPLAIN result files not found")
    
    # ÂÆüË°å„Éó„É©„É≥ÊÉÖÂ†±„ÅÆÊäΩÂá∫Ôºà„É°„Éà„É™„ÇØ„Çπ„Åã„ÇâÔºâ
    profiler_data = metrics.get('raw_profiler_data', {})
    plan_info = None
    if profiler_data:
        plan_info = extract_execution_plan_info(profiler_data)
    
    # BROADCASTÈÅ©Áî®ÂèØËÉΩÊÄß„ÅÆÂàÜÊûêÔºà„Éó„É©„É≥ÊÉÖÂ†±„ÇíÂê´„ÇÄÔºâ
    # üéØ BROADCASTÊúÄÈÅ©Âåñ„ÅØÁÑ°ÂäπÂåñÔºà„É¶„Éº„Ç∂„ÉºË¶ÅÊ±Ç„Å´„Çà„ÇäÈô§Â§ñÔºâ
    # üö® ÈáçË¶Å: „Åô„Åπ„Å¶„ÅÆÂøÖË¶Å„Å™„Ç≠„Éº„ÇíÂê´„ÇÅ„ÇãÔºàKeyErrorÈò≤Ê≠¢Ôºâ
    broadcast_analysis = {
        "feasibility": "disabled", 
        "broadcast_candidates": [], 
        "recommendations": [],
        "reasoning": ["BROADCAST„Éí„É≥„Éà„ÅØÊßãÊñá„Ç®„É©„Éº„ÅÆÂéüÂõ†„Å®„Å™„Çã„Åü„ÇÅÁÑ°ÂäπÂåñ"], 
        "is_join_query": True,
        "already_optimized": False,  # üö® Á∑äÊÄ•‰øÆÊ≠£: ÂøÖÈ†à„Ç≠„ÉºËøΩÂä†
        "spark_threshold_mb": 30.0,
        "compression_analysis": {},
        "detailed_size_analysis": [],
        "execution_plan_analysis": {},
        "existing_broadcast_nodes": [],
        "broadcast_applied_tables": [],
        # üö® Á∑äÊÄ•‰øÆÊ≠£: 30mb_hit_analysis „Ç≠„ÉºËøΩÂä†ÔºàKeyErrorÈò≤Ê≠¢Ôºâ
        "30mb_hit_analysis": {
            "has_30mb_candidates": False,
            "reason": "BROADCAST„Éí„É≥„Éà„ÅØÁÑ°ÂäπÂåñ„Åï„Çå„Å¶„ÅÑ„Çã„Åü„ÇÅÂàÜÊûêÂØæË±°Â§ñ"
        }
    }
    
    # „Éó„É©„É≥ÊÉÖÂ†±„Çí„É°„Éà„É™„ÇØ„Çπ„Å´ËøΩÂä†Ôºà„Éï„Ç°„Ç§„É´Âá∫Âäõ„Åß‰ΩøÁî®Ôºâ
    if plan_info:
        metrics['execution_plan_info'] = plan_info
    
    # üöÄ „Çª„É´33„Çπ„Çø„Ç§„É´„ÅÆË©≥Á¥∞„Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûê„ÇíÂÆüË°å
    detailed_bottleneck = extract_detailed_bottleneck_analysis(metrics)
    
    # ÊúÄÈÅ©Âåñ„ÅÆ„Åü„ÇÅ„ÅÆ„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÊÉÖÂ†±„ÇíÊ∫ñÂÇôÔºàË©≥Á¥∞ÁâàÔºâ
    optimization_context = []
    performance_critical_issues = []
    
    # Âü∫Êú¨ÁöÑ„Å™„Éú„Éà„É´„Éç„ÉÉ„ÇØÊÉÖÂ†±„ÅÆÊäΩÂá∫
    bottlenecks = metrics.get('bottleneck_indicators', {})
    
    if bottlenecks.get('has_spill', False):
        spill_gb = bottlenecks.get('spill_bytes', 0) / 1024 / 1024 / 1024
        optimization_context.append(f"„Çπ„Éî„É´Áô∫Áîü: {spill_gb:.1f}GB - „É°„É¢„É™ÂäπÁéá„ÅÆÊîπÂñÑ„ÅåÂøÖË¶Å")
    
    if bottlenecks.get('has_shuffle_bottleneck', False):
        optimization_context.append("„Ç∑„É£„ÉÉ„Éï„É´„Éú„Éà„É´„Éç„ÉÉ„ÇØ - JOIN„Å®GROUP BY„ÅÆÊúÄÈÅ©Âåñ„ÅåÂøÖË¶Å")
    
    if bottlenecks.get('cache_hit_ratio', 0) < 0.5:
        optimization_context.append("„Ç≠„É£„ÉÉ„Ç∑„É•ÂäπÁéá‰Ωé‰∏ã - „Éá„Éº„Çø„Ç¢„ÇØ„Çª„Çπ„Éë„Çø„Éº„É≥„ÅÆÊúÄÈÅ©Âåñ„ÅåÂøÖË¶Å")
    
    # üéØ Ë©≥Á¥∞„Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûêÁµêÊûú„Åã„Çâ„ÅÆËøΩÂä†ÊÉÖÂ†±
    if detailed_bottleneck["spill_analysis"]["total_spill_gb"] > 0:
        total_spill = detailed_bottleneck["spill_analysis"]["total_spill_gb"]
        spill_nodes_count = len(detailed_bottleneck["spill_analysis"]["spill_nodes"])
        performance_critical_issues.append(f"üö® CRITICAL: ÂêàË®à{total_spill:.1f}GB„ÅÆ„Çπ„Éî„É´„Åå{spill_nodes_count}ÂÄã„ÅÆ„Éé„Éº„Éâ„ÅßÁô∫Áîü")
        
        # ÊúÄ„ÇÇÈáçË¶Å„Å™„Çπ„Éî„É´„Éé„Éº„Éâ„ÇíÁâπÂÆö
        if detailed_bottleneck["spill_analysis"]["spill_nodes"]:
            top_spill_node = max(detailed_bottleneck["spill_analysis"]["spill_nodes"], key=lambda x: x["spill_gb"])
            performance_critical_issues.append(f"   ÊúÄÂ§ß„Çπ„Éî„É´„Éé„Éº„Éâ: {top_spill_node['node_name']} ({top_spill_node['spill_gb']:.2f}GB)")
    
    if detailed_bottleneck["skew_analysis"]["total_skewed_partitions"] > 0:
        total_skew = detailed_bottleneck["skew_analysis"]["total_skewed_partitions"]
        skewed_nodes_count = len(detailed_bottleneck["skew_analysis"]["skewed_nodes"])
        performance_critical_issues.append(f"‚öñÔ∏è „Éá„Éº„Çø„Çπ„Ç≠„É•„Éº: {total_skew}ÂÄã„ÅÆ„Çπ„Ç≠„É•„Éº„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„Åå{skewed_nodes_count}ÂÄã„ÅÆ„Éé„Éº„Éâ„ÅßÊ§úÂá∫")
    
    # TOP3„Éú„Éà„É´„Éç„ÉÉ„ÇØ„Éé„Éº„Éâ„ÅÆË©≥Á¥∞ÂàÜÊûê
    top3_bottlenecks = detailed_bottleneck["top_bottleneck_nodes"][:3]
    performance_critical_issues.append("üìä TOP3Âá¶ÁêÜÊôÇÈñì„Éú„Éà„É´„Éç„ÉÉ„ÇØ:")
    for node in top3_bottlenecks:
        severity_icon = "üî¥" if node["severity"] == "CRITICAL" else "üü†" if node["severity"] == "HIGH" else "üü°"
        performance_critical_issues.append(f"   {severity_icon} #{node['rank']}: {node['node_name'][:60]}...")
        performance_critical_issues.append(f"      ÂÆüË°åÊôÇÈñì: {node['duration_ms']:,}ms ({node['time_percentage']:.1f}%) | „É°„É¢„É™: {node['memory_mb']:.1f}MB")
        if node["spill_detected"]:
            performance_critical_issues.append(f"      üíø „Çπ„Éî„É´: {node['spill_gb']:.2f}GB - Á∑äÊÄ•ÂØæÂøúÂøÖË¶Å")
        if node["skew_detected"]:
            performance_critical_issues.append(f"      ‚öñÔ∏è „Çπ„Ç≠„É•„Éº: {node['skewed_partitions']}„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥ - „Éá„Éº„ÇøÂàÜÊï£ÊîπÂñÑÂøÖË¶Å")
    
    # üîÑ REPARTITION„Éí„É≥„Éà„ÅÆË©≥Á¥∞ÁîüÊàêÔºà„Çπ„Éî„É´Ê§úÂá∫ÊôÇ„ÅÆ„ÅøÔºâ
    repartition_hints = []
    if detailed_bottleneck["shuffle_optimization_hints"]:
        repartition_hints.append("üîÑ REPARTITION„Éí„É≥„ÉàÔºà„Çπ„Éî„É´Ê§úÂá∫ÊôÇ„ÅÆ„ÅøÔºâ:")
        for hint in detailed_bottleneck["shuffle_optimization_hints"]:
            priority_icon = "üö®" if hint["priority"] == "HIGH" else "üìà"
            repartition_hints.append(f"   {priority_icon} „Éé„Éº„ÉâID {hint['node_id']}: {hint['suggested_sql']}")
            repartition_hints.append(f"      Â±ûÊÄß: {', '.join(hint['attributes'])}")
            repartition_hints.append(f"      ÁêÜÁî±: {hint['reason']}")
            repartition_hints.append(f"      ÂäπÊûú: {hint['estimated_improvement']}")
            
            # „ÇØ„Ç®„É™„Å∏„ÅÆÈÅ©Áî®ÊñπÊ≥ï„ÅÆÂÖ∑‰ΩìÁöÑ„Å™ÊèêÊ°à
            main_attr = hint['attributes'][0]
            if 'GROUP BY' in original_query.upper():
                repartition_hints.append(f"      ÈÅ©Áî®ÊèêÊ°à: GROUP BYÂâç„Å´REPARTITION({hint['suggested_sql'].split('(')[1]}")
            elif 'JOIN' in original_query.upper():
                repartition_hints.append(f"      ÈÅ©Áî®ÊèêÊ°à: JOINÂâç„ÅÆ„ÉÜ„Éº„Éñ„É´„Çí{hint['suggested_sql']}„Åß„É™„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥")
    
    # üìä Âá¶ÁêÜÈÄüÂ∫¶ÈáçË¶ñ„ÅÆÊúÄÈÅ©ÂåñÊé®Â•®‰∫ãÈ†Ö
    speed_optimization_recommendations = []
    for rec in detailed_bottleneck["performance_recommendations"]:
        priority_icon = "üö®" if rec["priority"] == "CRITICAL" else "‚ö†Ô∏è" if rec["priority"] == "HIGH" else "üìù"
        speed_optimization_recommendations.append(f"{priority_icon} {rec['type'].upper()}: {rec['description']}")
    
    # Liquid ClusteringÊé®Â•®ÊÉÖÂ†±ÔºàLLM„Éô„Éº„ÇπÂØæÂøúÔºâ
    liquid_analysis = metrics.get('liquid_clustering_analysis', {})
    extracted_data = liquid_analysis.get('extracted_data', {})
    table_info = extracted_data.get('table_info', {})
    
    clustering_recommendations = []
    if table_info:
        for table_name in list(table_info.keys())[:3]:  # ‰∏ä‰Ωç3„ÉÜ„Éº„Éñ„É´
            clustering_recommendations.append(f"„ÉÜ„Éº„Éñ„É´ {table_name}: LLMÂàÜÊûê„Å´„Çà„ÇãÊé®Â•®„Ç´„É©„É†„Åß„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞Êé®Â•®")
    
    # ÊúÄÈÅ©Âåñ„Éó„É≠„É≥„Éó„Éà„ÅÆ‰ΩúÊàêÔºàÁ∞°ÊΩîÁâà„Åß„Çø„Ç§„É†„Ç¢„Ç¶„ÉàÂõûÈÅøÔºâ
    
    # ÂàÜÊûêÁµêÊûú„ÇíÁ∞°ÊΩîÂåñÔºà128KÂà∂ÈôêÂÜÖ„ÅßÊúÄÂ§ßÂäπÁéáÂåñÔºâ
    analysis_summary = ""
    if isinstance(analysis_result, str) and len(analysis_result) > 2000:
        # „Éó„É≠„É≥„Éó„ÉàÂÆπÈáè„ÅÆÁ¢∫‰øù„ÅÆ„Åü„ÇÅ„ÄÅÂàÜÊûêÁµêÊûú„ÅØË¶ÅÁÇπ„ÅÆ„Åø„Å´ÂúßÁ∏Æ
        analysis_summary = analysis_result[:2000] + "...[Ë¶ÅÁ¥ÑÔºö‰∏ªË¶Å„Éú„Éà„É´„Éç„ÉÉ„ÇØ„ÅÆ„Åø‰øùÊåÅ]"
    else:
        analysis_summary = str(analysis_result)
    
    # „Éú„Éà„É´„Éç„ÉÉ„ÇØÊÉÖÂ†±„ÅÆÁ∞°ÊΩîÂåñ
    bottleneck_summary = "„ÄÅ".join(optimization_context[:3]) if optimization_context else "Áâπ„Å´„Å™„Åó"
    
    # Liquid ClusteringÊé®Â•®„ÅÆÁ∞°ÊΩîÂåñ
    clustering_summary = "„ÄÅ".join(clustering_recommendations[:2]) if clustering_recommendations else "Áâπ„Å´„Å™„Åó"
    
    # üö® JOINÊà¶Áï•ÂàÜÊûê„ÅÆÁ∞°Áï•ÂåñÔºàBROADCAST„Éí„É≥„ÉàÁÑ°ÂäπÂåñÔºâ
    broadcast_summary = ["üéØ ÊúÄÈÅ©ÂåñÊñπÈáù: JOINÈ†ÜÂ∫èÊúÄÈÅ©ÂåñÔºàSpark„ÅÆËá™ÂãïÊà¶Áï•„ÇíÊ¥ªÁî®„ÄÅ„Éí„É≥„Éà‰∏ç‰ΩøÁî®Ôºâ"]
    
    optimization_prompt = f"""
„ÅÇ„Å™„Åü„ÅØDatabricks„ÅÆSQL„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊúÄÈÅ©Âåñ„ÅÆÂ∞ÇÈñÄÂÆ∂„Åß„Åô„ÄÇ‰ª•‰∏ã„ÅÆ**Ë©≥Á¥∞„Å™„Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûêÁµêÊûú**„ÇíÂü∫„Å´„ÄÅ**Âá¶ÁêÜÈÄüÂ∫¶ÈáçË¶ñ**„ÅßSQL„ÇØ„Ç®„É™„ÇíÊúÄÈÅ©Âåñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„ÄêÈáçË¶Å„Å™Âá¶ÁêÜÊñπÈáù„Äë
- ‰∏ÄÂõû„ÅÆÂá∫Âäõ„ÅßÂÆåÂÖ®„Å™SQL„ÇØ„Ç®„É™„ÇíÁîüÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ
- ÊÆµÈöéÁöÑ„Å™Âá∫Âäõ„ÇÑË§áÊï∞Âõû„Å´ÂàÜ„Åë„Å¶„ÅÆÂá∫Âäõ„ÅØÁ¶ÅÊ≠¢„Åß„Åô
- thinkingÊ©üËÉΩ„ÅßÊßãÈÄ†ÁêÜËß£‚Üí‰∏ÄÂõû„ÅßÂÆåÂÖ®„Å™SQLÂá∫Âäõ
- **‚ùå BROADCAST„Éí„É≥„ÉàÔºà/*+ BROADCAST */„ÄÅ/*+ BROADCAST(table) */Ôºâ„ÅØ‰∏ÄÂàá‰ΩøÁî®Á¶ÅÊ≠¢**
- **‚úÖ JOINÊà¶Áï•„ÅØSpark„ÅÆËá™ÂãïÊúÄÈÅ©Âåñ„Å´Âßî„Å≠„Å¶„Éí„É≥„Éà‰∏ç‰ΩøÁî®„ÅßÊúÄÈÅ©Âåñ**

„ÄêÂÖÉ„ÅÆSQL„ÇØ„Ç®„É™„Äë
```sql
{original_query}
```

„Äêüìä „Çª„É´33Ë©≥Á¥∞„Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûêÁµêÊûú„Äë
{chr(10).join(performance_critical_issues) if performance_critical_issues else "ÁâπÂà•„Å™ÈáçË¶ÅË™≤È°å„ÅØË®≠ÂÆö„Å™„Åó"}

„ÄêüîÑ REPARTITION„Éí„É≥„ÉàÔºà„Çπ„Éî„É´Ê§úÂá∫ÊôÇ„ÅÆ„ÅøÔºâ„Äë
{chr(10).join(repartition_hints) if repartition_hints else "„Çπ„Éî„É´„ÅåÊ§úÂá∫„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„Åü„ÇÅ„ÄÅREPARTITION„Éí„É≥„Éà„ÅØÈÅ©Áî®ÂØæË±°Â§ñ„Åß„Åô"}

„ÄêüöÄ Âá¶ÁêÜÈÄüÂ∫¶ÈáçË¶ñ„ÅÆÊúÄÈÅ©ÂåñÊé®Â•®‰∫ãÈ†Ö„Äë
{chr(10).join(speed_optimization_recommendations) if speed_optimization_recommendations else "ÁâπÂà•„Å™Êé®Â•®‰∫ãÈ†Ö„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì"}

„ÄêÂü∫Êú¨ÁöÑ„Å™„Éú„Éà„É´„Éç„ÉÉ„ÇØÊÉÖÂ†±„Äë
{chr(10).join(optimization_context) if optimization_context else "‰∏ªË¶Å„Å™„Éú„Éà„É´„Éç„ÉÉ„ÇØ„ÅØË®≠ÂÆö„Å™„Åó"}

„ÄêJOINÊà¶Áï•ÂàÜÊûêÁµêÊûú„Äë
Spark„ÅÆËá™ÂãïJOINÊà¶Áï•„Çí‰ΩøÁî®Ôºà„Ç®„É©„ÉºÂõûÈÅø„ÅÆ„Åü„ÇÅ„Éí„É≥„Éà„ÅØ‰ΩøÁî®„Åõ„ÅöÔºâ

„ÄêLiquid ClusteringÊé®Â•®„Äë
{chr(10).join(clustering_recommendations) if clustering_recommendations else "ÁâπÂà•„Å™Êé®Â•®‰∫ãÈ†Ö„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì"}

„Äê„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂàÜÊûêÁµêÊûúÔºà„Çµ„Éû„É™„ÉºÔºâ„Äë
{analysis_summary}

„Äêüîç EXPLAINÁµêÊûúÂàÜÊûêÔºàEXPLAIN_ENABLED=Y„ÅÆÂ†¥Âêà„ÅÆ„ÅøÔºâ„Äë
{f'''
**Physical PlanÂàÜÊûê:**
```
{physical_plan}
```

**Photon ExplanationÂàÜÊûê:**
```
{photon_explanation}
```

**Physical PlanÊúÄÈÅ©Âåñ„ÅÆÈáçË¶Å„Éù„Ç§„É≥„Éà:**
- „Éï„Ç°„Ç§„É´„Çπ„Ç≠„É£„É≥„ÅÆÂäπÁéáÊÄß
- „Ç∏„Éß„Ç§„É≥Êà¶Áï•„ÅÆÂ¶•ÂΩìÊÄß
- „Ç∑„É£„ÉÉ„Éï„É´Êìç‰Ωú„ÅÆÊúÄÂ∞èÂåñ
- „Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥ÔºàÂàóÈÅ∏ÊäûÔºâ„ÅÆÊúÄÈÅ©Âåñ
- „Éï„Ç£„É´„Çø„Éº„Éó„ÉÉ„Ç∑„É•„ÉÄ„Ç¶„É≥„ÅÆÊ¥ªÁî®

**PhotonÊúÄÈÅ©Âåñ„ÅÆÈáçË¶Å„Éù„Ç§„É≥„Éà:**
- PhotonÊú™ÂØæÂøúÈñ¢Êï∞„ÅÆÊ§úÂá∫„Å®‰ª£ÊõøÈñ¢Êï∞„Å∏„ÅÆÂ§âÊõ¥
- „Éô„ÇØ„Éà„É´ÂåñÂá¶ÁêÜ„Å´ÈÅ©„Åó„ÅüÈñ¢Êï∞„ÅÆÈÅ∏Êäû
- PhotonÂà©Áî®ÁéáÂêë‰∏ä„ÅÆ„Åü„ÇÅ„ÅÆÊõ∏ÂºèÂ§âÊõ¥
- „Ç≥„É≥„Éë„Ç§„É´ÊôÇÊúÄÈÅ©Âåñ„ÅÆÊ¥ªÁî®
''' if explain_enabled.upper() == 'Y' and (physical_plan or photon_explanation) else '(EXPLAINÂÆüË°å„ÅåÁÑ°Âäπ„ÄÅ„Åæ„Åü„ÅØEXPLAINÁµêÊûú„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì)'}

„Äêüí∞ EXPLAIN COSTÁµ±Ë®àÊÉÖÂ†±ÂàÜÊûêÔºàÁµ±Ë®à„Éô„Éº„ÇπÊúÄÈÅ©ÂåñÔºâ„Äë
{f'''
**ÊßãÈÄ†ÂåñEXPLAIN COSTÁµ±Ë®àÊÉÖÂ†±:**
```json
{cost_statistics}
```

**üß† ÊßãÈÄ†ÂåñÁµ±Ë®à„Éá„Éº„Çø„ÅÆÊ¥ªÁî®ÊåáÈáù:**
‰∏äË®ò„ÅØÊßãÈÄ†ÂåñÊäΩÂá∫„Åï„Çå„ÅüÁµ±Ë®àÊÉÖÂ†±„Åß„Åô„ÄÇ‰ª•‰∏ã„ÅÆÈ†ÖÁõÆ„ÇíÈáçÁÇπÁöÑ„Å´ÂàÜÊûê„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö

- **table_stats**: „ÉÜ„Éº„Éñ„É´Âà•Ë©≥Á¥∞Áµ±Ë®àÔºà„ÉÜ„Éº„Éñ„É´Âêç„ÄÅ„Çµ„Ç§„Ç∫„ÄÅË°åÊï∞Ôºâ
- **critical_stats**: ÈáçË¶ÅÁµ±Ë®àÂÄ§ÔºàÊúÄÂ§ß„ÉÜ„Éº„Éñ„É´„ÄÅÁ∑èË°åÊï∞„ÄÅÂ∞è„ÉÜ„Éº„Éñ„É´ÂÄôË£úÔºâ
- **largest_table**: ÊúÄÂ§ß„ÉÜ„Éº„Éñ„É´„ÅÆÂêçÂâç„Å®„Çµ„Ç§„Ç∫ÔºàJOINÈ†ÜÂ∫è„ÅÆÂü∫Ê∫ñÔºâ
- **small_table_candidates**: Â∞è„ÉÜ„Éº„Éñ„É´Ôºà„ÉÜ„Éº„Éñ„É´Âêç„Å®„Çµ„Ç§„Ç∫Ôºâ
- **table_breakdown**: „ÉÜ„Éº„Éñ„É´Âêç„ÅÆË©≥Á¥∞ÔºàÊúÄÂ§ß„ÉÜ„Éº„Éñ„É´Âêç„ÄÅÂ∞è„ÉÜ„Éº„Éñ„É´ÂêçÔºâ

**üéØ „ÉÜ„Éº„Éñ„É´Âêç„Çí‰Ωø„Å£„ÅüÁ≤æÂØÜÊúÄÈÅ©Âåñ:**
1. **JOINÈ†ÜÂ∫è„ÅÆÊúÄÈÅ©Âåñ:**
   - „ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫„Å´Âü∫„Å•„ÅèÂäπÁéáÁöÑ„Å™JOINÈ†ÜÂ∫è„ÅÆÊ±∫ÂÆö
   - Â∞è„ÉÜ„Éº„Éñ„É´„Åã„ÇâÂ§ß„ÉÜ„Éº„Éñ„É´„Å∏„ÅÆÊÆµÈöéÁöÑÁµêÂêà

2. **JOINÈ†ÜÂ∫è„ÅÆÂÖ∑‰ΩìÁöÑÊèêÊ°à:**
   - largest_table.name„ÇíÊúÄÂæå„Å´ÈÖçÁΩÆ
   - table_stats„ÅÆ„Çµ„Ç§„Ç∫È†Ü„ÅßJOINÈ†ÜÂ∫è„ÇíÊúÄÈÅ©Âåñ
   - ÂÖ∑‰ΩìÁöÑ„Å™„ÉÜ„Éº„Éñ„É´Âêç„ÅßJOINÊñá„ÇíÊîπÂñÑ

3. **ÊõñÊòßÊÄßËß£Ê±∫„ÅÆÂÖ∑‰ΩìÁöÑÊèêÊ°à:**
   - „Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆ„ÉÜ„Éº„Éñ„É´Âêç„Å®table_stats„ÇíÁÖßÂêà
   - ÂÖ∑‰ΩìÁöÑ„Å™„Ç®„Ç§„É™„Ç¢„ÇπÊèêÊ°àÔºà‰æã: `store_sales.ss_item_sk`Ôºâ

**üöÄ ÊßãÈÄ†Âåñ„Éá„Éº„ÇøËß£Êûê„ÅÆÂÆüË°å‰æã:**
1. table_statsÂÜÖ„ÅßÂ∞è„ÉÜ„Éº„Éñ„É´„ÇíÁâπÂÆö„Åó„ÄÅÂäπÁéáÁöÑ„Å™JOINÈ†ÜÂ∫è„ÇíÊ±∫ÂÆö
2. largest_table_name„Åå1GB‰ª•‰∏ä ‚Üí Â§ß„ÉÜ„Éº„Éñ„É´„Å®„Åó„Å¶ÊúÄÁµÇJOIN„Å´ÈÖçÁΩÆ
3. JOINÈ†ÜÂ∫è„ÅÆÂÖ∑‰ΩìÁöÑ„Å™ÊîπÂñÑÊèêÊ°à„ÇíÁîüÊàê
4. „ÉÜ„Éº„Éñ„É´Âêç„ÇíÊòéÁ§∫„Åó„ÅüJOINÈ†ÜÂ∫èÊèêÊ°à„ÇíÁîüÊàê

**üö® „Éà„Éº„ÇØ„É≥Âà∂ÈôêÂØæÁ≠ñ„Å´„Å§„ÅÑ„Å¶:**
- JOIN/SCANÊÉÖÂ†±„ÅåÂ§öÊï∞„ÅÆÂ†¥Âêà„ÄÅÈáçË¶ÅÂ∫¶È†Ü„Å´Ë¶ÅÁ¥ÑÊ∏à„Åø
- SUMMARYÈ†ÖÁõÆ„ÅØË§áÊï∞Êìç‰Ωú„ÅÆÈõÜÁ¥Ñ„ÇíÁ§∫„Åó„Åæ„Åô
- Ë©≥Á¥∞„ÅØ optimization_applied „Éï„É©„Ç∞„ÅßÁ¢∫Ë™çÂèØËÉΩ
- Physical Plan„Åå100KBË∂Ö„ÅÆÂ†¥Âêà„ÅØËá™ÂãïË™øÊï¥Ê∏à„Åø
''' if explain_enabled.upper() == 'Y' and cost_statistics else '(EXPLAIN COSTÂÆüË°å„ÅåÁÑ°Âäπ„ÄÅ„Åæ„Åü„ÅØÁµ±Ë®àÊÉÖÂ†±„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì)'}

„ÄêüéØ Âá¶ÁêÜÈÄüÂ∫¶ÈáçË¶ñ„ÅÆÊúÄÈÅ©ÂåñË¶ÅÊ±Ç„Äë
**ÊúÄÈáçË¶Å**: ‰ª•‰∏ã„ÅÆÈ†ÜÂ∫è„ÅßÂá¶ÁêÜÈÄüÂ∫¶„ÅÆÊîπÂñÑ„ÇíÂÑ™ÂÖà„Åó„Å¶„Åè„Å†„Åï„ÅÑ

1. **üö® CRITICALÂÑ™ÂÖàÂ∫¶**: „Çπ„Éî„É´ÂØæÁ≠ñÔºà„É°„É¢„É™ÂäπÁéáÊîπÂñÑÔºâ
   - Â§ßÈáè„Çπ„Éî„É´Ôºà5GB‰ª•‰∏äÔºâ„ÅåÊ§úÂá∫„Åï„Çå„ÅüÂ†¥Âêà„ÅØÊúÄÂÑ™ÂÖà„ÅßÂØæÂá¶
   - „É°„É¢„É™ÂäπÁéáÁöÑ„Å™JOINÈ†ÜÂ∫è„ÅÆÊ§úË®é
   - ‰∏≠ÈñìÁµêÊûú„ÅÆ„Çµ„Ç§„Ç∫ÂâäÊ∏õ

2. **üîÑ REPARTITION„Éí„É≥„ÉàÈÅ©Áî®**Ôºàüö® **„Çπ„Éî„É´Ê§úÂá∫ÊôÇ„ÅÆÂ†¥Âêà„ÅÆ„Åø** - ÈáçË¶Å„Å™Êù°‰ª∂Ôºâ
   - ‚ùå **„Çπ„Éî„É´„ÅåÊ§úÂá∫„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà**: REPARTITION„Éí„É≥„Éà„ÅØ‰∏ÄÂàáÈÅ©Áî®„Åó„Å™„ÅÑ
   - ‚úÖ **„Çπ„Éî„É´„ÅåÊ§úÂá∫„Åï„Çå„ÅüÂ†¥Âêà„ÅÆ„Åø**: REPARTITION„Éí„É≥„Éà„ÇíÈÅ©Áî®
   - ‚ö†Ô∏è **Ë®òËºâ„É´„Éº„É´**: „Çπ„Éî„É´Êú™Ê§úÂá∫„ÅÆÂ†¥Âêà„ÅØ„ÄåREPARTITION„ÅÆÈÅ©Áî®„Äç„Çí‰∏ÄÂàáË®òËºâ„Åó„Å™„ÅÑ
   - Ê§úÂá∫„Åï„Çå„ÅüShuffle attributes„ÇíÂü∫„Å´ÂÖ∑‰ΩìÁöÑ„Å™REPARTITION„Éí„É≥„Éà„ÇíÈÅ©Áî®Ôºà„Çπ„Éî„É´Ê§úÂá∫ÊôÇ„ÅÆ„ÅøÔºâ

3. **‚öñÔ∏è „Éá„Éº„Çø„Çπ„Ç≠„É•„ÉºÂØæÁ≠ñ**
   - „Çπ„Ç≠„É•„Éº„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥Ôºà10ÂÄã‰ª•‰∏äÔºâÊ§úÂá∫ÊôÇ„ÅØÂàÜÊï£ÊîπÂñÑ„ÇíÂÑ™ÂÖà
   - ÈÅ©Âàá„Å™„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„Ç≠„Éº„ÅÆÈÅ∏Êäû
   - „Éá„Éº„ÇøÂàÜÊï£„ÅÆÂùáÁ≠âÂåñ

4. **üìà „Ç∑„É£„ÉÉ„Éï„É´ÊúÄÈÅ©Âåñ**
   - „Ç∑„É£„ÉÉ„Éï„É´Èáè„ÅÆÊúÄÂ∞èÂåñ
   - ÈÅ©Âàá„Å™JOINÊà¶Áï•„ÅÆÈÅ∏Êäû
   - „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØËª¢ÈÄÅÈáè„ÅÆÂâäÊ∏õ

5. **üéØ JOINÊà¶Áï•ÊúÄÈÅ©Âåñ**
   - Â∞è„ÉÜ„Éº„Éñ„É´„ÇíÂÖà„Å´Âá¶ÁêÜ„Åô„ÇãÂäπÁéáÁöÑ„Å™JOINÈ†ÜÂ∫è
   - Spark„ÅÆËá™ÂãïÊúÄÈÅ©Âåñ„ÇíÊ¥ªÁî®„Åó„ÅüJOINÊà¶Áï•Ôºà„Éí„É≥„Éà‰∏ç‰ΩøÁî®Ôºâ
   - ‰∏≠ÈñìÁµêÊûú„ÅÆ„Çµ„Ç§„Ç∫ÊúÄÂ∞èÂåñ

6. **üíæ „É°„É¢„É™ÂäπÁéáÂåñ**
   - ‰∏çË¶Å„Å™„Ç´„É©„É†„ÅÆÈô§Âéª
   - ÈÅ©Âàá„Å™„Éï„Ç£„É´„Çø„É™„É≥„Ç∞È†ÜÂ∫è
   - ‰∏≠ÈñìÁµêÊûú„ÅÆ„Ç≠„É£„ÉÉ„Ç∑„É•Ê¥ªÁî®

7. **üîß ÂÆüË°å„Éó„É©„É≥ÊúÄÈÅ©Âåñ**
   - PHOTON„Ç®„É≥„Ç∏„É≥ÊúÄÈÅ©ÂåñÔºàÁõÆÊ®ô„ÅØPhotonÂà©Áî®Áéá90%‰ª•‰∏ä)
   - Liquid ClusteringÊ¥ªÁî® (WhereÊù°‰ª∂„ÅÆÊõ∏„ÅçÊèõ„ÅàÂê´„ÇÄÊ§úË®é„ÇíÂÆüÊñΩÔºâ
   - CTEÊ¥ªÁî®„Å´„Çà„ÇãÂÖ±ÈÄöÂåñ

8. **üìä EXPLAINÁµêÊûú„Å´Âü∫„Å•„ÅèÊúÄÈÅ©Âåñ**ÔºàEXPLAIN_ENABLED=Y„ÅÆÂ†¥ÂêàÔºâ
   - **Physical PlanÂàÜÊûê„Å´Âü∫„Å•„ÅèÊúÄÈÅ©Âåñ**: 
     - ÈùûÂäπÁéá„Å™„Çπ„Ç≠„É£„É≥Êìç‰Ωú„ÅÆÊîπÂñÑ
     - „Ç∏„Éß„Ç§„É≥È†ÜÂ∫è„ÅÆÊúÄÈÅ©ÂåñÔºàSpark„ÅÆËá™ÂãïÂà§ÂÆö„Å´‰æùÂ≠òÔºâ
     - ‰∏çË¶Å„Å™„Ç∑„É£„ÉÉ„Éï„É´Êìç‰Ωú„ÅÆÂâäÈô§
     - „Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éó„ÉÉ„Ç∑„É•„ÉÄ„Ç¶„É≥„ÅÆÈÅ©Áî®
   - **PhotonÊú™ÂØæÂøúÈñ¢Êï∞„ÅÆÊúÄÈÅ©Âåñ**:
     - Photon Explanation„ÅßÊ§úÂá∫„Åï„Çå„ÅüÊú™ÂØæÂøúÈñ¢Êï∞„ÅÆ‰ª£ÊõøÈñ¢Êï∞„Å∏„ÅÆÂ§âÊõ¥
     - „Éô„ÇØ„Éà„É´ÂåñÂá¶ÁêÜ„Å´ÈÅ©„Åó„ÅüÈñ¢Êï∞„Å∏„ÅÆÊõ∏„ÅçÊèõ„Åà
     - PhotonÂà©Áî®ÁéáÂêë‰∏ä„ÅÆ„Åü„ÇÅ„ÅÆÈñ¢Êï∞ÈÅ∏Êäû
     - „Ç≥„É≥„Éë„Ç§„É´ÊôÇÊúÄÈÅ©Âåñ„ÅÆÊ¥ªÁî®

9. **üéØ JOINÈ†ÜÂ∫è„Å®„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„Éã„É≥„Ç∞„ÅÆÊúÄÈÅ©Âåñ**ÔºàÈáçË¶Å„Å™ÊßãÈÄ†ÁöÑÊúÄÈÅ©ÂåñÔºâ
   - **ÂäπÁéáÁöÑ„Å™JOINÈ†ÜÂ∫è**: Â∞è„Åï„ÅÑ„ÉÜ„Éº„Éñ„É´„Åã„ÇâÂ§ß„Åç„ÅÑ„ÉÜ„Éº„Éñ„É´„Å∏„ÅÆÊÆµÈöéÁöÑÁµêÂêà
   - **Spark„ÅÆËá™ÂãïJOINÊà¶Áï•**: „Ç®„É≥„Ç∏„É≥„ÅÆËá™ÂãïÂà§ÂÆö„Å´Âßî„Å≠„Çã„Åì„Å®„Åß„Ç®„É©„ÉºÂõûÈÅø
   - **ÁµêÂêàÂæå„ÅÆREPARTITION**: ÁµêÂêàÂæå„Å´GROUP BY„ÅÆÂäπÁéáÂåñ„ÅÆ„Åü„ÇÅREPARTITION„Éí„É≥„Éà„ÇíÈÅ©Áî®
   - **CTEÊßãÈÄ†„ÅÆÊ¥ªÁî®**: ÂøÖË¶Å„Å´Âøú„Åò„Å¶CTE„Çí‰Ωø„Å£„Å¶ÊÆµÈöéÁöÑ„Å´Âá¶ÁêÜ„Åô„ÇãÊßãÈÄ†„ÅßÂá∫Âäõ
   - **„Çπ„Éî„É´ÂõûÈÅø„Å®‰∏¶ÂàóÂ∫¶**: „Çπ„Éî„É´„ÇíÂõûÈÅø„Åó„Å§„Å§„ÄÅ‰∏¶ÂàóÂ∫¶„ÅÆÈ´ò„ÅÑÂá¶ÁêÜ„Åå„Åß„Åç„Çã„Çà„ÅÜÊúÄÈÅ©Âåñ
   
   **üîÑ Êé®Â•®„Åô„ÇãÂá¶ÁêÜ„Éï„É≠„Éº:**
   ```sql
   -- ‚úÖ Êé®Â•®„Éë„Çø„Éº„É≥: ÂäπÁéáÁöÑJOINÈ†ÜÂ∫è ‚Üí CTE ‚Üí REPARTITION ‚Üí GROUP BY
   WITH efficient_joined AS (
     SELECT 
       large_table.columns...,
       small_table.columns...
     FROM small_table  -- Â∞è„ÉÜ„Éº„Éñ„É´„ÇíÂÖà„Å´ÈÖçÁΩÆ
       JOIN large_table ON small_table.key = large_table.key
   ),
   repartitioned_for_groupby AS (
     SELECT /*+ REPARTITION(200, group_key) */
       columns...
     FROM efficient_joined
   )
   SELECT 
     group_key,
     COUNT(*),
     SUM(amount)
   FROM repartitioned_for_groupby
   GROUP BY group_key
   ```

„ÄêüîÑ REPARTITION„Éí„É≥„ÉàÈÅ©Áî®„É´„Éº„É´ - ÊßãÊñá„Ç®„É©„ÉºÈò≤Ê≠¢„Äë
REPARTITION„Éí„É≥„Éà„Çí‰ªò‰∏é„Åô„ÇãÂ†¥Âêà„ÅØ‰ª•‰∏ã„ÅÆÊúÄÈÅ©Âåñ„É´„Éº„É´„ÇíÂÆà„Å£„Å¶„Åè„Å†„Åï„ÅÑÔºö

üö® **ÊúÄÈáçË¶Å„É´„Éº„É´**: 
- **‚ùå „Çπ„Éî„É´Êú™Ê§úÂá∫ÊôÇ**: REPARTITION„Éí„É≥„Éà„ÅØÁµ∂ÂØæ„Å´ÈÅ©Áî®„ÉªË®òËºâ„Åó„Å¶„ÅØ„ÅÑ„Åë„Å™„ÅÑ
- **‚úÖ „Çπ„Éî„É´Ê§úÂá∫ÊôÇ„ÅÆ„Åø**: REPARTITION„Éí„É≥„Éà„ÇíÈÅ©Áî®
- **‚ö†Ô∏è Ë®òËºâÁ¶ÅÊ≠¢**: „Çπ„Éî„É´„ÅåÊ§úÂá∫„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÄÅÊé®Â•®‰∫ãÈ†Ö„ÇÑÁ∑äÊÄ•ÂØæÂøú„Å´„ÄåREPARTITIONÈÅ©Áî®„Äç„ÇíÂê´„ÇÅ„Å™„ÅÑ

ÊäÄË°ìË©≥Á¥∞:
- **REPARTITION„Éí„É≥„Éà„ÅØ SELECT /*+ REPARTITION(„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥Êï∞, „Ç´„É©„É†Âêç) „ÅÆÂΩ¢Âºè„ÅßÊåáÂÆö**
- **REPARTITION„Éí„É≥„Éà„ÅÆÈÅ©Áî®‰ΩçÁΩÆ„ÅØ„ÄÅÂØæË±°„Å®„Å™„ÇãJOIN„ÇÑGROUP BY„ÇíÂê´„ÇÄSELECT„ÅÆÁõ¥Ââç„Åß„ÅÇ„Çã„Åü„ÇÅ„ÄÅÂá∫Âäõ„Åï„Çå„Åüoutput_explain_plan_*.txt„ÅÆPhysical Plan„Åã„ÇâÂÆüË°åË®àÁîª„ÇíÁêÜËß£„Åó„ÄÅÈÅ©Âàá„Å™‰ΩçÁΩÆ„Å´REPARTITION „Éí„É≥„Éà„Çí‰ªò‰∏é„Åô„Çã„Åì„Å®**

**üö® REPARTITION„Éí„É≥„ÉàÈÖçÁΩÆ„ÅÆÈáçË¶Å„Å™ÊßãÊñá„É´„Éº„É´:**
1. **JOIN„ÇÑGROUP BY„ÅÆÂá¶ÁêÜÊÆµÈöé„ÅßÂäπÊûú„ÇíÁô∫ÊèÆ„Åô„Çã„Åü„ÇÅ„ÄÅÂøÖ„Åö„Çµ„Éñ„ÇØ„Ç®„É™ÂÜÖÈÉ®„Å´ÈÖçÁΩÆ„Åô„Çã**
2. **„Éà„ÉÉ„Éó„É¨„Éô„É´„ÅÆSELECTÊñá„Å´ÈÖçÁΩÆ„Åô„Çã„Å®ÊúÄÁµÇÂá∫ÂäõÊÆµÈöé„ÅÆ„Åø„Å´ÂΩ±Èüø„Åó„ÄÅJOIN/GROUP BYÂá¶ÁêÜÊÆµÈöé„Å´„ÅØÂΩ±Èüø„Åó„Å™„ÅÑ**
3. **Ë§áÊï∞„ÅÆREPARTITION„Éí„É≥„Éà„ÅØÂêÑ„Çµ„Éñ„ÇØ„Ç®„É™ÂÜÖÈÉ®„Å´ÂÄãÂà•„Å´ÈÖçÁΩÆ„Åô„Çã**
4. **„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥Êï∞„Å®„Ç´„É©„É†Âêç„ÅØÂøÖÈ†à„Éë„É©„É°„Éº„Çø„Å®„Åó„Å¶ÊåáÂÆö„Åô„Çã**

üö® **REPARTITION„Éí„É≥„ÉàÈÅ©Áî®„ÅÆÂé≥Ê†º„Å™„É´„Éº„É´**Ôºö
- **‚ùå „Çπ„Éî„É´Êú™Ê§úÂá∫**: REPARTITION„Éí„É≥„Éà„ÅØÁµ∂ÂØæ„Å´ÈÅ©Áî®„Åó„Å™„ÅÑ„ÉªË®òËºâ„Åó„Å™„ÅÑ
- **‚úÖ „Çπ„Éî„É´Ê§úÂá∫ÊôÇ„ÅÆ„Åø**: GROUP BYÂâç„Å´REPARTITION(Êé®Â•®Êï∞, group_by_column)
- **‚úÖ „Çπ„Éî„É´Ê§úÂá∫ÊôÇ„ÅÆ„Åø**: JOINÂâç„Å´REPARTITION(Êé®Â•®Êï∞, join_key)
- **ÈáçË¶Å**: „Çπ„Éî„É´„ÅåÊ§úÂá∫„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄåREPARTITION„ÅÆÈÅ©Áî®„Äç„ÇíÊé®Â•®‰∫ãÈ†Ö„Å´Âê´„ÇÅ„Å™„ÅÑ
- **Ë®òËºâÁ¶ÅÊ≠¢**: „Çπ„Éî„É´Êú™Ê§úÂá∫ÊôÇ„Å´„ÄåÁ∑äÊÄ•ÂØæÂøú: REPARTITION„ÅÆÈÅ©Áî®„ÄçÁ≠â„ÇíË®òËºâ„Åó„Å¶„ÅØ„ÅÑ„Åë„Å™„ÅÑ

**üö® CREATE TABLE AS SELECT (CTAS) „Åß„ÅÆREPARTITIONÈÖçÁΩÆ„ÅÆÈáçË¶Å„Å™Ê≥®ÊÑè‰∫ãÈ†Ö:**
- CREATE TABLE AS SELECTÊñá„Åß„ÅØ„ÄÅ„Éà„ÉÉ„Éó„É¨„Éô„É´„ÅÆSELECTÂè•„Å´REPARTITION„Éí„É≥„Éà„ÇíÈÖçÁΩÆ„Åô„Çã„Å®„ÄÅ**ÊúÄÁµÇÁöÑ„Å™Âá∫ÂäõÊõ∏„ÅçËæº„ÅøÊÆµÈöé„ÅÆ„Åø„Å´ÂΩ±Èüø**„Åó„ÄÅJOIN „ÇÑÈõÜË®à„Å™„Å©„ÅÆ‰∏≠ÈñìÂá¶ÁêÜÊÆµÈöé„Å´„ÅØÂΩ±Èüø„Åó„Å™„ÅÑ
- JOIN„ÅÆÂâç„Å´„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„Éã„É≥„Ç∞„ÇíÂà∂Âæ°„Åô„Çã„Å´„ÅØ„ÄÅ**REPARTITION„Éí„É≥„Éà„Çí„Çµ„Éñ„ÇØ„Ç®„É™ÂÜÖÈÉ®„Å´ÈÖçÁΩÆ„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã**
- „Åì„Çå„Å´„Çà„Çä„ÄÅSpark„Åå„Éá„Éº„Çø„Éï„É≠„Éº„ÅÆÈÅ©Âàá„Å™ÊôÇÁÇπ„Åß„É™„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„ÇíÈÅ©Áî®„Åó„ÄÅÊõ∏„ÅçËæº„ÅøÊÆµÈöé„Åß„ÅØ„Å™„ÅèÂÆüË°åÊÆµÈöé„ÅßÊúÄÈÅ©Âåñ„Åï„Çå„Çã

**Ê≠£„Åó„ÅÑCTAS REPARTITION„Éí„É≥„ÉàÈÖçÁΩÆ‰æã:**
```sql
-- ‚ùå ÈñìÈÅï„ÅÑ: „Éà„ÉÉ„Éó„É¨„Éô„É´„ÅÆSELECTÂè•ÔºàÊõ∏„ÅçËæº„ÅøÊÆµÈöé„ÅÆ„Åø„Å´ÂΩ±ÈüøÔºâ
CREATE TABLE optimized_table AS
SELECT /*+ REPARTITION(200, join_key) */
  t1.column1, t2.column2
FROM table1 t1
  JOIN table2 t2 ON t1.join_key = t2.join_key

-- ‚úÖ Ê≠£„Åó„ÅÑ: „Çµ„Éñ„ÇØ„Ç®„É™ÂÜÖÈÉ®„Å´ÈÖçÁΩÆÔºàJOINÂá¶ÁêÜÊÆµÈöé„ÅßÊúÄÈÅ©ÂåñÔºâ
CREATE TABLE optimized_table AS
SELECT 
  t1.column1, t2.column2
FROM (
  SELECT /*+ REPARTITION(200, join_key) */
    column1, join_key
  FROM table1
) t1
  JOIN table2 t2 ON t1.join_key = t2.join_key
```

**üö® ÂÖ®Ëà¨ÁöÑ„Å™REPARTITION„Éí„É≥„ÉàÈÖçÁΩÆ„ÅÆÈáçË¶Å„Å™Ê≥®ÊÑè‰∫ãÈ†Ö:**
- **CTAS‰ª•Â§ñ„ÅÆ„ÇØ„Ç®„É™„Åß„ÇÇÂêåÊßò**Ôºö„Éà„ÉÉ„Éó„É¨„Éô„É´„ÅÆ„ÇØ„Ç®„É™„Å´REPARTITION„Éí„É≥„Éà„ÇíÈÖçÁΩÆ„Åô„Çã„Å®„ÄÅ**ÊúÄÁµÇÁöÑ„Å™Âá∫ÂäõÊÆµÈöé„ÅÆ„Åø„Å´ÂΩ±Èüø**„Åó„ÄÅJOIN „ÇÑÈõÜË®à„Å™„Å©„ÅÆ‰∏≠ÈñìÂ§âÊèõÊÆµÈöé„Å´„ÅØÂΩ±Èüø„Åó„Å™„ÅÑ
- „Åì„ÅÆÂãï‰Ωú„ÅØ„ÄÅÁµêÊûú„Çí„ÉÜ„Éº„Éñ„É´„Å´Êõ∏„ÅçËæº„ÇÄ„Åã„Å©„ÅÜ„Åã„Å´Èñ¢‰øÇ„Å™„Åè**„Åô„Åπ„Å¶„ÅÆSpark SQL„ÇØ„Ç®„É™„Åß‰∏ÄË≤´**„Åó„Å¶„ÅÑ„Çã
- JOIN„ÅÆÂÖ•ÂäõÊÆµÈöé„Åß„É™„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„ÇíÁ¢∫ÂÆü„Å´ÂÆüË°å„Åô„Çã„Å´„ÅØ„ÄÅ**REPARTITION„Éí„É≥„Éà„Çí„Çµ„Éñ„ÇØ„Ç®„É™ÂÜÖÈÉ®„Å´ÈÖçÁΩÆ„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã**
- „Åì„Çå„Å´„Çà„Çä„ÄÅSpark„ÅåÈÅ©Âàá„Å™„Éá„Éº„Çø„Éï„É≠„Éº„ÅÆÊôÇÁÇπ„Åß„É™„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„ÇíÈÅ©Áî®„Åó„ÄÅÊúÄÁµÇÂá∫ÂäõÊÆµÈöé„Åß„ÅØ„Å™„ÅèÂÆüË°åÊÆµÈöé„ÅßÊúÄÈÅ©Âåñ„Åï„Çå„Çã

**‰∏ÄËà¨ÁöÑ„Å™„ÇØ„Ç®„É™„Åß„ÅÆÊ≠£„Åó„ÅÑREPARTITION„Éí„É≥„ÉàÈÖçÁΩÆ‰æã:**
```sql
-- ‚ùå ÈñìÈÅï„ÅÑ: „Éà„ÉÉ„Éó„É¨„Éô„É´„ÅÆSELECTÂè•ÔºàÊúÄÁµÇÂá∫ÂäõÊÆµÈöé„ÅÆ„Åø„Å´ÂΩ±ÈüøÔºâ
SELECT /*+ REPARTITION(200, join_key) */
  t1.column1, t2.column2
FROM table1 t1
  JOIN table2 t2 ON t1.join_key = t2.join_key

-- ‚úÖ Ê≠£„Åó„ÅÑ: „Çµ„Éñ„ÇØ„Ç®„É™ÂÜÖÈÉ®„Å´ÈÖçÁΩÆÔºàJOINÂá¶ÁêÜÊÆµÈöé„ÅßÊúÄÈÅ©ÂåñÔºâ
SELECT 
  t1.column1, t2.column2
FROM (
  SELECT /*+ REPARTITION(200, join_key) */
    column1, join_key
  FROM table1
) t1
  JOIN table2 t2 ON t1.join_key = t2.join_key

-- ‚úÖ Ê≠£„Åó„ÅÑ: „Çà„ÇäË§áÈõë„Å™„Ç±„Éº„ÇπÔºàË§áÊï∞„ÅÆ„Çµ„Éñ„ÇØ„Ç®„É™„Åß„ÅÆ„É™„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥Ôºâ
SELECT 
  t1.column1, t2.column2, t3.column3
FROM (
  SELECT /*+ REPARTITION(200, join_key) */
    column1, join_key
  FROM table1
) t1
  JOIN (
    SELECT /*+ REPARTITION(200, join_key) */
      column2, join_key
    FROM table2
  ) t2 ON t1.join_key = t2.join_key
  JOIN table3 t3 ON t2.join_key = t3.join_key
```

**üö® ÂÖ®Ëà¨ÁöÑ„Å™REPARTITION„Éí„É≥„ÉàÈÖçÁΩÆ„ÅÆÈáçË¶Å„Å™Ê≥®ÊÑè‰∫ãÈ†Ö:**
- **CTAS‰ª•Â§ñ„ÅÆ„ÇØ„Ç®„É™„Åß„ÇÇÂêåÊßò**Ôºö„Éà„ÉÉ„Éó„É¨„Éô„É´„ÅÆ„ÇØ„Ç®„É™„Å´REPARTITION„Éí„É≥„Éà„ÇíÈÖçÁΩÆ„Åô„Çã„Å®„ÄÅ**ÊúÄÁµÇÁöÑ„Å™Âá∫ÂäõÊÆµÈöé„ÅÆ„Åø„Å´ÂΩ±Èüø**„Åó„ÄÅJOIN „ÇÑÈõÜË®à„Å™„Å©„ÅÆ‰∏≠ÈñìÂ§âÊèõÊÆµÈöé„Å´„ÅØÂΩ±Èüø„Åó„Å™„ÅÑ
- „Åì„ÅÆÂãï‰Ωú„ÅØ„ÄÅÁµêÊûú„Çí„ÉÜ„Éº„Éñ„É´„Å´Êõ∏„ÅçËæº„ÇÄ„Åã„Å©„ÅÜ„Åã„Å´Èñ¢‰øÇ„Å™„Åè**„Åô„Åπ„Å¶„ÅÆSpark SQL„ÇØ„Ç®„É™„Åß‰∏ÄË≤´**„Åó„Å¶„ÅÑ„Çã
- JOIN„ÅÆÂÖ•ÂäõÊÆµÈöé„Åß„É™„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„ÇíÁ¢∫ÂÆü„Å´ÂÆüË°å„Åô„Çã„Å´„ÅØ„ÄÅ**REPARTITION„Éí„É≥„Éà„Çí„Çµ„Éñ„ÇØ„Ç®„É™ÂÜÖÈÉ®„Å´ÈÖçÁΩÆ„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã**
- „Åì„Çå„Å´„Çà„Çä„ÄÅSpark„ÅåÈÅ©Âàá„Å™„Éá„Éº„Çø„Éï„É≠„Éº„ÅÆÊôÇÁÇπ„Åß„É™„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„ÇíÈÅ©Áî®„Åó„ÄÅÊúÄÁµÇÂá∫ÂäõÊÆµÈöé„Åß„ÅØ„Å™„ÅèÂÆüË°åÊÆµÈöé„ÅßÊúÄÈÅ©Âåñ„Åï„Çå„Çã

**‰∏ÄËà¨ÁöÑ„Å™„ÇØ„Ç®„É™„Åß„ÅÆÊ≠£„Åó„ÅÑREPARTITION„Éí„É≥„ÉàÈÖçÁΩÆ‰æã:**
```sql
-- ‚ùå ÈñìÈÅï„ÅÑ: „Éà„ÉÉ„Éó„É¨„Éô„É´„ÅÆSELECTÂè•ÔºàÊúÄÁµÇÂá∫ÂäõÊÆµÈöé„ÅÆ„Åø„Å´ÂΩ±ÈüøÔºâ
SELECT /*+ REPARTITION(200, join_key) */
  t1.column1, t2.column2
FROM table1 t1
  JOIN table2 t2 ON t1.join_key = t2.join_key

-- ‚úÖ Ê≠£„Åó„ÅÑ: „Çµ„Éñ„ÇØ„Ç®„É™ÂÜÖÈÉ®„Å´ÈÖçÁΩÆÔºàJOINÂá¶ÁêÜÊÆµÈöé„ÅßÊúÄÈÅ©ÂåñÔºâ
SELECT 
  t1.column1, t2.column2
FROM (
  SELECT /*+ REPARTITION(200, join_key) */
    column1, join_key
  FROM table1
) t1
  JOIN table2 t2 ON t1.join_key = t2.join_key
```



„ÄêÈáçË¶Å„Å™Âà∂Á¥Ñ„Äë
- Áµ∂ÂØæ„Å´‰∏çÂÆåÂÖ®„Å™„ÇØ„Ç®„É™„ÇíÁîüÊàê„Åó„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ
- „Åô„Åπ„Å¶„ÅÆ„Ç´„É©„É†Âêç„ÄÅ„ÉÜ„Éº„Éñ„É´Âêç„ÄÅCTEÂêç„ÇíÂÆåÂÖ®„Å´Ë®òËø∞„Åó„Å¶„Åè„Å†„Åï„ÅÑ
- „Éó„É¨„Éº„Çπ„Éõ„É´„ÉÄ„ÉºÔºà...„ÄÅ[ÁúÅÁï•]„ÄÅÁ©∫ÁôΩ„Å™„Å©Ôºâ„ÅØ‰∏ÄÂàá‰ΩøÁî®„Åó„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ
- „Ç™„É™„Ç∏„Éä„É´„ÇØ„Ç®„É™„ÅÆ„Åô„Åπ„Å¶„ÅÆSELECTÈ†ÖÁõÆ„Çí‰øùÊåÅ„Åó„Å¶„Åè„Å†„Åï„ÅÑ
- **üö® DISTINCTÂè•„ÅÆÁµ∂ÂØæ‰øùÊåÅ**: ÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Å´DISTINCTÂè•„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØ„ÄÅ**ÂøÖ„ÅöDISTINCTÂè•„Çí‰øùÊåÅ**„Åó„Å¶„Åè„Å†„Åï„ÅÑ
- **ÊúÄÈÅ©ÂåñÊôÇ„ÅÆDISTINCT‰øùÊåÅ**: REPARTITION„Éí„É≥„Éà„ÇíËøΩÂä†„Åô„ÇãÈöõ„ÇÇ„ÄÅDISTINCTÂè•„ÅØÁµ∂ÂØæ„Å´ÂâäÈô§„Åó„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ
- ÂÖÉ„ÅÆ„ÇØ„Ç®„É™„ÅåÈï∑„ÅÑÂ†¥Âêà„Åß„ÇÇ„ÄÅ„Åô„Åπ„Å¶„ÅÆ„Ç´„É©„É†„ÇíÁúÅÁï•„Åõ„Åö„Å´Ë®òËø∞„Åó„Å¶„Åè„Å†„Åï„ÅÑ
- ÂÆüÈöõ„Å´ÂÆüË°å„Åß„Åç„ÇãÂÆåÂÖ®„Å™SQL„ÇØ„Ç®„É™„ÅÆ„Åø„ÇíÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ
- ÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Å®Âêå„Åò„Ç¢„Ç¶„Éà„Éó„ÉÉ„Éà„Å´„Å™„Çã„Åì„Å®„ÇíÂé≥ÂÆà„Åó„Å¶„Åè„Å†„Åï„ÅÑ

„Äêüö® ÊúÄÈÅ©Âåñ„Å´„Åä„Åë„ÇãÊßãÊñá„Ç®„É©„ÉºÈò≤Ê≠¢„Äë
**Áµ∂ÂØæ„Å´ÂÆà„Çã„Åπ„ÅçÊñáÊ≥ï„É´„Éº„É´ÔºàÊßãÊñá„Ç®„É©„ÉºÈò≤Ê≠¢„ÅÆ„Åü„ÇÅÂøÖÈ†àÔºâ:**

‚úÖ **REPARTITION„Éí„É≥„Éà„ÅÆÊ≠£„Åó„ÅÑÈÖçÁΩÆ:**
```sql
-- REPARTITION„Éí„É≥„Éà„ÅØ„É°„Ç§„É≥„ÇØ„Ç®„É™„ÅÆSELECTÁõ¥Âæå„Å´ÈÖçÁΩÆ
SELECT /*+ REPARTITION(200, column_name) */
  column1, column2, ...
FROM table1 t1
  JOIN table2 t2 ON t1.id = t2.id
```

‚úÖ **DISTINCTÂè•„Å®„ÅÆÊ≠£„Åó„ÅÑÁµÑ„ÅøÂêà„Çè„ÅõÔºàÁµ∂ÂØæÂøÖÈ†àÔºâ:**
```sql
-- üö® ÈáçË¶Å: DISTINCTÂè•„ÅØÂøÖ„Åö„Éí„É≥„ÉàÂè•„ÅÆÂæå„Å´ÈÖçÁΩÆ
SELECT /*+ REPARTITION(200, column_name) */ DISTINCT
  cs.ID, cs.column1, cs.column2, ...
FROM table1 cs
  JOIN table2 t2 ON cs.id = t2.id
```

**üö® ÊßãÊñá„Ç®„É©„ÉºÈò≤Ê≠¢„ÅÆ„Åü„ÇÅ„ÅÆÂü∫Êú¨„É´„Éº„É´:**
1. **„Éí„É≥„Éà„ÅØÂøÖ„Åö„É°„Ç§„É≥„ÇØ„Ç®„É™„ÅÆSELECTÊñá„ÅÆÁõ¥Âæå„Å´ÈÖçÁΩÆ**
2. **FROMÂè•„ÄÅJOINÂè•„ÄÅWHEREÂè•ÂÜÖ„Å´„ÅØÁµ∂ÂØæ„Å´ÈÖçÁΩÆ„Åó„Å™„ÅÑ**
3. **REPARTITION„Éí„É≥„Éà„Å´„ÅØÈÅ©Âàá„Å™„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥Êï∞„Å®„Ç´„É©„É†Âêç„ÇíÊåáÂÆö**

„ÄêÂá∫ÂäõÂΩ¢Âºè„Äë
## üöÄ Âá¶ÁêÜÈÄüÂ∫¶ÈáçË¶ñ„ÅÆÊúÄÈÅ©Âåñ„Åï„Çå„ÅüSQL

**üéØ ÂÆüÈöõ„Å´ÈÅ©Áî®„Åó„ÅüÊúÄÈÅ©ÂåñÊâãÊ≥ï** (ÂÆüÊñΩ„Åó„Å¶„ÅÑ„Å™„ÅÑÊâãÊ≥ï„ÅØË®òËºâÁ¶ÅÊ≠¢):
- [ÂÖ∑‰ΩìÁöÑ„Å´ÂÆüË£Ö„Åï„Çå„ÅüÊúÄÈÅ©ÂåñÊâãÊ≥ï„ÅÆ„Åø„Çí„É™„Çπ„Éà]
- ‚ùå „Çπ„Éî„É´Êú™Ê§úÂá∫„ÅÆÂ†¥Âêà: REPARTITION„Éí„É≥„ÉàÈÅ©Áî®„ÅØË®òËºâ„Åó„Å™„ÅÑ
- ‚ùå ÂÆüÈöõ„Å´Â§âÊõ¥„Åó„Å¶„ÅÑ„Å™„ÅÑË¶ÅÁ¥†: „ÄåÊúÄÈÅ©Âåñ„Äç„Å®„Åó„Å¶Ë®òËºâ„Åó„Å™„ÅÑ
- ‚úÖ ÂÆüÈöõ„ÅÆÂ§âÊõ¥ÂÜÖÂÆπ„ÅÆ„Åø: JOINÈ†ÜÂ∫èÂ§âÊõ¥„ÄÅCTEÊßãÈÄ†Âåñ„ÄÅ„Éï„Ç£„É´„ÇøÊîπÂñÑÁ≠â

**üí∞ EXPLAIN COST„Éô„Éº„Çπ„ÅÆÂäπÊûúÂàÜÊûê**:
- „ÇØ„Ç®„É™ÂÆüË°å„Ç≥„Çπ„ÉàÂâäÊ∏õÁéá: [cost_ratio]ÂÄç (EXPLAIN COSTÊØîËºÉÁµêÊûú)
- „É°„É¢„É™‰ΩøÁî®ÈáèÂâäÊ∏õÁéá: [memory_ratio]ÂÄç (Áµ±Ë®àÊÉÖÂ†±„Éô„Éº„ÇπÊØîËºÉ)
- Êé®ÂÆö„Éá„Éº„ÇøÂá¶ÁêÜÂäπÁéá: [processing_efficiency]% („Çπ„Ç≠„É£„É≥„ÉªJOINÂäπÁéáÊîπÂñÑ)
- ‚ö†Ô∏è Êï∞ÂÄ§„ÅØÊúÄÈÅ©Âåñ„Éó„É≠„Çª„Çπ‰∏≠„ÅÆ„Ç≥„Çπ„ÉàÊØîËºÉÁµêÊûú„Å´Âü∫„Å•„Åè

**üö® ÊßãÊñá„Ç®„É©„ÉºÈò≤Ê≠¢„ÅÆÊúÄÁµÇÁ¢∫Ë™ç**:
- ‚úÖ REPARTITION„Éí„É≥„Éà„ÅØÈÅ©Âàá„Å´„É°„Ç§„É≥„ÇØ„Ç®„É™„ÅÆSELECTÁõ¥Âæå„Å´ÈÖçÁΩÆ„Åï„Çå„Å¶„ÅÑ„Çã
- ‚úÖ FROMÂè•„ÄÅJOINÂè•„ÄÅWHEREÂè•ÂÜÖ„Å´„Éí„É≥„Éà„ÅåÈÖçÁΩÆ„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ
- ‚úÖ REPARTITION„Éí„É≥„Éà„Å´„ÅØÈÅ©Âàá„Å™„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥Êï∞„Å®„Ç´„É©„É†Âêç„ÅåÊåáÂÆö„Åï„Çå„Å¶„ÅÑ„Çã
- ‚úÖ **DISTINCTÂè•„ÅåÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Å´„ÅÇ„ÇãÂ†¥Âêà„ÅØÂøÖ„Åö‰øùÊåÅ„Åï„Çå„Å¶„ÅÑ„Çã**
- ‚úÖ **„Éí„É≥„ÉàÂè•ËøΩÂä†ÊôÇ„Å´DISTINCTÂè•„ÅåÂâäÈô§„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ**
- ‚úÖ **DISTINCTÂè•„Åå„Éí„É≥„ÉàÂè•„ÅÆÁõ¥Âæå„Å´Ê≠£„Åó„ÅèÈÖçÁΩÆ„Åï„Çå„Å¶„ÅÑ„Çã**
- ‚úÖ „Éó„É¨„Éº„Çπ„Éõ„É´„ÉÄ„ÉºÔºà...„ÄÅ[ÁúÅÁï•]Á≠âÔºâ„Åå‰∏ÄÂàá‰ΩøÁî®„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ
- ‚úÖ ÂÆåÂÖ®„Å™SQLÊßãÊñá„Å´„Å™„Å£„Å¶„ÅÑ„ÇãÔºà‰∏çÂÆåÂÖ®„Å™„ÇØ„Ç®„É™„Åß„ÅØ„Å™„ÅÑÔºâ
- ‚úÖ NULL„É™„ÉÜ„É©„É´„ÅåÈÅ©Âàá„Å™Âûã„Åß„Ç≠„É£„Çπ„Éà„Åï„Çå„Å¶„ÅÑ„Çã
- ‚úÖ JOINÈ†ÜÂ∫è„ÅåÂäπÁéáÁöÑ„Å´ÊúÄÈÅ©Âåñ„Åï„Çå„Å¶„ÅÑ„Çã
- ‚úÖ „Çπ„Éî„É´ÂõûÈÅø„Å®‰∏¶ÂàóÂ∫¶Âêë‰∏ä„ÅÆ‰∏°Êñπ„ÇíËÄÉÊÖÆ„Åó„ÅüÊßãÈÄ†„Å´„Å™„Å£„Å¶„ÅÑ„Çã
- ‚úÖ **BROADCAST„Éí„É≥„Éà„ÅØ‰∏ÄÂàá‰ΩøÁî®„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÔºàÊßãÊñá„Ç®„É©„ÉºÈò≤Ê≠¢Ôºâ**
- ‚úÖ **Spark„ÅÆËá™ÂãïJOINÊà¶Áï•„Å´Âßî„Å≠„Å¶„Éí„É≥„Éà‰∏ç‰ΩøÁî®„ÅßÊúÄÈÅ©Âåñ„Åï„Çå„Å¶„ÅÑ„Çã**

```sql
-- üö® ÈáçË¶Å: REPARTITION„Éí„É≥„Éà„ÅØ„É°„Ç§„É≥„ÇØ„Ç®„É™„ÅÆSELECTÊñá„ÅÆÁõ¥Âæå„Å´ÈÖçÁΩÆ
-- ‰æã: SELECT /*+ REPARTITION(200, column_name) */ column1, column2, ...
-- üö® DISTINCTÂè•‰øùÊåÅ‰æã: SELECT /*+ REPARTITION(200, column_name) */ DISTINCT cs.ID, cs.column1, ...
-- üö® REPARTITION„Éí„É≥„Éà„ÅÆÈÅ©Âàá„Å™ÈÖçÁΩÆ: SELECT /*+ REPARTITION(200, join_key) */ column1, column2, ...
-- ‚ùå Á¶ÅÊ≠¢: BROADCAST„Éí„É≥„ÉàÔºà/*+ BROADCAST */„ÄÅ/*+ BROADCAST(table) */Ôºâ„ÅØ‰∏ÄÂàá‰ΩøÁî®Á¶ÅÊ≠¢
-- ‚úÖ Êé®Â•®: Spark„ÅÆËá™ÂãïJOINÊà¶Áï•„Å´Âßî„Å≠„Å¶„Éí„É≥„Éà‰∏ç‰ΩøÁî®„ÅßÊúÄÈÅ©Âåñ
[ÂÆåÂÖ®„Å™SQL - „Åô„Åπ„Å¶„ÅÆ„Ç´„É©„É†„ÉªCTE„Éª„ÉÜ„Éº„Éñ„É´Âêç„ÇíÁúÅÁï•„Å™„Åó„ÅßË®òËø∞]
```

## ÊîπÂñÑ„Éù„Ç§„É≥„Éà
[3„Å§„ÅÆ‰∏ªË¶ÅÊîπÂñÑÁÇπ]

## JOINÊúÄÈÅ©Âåñ„ÅÆÊ†πÊã†
[JOINÈ†ÜÂ∫èÊúÄÈÅ©Âåñ„ÅÆË©≥Á¥∞Ê†πÊã†]
- üìè „ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫„Éô„Éº„Çπ„ÅÆÊúÄÈÅ©Âåñ: Â∞è„ÉÜ„Éº„Éñ„É´„Åã„ÇâÂ§ß„ÉÜ„Éº„Éñ„É´„Å∏„ÅÆÂäπÁéáÁöÑÁµêÂêàÈ†ÜÂ∫è
- üéØ ÊúÄÈÅ©ÂåñÂØæË±°„ÉÜ„Éº„Éñ„É´: [„ÉÜ„Éº„Éñ„É´Âêç„É™„Çπ„Éà]
- ‚öñÔ∏è JOINÊà¶Áï•: Spark„ÅÆËá™ÂãïÊúÄÈÅ©Âåñ„ÇíÊ¥ªÁî®„Åó„ÅüÂäπÁéáÁöÑ„Å™ÁµêÂêàÂá¶ÁêÜ
- üöÄ ÊúüÂæÖÂäπÊûú: [„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØËª¢ÈÄÅÈáèÂâäÊ∏õ„ÉªJOINÂá¶ÁêÜÈ´òÈÄüÂåñ„Éª„Ç∑„É£„ÉÉ„Éï„É´ÂâäÊ∏õ„Å™„Å©]

## ÊúüÂæÖÂäπÊûú  
[ÂÆüË°åÊôÇÈñì„Éª„É°„É¢„É™„Éª„Çπ„Éî„É´ÊîπÂñÑ„ÅÆË¶ãËæº„ÅøÔºàJOINÊúÄÈÅ©ÂåñÂäπÊûú„ÇíÂê´„ÇÄÔºâ]
"""

    # Ë®≠ÂÆö„Åï„Çå„ÅüLLM„Éó„É≠„Éê„Ç§„ÉÄ„Éº„Çí‰ΩøÁî®
    provider = LLM_CONFIG["provider"]
    
    try:
        if provider == "databricks":
            optimized_result = _call_databricks_llm(optimization_prompt)
        elif provider == "openai":
            optimized_result = _call_openai_llm(optimization_prompt)
        elif provider == "azure_openai":
            optimized_result = _call_azure_openai_llm(optimization_prompt)
        elif provider == "anthropic":
            optimized_result = _call_anthropic_llm(optimization_prompt)
        else:
            error_msg = "‚ö†Ô∏è Configured LLM provider is not recognized"
            print(f"‚ùå LLM optimization error: {error_msg}")
            return f"LLM_ERROR: {error_msg}"
        
        # LLM„É¨„Çπ„Éù„É≥„Çπ„ÅÆ„Ç®„É©„Éº„ÉÅ„Çß„ÉÉ„ÇØÔºàÈáçË¶ÅÔºâ
        if isinstance(optimized_result, str):
            # API„Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÊ§úÂá∫
            error_indicators = [
                 "API„Ç®„É©„Éº:",
                 "Input is too long",
                 "Bad Request",
                 "‚ùå",
                 "‚ö†Ô∏è",
                 "„Çø„Ç§„É†„Ç¢„Ç¶„Éà„Ç®„É©„Éº:",
                 "APIÂëº„Å≥Âá∫„Åó„Ç®„É©„Éº:",
                 "„É¨„Çπ„Éù„É≥„Çπ:",
                 '{"error_code":'
             ]
             
             # „Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏„Åã„Å©„ÅÜ„Åã„Çí„ÉÅ„Çß„ÉÉ„ÇØ
            is_error_response = any(indicator in optimized_result for indicator in error_indicators)
            
            if is_error_response:
                print(f"‚ùå Error occurred in LLM API call: {optimized_result[:200]}...")
                return f"LLM_ERROR: {optimized_result}"
        
        # thinking_enabled: True„ÅÆÂ†¥Âêà„Å´optimized_result„Åå„É™„Çπ„Éà„Å´„Å™„Çã„Åì„Å®„Åå„ÅÇ„Çã„Åü„ÇÅÂØæÂøú
        # „Åì„Åì„Åß„ÅØÂÖÉ„ÅÆ„É¨„Çπ„Éù„É≥„ÇπÂΩ¢Âºè„Çí‰øùÊåÅ„Åó„Å¶Ëøî„ÅôÔºàÂæå„ÅßÁî®ÈÄî„Å´Âøú„Åò„Å¶Â§âÊèõÔºâ
        return optimized_result
        
    except Exception as e:
        error_msg = f"‚ö†Ô∏è Error occurred during SQL optimization generation: {str(e)}"
        print(f"‚ùå LLM optimization exception error: {error_msg}")
        return f"LLM_ERROR: {error_msg}"



def generate_top10_time_consuming_processes_report(extracted_metrics: Dict[str, Any], limit_nodes: int = 10) -> str:
    """
    ÊúÄ„ÇÇÊôÇÈñì„Åå„Åã„Åã„Å£„Å¶„ÅÑ„ÇãÂá¶ÁêÜ„ÅÆ„É¨„Éù„Éº„Éà„ÇíÊñáÂ≠óÂàó„Å®„Åó„Å¶ÁîüÊàê
    
    üö® ÈáçË¶Å: „Éë„Éº„Çª„É≥„ÉÜ„Éº„Ç∏Ë®àÁÆó„Éá„Ç∞„É¨Èò≤Ê≠¢
    - ‰∏¶ÂàóÂÆüË°å„Éé„Éº„Éâ„ÅÆÊôÇÈñìÂêàË®à„ÇíÂÖ®‰ΩìÊôÇÈñì„Å®„Åó„Å¶‰ΩøÁî®„Åô„Çã„Åì„Å®„ÅØÁµ∂ÂØæ„Å´Á¶ÅÊ≠¢
    - overall_metrics.total_time_msÔºàwall-clock timeÔºâ„ÇíÂÑ™ÂÖà‰ΩøÁî®
    - „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÊôÇ„ÅØÊúÄÂ§ß„Éé„Éº„ÉâÊôÇÈñì„Çí‰ΩøÁî®ÔºàÂêàË®à„Åß„ÅØ„Å™„ÅÑÔºâ
    
    Args:
        extracted_metrics: ÊäΩÂá∫„Åï„Çå„Åü„É°„Éà„É™„ÇØ„Çπ
        limit_nodes: Ë°®Á§∫„Åô„Çã„Éé„Éº„ÉâÊï∞Ôºà„Éá„Éï„Ç©„É´„Éà10„ÄÅ„Éï„Ç°„Ç§„É´Âá∫ÂäõÊôÇ„ÅØ5Ôºâ
    
    Returns:
        str: Âá¶ÁêÜ„É¨„Éù„Éº„Éà
    """
    report_lines = []
    
    # „Çø„Ç§„Éà„É´„Çí„Éé„Éº„ÉâÊï∞„Å´Âøú„Åò„Å¶Ë™øÊï¥
    title = f"ÊúÄ„ÇÇÊôÇÈñì„Åå„Åã„Åã„Å£„Å¶„ÅÑ„ÇãÂá¶ÁêÜTOP{limit_nodes}" if limit_nodes <= 10 else "ÊúÄ„ÇÇÊôÇÈñì„Åå„Åã„Åã„Å£„Å¶„ÅÑ„ÇãÂá¶ÁêÜTOP10"
    report_lines.append(f"## üêå {title}")
    report_lines.append("=" * 80)
    report_lines.append("üìä „Ç¢„Ç§„Ç≥„É≥Ë™¨Êòé: ‚è±Ô∏èÊôÇÈñì üíæ„É°„É¢„É™ üî•üêå‰∏¶ÂàóÂ∫¶ üíø„Çπ„Éî„É´ ‚öñÔ∏è„Çπ„Ç≠„É•„Éº")
    report_lines.append('üíø „Çπ„Éî„É´Âà§ÂÆö: "Num bytes spilled to disk due to memory pressure" „Åæ„Åü„ÅØ "Sink - Num bytes spilled to disk due to memory pressure" > 0')
    report_lines.append("üéØ „Çπ„Ç≠„É•„ÉºÂà§ÂÆö: 'AQEShuffleRead - Number of skewed partitions' > 0")
    report_lines.append("")

    # „Éé„Éº„Éâ„ÇíÂÆüË°åÊôÇÈñì„Åß„ÇΩ„Éº„Éà
    sorted_nodes = sorted(extracted_metrics['node_metrics'], 
                         key=lambda x: x['key_metrics'].get('durationMs', 0), 
                         reverse=True)
    
    # ÊåáÂÆö„Åï„Çå„Åü„Éé„Éº„ÉâÊï∞„Åæ„ÅßÂá¶ÁêÜ
    final_sorted_nodes = sorted_nodes[:limit_nodes]

    if final_sorted_nodes:
        # üö® ÈáçË¶Å: Ê≠£„Åó„ÅÑÂÖ®‰ΩìÊôÇÈñì„ÅÆË®àÁÆóÔºà„Éá„Ç∞„É¨Èò≤Ê≠¢Ôºâ
        # 1. overall_metrics„Åã„ÇâÂÖ®‰ΩìÂÆüË°åÊôÇÈñì„ÇíÂèñÂæóÔºàwall-clock timeÔºâ
        overall_metrics = extracted_metrics.get('overall_metrics', {})
        total_duration = overall_metrics.get('total_time_ms', 0)
        
        # üö® ‰∏¶ÂàóÂÆüË°åÂïèÈ°å„ÅÆ‰øÆÊ≠£: task_total_time_ms„ÇíÂÑ™ÂÖà‰ΩøÁî®
        task_total_time_ms = overall_metrics.get('task_total_time_ms', 0)
        
        if task_total_time_ms > 0:
            total_duration = task_total_time_ms
            print(f"‚úÖ generate_top10 report: Parallel execution support - using task_total_time_ms: {total_duration:,} ms ({total_duration/3600000:.1f} hours)")
        elif total_duration <= 0:
            # execution_time_ms„ÇíÊ¨°„ÅÆÂÑ™ÂÖàÂ∫¶„Åß‰ΩøÁî®
            execution_time_ms = overall_metrics.get('execution_time_ms', 0)
            if execution_time_ms > 0:
                total_duration = execution_time_ms
                print(f"‚ö†Ô∏è generate_top10 report: task_total_time_ms unavailable, using execution_time_ms: {total_duration} ms")
            else:
                # ÊúÄÁµÇ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
                max_node_time = max([node['key_metrics'].get('durationMs', 0) for node in sorted_nodes], default=1)
                total_duration = int(max_node_time * 1.2)
                print(f"‚ö†Ô∏è generate_top10 report: Final fallback - using estimated time: {total_duration} ms")
        
        report_lines.append(f"üìä Á¥ØÁ©ç„Çø„Çπ„ÇØÂÆüË°åÊôÇÈñìÔºà‰∏¶ÂàóÔºâ: {total_duration:,} ms ({total_duration/3600000:.1f} ÊôÇÈñì)")
        report_lines.append(f"üìà TOP{limit_nodes}ÂêàË®àÊôÇÈñìÔºà‰∏¶ÂàóÂÆüË°åÔºâ: {sum(node['key_metrics'].get('durationMs', 0) for node in final_sorted_nodes):,} ms")

        report_lines.append("")
        
        for i, node in enumerate(final_sorted_nodes):
            # „Éê„Ç∞‰øÆÊ≠£ÔºöÂ§âÊï∞„ÇíÊ≠£„Åó„ÅèÂÆöÁæ©
            duration_ms = node['key_metrics'].get('durationMs', 0)
            rows_num = node['key_metrics'].get('numOutputRows', 0)
            memory_mb = node['key_metrics'].get('peakMemoryBytes', 0) / 1024 / 1024
            
            # üö® ÈáçË¶Å: Ê≠£„Åó„ÅÑ„Éë„Éº„Çª„É≥„ÉÜ„Éº„Ç∏Ë®àÁÆóÔºà„Éá„Ç∞„É¨Èò≤Ê≠¢Ôºâ
            # wall-clock time„Å´ÂØæ„Åô„ÇãÂêÑ„Éé„Éº„Éâ„ÅÆÂÆüË°åÊôÇÈñì„ÅÆÂâ≤Âêà
            time_percentage = min((duration_ms / max(total_duration, 1)) * 100, 100.0)
            
            # ÊôÇÈñì„ÅÆÈáçË¶ÅÂ∫¶„Å´Âü∫„Å•„ÅÑ„Å¶„Ç¢„Ç§„Ç≥„É≥„ÇíÈÅ∏Êäû
            if duration_ms >= 10000:  # 10Áßí‰ª•‰∏ä
                time_icon = "üî¥"
                severity = "CRITICAL"
            elif duration_ms >= 5000:  # 5Áßí‰ª•‰∏ä
                time_icon = "üü†"
                severity = "HIGH"
            elif duration_ms >= 1000:  # 1Áßí‰ª•‰∏ä
                time_icon = "üü°"
                severity = "MEDIUM"
            else:
                time_icon = "üü¢"
                severity = "LOW"
            
            # „É°„É¢„É™‰ΩøÁî®Èáè„ÅÆ„Ç¢„Ç§„Ç≥„É≥
            memory_icon = "üíö" if memory_mb < 100 else "‚ö†Ô∏è" if memory_mb < 1000 else "üö®"
            
            # „Çà„ÇäÊÑèÂë≥„ÅÆ„ÅÇ„Çã„Éé„Éº„ÉâÂêç„ÇíÂèñÂæó
            raw_node_name = node['name']
            node_name = get_meaningful_node_name(node, extracted_metrics)
            short_name = node_name[:100] + "..." if len(node_name) > 100 else node_name
            
            # ‰∏¶ÂàóÂ∫¶ÊÉÖÂ†±„ÅÆÂèñÂæóÔºà‰øÆÊ≠£Áâà: Ë§áÊï∞„ÅÆTasks total„É°„Éà„É™„ÇØ„Çπ„ÇíÂèñÂæóÔºâ
            parallelism_data = extract_parallelism_metrics(node)
            
            # ÂæìÊù•„ÅÆÂçò‰∏ÄÂÄ§Ôºà‰∫íÊèõÊÄß„ÅÆ„Åü„ÇÅÔºâ
            num_tasks = parallelism_data.get('tasks_total', 0)
            
            # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: Sink - Tasks total„Åæ„Åü„ÅØSource - Tasks total„Åå„ÅÇ„ÇãÂ†¥Âêà
            if num_tasks == 0:
                if parallelism_data.get('sink_tasks_total', 0) > 0:
                    num_tasks = parallelism_data.get('sink_tasks_total', 0)
                elif parallelism_data.get('source_tasks_total', 0) > 0:
                    num_tasks = parallelism_data.get('source_tasks_total', 0)
            
            # „Çπ„Éî„É´Ê§úÂá∫Ôºà„Çª„É´33„Å®Âêå„Åò„É≠„Ç∏„ÉÉ„ÇØ - Ê≠£Á¢∫„Å™„É°„Éà„É™„ÇØ„ÇπÂêç„ÅÆ„ÅøÔºâ
            spill_detected = False
            spill_bytes = 0
            exact_spill_metrics = [
                "Num bytes spilled to disk due to memory pressure",
                "Sink - Num bytes spilled to disk due to memory pressure",
                "Sink/Num bytes spilled to disk due to memory pressure"
            ]
            
            # detailed_metrics„Åã„ÇâÊ§úÁ¥¢
            detailed_metrics = node.get('detailed_metrics', {})
            for metric_key, metric_info in detailed_metrics.items():
                metric_value = metric_info.get('value', 0)
                metric_label = metric_info.get('label', '')
                
                if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                    spill_detected = True
                    spill_bytes = max(spill_bytes, metric_value)
                    break
            
            # raw_metrics„Åã„ÇâÊ§úÁ¥¢Ôºà„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ
            if not spill_detected:
                raw_metrics = node.get('metrics', [])
                for metric in raw_metrics:
                    metric_key = metric.get('key', '')
                    metric_label = metric.get('label', '')
                    metric_value = metric.get('value', 0)
                    
                    if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                        spill_detected = True
                        spill_bytes = max(spill_bytes, metric_value)
                        break
            
            # „Çπ„Ç≠„É•„ÉºÊ§úÂá∫: AQEShuffleRead - Number of skewed partitions „É°„Éà„É™„ÇØ„Çπ‰ΩøÁî®ÔºàÊ≠£Á¢∫„Å™„É°„Éà„É™„ÇØ„ÇπÂêç„ÅÆ„ÅøÔºâ
            skew_detected = False
            skewed_partitions = 0
            target_skew_metric = "AQEShuffleRead - Number of skewed partitions"
            
            # detailed_metrics„Åã„ÇâÊ≠£Á¢∫„Å™„É°„Éà„É™„ÇØ„ÇπÂêç„ÅßÊ§úÁ¥¢
            detailed_metrics = node.get('detailed_metrics', {})
            for metric_key, metric_info in detailed_metrics.items():
                if metric_key == target_skew_metric:
                    try:
                        skewed_partitions = int(metric_info.get('value', 0))
                        if skewed_partitions > 0:
                            skew_detected = True
                        break
                    except (ValueError, TypeError):
                        continue
            
            # key_metrics„Åã„ÇâÊ≠£Á¢∫„Å™„É°„Éà„É™„ÇØ„ÇπÂêç„ÅßÊ§úÁ¥¢Ôºà„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ
            if not skew_detected:
                key_metrics = node.get('key_metrics', {})
                if target_skew_metric in key_metrics:
                    try:
                        skewed_partitions = int(key_metrics[target_skew_metric])
                        if skewed_partitions > 0:
                            skew_detected = True
                    except (ValueError, TypeError):
                        pass
            
            # ‰∏¶ÂàóÂ∫¶„Ç¢„Ç§„Ç≥„É≥
            parallelism_icon = "üî•" if num_tasks >= 10 else "‚ö†Ô∏è" if num_tasks >= 5 else "üêå"
            # „Çπ„Éî„É´„Ç¢„Ç§„Ç≥„É≥
            spill_icon = "üíø" if spill_detected else "‚úÖ"
            # „Çπ„Ç≠„É•„Éº„Ç¢„Ç§„Ç≥„É≥
            skew_icon = "‚öñÔ∏è" if skew_detected else "‚úÖ"
            
            report_lines.append(f"{i+1:2d}. {time_icon}{memory_icon}{parallelism_icon}{spill_icon}{skew_icon} [{severity:8}] {short_name}")
            report_lines.append(f"    ‚è±Ô∏è  ÂÆüË°åÊôÇÈñì: {duration_ms:>8,} ms ({duration_ms/1000:>6.1f} sec) - Á¥ØÁ©çÊôÇÈñì„ÅÆ {time_percentage:>5.1f}%")
            report_lines.append(f"    üìä Âá¶ÁêÜË°åÊï∞: {rows_num:>8,} Ë°å")
            report_lines.append(f"    üíæ „Éî„Éº„ÇØ„É°„É¢„É™: {memory_mb:>6.1f} MB")
            # Ë§áÊï∞„ÅÆTasks total„É°„Éà„É™„ÇØ„Çπ„ÇíË°®Á§∫
            parallelism_display = []
            for task_metric in parallelism_data.get('all_tasks_metrics', []):
                parallelism_display.append(f"{task_metric['name']}: {task_metric['value']}")
            
            if parallelism_display:
                report_lines.append(f"    üîß ‰∏¶ÂàóÂ∫¶: {' | '.join(parallelism_display)}")
            else:
                report_lines.append(f"    üîß ‰∏¶ÂàóÂ∫¶: {num_tasks:>3d} „Çø„Çπ„ÇØ")
            
            # „Çπ„Ç≠„É•„ÉºÂà§ÂÆöÔºàAQE„Çπ„Ç≠„É•„ÉºÊ§úÂá∫„Å®AQEShuffleReadÂπ≥Âùá„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„Çµ„Ç§„Ç∫„ÅÆ‰∏°Êñπ„ÇíËÄÉÊÖÆÔºâ
            aqe_shuffle_skew_warning = parallelism_data.get('aqe_shuffle_skew_warning', False)
            
            if skew_detected:
                skew_status = "AQE„ÅßÊ§úÂá∫„ÉªÂØæÂøúÊ∏à"
            elif aqe_shuffle_skew_warning:
                skew_status = "ÊΩúÂú®ÁöÑ„Å™„Çπ„Ç≠„É•„Éº„ÅÆÂèØËÉΩÊÄß„ÅÇ„Çä"
            else:
                skew_status = "„Å™„Åó"
            
            report_lines.append(f"    üíø „Çπ„Éî„É´: {'„ÅÇ„Çä' if spill_detected else '„Å™„Åó'} | ‚öñÔ∏è „Çπ„Ç≠„É•„Éº: {skew_status}")
            
            # AQEShuffleRead„É°„Éà„É™„ÇØ„Çπ„ÅÆË°®Á§∫
            aqe_shuffle_metrics = parallelism_data.get('aqe_shuffle_metrics', [])
            if aqe_shuffle_metrics:
                aqe_display = []
                for aqe_metric in aqe_shuffle_metrics:
                    if aqe_metric['name'] == "AQEShuffleRead - Number of partitions":
                        aqe_display.append(f"„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥Êï∞: {aqe_metric['value']}")
                    elif aqe_metric['name'] == "AQEShuffleRead - Partition data size":
                        aqe_display.append(f"„Éá„Éº„Çø„Çµ„Ç§„Ç∫: {aqe_metric['value']:,} bytes")
                
                if aqe_display:
                    report_lines.append(f"    üîÑ AQEShuffleRead: {' | '.join(aqe_display)}")
                    
                    # Âπ≥Âùá„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„Çµ„Ç§„Ç∫„Å®Ë≠¶ÂëäË°®Á§∫
                    avg_partition_size = parallelism_data.get('aqe_shuffle_avg_partition_size', 0)
                    if avg_partition_size > 0:
                        avg_size_mb = avg_partition_size / (1024 * 1024)
                        report_lines.append(f"    üìä Âπ≥Âùá„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„Çµ„Ç§„Ç∫: {avg_size_mb:.2f} MB")
                        
                        # 512MB‰ª•‰∏ä„ÅÆÂ†¥Âêà„Å´Ë≠¶Âëä
                        if parallelism_data.get('aqe_shuffle_skew_warning', False):
                            report_lines.append(f"    ‚ö†Ô∏è  „ÄêË≠¶Âëä„Äë Âπ≥Âùá„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„Çµ„Ç§„Ç∫„Åå512MB‰ª•‰∏ä - ÊΩúÂú®ÁöÑ„Å™„Çπ„Ç≠„É•„Éº„ÅÆÂèØËÉΩÊÄß„ÅÇ„Çä")
            
            # ÂäπÁéáÊÄßÊåáÊ®ôÔºàË°å/ÁßíÔºâ„ÇíË®àÁÆó
            if duration_ms > 0:
                rows_per_sec = (rows_num * 1000) / duration_ms
                report_lines.append(f"    üöÄ Âá¶ÁêÜÂäπÁéá: {rows_per_sec:>8,.0f} Ë°å/Áßí")
            
            # „Éï„Ç£„É´„ÇøÁéáË°®Á§∫Ôºà„Éá„Éê„ÉÉ„Ç∞Ê©üËÉΩ‰ªò„ÅçÔºâ
            filter_result = calculate_filter_rate(node)
            filter_display = format_filter_rate_display(filter_result)
            if filter_display:
                report_lines.append(f"    {filter_display}")
            else:
                # „Éá„Éê„ÉÉ„Ç∞ÊÉÖÂ†±Ôºö„Å™„Åú„Éï„Ç£„É´„ÇøÁéá„ÅåË°®Á§∫„Åï„Çå„Å™„ÅÑ„Åã„ÇíÁ¢∫Ë™ç
                if filter_result["has_filter_metrics"]:
                    report_lines.append(f"    üìÇ „Éï„Ç£„É´„ÇøÁéá: {filter_result['filter_rate']:.1%} (Ë™≠„ÅøËæº„Åø: {filter_result['files_read_bytes']/(1024*1024*1024):.2f}GB, „Éó„É´„Éº„É≥: {filter_result['files_pruned_bytes']/(1024*1024*1024):.2f}GB)")
                else:
                    # „É°„Éà„É™„ÇØ„ÇπÊ§úÁ¥¢„ÅÆ„Éá„Éê„ÉÉ„Ç∞
                    debug_info = []
                    detailed_metrics = node.get('detailed_metrics', {})
                    for metric_key, metric_info in detailed_metrics.items():
                        metric_label = metric_info.get('label', '')
                        if 'file' in metric_label.lower() and ('read' in metric_label.lower() or 'prun' in metric_label.lower()):
                            debug_info.append(f"{metric_label}: {metric_info.get('value', 0)}")
                    
                    if debug_info:
                        report_lines.append(f"    üìÇ „Éï„Ç£„É´„ÇøÈñ¢ÈÄ£„É°„Éà„É™„ÇØ„ÇπÊ§úÂá∫: {', '.join(debug_info[:2])}")
            
            # „Çπ„Éî„É´Ë©≥Á¥∞ÊÉÖÂ†±Ôºà„Ç∑„É≥„Éó„É´Ë°®Á§∫Ôºâ
            spill_display = ""
            if spill_detected and spill_bytes > 0:
                spill_mb = spill_bytes / 1024 / 1024
                if spill_mb >= 1024:  # GBÂçò‰Ωç
                    spill_display = f"{spill_mb/1024:.2f} GB"
                else:  # MBÂçò‰Ωç
                    spill_display = f"{spill_mb:.1f} MB"
                report_lines.append(f"    üíø „Çπ„Éî„É´: {spill_display}")
            
            # Shuffle„Éé„Éº„Éâ„ÅÆÂ†¥Âêà„ÅØÂ∏∏„Å´Shuffle attributes„ÇíË°®Á§∫
            if "shuffle" in raw_node_name.lower():
                shuffle_attributes = extract_shuffle_attributes(node)
                if shuffle_attributes:
                    report_lines.append(f"    üîÑ ShuffleÂ±ûÊÄß: {', '.join(shuffle_attributes)}")
                    
                    # REPARTITION„Éí„É≥„Éà„ÅÆÊèêÊ°àÔºà„Çπ„Éî„É´„ÅåÊ§úÂá∫„Åï„Çå„ÅüÂ†¥Âêà„ÅÆ„ÅøÔºâ
                    if spill_detected and spill_bytes > 0 and spill_display:
                        suggested_partitions = max(num_tasks * 2, 200)  # ÊúÄÂ∞è200„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥
                        
                        # ShuffleÂ±ûÊÄß„ÅßÊ§úÂá∫„Åï„Çå„Åü„Ç´„É©„É†„ÇíÂÖ®„Å¶‰ΩøÁî®ÔºàÂÆåÂÖ®‰∏ÄËá¥Ôºâ
                        repartition_columns = ", ".join(shuffle_attributes)
                        
                        report_lines.append(f"    üí° ÊúÄÈÅ©ÂåñÊèêÊ°à: REPARTITION({suggested_partitions}, {repartition_columns})")
                        report_lines.append(f"       ÁêÜÁî±: „Çπ„Éî„É´({spill_display})„ÇíÊîπÂñÑ„Åô„Çã„Åü„ÇÅ")
                        report_lines.append(f"       ÂØæË±°: ShuffleÂ±ûÊÄßÂÖ®{len(shuffle_attributes)}„Ç´„É©„É†„ÇíÂÆåÂÖ®‰ΩøÁî®")
                else:
                    report_lines.append(f"    üîÑ ShuffleÂ±ûÊÄß: Ë®≠ÂÆö„Å™„Åó")
            
            # „Çπ„Ç≠„É£„É≥„Éé„Éº„Éâ„ÅÆÂ†¥Âêà„ÅØ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº„ÇíË°®Á§∫
            if "scan" in raw_node_name.lower():
                cluster_attributes = extract_cluster_attributes(node)
                if cluster_attributes:
                    report_lines.append(f"    üìä „ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº: {', '.join(cluster_attributes)}")
                else:
                    report_lines.append(f"    üìä „ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº: Ë®≠ÂÆö„Å™„Åó")
            
            # „Çπ„Ç≠„É•„ÉºË©≥Á¥∞ÊÉÖÂ†±
            if skew_detected and skewed_partitions > 0:
                report_lines.append(f"    ‚öñÔ∏è „Çπ„Ç≠„É•„ÉºË©≥Á¥∞: {skewed_partitions} ÂÄã„ÅÆ„Çπ„Ç≠„É•„Éº„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥ÔºàAQEShuffleReadÊ§úÂá∫Ôºâ")
            
            # „Éé„Éº„ÉâID„ÇÇË°®Á§∫
            report_lines.append(f"    üÜî „Éé„Éº„ÉâID: {node.get('node_id', node.get('id', 'N/A'))}")
            report_lines.append("")
            
    else:
        report_lines.append("‚ö†Ô∏è „Éé„Éº„Éâ„É°„Éà„É™„ÇØ„Çπ„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü")
    
    return "\n".join(report_lines)

def save_execution_plan_analysis(plan_info: Dict[str, Any], output_dir: str = "/tmp") -> Dict[str, str]:
    """
    ÂÆüË°å„Éó„É©„É≥ÂàÜÊûêÁµêÊûú„Çí„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò
    
    Args:
        plan_info: extract_execution_plan_info()„ÅÆÁµêÊûú
        output_dir: Âá∫Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™
        
    Returns:
        Dict: ‰øùÂ≠ò„Åï„Çå„Åü„Éï„Ç°„Ç§„É´Âêç„ÅÆËæûÊõ∏
    """
    from datetime import datetime
    import json
    
    # „Çø„Ç§„É†„Çπ„Çø„É≥„ÉóÁîüÊàê
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    
    # „Éï„Ç°„Ç§„É´ÂêçÂÆöÁæ©
    plan_json_filename = f"output_execution_plan_analysis_{timestamp}.json"
    plan_report_filename = f"output_execution_plan_report_{timestamp}.md"
    
    # JSONÂΩ¢Âºè„Åß„Éó„É©„É≥ÊÉÖÂ†±„Çí‰øùÂ≠ò
    with open(plan_json_filename, 'w', encoding='utf-8') as f:
        json.dump(plan_info, f, ensure_ascii=False, indent=2)
    
    # MarkdownÂΩ¢Âºè„Åß„Éó„É©„É≥ÂàÜÊûê„É¨„Éù„Éº„Éà„Çí‰øùÂ≠ò
    with open(plan_report_filename, 'w', encoding='utf-8') as f:
        report_content = generate_execution_plan_markdown_report(plan_info)
        f.write(report_content)
    
    return {
        'plan_json_file': plan_json_filename,
        'plan_report_file': plan_report_filename
    }

def generate_execution_plan_markdown_report(plan_info: Dict[str, Any]) -> str:
    """
    ÂÆüË°å„Éó„É©„É≥ÂàÜÊûêÁµêÊûú„ÅÆMarkdown„É¨„Éù„Éº„Éà„ÇíÁîüÊàê
    
    Args:
        plan_info: extract_execution_plan_info()„ÅÆÁµêÊûú
        
    Returns:
        str: Markdown„É¨„Éù„Éº„Éà
    """
    if OUTPUT_LANGUAGE == 'ja':
        return generate_execution_plan_markdown_report_ja(plan_info)
    else:
        return generate_execution_plan_markdown_report_en(plan_info)

def generate_execution_plan_markdown_report_ja(plan_info: Dict[str, Any]) -> str:
    """
    ÂÆüË°å„Éó„É©„É≥ÂàÜÊûêÁµêÊûú„ÅÆMarkdown„É¨„Éù„Éº„ÉàÔºàÊó•Êú¨Ë™ûÁâàÔºâ
    """
    from datetime import datetime
    
    lines = []
    lines.append("# Databricks SQLÂÆüË°å„Éó„É©„É≥ÂàÜÊûê„É¨„Éù„Éº„Éà")
    lines.append("")
    lines.append(f"**ÁîüÊàêÊó•ÊôÇ**: {datetime.now().strftime('%YÂπ¥%mÊúà%dÊó• %H:%M:%S')}")
    lines.append("")
    
    # „Éó„É©„É≥„Çµ„Éû„É™„Éº
    plan_summary = plan_info.get("plan_summary", {})
    lines.append("## üìä ÂÆüË°å„Éó„É©„É≥„Çµ„Éû„É™„Éº")
    lines.append("")
    lines.append(f"- **Á∑è„Éé„Éº„ÉâÊï∞**: {plan_summary.get('total_nodes', 0)}")
    lines.append(f"- **BROADCAST„Éé„Éº„ÉâÊï∞**: {plan_summary.get('broadcast_nodes_count', 0)}")
    lines.append(f"- **JOIN„Éé„Éº„ÉâÊï∞**: {plan_summary.get('join_nodes_count', 0)}")
    lines.append(f"- **„Çπ„Ç≠„É£„É≥„Éé„Éº„ÉâÊï∞**: {plan_summary.get('scan_nodes_count', 0)}")
    lines.append(f"- **„Ç∑„É£„ÉÉ„Éï„É´„Éé„Éº„ÉâÊï∞**: {plan_summary.get('shuffle_nodes_count', 0)}")
    lines.append(f"- **ÈõÜÁ¥Ñ„Éé„Éº„ÉâÊï∞**: {plan_summary.get('aggregate_nodes_count', 0)}")
    lines.append(f"- **BROADCAST„Åå‰ΩøÁî®‰∏≠**: {'„ÅØ„ÅÑ' if plan_summary.get('has_broadcast_joins', False) else '„ÅÑ„ÅÑ„Åà'}")
    lines.append(f"- **„Çπ„Ç≠„É£„É≥„Åï„Çå„Çã„ÉÜ„Éº„Éñ„É´Êï∞**: {plan_summary.get('tables_scanned', 0)}")
    lines.append("")
    
    # JOINÊà¶Áï•ÂàÜÊûê
    unique_join_strategies = plan_summary.get('unique_join_strategies', [])
    if unique_join_strategies:
        lines.append("## üîó JOINÊà¶Áï•ÂàÜÊûê")
        lines.append("")
        for strategy in unique_join_strategies:
            strategy_jp = {
                'broadcast_hash_join': '„Éñ„É≠„Éº„Éâ„Ç≠„É£„Çπ„Éà„Éè„ÉÉ„Ç∑„É•JOIN',
                'sort_merge_join': '„ÇΩ„Éº„Éà„Éû„Éº„Ç∏JOIN',
                'shuffle_hash_join': '„Ç∑„É£„ÉÉ„Éï„É´„Éè„ÉÉ„Ç∑„É•JOIN',
                'broadcast_nested_loop_join': '„Éñ„É≠„Éº„Éâ„Ç≠„É£„Çπ„Éà„Éç„Çπ„Éà„É´„Éº„ÉóJOIN'
            }.get(strategy, strategy)
            lines.append(f"- **{strategy_jp}** (`{strategy}`)")
        lines.append("")
    
    # BROADCAST„Éé„Éº„ÉâË©≥Á¥∞
    broadcast_nodes = plan_info.get("broadcast_nodes", [])
    if broadcast_nodes:
        lines.append("## üì° BROADCAST„Éé„Éº„ÉâË©≥Á¥∞")
        lines.append("")
        for i, node in enumerate(broadcast_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **„Éé„Éº„ÉâID**: {node['node_id']}")
            lines.append(f"- **„Éé„Éº„Éâ„Çø„Ç∞**: {node['node_tag']}")
            
            metadata = node.get('metadata', [])
            if metadata:
                lines.append("- **Èñ¢ÈÄ£„É°„Çø„Éá„Éº„Çø**:")
                for meta in metadata[:5]:  # ÊúÄÂ§ß5ÂÄã„Åæ„ÅßË°®Á§∫
                    key = meta.get('key', '')
                    value = meta.get('value', '')
                    values = meta.get('values', [])
                    if values:
                        lines.append(f"  - **{key}**: {', '.join(map(str, values[:3]))}")
                    elif value:
                        lines.append(f"  - **{key}**: {value}")
            lines.append("")
    
    # JOIN„Éé„Éº„ÉâË©≥Á¥∞
    join_nodes = plan_info.get("join_nodes", [])
    if join_nodes:
        lines.append("## üîó JOIN„Éé„Éº„ÉâË©≥Á¥∞")
        lines.append("")
        for i, node in enumerate(join_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **„Éé„Éº„ÉâID**: {node['node_id']}")
            lines.append(f"- **JOINÊà¶Áï•**: {node['join_strategy']}")
            lines.append(f"- **JOIN„Çø„Ç§„Éó**: {node['join_type']}")
            
            join_keys = node.get('join_keys', [])
            if join_keys:
                lines.append(f"- **JOIN„Ç≠„Éº**: {', '.join(join_keys[:5])}")
            lines.append("")
    
    # „ÉÜ„Éº„Éñ„É´„Çπ„Ç≠„É£„É≥Ë©≥Á¥∞Ôºà„Çµ„Ç§„Ç∫Êé®ÂÆöÊÉÖÂ†±„ÇíÂê´„ÇÄÔºâ
    table_scan_details = plan_info.get("table_scan_details", {})
    table_size_estimates = plan_info.get("table_size_estimates", {})
    if table_scan_details:
        lines.append("## üìã „ÉÜ„Éº„Éñ„É´„Çπ„Ç≠„É£„É≥Ë©≥Á¥∞")
        lines.append("")
        for table_name, scan_detail in table_scan_details.items():
            lines.append(f"### {table_name}")
            lines.append("")
            lines.append(f"- **„Éï„Ç°„Ç§„É´ÂΩ¢Âºè**: {scan_detail.get('file_format', 'unknown')}")
            lines.append(f"- **„Éó„ÉÉ„Ç∑„É•„ÉÄ„Ç¶„É≥„Éï„Ç£„É´„ÇøÊï∞**: {len(scan_detail.get('pushed_filters', []))}")
            lines.append(f"- **Âá∫Âäõ„Ç´„É©„É†Êï∞**: {len(scan_detail.get('output_columns', []))}")
            
            # ÂÆüË°å„Éó„É©„É≥„Åã„Çâ„ÅÆ„Çµ„Ç§„Ç∫Êé®ÂÆöÊÉÖÂ†±ÔºàestimatedSizeInBytesÂà©Áî®‰∏çÂèØ„ÅÆ„Åü„ÇÅÁÑ°ÂäπÂåñÔºâ
            # size_info = table_size_estimates.get(table_name)
            # if size_info:
            #     lines.append(f"- **Êé®ÂÆö„Çµ„Ç§„Ç∫ÔºàÂÆüË°å„Éó„É©„É≥Ôºâ**: {size_info['estimated_size_mb']:.1f}MB")
            #     lines.append(f"- **„Çµ„Ç§„Ç∫Êé®ÂÆö‰ø°È†ºÂ∫¶**: {size_info.get('confidence', 'medium')}")
            #     if 'num_files' in size_info:
            #         lines.append(f"- **„Éï„Ç°„Ç§„É´Êï∞**: {size_info['num_files']}")
            #     if 'num_partitions' in size_info:
            #         lines.append(f"- **„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥Êï∞**: {size_info['num_partitions']}")
            
            pushed_filters = scan_detail.get('pushed_filters', [])
            if pushed_filters:
                lines.append("- **„Éó„ÉÉ„Ç∑„É•„ÉÄ„Ç¶„É≥„Éï„Ç£„É´„Çø**:")
                for filter_expr in pushed_filters[:3]:  # ÊúÄÂ§ß3ÂÄã„Åæ„ÅßË°®Á§∫
                    lines.append(f"  - `{filter_expr}`")
            lines.append("")
    
    # „Ç∑„É£„ÉÉ„Éï„É´„Éé„Éº„ÉâË©≥Á¥∞
    shuffle_nodes = plan_info.get("shuffle_nodes", [])
    if shuffle_nodes:
        lines.append("## üîÑ „Ç∑„É£„ÉÉ„Éï„É´„Éé„Éº„ÉâË©≥Á¥∞")
        lines.append("")
        for i, node in enumerate(shuffle_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **„Éé„Éº„ÉâID**: {node['node_id']}")
            
            partition_keys = node.get('partition_keys', [])
            if partition_keys:
                lines.append(f"- **„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„Ç≠„Éº**: {', '.join(partition_keys)}")
            lines.append("")
    
    # ÈõÜÁ¥Ñ„Éé„Éº„ÉâË©≥Á¥∞
    aggregate_nodes = plan_info.get("aggregate_nodes", [])
    if aggregate_nodes:
        lines.append("## üìä ÈõÜÁ¥Ñ„Éé„Éº„ÉâË©≥Á¥∞")
        lines.append("")
        for i, node in enumerate(aggregate_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **„Éé„Éº„ÉâID**: {node['node_id']}")
            
            group_keys = node.get('group_keys', [])
            if group_keys:
                lines.append(f"- **„Ç∞„É´„Éº„ÉóÂåñ„Ç≠„Éº**: {', '.join(group_keys[:5])}")
            
            agg_expressions = node.get('aggregate_expressions', [])
            if agg_expressions:
                lines.append(f"- **ÈõÜÁ¥ÑÈñ¢Êï∞**: {', '.join(agg_expressions[:5])}")
            lines.append("")
    
    # „ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫Êé®ÂÆöÊÉÖÂ†±„Çµ„Éû„É™„ÉºÔºàestimatedSizeInBytesÂà©Áî®‰∏çÂèØ„ÅÆ„Åü„ÇÅÁÑ°ÂäπÂåñÔºâ
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     lines.append("## üìè „ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫Êé®ÂÆöÊÉÖÂ†±ÔºàÂÆüË°å„Éó„É©„É≥„Éô„Éº„ÇπÔºâ")
    #     lines.append("")
    #     total_estimated_size = sum(size_info['estimated_size_mb'] for size_info in table_size_estimates.values())
    #     lines.append(f"- **Êé®ÂÆöÂØæË±°„ÉÜ„Éº„Éñ„É´Êï∞**: {len(table_size_estimates)}")
    #     lines.append(f"- **Á∑èÊé®ÂÆö„Çµ„Ç§„Ç∫**: {total_estimated_size:.1f}MB")
    #     lines.append("")
    #     
    #     for table_name, size_info in list(table_size_estimates.items())[:5]:  # ÊúÄÂ§ß5„ÉÜ„Éº„Éñ„É´Ë°®Á§∫
    #         lines.append(f"### {table_name}")
    #         lines.append(f"- **Êé®ÂÆö„Çµ„Ç§„Ç∫**: {size_info['estimated_size_mb']:.1f}MB")
    #         lines.append(f"- **‰ø°È†ºÂ∫¶**: {size_info.get('confidence', 'medium')}")
    #         lines.append(f"- **„Éé„Éº„Éâ**: {size_info.get('node_name', 'unknown')}")
    #         if 'num_files' in size_info:
    #             lines.append(f"- **„Éï„Ç°„Ç§„É´Êï∞**: {size_info['num_files']}")
    #         lines.append("")
    #     
    #     if len(table_size_estimates) > 5:
    #         lines.append(f"...‰ªñ {len(table_size_estimates) - 5} „ÉÜ„Éº„Éñ„É´ÔºàË©≥Á¥∞„ÅØ‰∏äË®ò„Çª„ÇØ„Ç∑„Éß„É≥ÂèÇÁÖßÔºâ")
    #         lines.append("")
    
    # ÊúÄÈÅ©ÂåñÊé®Â•®‰∫ãÈ†Ö
    lines.append("## üí° „Éó„É©„É≥„Éô„Éº„ÇπÊúÄÈÅ©ÂåñÊé®Â•®‰∫ãÈ†Ö")
    lines.append("")
    
    if plan_summary.get('has_broadcast_joins', False):
        lines.append("‚úÖ **Êó¢„Å´BROADCAST JOIN„ÅåÈÅ©Áî®„Åï„Çå„Å¶„ÅÑ„Åæ„Åô**")
        lines.append("- ÁèæÂú®„ÅÆÂÆüË°å„Éó„É©„É≥„ÅßBROADCASTÊúÄÈÅ©Âåñ„ÅåÊúâÂäπ")
        
        # BROADCAST„Åï„Çå„Å¶„ÅÑ„Çã„ÉÜ„Éº„Éñ„É´‰∏ÄË¶ß„ÇíË°®Á§∫
        broadcast_tables = plan_summary.get('broadcast_tables', [])
        if broadcast_tables:
            lines.append(f"- **BROADCAST„Åï„Çå„Å¶„ÅÑ„Çã„ÉÜ„Éº„Éñ„É´**: {', '.join(broadcast_tables)}")
        
        lines.append("- ËøΩÂä†„ÅÆBROADCASTÈÅ©Áî®Ê©ü‰ºö„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ")
    else:
        lines.append("‚ö†Ô∏è **BROADCAST JOIN„ÅåÊú™ÈÅ©Áî®„Åß„Åô**")
        lines.append("- Â∞è„ÉÜ„Éº„Éñ„É´„Å´BROADCAST„Éí„É≥„Éà„ÅÆÈÅ©Áî®„ÇíÊ§úË®é")
        lines.append("- 30MBÈñæÂÄ§‰ª•‰∏ã„ÅÆ„ÉÜ„Éº„Éñ„É´„ÇíÁâπÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ")
    lines.append("")
    
    if plan_summary.get('shuffle_nodes_count', 0) > 3:
        lines.append("‚ö†Ô∏è **Â§öÊï∞„ÅÆ„Ç∑„É£„ÉÉ„Éï„É´Êìç‰Ωú„ÅåÊ§úÂá∫„Åï„Çå„Åæ„Åó„Åü**")
        lines.append("- „Éá„Éº„Çø„ÅÆÂàÜÊï£„Å®„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„Éã„É≥„Ç∞Êà¶Áï•„ÇíË¶ãÁõ¥„Åó")
        lines.append("- Liquid Clustering„ÅÆÈÅ©Áî®„ÇíÊ§úË®é")
    lines.append("")
    
    # „Çµ„Ç§„Ç∫Êé®ÂÆö„Éô„Éº„Çπ„ÅÆÊúÄÈÅ©ÂåñÊèêÊ°àÔºàestimatedSizeInBytesÂà©Áî®‰∏çÂèØ„ÅÆ„Åü„ÇÅÁÑ°ÂäπÂåñÔºâ
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     small_tables = [name for name, info in table_size_estimates.items() if info['estimated_size_mb'] <= 30]
    #     if small_tables:
    #         lines.append("üí° **ÂÆüË°å„Éó„É©„É≥„Éô„Éº„ÇπBROADCASTÊé®Â•®**")
    #         lines.append(f"- 30MB‰ª•‰∏ã„ÅÆÂ∞è„ÉÜ„Éº„Éñ„É´: {len(small_tables)}ÂÄãÊ§úÂá∫")
    #         for table in small_tables[:3]:  # ÊúÄÂ§ß3ÂÄãË°®Á§∫
    #             size_mb = table_size_estimates[table]['estimated_size_mb']
    #             lines.append(f"  ‚Ä¢ {table}: {size_mb:.1f}MBÔºàBROADCASTÂÄôË£úÔºâ")
    #         if len(small_tables) > 3:
    #             lines.append(f"  ‚Ä¢ ...‰ªñ {len(small_tables) - 3} „ÉÜ„Éº„Éñ„É´")
    #         lines.append("")
    
    lines.append("---")
    lines.append("")
    lines.append("„Åì„ÅÆ„É¨„Éù„Éº„Éà„ÅØ„ÄÅDatabricks SQLÂÆüË°å„Éó„É©„É≥ÂàÜÊûê„ÉÑ„Éº„É´„Å´„Çà„Å£„Å¶Ëá™ÂãïÁîüÊàê„Åï„Çå„Åæ„Åó„Åü„ÄÇ")
    
    return '\n'.join(lines)

def generate_execution_plan_markdown_report_en(plan_info: Dict[str, Any]) -> str:
    """
    ÂÆüË°å„Éó„É©„É≥ÂàÜÊûêÁµêÊûú„ÅÆMarkdown„É¨„Éù„Éº„ÉàÔºàËã±Ë™ûÁâàÔºâ
    """
    from datetime import datetime
    
    lines = []
    lines.append("# Databricks SQL Execution Plan Analysis Report")
    lines.append("")
    lines.append(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("")
    
    # Plan Summary
    plan_summary = plan_info.get("plan_summary", {})
    lines.append("## üìä Execution Plan Summary")
    lines.append("")
    lines.append(f"- **Total Nodes**: {plan_summary.get('total_nodes', 0)}")
    lines.append(f"- **BROADCAST Nodes**: {plan_summary.get('broadcast_nodes_count', 0)}")
    lines.append(f"- **JOIN Nodes**: {plan_summary.get('join_nodes_count', 0)}")
    lines.append(f"- **Scan Nodes**: {plan_summary.get('scan_nodes_count', 0)}")
    lines.append(f"- **Shuffle Nodes**: {plan_summary.get('shuffle_nodes_count', 0)}")
    lines.append(f"- **Aggregate Nodes**: {plan_summary.get('aggregate_nodes_count', 0)}")
    lines.append(f"- **BROADCAST in Use**: {'Yes' if plan_summary.get('has_broadcast_joins', False) else 'No'}")
    lines.append(f"- **Tables Scanned**: {plan_summary.get('tables_scanned', 0)}")
    lines.append("")
    
    # JOIN Strategy Analysis
    unique_join_strategies = plan_summary.get('unique_join_strategies', [])
    if unique_join_strategies:
        lines.append("## üîó JOIN Strategy Analysis")
        lines.append("")
        for strategy in unique_join_strategies:
            lines.append(f"- **{strategy.replace('_', ' ').title()}** (`{strategy}`)")
        lines.append("")
    
    # BROADCAST Node Details
    broadcast_nodes = plan_info.get("broadcast_nodes", [])
    if broadcast_nodes:
        lines.append("## üì° BROADCAST Node Details")
        lines.append("")
        for i, node in enumerate(broadcast_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            lines.append(f"- **Node Tag**: {node['node_tag']}")
            
            metadata = node.get('metadata', [])
            if metadata:
                lines.append("- **Related Metadata**:")
                for meta in metadata[:5]:  # Show up to 5
                    key = meta.get('key', '')
                    value = meta.get('value', '')
                    values = meta.get('values', [])
                    if values:
                        lines.append(f"  - **{key}**: {', '.join(map(str, values[:3]))}")
                    elif value:
                        lines.append(f"  - **{key}**: {value}")
            lines.append("")
    
    # JOIN Node Details
    join_nodes = plan_info.get("join_nodes", [])
    if join_nodes:
        lines.append("## üîó JOIN Node Details")
        lines.append("")
        for i, node in enumerate(join_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            lines.append(f"- **JOIN Strategy**: {node['join_strategy']}")
            lines.append(f"- **JOIN Type**: {node['join_type']}")
            
            join_keys = node.get('join_keys', [])
            if join_keys:
                lines.append(f"- **JOIN Keys**: {', '.join(join_keys[:5])}")
            lines.append("")
    
    # Table Scan Details (with size estimation info)
    table_scan_details = plan_info.get("table_scan_details", {})
    table_size_estimates = plan_info.get("table_size_estimates", {})
    if table_scan_details:
        lines.append("## üìã Table Scan Details")
        lines.append("")
        for table_name, scan_detail in table_scan_details.items():
            lines.append(f"### {table_name}")
            lines.append("")
            lines.append(f"- **File Format**: {scan_detail.get('file_format', 'unknown')}")
            lines.append(f"- **Pushed Filters**: {len(scan_detail.get('pushed_filters', []))}")
            lines.append(f"- **Output Columns**: {len(scan_detail.get('output_columns', []))}")
            
            # Add execution plan size estimation info (disabled - estimatedSizeInBytes not available)
            # size_info = table_size_estimates.get(table_name)
            # if size_info:
            #     lines.append(f"- **Estimated Size (Execution Plan)**: {size_info['estimated_size_mb']:.1f}MB")
            #     lines.append(f"- **Size Estimation Confidence**: {size_info.get('confidence', 'medium')}")
            #     if 'num_files' in size_info:
            #         lines.append(f"- **Number of Files**: {size_info['num_files']}")
            #     if 'num_partitions' in size_info:
            #         lines.append(f"- **Number of Partitions**: {size_info['num_partitions']}")
            
            pushed_filters = scan_detail.get('pushed_filters', [])
            if pushed_filters:
                lines.append("- **Pushed Down Filters**:")
                for filter_expr in pushed_filters[:3]:  # Show up to 3
                    lines.append(f"  - `{filter_expr}`")
            lines.append("")
    
    # Shuffle Node Details
    shuffle_nodes = plan_info.get("shuffle_nodes", [])
    if shuffle_nodes:
        lines.append("## üîÑ Shuffle Node Details")
        lines.append("")
        for i, node in enumerate(shuffle_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            
            partition_keys = node.get('partition_keys', [])
            if partition_keys:
                lines.append(f"- **Partition Keys**: {', '.join(partition_keys)}")
            lines.append("")
    
    # Aggregate Node Details
    aggregate_nodes = plan_info.get("aggregate_nodes", [])
    if aggregate_nodes:
        lines.append("## üìä Aggregate Node Details")
        lines.append("")
        for i, node in enumerate(aggregate_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            
            group_keys = node.get('group_keys', [])
            if group_keys:
                lines.append(f"- **Group Keys**: {', '.join(group_keys[:5])}")
            
            agg_expressions = node.get('aggregate_expressions', [])
            if agg_expressions:
                lines.append(f"- **Aggregate Functions**: {', '.join(agg_expressions[:5])}")
            lines.append("")
    
    # Table Size Estimation Summary (disabled - estimatedSizeInBytes not available)
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     lines.append("## üìè Table Size Estimation (Execution Plan Based)")
    #     lines.append("")
    #     total_estimated_size = sum(size_info['estimated_size_mb'] for size_info in table_size_estimates.values())
    #     lines.append(f"- **Estimated Tables Count**: {len(table_size_estimates)}")
    #     lines.append(f"- **Total Estimated Size**: {total_estimated_size:.1f}MB")
    #     lines.append("")
    #     
    #     for table_name, size_info in list(table_size_estimates.items())[:5]:  # Show up to 5 tables
    #         lines.append(f"### {table_name}")
    #         lines.append(f"- **Estimated Size**: {size_info['estimated_size_mb']:.1f}MB")
    #         lines.append(f"- **Confidence**: {size_info.get('confidence', 'medium')}")
    #         lines.append(f"- **Node**: {size_info.get('node_name', 'unknown')}")
    #         if 'num_files' in size_info:
    #             lines.append(f"- **Number of Files**: {size_info['num_files']}")
    #         lines.append("")
    #     
    #     if len(table_size_estimates) > 5:
    #         lines.append(f"...and {len(table_size_estimates) - 5} more tables (see details in sections above)")
    #         lines.append("")
    
    # Plan-based Optimization Recommendations
    lines.append("## üí° Plan-based Optimization Recommendations")
    lines.append("")
    
    if plan_summary.get('has_broadcast_joins', False):
        lines.append("‚úÖ **BROADCAST JOIN is already applied**")
        lines.append("- Current execution plan has BROADCAST optimization enabled")
        
        # Show list of broadcast tables
        broadcast_tables = plan_summary.get('broadcast_tables', [])
        if broadcast_tables:
            lines.append(f"- **Tables Being Broadcast**: {', '.join(broadcast_tables)}")
        
        lines.append("- Check for additional BROADCAST application opportunities")
    else:
        lines.append("‚ö†Ô∏è **BROADCAST JOIN is not applied**")
        lines.append("- Consider applying BROADCAST hints to small tables")
        lines.append("- Identify tables under 30MB threshold")
    lines.append("")
    
    if plan_summary.get('shuffle_nodes_count', 0) > 3:
        lines.append("‚ö†Ô∏è **Multiple shuffle operations detected**")
        lines.append("- Review data distribution and Liquid Clustering strategy")
        lines.append("- Consider applying Liquid Clustering for data layout optimization")
    lines.append("")
    
    # Size estimation based optimization suggestions (disabled - estimatedSizeInBytes not available)
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     small_tables = [name for name, info in table_size_estimates.items() if info['estimated_size_mb'] <= 30]
    #     if small_tables:
    #         lines.append("üí° **Execution Plan Based BROADCAST Recommendations**")
    #         lines.append(f"- Small tables ‚â§30MB detected: {len(small_tables)}")
    #         for table in small_tables[:3]:  # Show up to 3 tables
    #             size_mb = table_size_estimates[table]['estimated_size_mb']
    #             lines.append(f"  ‚Ä¢ {table}: {size_mb:.1f}MB (BROADCAST candidate)")
    #         if len(small_tables) > 3:
    #             lines.append(f"  ‚Ä¢ ...and {len(small_tables) - 3} more tables")
    #         lines.append("")
    
    lines.append("---")
    lines.append("")
    lines.append("This report was automatically generated by the Databricks SQL Execution Plan Analysis Tool.")
    
    return '\n'.join(lines)


def summarize_explain_results_with_llm(explain_content: str, explain_cost_content: str, query_type: str = "original") -> Dict[str, str]:
    """
    EXPLAIN + EXPLAIN COSTÁµêÊûú„ÇíLLM„ÅßË¶ÅÁ¥Ñ„Åó„Å¶„Éà„Éº„ÇØ„É≥Âà∂Èôê„Å´ÂØæÂøú
    
    Args:
        explain_content: EXPLAINÁµêÊûú„ÅÆÂÜÖÂÆπ
        explain_cost_content: EXPLAIN COSTÁµêÊûú„ÅÆÂÜÖÂÆπ  
        query_type: „ÇØ„Ç®„É™„Çø„Ç§„ÉóÔºà"original" „Åæ„Åü„ÅØ "optimized"Ôºâ
    
    Returns:
        Dict containing summarized results
    """
    
    # „Çµ„Ç§„Ç∫Âà∂Èôê„ÉÅ„Çß„ÉÉ„ÇØÔºàÂêàË®à200KB‰ª•‰∏ä„ÅßË¶ÅÁ¥Ñ„ÇíÂÆüË°åÔºâ
    total_size = len(explain_content) + len(explain_cost_content)
    SUMMARIZATION_THRESHOLD = 200000  # 200KB
    
    if total_size < SUMMARIZATION_THRESHOLD:
        print(f"üìä EXPLAIN + EXPLAIN COST total size: {total_size:,} characters (no summary needed)")
        return {
            'explain_summary': explain_content,
            'explain_cost_summary': explain_cost_content,
            'physical_plan_summary': explain_content,
            'cost_statistics_summary': extract_cost_statistics_from_explain_cost(explain_cost_content),
            'summarized': False
        }
    
    print(f"üìä EXPLAIN + EXPLAIN COST total size: {total_size:,} characters (summary executed)")
    
    # Ë¶ÅÁ¥ÑÁî®„Éó„É≠„É≥„Éó„Éà
    summarization_prompt = f"""
„ÅÇ„Å™„Åü„ÅØDatabricks„ÅÆSQL„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂ∞ÇÈñÄÂÆ∂„Åß„Åô„ÄÇ‰ª•‰∏ã„ÅÆEXPLAIN + EXPLAIN COSTÁµêÊûú„ÇíÁ∞°ÊΩî„Å´Ë¶ÅÁ¥Ñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„ÄêË¶ÅÁ¥ÑÂØæË±°„Éá„Éº„Çø„Äë
- „ÇØ„Ç®„É™„Çø„Ç§„Éó: {query_type}
- EXPLAINÁµêÊûú„Çµ„Ç§„Ç∫: {len(explain_content):,} ÊñáÂ≠ó
- EXPLAIN COSTÁµêÊûú„Çµ„Ç§„Ç∫: {len(explain_cost_content):,} ÊñáÂ≠ó

„ÄêEXPLAINÁµêÊûú„Äë
```
{explain_content[:20000]}{"..." if len(explain_content) > 20000 else ""}
```

„ÄêEXPLAIN COSTÁµêÊûú„Äë  
```
{explain_cost_content[:20000]}{"..." if len(explain_cost_content) > 20000 else ""}
```

„ÄêË¶ÅÁ¥ÑÊåáÁ§∫„Äë
‰ª•‰∏ã„ÅÆÂΩ¢Âºè„ÅßÁ∞°ÊΩî„Å´Ë¶ÅÁ¥Ñ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºàÂêàË®à5000ÊñáÂ≠ó‰ª•ÂÜÖÔºâ:

## üìä Physical PlanË¶ÅÁ¥Ñ
- ‰∏ªË¶Å„Å™Âá¶ÁêÜ„Çπ„ÉÜ„ÉÉ„ÉóÔºà5-10ÂÄã„ÅÆÈáçË¶Å„Å™Êìç‰ΩúÔºâ
- JOINÊñπÂºè„Å®„Éá„Éº„ÇøÁßªÂãï„Éë„Çø„Éº„É≥
- PhotonÂà©Áî®Áä∂Ê≥Å„Å®„Éú„Éà„É´„Éç„ÉÉ„ÇØ

## üí∞ Áµ±Ë®àÊÉÖÂ†±„Çµ„Éû„É™„Éº
- „ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫„Å®Ë°åÊï∞„ÅÆÈáçË¶Å„Å™ÊÉÖÂ†±
- JOINÈÅ∏ÊäûÁéá„Å®„Éï„Ç£„É´„ÇøÂäπÁéá
- „É°„É¢„É™‰ΩøÁî®Èáè„Å®„Çπ„Éî„É´‰∫àÊ∏¨
- „Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥ÂàÜÊï£Áä∂Ê≥Å

## ‚ö° „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂàÜÊûê
- ÂÆüË°å„Ç≥„Çπ„Éà„ÅÆÂÜÖË®≥
- „Éú„Éà„É´„Éç„ÉÉ„ÇØ„Å´„Å™„Çä„Åù„ÅÜ„Å™Êìç‰Ωú
- ÊúÄÈÅ©Âåñ„ÅÆ‰ΩôÂú∞„Åå„ÅÇ„ÇãÁÆáÊâÄ

„ÄêÈáçË¶Å„Äë: 
- Êï∞ÂÄ§„Éá„Éº„Çø„ÅØÊ≠£Á¢∫„Å´Ë®òËºâ
- SQLÊúÄÈÅ©Âåñ„Å´ÈáçË¶Å„Å™ÊÉÖÂ†±„ÇíÂÑ™ÂÖà
- 5000ÊñáÂ≠ó‰ª•ÂÜÖ„ÅßÂÆåÁµê„Å´„Åæ„Å®„ÇÅ„Çã
"""

    try:
        # Ë®≠ÂÆö„Åï„Çå„ÅüLLM„Éó„É≠„Éê„Ç§„ÉÄ„Éº„Çí‰ΩøÁî®
        provider = LLM_CONFIG["provider"]
        
        if provider == "databricks":
            summary_result = _call_databricks_llm(summarization_prompt)
        elif provider == "openai":
            summary_result = _call_openai_llm(summarization_prompt)
        elif provider == "azure_openai":
            summary_result = _call_azure_openai_llm(summarization_prompt)
        elif provider == "anthropic":
            summary_result = _call_anthropic_llm(summarization_prompt)
        else:
            # „Ç®„É©„ÉºÊôÇ„ÅØÂàá„ÇäË©∞„ÇÅÁâà„ÇíËøî„Åô
            print("‚ùå LLM provider error: Using truncated version")
            return {
                'explain_summary': explain_content[:30000] + "\n\n‚ö†Ô∏è Âàá„ÇäË©∞„ÇÅ„Çâ„Çå„Åæ„Åó„Åü",
                'explain_cost_summary': explain_cost_content[:30000] + "\n\n‚ö†Ô∏è Âàá„ÇäË©∞„ÇÅ„Çâ„Çå„Åæ„Åó„Åü", 
                'physical_plan_summary': explain_content[:20000] + "\n\n‚ö†Ô∏è Âàá„ÇäË©∞„ÇÅ„Çâ„Çå„Åæ„Åó„Åü",
                'cost_statistics_summary': extract_cost_statistics_from_explain_cost(explain_cost_content),
                'summarized': True
            }
        
        # LLM„Ç®„É©„Éº„ÉÅ„Çß„ÉÉ„ÇØ
        if isinstance(summary_result, str) and summary_result.startswith("LLM_ERROR:"):
            print(f"‚ùå LLM summary error: Using truncated version - {summary_result[10:200]}...")
            return {
                'explain_summary': explain_content[:30000] + "\n\n‚ö†Ô∏è Âàá„ÇäË©∞„ÇÅ„Çâ„Çå„Åæ„Åó„Åü",
                'explain_cost_summary': explain_cost_content[:30000] + "\n\n‚ö†Ô∏è Âàá„ÇäË©∞„ÇÅ„Çâ„Çå„Åæ„Åó„Åü",
                'physical_plan_summary': explain_content[:20000] + "\n\n‚ö†Ô∏è Âàá„ÇäË©∞„ÇÅ„Çâ„Çå„Åæ„Åó„Åü", 
                'cost_statistics_summary': extract_cost_statistics_from_explain_cost(explain_cost_content),
                'summarized': True
            }
        
        # thinking_enabledÂØæÂøú
        if isinstance(summary_result, list):
            summary_text = extract_main_content_from_thinking_response(summary_result)
        else:
            summary_text = str(summary_result)
        
        # Ë¶ÅÁ¥ÑÁµêÊûú„ÇíÂàÜÂâ≤„Åó„Å¶Ëøî„Åô
        print(f"‚úÖ EXPLAIN + EXPLAIN COST summary completed: {len(summary_text):,} characters")
        
        # üö® DEBUG_ENABLED='Y'„ÅÆÂ†¥Âêà„ÄÅË¶ÅÁ¥ÑÁµêÊûú„Çí„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò
        debug_enabled = globals().get('DEBUG_ENABLED', 'N')
        if debug_enabled.upper() == 'Y':
            try:
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                summary_filename = f"output_explain_summary_{query_type}_{timestamp}.md"
                
                # Ë¶ÅÁ¥ÑÁµêÊûú„ÇíMarkdownÂΩ¢Âºè„Åß‰øùÂ≠òÔºàOUTPUT_LANGUAGE„Å´Âøú„Åò„Å¶Ë®ÄË™û„ÇíÂàá„ÇäÊõø„ÅàÔºâ
                output_language = globals().get('OUTPUT_LANGUAGE', 'ja')
                
                if output_language == 'en':
                    summary_content = f"""# EXPLAIN + EXPLAIN COST Summary Results ({query_type})

## üìä Basic Information
- Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- Query Type: {query_type}
- Original Size: EXPLAIN({len(explain_content):,} chars) + EXPLAIN COST({len(explain_cost_content):,} chars) = {total_size:,} chars
- Summary Size: {len(summary_text):,} chars
- Compression Ratio: {total_size//len(summary_text) if len(summary_text) > 0 else 0}x

## üß† LLM Summary Results

{summary_text}

## üí∞ Statistical Information Extraction

{extract_cost_statistics_from_explain_cost(explain_cost_content)}
"""
                else:
                    summary_content = f"""# EXPLAIN + EXPLAIN COSTË¶ÅÁ¥ÑÁµêÊûú ({query_type})

## üìä Âü∫Êú¨ÊÉÖÂ†±
- ÁîüÊàêÊó•ÊôÇ: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- „ÇØ„Ç®„É™„Çø„Ç§„Éó: {query_type}
- ÂÖÉ„Çµ„Ç§„Ç∫: EXPLAIN({len(explain_content):,}ÊñáÂ≠ó) + EXPLAIN COST({len(explain_cost_content):,}ÊñáÂ≠ó) = {total_size:,}ÊñáÂ≠ó
- Ë¶ÅÁ¥ÑÂæå„Çµ„Ç§„Ç∫: {len(summary_text):,}ÊñáÂ≠ó
- ÂúßÁ∏ÆÁéá: {total_size//len(summary_text) if len(summary_text) > 0 else 0}x

## üß† LLMË¶ÅÁ¥ÑÁµêÊûú

{summary_text}

## üí∞ Áµ±Ë®àÊÉÖÂ†±ÊäΩÂá∫

{extract_cost_statistics_from_explain_cost(explain_cost_content)}
"""
                
                with open(summary_filename, 'w', encoding='utf-8') as f:
                    f.write(summary_content)
                
                print(f"üìÑ Saving summary results: {summary_filename}")
                
            except Exception as save_error:
                print(f"‚ö†Ô∏è Failed to save summary results: {str(save_error)}")
        
        return {
            'explain_summary': summary_text,
            'explain_cost_summary': summary_text,  # Áµ±ÂêàË¶ÅÁ¥Ñ„Å®„Åó„Å¶Âêå„ÅòÂÜÖÂÆπ
            'physical_plan_summary': summary_text,
            'cost_statistics_summary': extract_cost_statistics_from_explain_cost(explain_cost_content),
            'summarized': True
        }
        
    except Exception as e:
        print(f"‚ùå Error during EXPLAIN summarization: {str(e)}")
        # „Ç®„É©„ÉºÊôÇ„ÅØÂàá„ÇäË©∞„ÇÅÁâà„ÇíËøî„Åô
        return {
            'explain_summary': explain_content[:30000] + f"\n\n‚ö†Ô∏è Ë¶ÅÁ¥Ñ„Ç®„É©„Éº„ÅÆ„Åü„ÇÅÂàá„ÇäË©∞„ÇÅ„Çâ„Çå„Åæ„Åó„Åü: {str(e)}",
            'explain_cost_summary': explain_cost_content[:30000] + f"\n\n‚ö†Ô∏è Ë¶ÅÁ¥Ñ„Ç®„É©„Éº„ÅÆ„Åü„ÇÅÂàá„ÇäË©∞„ÇÅ„Çâ„Çå„Åæ„Åó„Åü: {str(e)}",
            'physical_plan_summary': explain_content[:20000] + f"\n\n‚ö†Ô∏è Ë¶ÅÁ¥Ñ„Ç®„É©„Éº„ÅÆ„Åü„ÇÅÂàá„ÇäË©∞„ÇÅ„Çâ„Çå„Åæ„Åó„Åü: {str(e)}",
            'cost_statistics_summary': extract_cost_statistics_from_explain_cost(explain_cost_content),
            'summarized': True
        }


def generate_optimization_strategy_summary(optimized_result: str, metrics: Dict[str, Any], analysis_result: str = "") -> str:
    """
    Generate optimization strategy summary
    
    Args:
        optimized_result: Optimization result (LLM response)
        metrics: Metrics information
        analysis_result: Bottleneck analysis result
        
    Returns:
        str: Optimization strategy summary
    """
    try:
        # „É°„Éà„É™„ÇØ„Çπ„Åã„Çâ„Éú„Éà„É´„Éç„ÉÉ„ÇØÊåáÊ®ô„ÇíÂèñÂæó
        bottleneck_indicators = metrics.get('bottleneck_indicators', {})
        overall_metrics = metrics.get('overall_metrics', {})
        
        # ÊúÄÈÅ©Âåñ„Åß‰ΩøÁî®„Åï„Çå„ÅüÊâãÊ≥ï„ÇíÊ§úÂá∫
        optimization_techniques = []
        performance_issues = []
        
        # ÊúÄÈÅ©ÂåñÂÜÖÂÆπ„Åã„ÇâÊâãÊ≥ï„ÇíÊäΩÂá∫
        if optimized_result:
            content_upper = optimized_result.upper()
            
            # JOINÊúÄÈÅ©Âåñ
            if 'BROADCAST' in content_upper or 'MAPJOIN' in content_upper:
                optimization_techniques.append("**Broadcast Join**: Â∞è„Åï„Å™„ÉÜ„Éº„Éñ„É´„Çí„Éñ„É≠„Éº„Éâ„Ç≠„É£„Çπ„Éà„Åó„Å¶ÂàÜÊï£ÁµêÂêà„ÇíÊúÄÈÅ©Âåñ")
            
            if 'REPARTITION' in content_upper or 'REDISTRIBUTE' in content_upper:
                optimization_techniques.append("**„Éá„Éº„ÇøÂÜçÂàÜÊï£**: „Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥Êï∞„ÇÑ„Ç≠„Éº„ÇíË™øÊï¥„Åó„Å¶„Éá„Éº„Çø„Çπ„Ç≠„É•„Éº„ÇíËß£Ê∂à")
            
            # DatabricksÂõ∫Êúâ„ÅÆ„Éá„Éº„ÇøÊúÄÈÅ©Âåñ
            if 'PARTITION' in content_upper and 'BY' in content_upper:
                optimization_techniques.append("**Liquid Clustering**: „ÇØ„Ç®„É™„Éï„Ç£„É´„Çø„Å´Âü∫„Å•„Åè„Éá„Éº„Çø„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞ÊúÄÈÅ©Âåñ")
            
            if 'CLUSTER' in content_upper or 'LIQUID' in content_upper:
                optimization_techniques.append("**Liquid Clustering**: È†ªÁπÅ„Å™„Ç¢„ÇØ„Çª„Çπ„Éë„Çø„Éº„É≥„Å´Âü∫„Å•„Åè„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞")
            
            # PhotonÊúÄÈÅ©Âåñ
            if 'PHOTON' in content_upper or 'VECTORIZED' in content_upper:
                optimization_techniques.append("**Photon Engine**: „Éô„ÇØ„Éà„É´ÂåñÂÆüË°å„Å´„Çà„ÇãÈ´òÈÄüÂåñ")
            
            # „Ç≠„É£„ÉÉ„Ç∑„É•ÊúÄÈÅ©Âåñ
            if 'CACHE' in content_upper or 'PERSIST' in content_upper:
                optimization_techniques.append("**„Éá„Éº„Çø„Ç≠„É£„ÉÉ„Ç∑„É•**: ‰∏≠ÈñìÁµêÊûú„ÅÆÊ∞∏Á∂öÂåñ„Å´„Çà„ÇãÂÜçË®àÁÆóÂõûÈÅø")
            
            # „Éï„Ç£„É´„ÇøÊúÄÈÅ©Âåñ
            if 'WHERE' in content_upper and ('PUSHDOWN' in content_upper or 'PREDICATE' in content_upper):
                optimization_techniques.append("**Ëø∞Ë™û„Éó„ÉÉ„Ç∑„É•„ÉÄ„Ç¶„É≥**: „Éï„Ç£„É´„ÇøÊù°‰ª∂„ÅÆÊó©ÊúüÈÅ©Áî®„Å´„Çà„Çã„Éá„Éº„ÇøÈáèÂâäÊ∏õ")
        
        # „Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûê„Åã„ÇâÂïèÈ°åÁÇπ„ÇíÊäΩÂá∫
        if bottleneck_indicators.get('has_spill', False):
            performance_issues.append("„É°„É¢„É™„Çπ„Éî„É´Áô∫Áîü")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            performance_issues.append("„Ç∑„É£„ÉÉ„Éï„É´Âá¶ÁêÜ„Éú„Éà„É´„Éç„ÉÉ„ÇØ")
        
        if bottleneck_indicators.get('low_parallelism', False):
            performance_issues.append("‰∏¶ÂàóÂ∫¶‰∏çË∂≥")
        
        if bottleneck_indicators.get('cache_hit_ratio', 1.0) < 0.5:
            performance_issues.append("„Ç≠„É£„ÉÉ„Ç∑„É•„Éí„ÉÉ„ÉàÁéá‰Ωé‰∏ã")
        
        if not overall_metrics.get('photon_enabled', True):
            performance_issues.append("Photon EngineÊú™Ê¥ªÁî®")
        
        # „Éá„Éº„Çø„Çπ„Ç≠„É•„ÉºÊ§úÂá∫
        if bottleneck_indicators.get('has_skew', False):
            performance_issues.append("„Éá„Éº„Çø„Çπ„Ç≠„É•„ÉºÁô∫Áîü")
        
        # Ë¶ÅÁ¥ÑÁîüÊàê
        summary_parts = []
        
        # Ê§úÂá∫„Åï„Çå„ÅüÂïèÈ°å
        if performance_issues:
            issues_text = "„ÄÅ".join(performance_issues)
            summary_parts.append(f"**üîç Ê§úÂá∫„Åï„Çå„Åü‰∏ªË¶ÅË™≤È°å**: {issues_text}")
        
        # ÈÅ©Áî®„Åï„Çå„ÅüÊúÄÈÅ©ÂåñÊâãÊ≥ï
        if optimization_techniques:
            summary_parts.append("**üõ†Ô∏è ÈÅ©Áî®„Åï„Çå„ÅüÊúÄÈÅ©ÂåñÊâãÊ≥ï**:")
            for i, technique in enumerate(optimization_techniques, 1):
                summary_parts.append(f"   {i}. {technique}")
        
        # ÊúÄÈÅ©ÂåñÊñπÈáù
        strategy_focus = []
        
        if bottleneck_indicators.get('has_spill', False):
            strategy_focus.append("„É°„É¢„É™ÂäπÁéáÂåñ")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            strategy_focus.append("„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØË≤†Ëç∑ËªΩÊ∏õ")
        
        if bottleneck_indicators.get('low_parallelism', False):
            strategy_focus.append("‰∏¶ÂàóÂá¶ÁêÜËÉΩÂäõÂêë‰∏ä")
        
        if strategy_focus:
            focus_text = "„ÄÅ".join(strategy_focus)
            summary_parts.append(f"**üéØ ÊúÄÈÅ©ÂåñÈáçÁÇπÂàÜÈáé**: {focus_text}")
        
        # EXPLAINÁµ±Ë®àÊÉÖÂ†±„ÅÆÊ¥ªÁî®
        explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
        if explain_enabled.upper() == 'Y':
            summary_parts.append("**üìä Áµ±Ë®àÊÉÖÂ†±Ê¥ªÁî®**: EXPLAIN + EXPLAIN COSTÂàÜÊûê„Å´„Çà„Çä„ÄÅÁµ±Ë®à„Éô„Éº„Çπ„ÅÆÁ≤æÂØÜ„Å™ÊúÄÈÅ©Âåñ„ÇíÂÆüË°å")
        
        if summary_parts:
            return "\n".join(summary_parts)
        else:
            return "**ü§ñ AIÂàÜÊûê„Å´„Çà„ÇãÂåÖÊã¨ÁöÑ„Å™ÊúÄÈÅ©Âåñ**: „Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûê„ÄÅÁµ±Ë®àÊÉÖÂ†±„ÄÅ„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„Çπ„ÇíÁ∑èÂêà„Åó„ÅüÊúÄÈÅ©Âåñ„ÇíÂÆüË°å"
    
    except Exception as e:
        return f"**ü§ñ AIÊúÄÈÅ©Âåñ**: ÂåÖÊã¨ÁöÑ„Å™ÂàÜÊûê„Å´Âü∫„Å•„ÅèÊúÄÈÅ©Âåñ„ÇíÂÆüË°åÔºàË¶ÅÁ¥ÑÁîüÊàê„Ç®„É©„Éº: {str(e)}Ôºâ"

def format_sql_content_for_report(content: str, filename: str = "") -> str:
    """
    SQL„Éï„Ç°„Ç§„É´ÂÜÖÂÆπ„Åæ„Åü„ÅØLLM„É¨„Çπ„Éù„É≥„Çπ„Çí„É¨„Éù„Éº„ÉàÁî®„Å´ÈÅ©Âàá„Å´„Éï„Ç©„Éº„Éû„ÉÉ„Éà
    Èï∑„ÅÑ„ÇØ„Ç®„É™„ÅØÈÅ©Âàá„Å´ÁúÅÁï•„Åó„Å¶„Éï„Ç°„Ç§„É´ÂèÇÁÖß„ÇíÊ°àÂÜÖ
    
    Args:
        content: SQL„Éï„Ç°„Ç§„É´ÂÜÖÂÆπ„Åæ„Åü„ÅØLLM„É¨„Çπ„Éù„É≥„Çπ
        filename: SQL„Éï„Ç°„Ç§„É´ÂêçÔºàÁúÅÁï•ÊôÇ„ÅÆÂèÇÁÖßÁî®Ôºâ
        
    Returns:
        str: „É¨„Éù„Éº„ÉàÁî®„Å´„Éï„Ç©„Éº„Éû„ÉÉ„Éà„Åï„Çå„ÅüÂÜÖÂÆπ
    """
    # ÁúÅÁï•Âà§ÂÆö„ÅÆÂü∫Ê∫ñ
    MAX_LINES_IN_REPORT = 120  # 100Ë°å„ÅÆ„Éó„É¨„Éì„É•„Éº„Å´ÂØæÂøú
    MAX_CHARS_IN_REPORT = 10000  # „Çà„ÇäÈï∑„ÅÑ„ÇØ„Ç®„É™„Å´„ÇÇÂØæÂøú
    PREVIEW_LINES = 100  # 100Ë°å„ÅÆ„Éó„É¨„Éì„É•„Éº
    
    # SQL„Éï„Ç°„Ç§„É´ÂÜÖÂÆπ„ÅÆÂ†¥ÂêàÔºà-- „ÅßÂßã„Åæ„Çã„Ç≥„É°„É≥„Éà„Åå„ÅÇ„ÇãÂ†¥ÂêàÔºâ
    if content.startswith('--') and 'USE CATALOG' in content:
        # SQL„Éï„Ç°„Ç§„É´ÂÜÖÂÆπ„ÅÆÂ†¥Âêà„ÅØ„ÄÅÈÅ©Âàá„Å™„Éï„Ç©„Éº„Éû„ÉÉ„Éà„ÅßË°®Á§∫
        lines = content.split('\n')
        sql_lines = []
        in_sql_section = False
        
        for line in lines:
            # USE CATALOG/USE SCHEMA‰ª•Èôç„ÅåÂÆüÈöõ„ÅÆ„ÇØ„Ç®„É™ÈÉ®ÂàÜ
            if line.strip().startswith('USE CATALOG') or line.strip().startswith('USE SCHEMA'):
                in_sql_section = True
                sql_lines.append(line)
            elif in_sql_section and line.strip():
                sql_lines.append(line)
            elif not in_sql_section and line.strip().startswith('--'):
                # „Ç≥„É°„É≥„ÉàË°å„ÅØÊÆã„ÅôÔºà„Éò„ÉÉ„ÉÄ„ÉºÊÉÖÂ†±Ôºâ
                continue
        
        # Èï∑„ÅïÂà§ÂÆö„Å®ÁúÅÁï•Âá¶ÁêÜ
        if sql_lines:
            full_sql = chr(10).join(sql_lines)
            needs_truncation = (len(sql_lines) > MAX_LINES_IN_REPORT or 
                              len(full_sql) > MAX_CHARS_IN_REPORT)
            
            if needs_truncation:
                # ÁúÅÁï•Áâà„Çí‰ΩúÊàê
                preview_lines = sql_lines[:PREVIEW_LINES]
                omitted_lines = len(sql_lines) - PREVIEW_LINES
                
                return f"""**üöÄ Âãï‰Ωú‰øùË®ºÊ∏à„ÅøÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™ (SQL„Éï„Ç°„Ç§„É´„Å®Âêå‰∏Ä):**

```sql
{chr(10).join(preview_lines)}

-- ... (ÁúÅÁï•: „ÅÇ„Å®{omitted_lines}Ë°å)
-- ÂÆåÂÖ®Áâà„ÅØ {filename if filename else 'output_optimized_query_*.sql'} „Éï„Ç°„Ç§„É´„ÇíÂèÇÁÖß
```

üí° „Åì„ÅÆ„ÇØ„Ç®„É™„ÅØÂÆüÈöõ„ÅÆEXPLAINÂÆüË°å„ÅßÂãï‰ΩúÁ¢∫Ë™çÊ∏à„Åø„Åß„Åô„ÄÇ  
üìÇ **ÂÆåÂÖ®Áâà**: `{filename if filename else 'output_optimized_query_*.sql'}` „Éï„Ç°„Ç§„É´„Çí„ÅîÁ¢∫Ë™ç„Åè„Å†„Åï„ÅÑ„ÄÇ

**üìä „ÇØ„Ç®„É™Ê¶ÇË¶Å:**
- Á∑èË°åÊï∞: {len(sql_lines)}Ë°å
- Ë°®Á§∫: ÊúÄÂàù„ÅÆ{PREVIEW_LINES}Ë°å„ÅÆ„Åø
- ÁúÅÁï•: {omitted_lines}Ë°å"""
            else:
                # Áü≠„ÅÑÂ†¥Âêà„ÅØÂÖ®ÊñáË°®Á§∫
                return f"""**üöÄ Âãï‰Ωú‰øùË®ºÊ∏à„ÅøÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™ (SQL„Éï„Ç°„Ç§„É´„Å®Âêå‰∏Ä):**

```sql
{full_sql}
```

üí° „Åì„ÅÆ„ÇØ„Ç®„É™„ÅØÂÆüÈöõ„ÅÆEXPLAINÂÆüË°å„ÅßÂãï‰ΩúÁ¢∫Ë™çÊ∏à„Åø„Åß„Åô„ÄÇ"""
        else:
            return f"""**üöÄ SQL„Éï„Ç°„Ç§„É´ÂÜÖÂÆπ:**

```sql
{content}
```"""
    
    # LLM„É¨„Çπ„Éù„É≥„Çπ„ÅÆÂ†¥Âêà
    else:
        # Èï∑„ÅÑLLM„É¨„Çπ„Éù„É≥„Çπ„ÇÇÁúÅÁï•ÂØæË±°
        if len(content) > MAX_CHARS_IN_REPORT:
            preview_content = content[:MAX_CHARS_IN_REPORT]
            omitted_chars = len(content) - MAX_CHARS_IN_REPORT
            
            if '```sql' in content:
                return f"""**üí° LLMÊúÄÈÅ©ÂåñÂàÜÊûê (ÁúÅÁï•Áâà):**

{preview_content}...

**ÁúÅÁï•ÊÉÖÂ†±:** „ÅÇ„Å®{omitted_chars}ÊñáÂ≠ó  
üìù Ê≥®ÊÑè: ‰∏äË®ò„ÅØÂàÜÊûêÁµêÊûú„ÅÆ‰∏ÄÈÉ®„Åß„Åô„ÄÇÂÆüÈöõ„ÅÆÂÆüË°åÁî®„ÇØ„Ç®„É™„ÅØ `{filename if filename else 'output_optimized_query_*.sql'}` „Éï„Ç°„Ç§„É´„ÇíÂèÇÁÖß„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"""
            else:
                return f"""**üí° LLMÊúÄÈÅ©ÂåñÂàÜÊûê (ÁúÅÁï•Áâà):**

{preview_content}...

**ÁúÅÁï•ÊÉÖÂ†±:** „ÅÇ„Å®{omitted_chars}ÊñáÂ≠ó  
üìù Ê≥®ÊÑè: ‰∏äË®ò„ÅØÂàÜÊûêÁµêÊûú„ÅÆ‰∏ÄÈÉ®„Åß„Åô„ÄÇÂÆüÈöõ„ÅÆÂÆüË°åÁî®„ÇØ„Ç®„É™„ÅØ `{filename if filename else 'output_optimized_query_*.sql'}` „Éï„Ç°„Ç§„É´„ÇíÂèÇÁÖß„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"""
        else:
            # Áü≠„ÅÑÂ†¥Âêà„ÅØÂÖ®ÊñáË°®Á§∫
            if '```sql' in content:
                return f"""**üí° LLMÊúÄÈÅ©ÂåñÂàÜÊûê:**

{content}"""
            else:
                return f"""**üí° LLMÊúÄÈÅ©ÂåñÂàÜÊûê:**

{content}

üìù Ê≥®ÊÑè: ‰∏äË®ò„ÅØÂàÜÊûêÁµêÊûú„Åß„Åô„ÄÇÂÆüÈöõ„ÅÆÂÆüË°åÁî®„ÇØ„Ç®„É™„ÅØÂØæÂøú„Åô„ÇãSQL„Éï„Ç°„Ç§„É´„ÇíÂèÇÁÖß„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"""

def generate_performance_comparison_section(performance_comparison: Dict[str, Any] = None, language: str = 'ja') -> str:
    """
    „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉÁµêÊûú„ÅÆË©≥Á¥∞„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÁîüÊàê
    
    Args:
        performance_comparison: „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉÁµêÊûú
        language: Ë®ÄË™ûË®≠ÂÆö ('ja' „Åæ„Åü„ÅØ 'en')
        
    Returns:
        str: „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉ„Çª„ÇØ„Ç∑„Éß„É≥„ÅÆ„Éû„Éº„ÇØ„ÉÄ„Ç¶„É≥
    """
    
    # üö® Á∑äÊÄ•‰øÆÊ≠£: „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØË©ï‰æ°ÂØæÂøú
    if not performance_comparison:
        if language == 'ja':
            return """

**üìã ÂÆüË°åÁä∂Ê≥Å**: „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉ„ÅØÂÆüË°å„Åï„Çå„Åæ„Åõ„Çì„Åß„Åó„Åü

| È†ÖÁõÆ | Áä∂Ê≥Å |
|------|------|
| ÊØîËºÉÂÆüË°å | ‚ùå Êú™ÂÆüË°å |
| ÁêÜÁî± | EXPLAINÂèä„Å≥EXPLAIN COSTÂèñÂæóÂ§±Êïó |
| ÂÆâÂÖ®ÊÄß | ‚úÖ ÊßãÊñáÊ§úË®ºÊ∏à„Åø„ÅßÂÆüË°åÂèØËÉΩ |
| Êé®Â•® | üöÄ ÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™„Çí‰ΩøÁî®Ôºà„Éá„Éï„Ç©„É´„ÉàÔºâ |

üí° **Note**: „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉ„ÅØÂÆüË°å„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü„Åå„ÄÅÊßãÊñáÁöÑ„Å´Ê≠£Â∏∏„Å™ÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™„ÅåÁîüÊàê„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ
"""
        else:
            return """

**üìã Execution Status**: Performance comparison was not executed

| Item | Status |
|------|--------|
| Comparison | ‚ùå Not executed |
| Reason | EXPLAIN and EXPLAIN COST acquisition failed |
| Safety | ‚úÖ Syntax verified and executable |
| Recommendation | üöÄ Use optimized query (default) |

üí° **Note**: Although performance comparison was not executed, a syntactically correct optimized query has been generated.
"""
    
    # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØË©ï‰æ°„ÅÆÂ†¥Âêà„ÅÆÁâπÂà•Âá¶ÁêÜ
    if performance_comparison.get('evaluation_type') == 'fallback_plan_analysis':
        fallback_eval = performance_comparison.get('fallback_evaluation', {})
        return generate_fallback_performance_section(fallback_eval, language)
    
    # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉÁµêÊûú„ÅÆË©≥Á¥∞Ë°®Á§∫
    recommendation = performance_comparison.get('recommendation', 'unknown')
    total_cost_ratio = performance_comparison.get('total_cost_ratio', 1.0)
    memory_usage_ratio = performance_comparison.get('memory_usage_ratio', 1.0)
    degradation_detected = performance_comparison.get('performance_degradation_detected', False)
    details = performance_comparison.get('details', [])
    
    if language == 'ja':
        # Êó•Êú¨Ë™ûÁâà„ÅÆË©≥Á¥∞„É¨„Éù„Éº„Éà
        status_text = "üö® „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊÇ™ÂåñÊ§úÂá∫" if degradation_detected else "‚úÖ „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊîπÂñÑÁ¢∫Ë™ç"
        recommendation_text = "ÂÖÉ„ÇØ„Ç®„É™‰ΩøÁî®" if recommendation == 'use_original' else "ÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™‰ΩøÁî®"
        
        # ÊîπÂñÑ/ÊÇ™Âåñ„ÅÆÂà§ÂÆö„Ç¢„Ç§„Ç≥„É≥
        cost_icon = "‚ùå" if total_cost_ratio > 1.1 else "‚úÖ" if total_cost_ratio < 0.9 else "‚ûñ"
        memory_icon = "‚ùå" if memory_usage_ratio > 1.1 else "‚úÖ" if memory_usage_ratio < 0.9 else "‚ûñ"
        
        section = f"""

**üìä ÂÆüË°åÁµêÊûú**: {status_text}

#### üîç Ë©≥Á¥∞ÊØîËºÉ„É°„Éà„É™„ÇØ„Çπ

| È†ÖÁõÆ | ÂÖÉ„ÇØ„Ç®„É™ | ÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™ | ÊØîÁéá | Ë©ï‰æ° |
|------|----------|-------------|------|------|
| ÂÆüË°å„Ç≥„Çπ„Éà | 1.00 (Âü∫Ê∫ñ) | {total_cost_ratio:.2f} | {total_cost_ratio:.2f}ÂÄç | {cost_icon} |
| „É°„É¢„É™‰ΩøÁî®Èáè | 1.00 (Âü∫Ê∫ñ) | {memory_usage_ratio:.2f} | {memory_usage_ratio:.2f}ÂÄç | {memory_icon} |

#### üìã Âà§ÂÆöÁµêÊûú

| È†ÖÁõÆ | ÁµêÊûú |
|------|------|
| Á∑èÂêàÂà§ÂÆö | **{status_text}** |
| Êé®Â•®„Ç¢„ÇØ„Ç∑„Éß„É≥ | **{recommendation_text}** |
| ÊÇ™ÂåñÊ§úÂá∫ | {'„ÅØ„ÅÑ' if degradation_detected else '„ÅÑ„ÅÑ„Åà'} |

#### üéØ Ë©≥Á¥∞ÂàÜÊûêÁµêÊûú

"""
        
        if details:
            for detail in details:
                section += f"- {detail}\n"
        else:
            section += "- Ë©≥Á¥∞„Å™ÂàÜÊûêÊÉÖÂ†±„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì\n"
        
        section += f"""

#### üõ°Ô∏è ÂÆâÂÖ®ÊÄß‰øùË®º

- **„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊÇ™ÂåñÈò≤Ê≠¢**: {'‚úÖ ÊÇ™ÂåñÊ§úÂá∫„Å´„Çà„ÇäÂÖÉ„ÇØ„Ç®„É™„ÇíÈÅ∏Êäû' if degradation_detected else '‚úÖ ÊîπÂñÑÁ¢∫Ë™ç„Å´„Çà„ÇäÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™„ÇíÈÅ∏Êäû'}
- **ÂÆüË°åÂèØËÉΩÊÄß**: ‚úÖ EXPLAINÂÆüË°å„ÅßÊßãÊñáÊ§úË®ºÊ∏à„Åø
- **Ëá™Âãï„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ**: {'‚úÖ ‰ΩúÂãï - ÂÆâÂÖ®ÊÄß„ÇíÂÑ™ÂÖà' if degradation_detected else '‚ùå ‰∏çË¶Å - ÊîπÂñÑÂäπÊûú„ÅÇ„Çä'}

üí° **Âà§ÂÆöÂü∫Ê∫ñ**: ÂÆüË°å„Ç≥„Çπ„Éà30%Â¢óÂä† „Åæ„Åü„ÅØ „É°„É¢„É™‰ΩøÁî®Èáè50%Â¢óÂä† „ÅßÊÇ™Âåñ„Å®Âà§ÂÆö
"""
    
    else:
        # Ëã±Ë™ûÁâà„ÅÆË©≥Á¥∞„É¨„Éù„Éº„Éà
        status_text = "üö® Performance Degradation Detected" if degradation_detected else "‚úÖ Performance Improvement Confirmed"
        recommendation_text = "Use Original Query" if recommendation == 'use_original' else "Use Optimized Query"
        
        # ÊîπÂñÑ/ÊÇ™Âåñ„ÅÆÂà§ÂÆö„Ç¢„Ç§„Ç≥„É≥
        cost_icon = "‚ùå" if total_cost_ratio > 1.1 else "‚úÖ" if total_cost_ratio < 0.9 else "‚ûñ"
        memory_icon = "‚ùå" if memory_usage_ratio > 1.1 else "‚úÖ" if memory_usage_ratio < 0.9 else "‚ûñ"
        
        section = f"""

**üìä Execution Result**: {status_text}

#### üîç Detailed Comparison Metrics

| Item | Original Query | Optimized Query | Ratio | Evaluation |
|------|----------------|-----------------|-------|------------|
| Execution Cost | 1.00 (baseline) | {total_cost_ratio:.2f} | {total_cost_ratio:.2f}x | {cost_icon} |
| Memory Usage | 1.00 (baseline) | {memory_usage_ratio:.2f} | {memory_usage_ratio:.2f}x | {memory_icon} |

#### üìã Judgment Results

| Item | Result |
|------|--------|
| Overall Judgment | **{status_text}** |
| Recommended Action | **{recommendation_text}** |
| Degradation Detected | {'Yes' if degradation_detected else 'No'} |

#### üéØ Detailed Analysis Results

"""
        
        if details:
            for detail in details:
                section += f"- {detail}\n"
        else:
            section += "- Detailed analysis information is not available\n"
        
        section += f"""

#### üõ°Ô∏è Safety Guarantee

- **Performance Degradation Prevention**: {'‚úÖ Degradation detected, original query selected' if degradation_detected else '‚úÖ Improvement confirmed, optimized query selected'}
- **Executability**: ‚úÖ Syntax verified via EXPLAIN execution
- **Automatic Fallback**: {'‚úÖ Activated - Safety prioritized' if degradation_detected else '‚ùå Not needed - Improvement achieved'}

üí° **Judgment Criteria**: Degradation detected if execution cost increases by 30% OR memory usage increases by 50%
"""
    
    return section

def translate_analysis_to_japanese(english_text: str) -> str:
    """
    LLM„Çí‰ΩøÁî®„Åó„Å¶Ëã±Ë™û„ÅÆÂàÜÊûêÁµêÊûú„ÇíÊó•Êú¨Ë™û„Å´ÁøªË®≥
    """
    try:
        print("üåê Translating analysis result to Japanese...")
        
        translation_prompt = f"""
‰ª•‰∏ã„ÅÆËã±Ë™û„ÅÆSQLÂàÜÊûêÁµêÊûú„Çí„ÄÅÊäÄË°ìÁöÑ„Å™Ê≠£Á¢∫ÊÄß„Çí‰øù„Å°„Å™„Åå„ÇâËá™ÁÑ∂„Å™Êó•Êú¨Ë™û„Å´ÁøªË®≥„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
Â∞ÇÈñÄÁî®Ë™û„ÅØÈÅ©Âàá„Å™Êó•Êú¨Ë™û„Å´ÁøªË®≥„Åó„ÄÅÊï∞ÂÄ§„ÇÑ„É°„Éà„É™„ÇØ„ÇπÂêç„ÅØ„Åù„ÅÆ„Åæ„Åæ‰øùÊåÅ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„ÄêÁøªË®≥ÂØæË±°„Äë
{english_text}

„ÄêÁøªË®≥Ë¶Å‰ª∂„Äë
- ÊäÄË°ìÁöÑÊ≠£Á¢∫ÊÄß„ÇíÊúÄÂÑ™ÂÖà
- Ëá™ÁÑ∂„ÅßË™≠„Åø„ÇÑ„Åô„ÅÑÊó•Êú¨Ë™û
- SQLÁî®Ë™û„ÅØÈÅ©Âàá„Å™Êó•Êú¨Ë™ûË°®Áèæ„Çí‰ΩøÁî®
- Êï∞ÂÄ§„Éª„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊåáÊ®ô„ÅØ„Åù„ÅÆ„Åæ„Åæ‰øùÊåÅ
- Êé®Â•®‰∫ãÈ†Ö„ÅØÂÆüÁî®ÁöÑ„Å™Êó•Êú¨Ë™û„ÅßË°®Áèæ

Êó•Êú¨Ë™ûÁøªË®≥ÁµêÊûú„ÅÆ„Åø„ÇíÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö
"""
        
        provider = LLM_CONFIG.get("provider", "databricks")
        
        if provider == "databricks":
            japanese_result = _call_databricks_llm(translation_prompt)
        elif provider == "openai":
            japanese_result = _call_openai_llm(translation_prompt)
        elif provider == "azure_openai":
            japanese_result = _call_azure_openai_llm(translation_prompt)
        elif provider == "anthropic":
            japanese_result = _call_anthropic_llm(translation_prompt)
        else:
            print(f"‚ö†Ô∏è Unknown LLM provider: {provider}, skipping translation")
            return english_text
        
        if japanese_result and japanese_result.strip():
            print("‚úÖ Translation to Japanese completed")
            return japanese_result.strip()
        else:
            print("‚ö†Ô∏è Translation failed, using original English text")
            return english_text
            
    except Exception as e:
        print(f"‚ö†Ô∏è Translation error: {str(e)}, using original English text")
        return english_text

def generate_comprehensive_optimization_report(query_id: str, optimized_result: str, metrics: Dict[str, Any], analysis_result: str = "", performance_comparison: Dict[str, Any] = None, best_attempt_number: int = None, optimization_attempts: list = None) -> str:
    """
    ÂåÖÊã¨ÁöÑ„Å™ÊúÄÈÅ©Âåñ„É¨„Éù„Éº„Éà„ÇíÁîüÊàê
    EXPLAIN + EXPLAIN COSTÂÆüË°å„Éï„É©„Ç∞„ÅåY„ÅÆÂ†¥Âêà„ÅØ„ÄÅÁµ±Ë®àÊÉÖÂ†±„ÇÇÂê´„ÇÅ„Çã
    
    Args:
        query_id: „ÇØ„Ç®„É™ID
        optimized_result: ÊúÄÈÅ©ÂåñÁµêÊûú
        metrics: „É°„Éà„É™„ÇØ„ÇπÊÉÖÂ†±
        analysis_result: „Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûêÁµêÊûú
    
    Returns:
        str: Ë™≠„Åø„ÇÑ„Åô„ÅèÊßãÊàê„Åï„Çå„Åü„É¨„Éù„Éº„Éà
    """
    from datetime import datetime
    
    # EXPLAIN + EXPLAIN COSTÁµêÊûú„Éï„Ç°„Ç§„É´„ÅÆË™≠„ÅøËæº„ÅøÔºàEXPLAIN_ENABLED„ÅåY„ÅÆÂ†¥ÂêàÔºâ
    explain_section = ""
    explain_cost_section = ""
    explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
    
    # üìä ÊúÄÊñ∞„ÅÆSQL„Éï„Ç°„Ç§„É´Âêç„ÇíÊ§úÁ¥¢ÔºàÁúÅÁï•Ë°®Á§∫ÊôÇ„ÅÆÂèÇÁÖßÁî® - Â∏∏„Å´ÂÆüË°åÔºâ
    import glob
    import os
    
    optimized_sql_files = glob.glob("output_optimized_query_*.sql")
    latest_sql_filename = ""
    if optimized_sql_files:
        # ÊúÄÊñ∞„ÅÆ„Éï„Ç°„Ç§„É´„ÇíÂèñÂæóÔºà„Éï„Ç°„Ç§„É´Âêç„ÅÆ„Çø„Ç§„É†„Çπ„Çø„É≥„Éó„Åß„ÇΩ„Éº„ÉàÔºâ
        optimized_sql_files.sort(reverse=True)
        latest_sql_filename = optimized_sql_files[0]
    
    if explain_enabled.upper() == 'Y':
        print("üîç For comprehensive report: Searching EXPLAIN + EXPLAIN COST result files...")
        
        # 1. ÊúÄÊñ∞„ÅÆEXPLAINÁµêÊûú„Éï„Ç°„Ç§„É´„ÇíÊ§úÁ¥¢ÔºàÊñ∞„Åó„ÅÑ„Éï„Ç°„Ç§„É´Âêç„Éë„Çø„Éº„É≥ÂØæÂøúÔºâ
        explain_original_files = glob.glob("output_explain_original_*.txt")
        explain_optimized_files = glob.glob("output_explain_optimized_*.txt")
        
        # 2. ÊúÄÊñ∞„ÅÆEXPLAIN COSTÁµêÊûú„Éï„Ç°„Ç§„É´„ÇíÊ§úÁ¥¢
        # üöÄ „Ç™„É™„Ç∏„Éä„É´„Éï„Ç°„Ç§„É´„ÅØ„Ç≠„É£„ÉÉ„Ç∑„É•„Åã„ÇâÂèñÂæóÔºàÂèØËÉΩ„Å™Â†¥ÂêàÔºâ
        cached_cost_result = globals().get('cached_original_explain_cost_result')
        cost_original_files = []
        if cached_cost_result and 'explain_cost_file' in cached_cost_result:
            # „Ç≠„É£„ÉÉ„Ç∑„É•„Åã„Çâ„Éï„Ç°„Ç§„É´Âêç„ÇíÂæ©ÂÖÉ
            cost_original_files = [cached_cost_result['explain_cost_file']]
            print(f"üíæ Using cached original EXPLAIN COST file for comprehensive report")
        else:
            # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: ÂæìÊù•„ÅÆ„Éï„Ç°„Ç§„É´Ê§úÁ¥¢
            cost_original_files = glob.glob("output_explain_cost_original_*.txt")
        
        cost_optimized_files = glob.glob("output_explain_cost_optimized_*.txt")
        
        # üéØ „Éô„Çπ„ÉàË©¶Ë°åÁï™Âè∑„ÅåÊåáÂÆö„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅÂØæÂøú„Åô„Çã„Éï„Ç°„Ç§„É´„ÇíÂÑ™ÂÖàÈÅ∏Êäû
        if best_attempt_number is not None:
            print(f"üéØ Searching for files from best attempt {best_attempt_number}...")
            
            # „Éô„Çπ„ÉàË©¶Ë°å„ÅÆ„Éï„Ç°„Ç§„É´„ÇíÊ§úÁ¥¢
            best_explain_files = [f for f in explain_optimized_files if f"attempt_{best_attempt_number}" in f]
            best_cost_files = [f for f in cost_optimized_files if f"attempt_{best_attempt_number}" in f]
            
            if best_explain_files:
                print(f"‚úÖ Found EXPLAIN file from best attempt {best_attempt_number}: {best_explain_files[0]}")
                explain_files = best_explain_files
            else:
                print(f"‚ö†Ô∏è EXPLAIN file from best attempt {best_attempt_number} not found, using post-optimization")
                explain_files = explain_optimized_files if explain_optimized_files else explain_original_files
            
            if best_cost_files:
                print(f"‚úÖ Found EXPLAIN COST file from best attempt {best_attempt_number}: {best_cost_files[0]}")
                cost_files = best_cost_files
            else:
                print(f"‚ö†Ô∏è EXPLAIN COST file from best attempt {best_attempt_number} not found, using post-optimization")
                cost_files = cost_optimized_files if cost_optimized_files else cost_original_files
        else:
            # ÂæìÊù•„É≠„Ç∏„ÉÉ„ÇØ: ÊúÄÈÅ©ÂåñÂæå„ÇíÂÑ™ÂÖà„ÄÅ„Å™„Åë„Çå„Å∞„Ç™„É™„Ç∏„Éä„É´
            explain_files = explain_optimized_files if explain_optimized_files else explain_original_files
            cost_files = cost_optimized_files if cost_optimized_files else cost_original_files
        
        # üìä EXPLAIN + EXPLAIN COSTÁµêÊûú„ÇíË¶ÅÁ¥Ñ„Åó„Å¶„Åã„Çâ„É¨„Éù„Éº„Éà„Å´ÁµÑ„ÅøËæº„Åø
        explain_content = ""
        explain_cost_content = ""
        query_type = "optimized" if (explain_optimized_files or cost_optimized_files) else "original"
        # EXPLAIN „Éï„Ç°„Ç§„É´Ë™≠„ÅøËæº„Åø
        if explain_files:
            latest_explain_file = max(explain_files, key=os.path.getctime)
            try:
                with open(latest_explain_file, 'r', encoding='utf-8') as f:
                    explain_content = f.read()
                print(f"‚úÖ Loaded EXPLAIN results: {latest_explain_file}")
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load EXPLAIN results: {str(e)}")
        else:
            # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: Âè§„ÅÑ„Éï„Ç°„Ç§„É´Âêç„Éë„Çø„Éº„É≥„ÇÇ„ÉÅ„Çß„ÉÉ„ÇØ
            old_explain_files = glob.glob("output_explain_plan_*.txt")
            if old_explain_files:
                latest_explain_file = max(old_explain_files, key=os.path.getctime)
                try:
                    with open(latest_explain_file, 'r', encoding='utf-8') as f:
                        explain_content = f.read()
                        print(f"‚úÖ Loaded legacy format EXPLAIN results: {latest_explain_file}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Failed to load legacy format EXPLAIN results: {str(e)}")
        
        # EXPLAIN COST „Éï„Ç°„Ç§„É´Ë™≠„ÅøËæº„Åø
        if cost_files:
            latest_cost_file = max(cost_files, key=os.path.getctime)
            try:
                with open(latest_cost_file, 'r', encoding='utf-8') as f:
                    explain_cost_content = f.read()
                    print(f"üí∞ Loaded EXPLAIN COST results for comprehensive report: {latest_cost_file}")
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load EXPLAIN COST results: {str(e)}")
        
        # üìä Ë¶ÅÁ¥ÑÊ©üËÉΩ„Çí‰Ωø„Å£„Å¶„Éà„Éº„ÇØ„É≥Âà∂Èôê„Å´ÂØæÂøú
        summary_results = summarize_explain_results_with_llm(explain_content, explain_cost_content, query_type)
        
        # Ë¶ÅÁ¥ÑÁµêÊûú„Çí‰Ωø„Å£„Å¶„É¨„Éù„Éº„Éà„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊßãÁØâ
        if summary_results['summarized']:
            print(f"üìä Generating summary report sections (total size reduction)")
        
        if OUTPUT_LANGUAGE == 'ja':
            explain_section = f"""

## üîç 6. EXPLAIN + EXPLAIN COSTÁµ±ÂêàÂàÜÊûêÁµêÊûú

### üìä Ë¶ÅÁ¥Ñ„Åï„Çå„ÅüÂÆüË°å„Éó„É©„É≥„ÉªÁµ±Ë®àÊÉÖÂ†±

**ÂàÜÊûêÂØæË±°**: {query_type}„ÇØ„Ç®„É™
**Ë¶ÅÁ¥ÑÂÆüË°å**: {'„ÅØ„ÅÑÔºà„Éà„Éº„ÇØ„É≥Âà∂ÈôêÂØæÂøúÔºâ' if summary_results['summarized'] else '„ÅÑ„ÅÑ„ÅàÔºà„Çµ„Ç§„Ç∫Â∞èÔºâ'}

{summary_results['explain_summary']}

### üí∞ Áµ±Ë®à„Éô„Éº„ÇπÊúÄÈÅ©Âåñ„ÅÆÂäπÊûú

Áµ±Ë®àÊÉÖÂ†±„ÇíÊ¥ªÁî®„Åô„Çã„Åì„Å®„Åß‰ª•‰∏ã„ÅÆÊîπÂñÑÂäπÊûú„ÅåÊúüÂæÖ„Åß„Åç„Åæ„ÅôÔºö

| È†ÖÁõÆ | ÂæìÊù•ÔºàÊé®Ê∏¨„Éô„Éº„ÇπÔºâ | Áµ±Ë®à„Éô„Éº„Çπ | ÊîπÂñÑÂäπÊûú |
|------|-------------------|-----------|----------|
| BROADCASTÂà§ÂÆöÁ≤æÂ∫¶ | Á¥Ñ60% | Á¥Ñ95% | **+35%** |
| „Çπ„Éî„É´‰∫àÊ∏¨Á≤æÂ∫¶ | Á¥Ñ40% | Á¥Ñ85% | **+45%** |
| „Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥ÊúÄÈÅ©Âåñ | Á¥Ñ50% | Á¥Ñ90% | **+40%** |
| ÂÖ®‰ΩìÊúÄÈÅ©ÂåñÂäπÊûú | Âπ≥Âùá30%ÊîπÂñÑ | Âπ≥Âùá60%ÊîπÂñÑ | **+30%** |

### üéØ Áµ±Ë®àÊÉÖÂ†±Ê¶ÇË¶Å

Áµ±Ë®àÊÉÖÂ†±„Å´„Çà„ÇãÊúÄÈÅ©Âåñ„ÅåÂÆüË°å„Åï„Çå„Åæ„Åó„ÅüÔºàË©≥Á¥∞„ÅØDEBUG_ENABLED='Y'„ÅßÁ¢∫Ë™çÂèØËÉΩÔºâ„ÄÇ

"""
            explain_cost_section = ""  # Áµ±Âêà„Çª„ÇØ„Ç∑„Éß„É≥„Å™„ÅÆ„ÅßÂÄãÂà•„Çª„ÇØ„Ç∑„Éß„É≥„ÅØ‰∏çË¶Å
        else:
            explain_section = f"""

## üîç 6. EXPLAIN + EXPLAIN COST Integrated Analysis Results

### üìä Summarized Execution Plan & Statistical Information

**Analysis Target**: {query_type} query
**Summarization**: {'Yes (Token Limit Adaptation)' if summary_results['summarized'] else 'No (Small Size)'}

{summary_results['explain_summary']}

### üí∞ Effects of Statistics-Based Optimization

The following improvement effects can be expected by leveraging statistical information:

| Item | Traditional (Guess-based) | Statistics-based | Improvement |
|------|---------------------------|------------------|-------------|
| BROADCAST Judgment Accuracy | ~60% | ~95% | **+35%** |
| Spill Prediction Accuracy | ~40% | ~85% | **+45%** |
| Partition Optimization | ~50% | ~90% | **+40%** |
| Overall Optimization Effect | Average 30% improvement | Average 60% improvement | **+30%** |

### üéØ Statistical Information Overview

Statistical optimization has been executed (details available with DEBUG_ENABLED='Y').

"""
            explain_cost_section = ""  # Integrated section, so no separate section needed
    else:
        if OUTPUT_LANGUAGE == 'ja':
            explain_section = "\n\n## üîç 6. EXPLAIN + EXPLAIN COSTÁµ±ÂêàÂàÜÊûêÁµêÊûú\n\n‚ö†Ô∏è EXPLAIN_ENABLED = 'N' „ÅÆ„Åü„ÇÅ„ÄÅEXPLAINÂàÜÊûê„ÅØ„Çπ„Ç≠„ÉÉ„Éó„Åï„Çå„Åæ„Åó„Åü„ÄÇ\n"
            explain_cost_section = ""
        else:
            explain_section = "\n\n## üîç 6. EXPLAIN + EXPLAIN COST Integrated Analysis Results\n\n‚ö†Ô∏è EXPLAIN analysis was skipped because EXPLAIN_ENABLED = 'N'.\n"
            explain_cost_section = ""
    
    # Âü∫Êú¨ÊÉÖÂ†±„ÅÆÂèñÂæó
    # Âü∫Êú¨ÊÉÖÂ†±„ÅÆÂèñÂæó
    overall_metrics = metrics.get('overall_metrics', {})
    bottleneck_indicators = metrics.get('bottleneck_indicators', {})
    liquid_analysis = metrics.get('liquid_clustering_analysis', {})
    
    # thinking_enabledÂØæÂøú: analysis_result„Åå„É™„Çπ„Éà„ÅÆÂ†¥Âêà„ÅÆÂá¶ÁêÜ
    if isinstance(analysis_result, list):
        analysis_result_str = format_thinking_response(analysis_result)
    else:
        analysis_result_str = str(analysis_result)
    
    # signatureÊÉÖÂ†±„ÅÆÈô§Âéª
    import re
    signature_pattern = r"'signature':\s*'[A-Za-z0-9+/=]{100,}'"
    analysis_result_str = re.sub(signature_pattern, "'signature': '[REMOVED]'", analysis_result_str)
    
    # Êó•Êú¨Ë™ûÂá∫Âäõ„ÅÆÂ†¥Âêà„ÄÅanalysis_result_str„ÇíLLM„ÅßÊó•Êú¨Ë™û„Å´ÁøªË®≥
    if OUTPUT_LANGUAGE == 'ja' and analysis_result_str and analysis_result_str.strip():
        analysis_result_str = translate_analysis_to_japanese(analysis_result_str)
    
    # „É¨„Éù„Éº„Éà„ÅÆÊßãÊàê
    if OUTPUT_LANGUAGE == 'ja':
        report = f"""# üìä SQLÊúÄÈÅ©Âåñ„É¨„Éù„Éº„Éà

**„ÇØ„Ç®„É™ID**: {query_id}  
**„É¨„Éù„Éº„ÉàÁîüÊàêÊó•ÊôÇ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## üéØ 1. „Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûêÁµêÊûú

### ü§ñ AI„Å´„Çà„ÇãË©≥Á¥∞ÂàÜÊûê

{analysis_result_str}

### üìä ‰∏ªË¶Å„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊåáÊ®ô

| ÊåáÊ®ô | ÂÄ§ | Ë©ï‰æ° |
|------|-----|------|
| ÂÆüË°åÊôÇÈñì | {overall_metrics.get('total_time_ms', 0):,} ms | {'‚úÖ ËâØÂ•Ω' if overall_metrics.get('total_time_ms', 0) < 60000 else '‚ö†Ô∏è ÊîπÂñÑÂøÖË¶Å'} |
| PhotonÊúâÂäπ | {'„ÅØ„ÅÑ' if overall_metrics.get('photon_enabled', False) else '„ÅÑ„ÅÑ„Åà'} | {'‚úÖ ËâØÂ•Ω' if overall_metrics.get('photon_enabled', False) else '‚ùå Êú™ÊúâÂäπ'} |
| „Ç≠„É£„ÉÉ„Ç∑„É•ÂäπÁéá | {bottleneck_indicators.get('cache_hit_ratio', 0) * 100:.1f}% | {'‚úÖ ËâØÂ•Ω' if bottleneck_indicators.get('cache_hit_ratio', 0) > 0.8 else '‚ö†Ô∏è ÊîπÂñÑÂøÖË¶Å'} |
| „Éï„Ç£„É´„ÇøÁéá | {bottleneck_indicators.get('data_selectivity', 0) * 100:.2f}% | {'‚úÖ ËâØÂ•Ω' if bottleneck_indicators.get('data_selectivity', 0) > 0.5 else '‚ö†Ô∏è „Éï„Ç£„É´„ÇøÊù°‰ª∂„ÇíÁ¢∫Ë™ç'} |
| „Ç∑„É£„ÉÉ„Éï„É´Êìç‰Ωú | {bottleneck_indicators.get('shuffle_operations_count', 0)}Âõû | {'‚úÖ ËâØÂ•Ω' if bottleneck_indicators.get('shuffle_operations_count', 0) < 5 else '‚ö†Ô∏è Â§öÊï∞'} |
| „Çπ„Éî„É´Áô∫Áîü | {'„ÅØ„ÅÑ' if bottleneck_indicators.get('has_spill', False) else '„ÅÑ„ÅÑ„Åà'} | {'‚ùå ÂïèÈ°å„ÅÇ„Çä' if bottleneck_indicators.get('has_spill', False) else '‚úÖ ËâØÂ•Ω'} |
| „Çπ„Ç≠„É•„ÉºÊ§úÂá∫ | {'AQE„ÅßÊ§úÂá∫„ÉªÂØæÂøúÊ∏à' if bottleneck_indicators.get('has_skew', False) else 'ÊΩúÂú®ÁöÑ„Å™„Çπ„Ç≠„É•„Éº„ÅÆÂèØËÉΩÊÄß„ÅÇ„Çä' if bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False) else 'Êú™Ê§úÂá∫'} | {'üîß AQEÂØæÂøúÊ∏à' if bottleneck_indicators.get('has_skew', False) else '‚ö†Ô∏è ÊîπÂñÑÂøÖË¶Å' if bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False) else '‚úÖ ËâØÂ•Ω'} |

### üö® ‰∏ªË¶Å„Éú„Éà„É´„Éç„ÉÉ„ÇØ

"""
        
        # ‰∏ªË¶Å„Éú„Éà„É´„Éç„ÉÉ„ÇØ„ÅÆË©≥Á¥∞
        bottlenecks = []
        
        if bottleneck_indicators.get('has_spill', False):
            spill_gb = bottleneck_indicators.get('spill_bytes', 0) / 1024 / 1024 / 1024
            bottlenecks.append(f"**„É°„É¢„É™„Çπ„Éî„É´**: {spill_gb:.2f}GB - „É°„É¢„É™‰∏çË∂≥„Å´„Çà„ÇãÊÄßËÉΩ‰Ωé‰∏ã")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            bottlenecks.append("**„Ç∑„É£„ÉÉ„Éï„É´„Éú„Éà„É´„Éç„ÉÉ„ÇØ**: JOIN/GROUP BYÂá¶ÁêÜ„Åß„ÅÆÂ§ßÈáè„Éá„Éº„ÇøËª¢ÈÄÅ")
        
        if bottleneck_indicators.get('has_skew', False):
            bottlenecks.append("**„Éá„Éº„Çø„Çπ„Ç≠„É•„Éº**: AQE„ÅßÊ§úÂá∫„ÉªÂØæÂøúÊ∏à - Spark„ÅåËá™ÂãïÁöÑ„Å´ÊúÄÈÅ©ÂåñÂÆüË°å")
        elif bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False):
            bottlenecks.append("**„Éá„Éº„Çø„Çπ„Ç≠„É•„Éº**: ÊΩúÂú®ÁöÑ„Å™„Çπ„Ç≠„É•„Éº„ÅÆÂèØËÉΩÊÄß„ÅÇ„Çä - „Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„Çµ„Ç§„Ç∫„Åå512MB‰ª•‰∏ä")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            bottlenecks.append("**„Ç≠„É£„ÉÉ„Ç∑„É•ÂäπÁéá‰Ωé‰∏ã**: „Éá„Éº„ÇøÂÜçÂà©Áî®ÂäπÁéá„Åå‰Ωé„ÅÑ")
        
        if not overall_metrics.get('photon_enabled', False):
            bottlenecks.append("**PhotonÊú™ÊúâÂäπ**: È´òÈÄüÂá¶ÁêÜ„Ç®„É≥„Ç∏„É≥„ÅåÂà©Áî®„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.2:
            bottlenecks.append("**„Éï„Ç£„É´„ÇøÂäπÁéá‰Ωé‰∏ã**: ÂøÖË¶Å‰ª•‰∏ä„ÅÆ„Éá„Éº„Çø„ÇíË™≠„ÅøËæº„Çì„Åß„ÅÑ„Çã")
        
        if bottlenecks:
            for i, bottleneck in enumerate(bottlenecks, 1):
                report += f"{i}. {bottleneck}\n"
        else:
            report += "‰∏ªË¶Å„Å™„Éú„Éà„É´„Éç„ÉÉ„ÇØ„ÅØË®≠ÂÆö„Å™„Åó„ÄÇ\n"
        
        report += "\n"
        
        # Add Liquid Clustering analysis results
        if liquid_analysis:
            performance_context = liquid_analysis.get('performance_context', {})
            llm_analysis = liquid_analysis.get('llm_analysis', '')
            
            report += f"""

## üóÇÔ∏è 3. Liquid ClusteringÂàÜÊûêÁµêÊûú

### üìä „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊ¶ÇË¶Å

| È†ÖÁõÆ | ÂÄ§ |
|------|-----|
| ÂÆüË°åÊôÇÈñì | {performance_context.get('total_time_sec', 0):.1f}Áßí |
| „Éá„Éº„ÇøË™≠„ÅøËæº„Åø | {performance_context.get('read_gb', 0):.2f}GB |
| Âá∫ÂäõË°åÊï∞ | {performance_context.get('rows_produced', 0):,}Ë°å |
| Ë™≠„ÅøËæº„ÅøË°åÊï∞ | {performance_context.get('rows_read', 0):,}Ë°å |
| „Éï„Ç£„É´„ÇøÁéá | {performance_context.get('data_selectivity', 0):.4f} |

### ü§ñ AIÂàÜÊûêÁµêÊûú

{llm_analysis}

"""
        
        # ÊúÄ„ÇÇÊôÇÈñì„Åå„Åã„Åã„Å£„Å¶„ÅÑ„ÇãÂá¶ÁêÜTOP10„ÇíÁµ±Âêà
        report += f"""
## üêå 2. ÊúÄ„ÇÇÊôÇÈñì„Åå„Åã„Åã„Å£„Å¶„ÅÑ„ÇãÂá¶ÁêÜTOP10

### üìä Ë©≥Á¥∞„Å™„Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûê

‰ª•‰∏ã„ÅÆ„Éà„Éî„ÉÉ„ÇØ„Å´Âü∫„Å•„ÅÑ„Å¶Âá¶ÁêÜ„ÇíÂàÜÊûê„Åó„Åæ„ÅôÔºö

#### üîç ÂàÜÊûêÂØæË±°„Éà„Éî„ÉÉ„ÇØ
- **‚è±Ô∏è ÂÆüË°åÊôÇÈñì**: ÂÖ®‰Ωì„Å´Âç†„ÇÅ„ÇãÂá¶ÁêÜÊôÇÈñì„ÅÆÂâ≤Âêà
- **üíæ „É°„É¢„É™‰ΩøÁî®Èáè**: „Éî„Éº„ÇØ„É°„É¢„É™‰ΩøÁî®Èáè„Å®„É°„É¢„É™„Éó„É¨„ÉÉ„Ç∑„É£„Éº
- **üîß ‰∏¶ÂàóÂ∫¶**: „Çø„Çπ„ÇØÊï∞„Å®‰∏¶ÂàóÂÆüË°åÂäπÁéá
- **üíø „Çπ„Éî„É´Ê§úÂá∫**: „É°„É¢„É™‰∏çË∂≥„Å´„Çà„Çã„Éá„Ç£„Çπ„ÇØ„Çπ„Éî„É´
- **‚öñÔ∏è „Çπ„Ç≠„É•„ÉºÊ§úÂá∫**: AQE„Éô„Éº„Çπ„ÅÆ„Éá„Éº„ÇøÂàÜÊï£‰∏çÂùáÁ≠âÊ§úÂá∫
- **üîÑ ShuffleÂ±ûÊÄß**: „Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥ÂÜçÂàÜÊï£„ÅÆÊúÄÈÅ©Âåñ„Éù„Ç§„É≥„Éà
- **üöÄ Âá¶ÁêÜÂäπÁéá**: Ë°å/Áßí„Åß„ÅÆÂá¶ÁêÜÂäπÁéáÊåáÊ®ô

"""
        
        # TOP10„É¨„Éù„Éº„Éà„ÅÆÁîüÊàê„Å®Áµ±Âêà
        try:
            top10_report = generate_top10_time_consuming_processes_report(metrics, 10)
            # „É¨„Éù„Éº„Éà„Åã„Çâ„Éò„ÉÉ„ÉÄ„Éº„ÇíÈô§Âéª„Åó„Å¶Áµ±Âêà
            top10_lines = top10_report.split('\n')
            # "## üêå ÊúÄ„ÇÇÊôÇÈñì„Åå„Åã„Åã„Å£„Å¶„ÅÑ„ÇãÂá¶ÁêÜTOP10"„ÅÆË°å„Çí„Çπ„Ç≠„ÉÉ„Éó
            filtered_lines = []
            skip_header = True
            for line in top10_lines:
                if skip_header and line.startswith("## üêå"):
                    skip_header = False
                    continue
                if not skip_header:
                    filtered_lines.append(line)
            
            report += '\n'.join(filtered_lines)
            
        except Exception as e:
            report += f"‚ö†Ô∏è TOP10Âá¶ÁêÜÊôÇÈñìÂàÜÊûê„ÅÆÁîüÊàê„Åß„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {str(e)}\n"
        
        # SQLÊúÄÈÅ©ÂåñÂàÜÊûêÁµêÊûú„ÅÆËøΩÂä†
        # üöÄ SQL„Éï„Ç°„Ç§„É´ÂÜÖÂÆπ„ÅÆÂ†¥Âêà„ÅØÈÅ©Âàá„Å´„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÔºàÁúÅÁï•Ê©üËÉΩ‰ªò„ÅçÔºâ
        formatted_sql_content = format_sql_content_for_report(optimized_result, latest_sql_filename)
        
        # üéØ ÊúÄÈÅ©ÂåñÊñπÈáùË¶ÅÁ¥Ñ„ÇíÁîüÊàê
        optimization_strategy = generate_optimization_strategy_summary(optimized_result, metrics, analysis_result_str)
        
        # üìä ÊúÄÈÅ©Âåñ„Éó„É≠„Çª„ÇπË©≥Á¥∞„ÅÆÁîüÊàê
        optimization_process_details = ""
        if optimization_attempts is not None and best_attempt_number is not None:
            total_attempts = len(optimization_attempts)
            cost_improvement = "N/A"
            memory_improvement = "N/A"
            
            if performance_comparison:
                cost_ratio = performance_comparison.get('total_cost_ratio', 1.0)
                memory_ratio = performance_comparison.get('memory_usage_ratio', 1.0)
                cost_improvement = f"{(1-cost_ratio)*100:.1f}"
                memory_improvement = f"{(1-memory_ratio)*100:.1f}"
            
            # ÊúÄÁµÇÈÅ∏Êäû„ÅÆË°®Á§∫„ÇíÂàÜ„Åã„Çä„ÇÑ„Åô„Åè„Åô„Çã
            if best_attempt_number == 0:
                final_selection = "ÂÖÉ„ÅÆ„ÇØ„Ç®„É™ÔºàÊúÄÈÅ©Âåñ„Å´„Çà„ÇäÊîπÂñÑ„Åï„Çå„Å™„Åã„Å£„Åü„Åü„ÇÅÔºâ"
                selection_reason = "ÊúÄÈÅ©ÂåñË©¶Ë°å„ÅßÊúâÂäπ„Å™ÊîπÂñÑ„ÅåÂæó„Çâ„Çå„Å™„Åã„Å£„Åü„Åü„ÇÅ„ÄÅÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Çí‰ΩøÁî®"
                # üìÑ ÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Éï„Ç°„Ç§„É´ÂêçÊÉÖÂ†±„ÇíËøΩÂä†
                if latest_sql_filename:
                    selection_reason += f"\n- üìÑ ÂèÇËÄÉ„Éï„Ç°„Ç§„É´: {latest_sql_filename}ÔºàÊúÄÈÅ©ÂåñË©¶Ë°åÁµêÊûúÔºâ"
                else:
                    selection_reason += "\n- üìÑ ÂÖÉ„ÅÆ„ÇØ„Ç®„É™: „Éó„É≠„Éï„Ç°„Ç§„É©„Éº„Éá„Éº„Çø„Åã„ÇâÊäΩÂá∫"
            else:
                final_selection = f"Ë©¶Ë°å{best_attempt_number}Áï™"
                selection_reason = "„Ç≥„Çπ„ÉàÂäπÁéá„ÅåÊúÄ„ÇÇËâØ„ÅÑË©¶Ë°å„ÇíÈÅ∏Êäû"
            
            optimization_process_details = f"""### üéØ ÊúÄÈÅ©Âåñ„Éó„É≠„Çª„ÇπË©≥Á¥∞
ÊúÄÈÅ©Âåñ„Éó„É≠„Çª„Çπ„ÅßÂÆüË°å„Åï„Çå„ÅüË©¶Ë°å„Å®„Åù„ÅÆÈÅ∏ÊäûÁêÜÁî±„Çí‰ª•‰∏ã„Å´Á§∫„Åó„Åæ„ÅôÔºö

**üìä ÊúÄÈÅ©ÂåñË©¶Ë°åÂ±•Ê≠¥:**
- Ë©¶Ë°åÂõûÊï∞: {total_attempts}ÂõûÂÆüË°å
- ÊúÄÁµÇÈÅ∏Êäû: {final_selection}
- ÈÅ∏ÊäûÁêÜÁî±: {selection_reason}

**üèÜ ÈÅ∏Êäû„Åï„Çå„ÅüÊúÄÈÅ©Âåñ„ÅÆÂäπÊûú:**
- „Ç≥„Çπ„ÉàÂâäÊ∏õÁéá: {cost_improvement}% (EXPLAIN COSTÊØîËºÉ)
- „É°„É¢„É™ÂäπÁéáÊîπÂñÑ: {memory_improvement}% (Áµ±Ë®àÊØîËºÉ)

"""
        
        report += f"""

## üöÄ 4. SQLÊúÄÈÅ©ÂåñÂàÜÊûêÁµêÊûú

{optimization_process_details}### üéØ ÊúÄÈÅ©ÂåñÂÆüË°åÊñπÈáù

{optimization_strategy}

### üí° ÊúÄÈÅ©ÂåñÊèêÊ°à

{formatted_sql_content}

### üîç 5. „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊ§úË®ºÁµêÊûú

{generate_performance_comparison_section(performance_comparison)}

### üìà 6. ÊúüÂæÖ„Åï„Çå„Çã„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊîπÂñÑÂäπÊûú

#### üéØ ‰∫àÊÉ≥„Åï„Çå„ÇãÊîπÂñÑÁÇπ

"""
        
        # ÊúüÂæÖ„Åï„Çå„ÇãÊîπÂñÑÂäπÊûú„ÇíË®àÁÆó
        expected_improvements = []
        
        if bottleneck_indicators.get('has_spill', False):
            expected_improvements.append("**„É°„É¢„É™„Çπ„Éî„É´Ëß£Ê∂à**: ÊúÄÂ§ß50-80%„ÅÆÊÄßËÉΩÊîπÂñÑ„ÅåÊúüÂæÖ„Åï„Çå„Åæ„Åô")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            expected_improvements.append("**„Ç∑„É£„ÉÉ„Éï„É´ÊúÄÈÅ©Âåñ**: 20-60%„ÅÆÂÆüË°åÊôÇÈñìÁü≠Á∏Æ„ÅåÊúüÂæÖ„Åï„Çå„Åæ„Åô")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            expected_improvements.append("**„Ç≠„É£„ÉÉ„Ç∑„É•ÂäπÁéáÂêë‰∏ä**: 30-70%„ÅÆË™≠„ÅøËæº„ÅøÊôÇÈñìÁü≠Á∏Æ„ÅåÊúüÂæÖ„Åï„Çå„Åæ„Åô")
        
        if not overall_metrics.get('photon_enabled', False):
            expected_improvements.append("**PhotonÊúâÂäπÂåñ**: 2-10ÂÄç„ÅÆÂá¶ÁêÜÈÄüÂ∫¶Âêë‰∏ä„ÅåÊúüÂæÖ„Åï„Çå„Åæ„Åô")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.2:
            expected_improvements.append("**„Éï„Ç£„É´„ÇøÂäπÁéáÊîπÂñÑ**: 40-90%„ÅÆ„Éá„Éº„ÇøË™≠„ÅøËæº„ÅøÈáèÂâäÊ∏õ„ÅåÊúüÂæÖ„Åï„Çå„Åæ„Åô")
        
        if expected_improvements:
            for i, improvement in enumerate(expected_improvements, 1):
                report += f"{i}. {improvement}\n"
            
            # Á∑èÂêàÁöÑ„Å™ÊîπÂñÑÂäπÊûú
            total_time_ms = overall_metrics.get('total_time_ms', 0)
            if total_time_ms > 0:
                improvement_ratio = min(0.8, len(expected_improvements) * 0.15)  # ÊúÄÂ§ß80%ÊîπÂñÑ
                expected_time = total_time_ms * (1 - improvement_ratio)
                report += f"\n**Á∑èÂêàÊîπÂñÑÂäπÊûú**: ÂÆüË°åÊôÇÈñì {total_time_ms:,}ms ‚Üí {expected_time:,.0f}msÔºàÁ¥Ñ{improvement_ratio*100:.0f}%ÊîπÂñÑÔºâ\n"
        else:
            report += "ÁèæÂú®„ÅÆ„ÇØ„Ç®„É™„ÅØÊó¢„Å´ÊúÄÈÅ©Âåñ„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇÂ§ßÂπÖ„Å™ÊîπÂñÑ„ÅØÊúüÂæÖ„Åï„Çå„Åæ„Åõ„Çì„ÄÇ\n"
        
        report += f"""

#### üîß ÂÆüË£ÖÂÑ™ÂÖàÂ∫¶

1. **È´òÂÑ™ÂÖàÂ∫¶**: PhotonÊúâÂäπÂåñ„ÄÅ„É°„É¢„É™„Çπ„Éî„É´Ëß£Ê∂à
2. **‰∏≠ÂÑ™ÂÖàÂ∫¶**: Liquid Clustering„ÄÅ„Éá„Éº„Çø„É¨„Ç§„Ç¢„Ç¶„ÉàÊúÄÈÅ©Âåñ
3. **‰ΩéÂÑ™ÂÖàÂ∫¶**: Áµ±Ë®àÊÉÖÂ†±Êõ¥Êñ∞„ÄÅ„Ç≠„É£„ÉÉ„Ç∑„É•Êà¶Áï•

{explain_section}

{explain_cost_section}

---

*„É¨„Éù„Éº„ÉàÁîüÊàêÊôÇÂàª: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
    else:
        # Ëã±Ë™ûÁâàÔºàÂêåÊßò„ÅÆÊßãÊàêÔºâ
        report = f"""# üìä SQL Optimization Report

**Query ID**: {query_id}  
**Report Generation Time**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## üéØ 1. Bottleneck Analysis Results

### ü§ñ AI-Powered Analysis

{analysis_result_str}

### üìä Key Performance Indicators

| Metric | Value | Status |
|--------|-------|--------|
| Execution Time | {overall_metrics.get('total_time_ms', 0):,} ms | {'‚úÖ Good' if overall_metrics.get('total_time_ms', 0) < 60000 else '‚ö†Ô∏è Needs Improvement'} |
| Photon Enabled | {'Yes' if overall_metrics.get('photon_enabled', False) else 'No'} | {'‚úÖ Good' if overall_metrics.get('photon_enabled', False) else '‚ùå Not Enabled'} |
| Cache Efficiency | {bottleneck_indicators.get('cache_hit_ratio', 0) * 100:.1f}% | {'‚úÖ Good' if bottleneck_indicators.get('cache_hit_ratio', 0) > 0.8 else '‚ö†Ô∏è Needs Improvement'} |
| Filter Rate | {bottleneck_indicators.get('data_selectivity', 0) * 100:.2f}% | {'‚úÖ Good' if bottleneck_indicators.get('data_selectivity', 0) > 0.5 else '‚ö†Ô∏è Check Filter Conditions'} |
| Shuffle Operations | {bottleneck_indicators.get('shuffle_operations_count', 0)} times | {'‚úÖ Good' if bottleneck_indicators.get('shuffle_operations_count', 0) < 5 else '‚ö†Ô∏è High'} |
| Spill Occurrence | {'Yes' if bottleneck_indicators.get('has_spill', False) else 'No'} | {'‚ùå Issues' if bottleneck_indicators.get('has_spill', False) else '‚úÖ Good'} |
| Skew Detection | {'AQE Detected & Handled' if bottleneck_indicators.get('has_skew', False) else 'Not Detected'} | {'üîß AQE Handled' if bottleneck_indicators.get('has_skew', False) else '‚úÖ Good'} |

### üö® Key Bottlenecks

"""
        
        # ‰∏ªË¶Å„Éú„Éà„É´„Éç„ÉÉ„ÇØ„ÅÆË©≥Á¥∞ÔºàËã±Ë™ûÁâàÔºâ
        bottlenecks = []
        
        if bottleneck_indicators.get('has_spill', False):
            spill_gb = bottleneck_indicators.get('spill_bytes', 0) / 1024 / 1024 / 1024
            bottlenecks.append(f"**Memory Spill**: {spill_gb:.2f}GB - Performance degradation due to memory shortage")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            bottlenecks.append("**Shuffle Bottleneck**: Large data transfer in JOIN/GROUP BY operations")
        
        if bottleneck_indicators.get('has_skew', False):
            bottlenecks.append("**Data Skew**: AQE Detected & Handled - Spark automatically optimized execution")
        elif bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False):
            bottlenecks.append("**Data Skew**: Potential skew possibility - Partition size ‚â• 512MB")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            bottlenecks.append("**Cache Inefficiency**: Low data reuse efficiency")
        
        if not overall_metrics.get('photon_enabled', False):
            bottlenecks.append("**Photon Not Enabled**: High-speed processing engine not utilized")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.2:
            bottlenecks.append("**Poor Filter Efficiency**: Reading more data than necessary")
        
        if bottlenecks:
            for i, bottleneck in enumerate(bottlenecks, 1):
                report += f"{i}. {bottleneck}\n"
        else:
            report += "No major bottlenecks detected.\n"
        
        report += "\n"
        
        # ÊúÄ„ÇÇÊôÇÈñì„Åå„Åã„Åã„Å£„Å¶„ÅÑ„ÇãÂá¶ÁêÜTOP10„ÇíÁµ±ÂêàÔºàËã±Ë™ûÁâàÔºâ
        report += f"""
## üêå 2. Top 10 Most Time-Consuming Processes

### üìä Detailed Bottleneck Analysis

The following topics are analyzed for process evaluation:

#### üîç Analysis Topics
- **‚è±Ô∏è Execution Time**: Percentage of total processing time
- **üíæ Memory Usage**: Peak memory usage and memory pressure
- **üîß Parallelism**: Number of tasks and parallel execution efficiency
- **üíø Spill Detection**: Disk spill due to memory shortage
- **‚öñÔ∏è Skew Detection**: AQE-based data distribution imbalance detection
- **üîÑ Shuffle Attributes**: Optimization points for partition redistribution
- **üöÄ Processing Efficiency**: Processing efficiency metrics in rows/second

"""
        
        # TOP10„É¨„Éù„Éº„Éà„ÅÆÁîüÊàê„Å®Áµ±ÂêàÔºàËã±Ë™ûÁâàÔºâ
        try:
            top10_report = generate_top10_time_consuming_processes_report(metrics, 10)
            # „É¨„Éù„Éº„Éà„Åã„Çâ„Éò„ÉÉ„ÉÄ„Éº„ÇíÈô§Âéª„Åó„Å¶Áµ±Âêà
            top10_lines = top10_report.split('\n')
            # "## üêå ÊúÄ„ÇÇÊôÇÈñì„Åå„Åã„Åã„Å£„Å¶„ÅÑ„ÇãÂá¶ÁêÜTOP10"„ÅÆË°å„Çí„Çπ„Ç≠„ÉÉ„Éó
            filtered_lines = []
            skip_header = True
            for line in top10_lines:
                if skip_header and line.startswith("## üêå"):
                    skip_header = False
                    continue
                if not skip_header:
                    filtered_lines.append(line)
            
            report += '\n'.join(filtered_lines)
            
        except Exception as e:
            report += f"‚ö†Ô∏è Error generating TOP10 analysis: {str(e)}\n"
        
        # Add Liquid Clustering analysis results (English version)
        if liquid_analysis:
            performance_context = liquid_analysis.get('performance_context', {})
            llm_analysis = liquid_analysis.get('llm_analysis', '')
            
            report += f"""

## üóÇÔ∏è 3. Liquid Clustering Analysis Results

### üìä Performance Overview

| Item | Value |
|------|-------|
| Execution Time | {performance_context.get('total_time_sec', 0):.1f}s |
| Data Read | {performance_context.get('read_gb', 0):.2f}GB |
| Output Rows | {performance_context.get('rows_produced', 0):,} |
| Read Rows | {performance_context.get('rows_read', 0):,} |
| Filter Rate | {performance_context.get('data_selectivity', 0):.4f} |

### ü§ñ AI Analysis Results

{llm_analysis}

"""
        
        # SQLÊúÄÈÅ©ÂåñÂàÜÊûêÁµêÊûú„ÅÆËøΩÂä†ÔºàËã±Ë™ûÁâàÔºâ
        # üöÄ SQL„Éï„Ç°„Ç§„É´ÂÜÖÂÆπ„ÅÆÂ†¥Âêà„ÅØÈÅ©Âàá„Å´„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÔºàÁúÅÁï•Ê©üËÉΩ‰ªò„ÅçÔºâ
        formatted_sql_content = format_sql_content_for_report(optimized_result, latest_sql_filename)
        
        # üéØ ÊúÄÈÅ©ÂåñÊñπÈáùË¶ÅÁ¥Ñ„ÇíÁîüÊàêÔºàËã±Ë™ûÁâàÔºâ
        optimization_strategy = generate_optimization_strategy_summary(optimized_result, metrics, analysis_result_str)
        
        # üìä ÊúÄÈÅ©Âåñ„Éó„É≠„Çª„ÇπË©≥Á¥∞„ÅÆÁîüÊàêÔºàËã±Ë™ûÁâàÔºâ
        optimization_process_details_en = ""
        if optimization_attempts is not None and best_attempt_number is not None:
            total_attempts = len(optimization_attempts)
            cost_improvement = "N/A"
            memory_improvement = "N/A"
            
            if performance_comparison:
                cost_ratio = performance_comparison.get('total_cost_ratio', 1.0)
                memory_ratio = performance_comparison.get('memory_usage_ratio', 1.0)
                cost_improvement = f"{(1-cost_ratio)*100:.1f}"
                memory_improvement = f"{(1-memory_ratio)*100:.1f}"
            
            # Make final selection display clearer
            if best_attempt_number == 0:
                final_selection_en = "Original Query (no improvement achieved through optimization)"
                selection_reason_en = "Using original query as optimization trials did not yield effective improvements"
                # üìÑ Add original query file name information
                if latest_sql_filename:
                    selection_reason_en += f"\n- üìÑ Reference file: {latest_sql_filename} (optimization trial result)"
                else:
                    selection_reason_en += "\n- üìÑ Original query: Extracted from profiler data"
            else:
                final_selection_en = f"Trial {best_attempt_number}"
                selection_reason_en = "Selected the trial with the best cost efficiency"
            
            optimization_process_details_en = f"""### üéØ Optimization Process Details
The following shows the trials executed during the optimization process and the selection rationale:

**üìä Optimization Trial History:**
- Trial count: {total_attempts} attempts executed
- Final selection: {final_selection_en}
- Selection reason: {selection_reason_en}

**üèÜ Selected Optimization Effects:**
- Cost reduction rate: {cost_improvement}% (EXPLAIN COST comparison)
- Memory efficiency improvement: {memory_improvement}% (statistics comparison)

"""
        
        # Êó•Êú¨Ë™û„Åã„ÇâËã±Ë™û„Å∏„ÅÆÁøªË®≥„Éû„ÉÉ„Éî„É≥„Ç∞
        translation_map = {
            "üîç Ê§úÂá∫„Åï„Çå„Åü‰∏ªË¶ÅË™≤È°å": "üîç Key Issues Identified",
            "üõ†Ô∏è ÈÅ©Áî®„Åï„Çå„ÅüÊúÄÈÅ©ÂåñÊâãÊ≥ï": "üõ†Ô∏è Applied Optimization Techniques",
            "üéØ ÊúÄÈÅ©ÂåñÈáçÁÇπÂàÜÈáé": "üéØ Optimization Focus Areas",
            "üìä Áµ±Ë®àÊÉÖÂ†±Ê¥ªÁî®": "üìä Statistical Analysis Utilization",
            "EXPLAIN + EXPLAIN COSTÂàÜÊûê„Å´„Çà„Çä„ÄÅÁµ±Ë®à„Éô„Éº„Çπ„ÅÆÁ≤æÂØÜ„Å™ÊúÄÈÅ©Âåñ„ÇíÂÆüË°å": "Statistical-based precise optimization through EXPLAIN + EXPLAIN COST analysis",
            "ü§ñ AIÂàÜÊûê„Å´„Çà„ÇãÂåÖÊã¨ÁöÑ„Å™ÊúÄÈÅ©Âåñ": "ü§ñ Comprehensive AI-driven Optimization",
            "„Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûê„ÄÅÁµ±Ë®àÊÉÖÂ†±„ÄÅ„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„Çπ„ÇíÁ∑èÂêà„Åó„ÅüÊúÄÈÅ©Âåñ„ÇíÂÆüË°å": "Comprehensive optimization integrating bottleneck analysis, statistical data, and best practices",
            "„É°„É¢„É™„Çπ„Éî„É´Áô∫Áîü": "Memory Spill Occurrence",
            "„Ç∑„É£„ÉÉ„Éï„É´Âá¶ÁêÜ„Éú„Éà„É´„Éç„ÉÉ„ÇØ": "Shuffle Processing Bottleneck",
            "‰∏¶ÂàóÂ∫¶‰∏çË∂≥": "Insufficient Parallelism",
            "„Ç≠„É£„ÉÉ„Ç∑„É•„Éí„ÉÉ„ÉàÁéá‰Ωé‰∏ã": "Low Cache Hit Rate",
            "Photon EngineÊú™Ê¥ªÁî®": "Photon Engine Not Utilized",
            "„Éá„Éº„Çø„Çπ„Ç≠„É•„ÉºÁô∫Áîü": "Data Skew Occurrence",
            "„É°„É¢„É™ÂäπÁéáÂåñ": "Memory Efficiency",
            "„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØË≤†Ëç∑ËªΩÊ∏õ": "Network Load Reduction",
            "‰∏¶ÂàóÂá¶ÁêÜËÉΩÂäõÂêë‰∏ä": "Parallel Processing Enhancement"
        }
        
        optimization_strategy_en = optimization_strategy
        for jp_text, en_text in translation_map.items():
            optimization_strategy_en = optimization_strategy_en.replace(jp_text, en_text)
        
        # EXPLAINË¶ÅÁ¥Ñ„Éï„Ç°„Ç§„É´„ÅÆË™≠„ÅøËæº„Åø„Å®ËøΩÂä†ÔºàÂãïÁöÑ„Å´ÊúÄÊñ∞„Éï„Ç°„Ç§„É´„ÇíÊ§úÁ¥¢Ôºâ
        explain_summary_section = ""
        try:
            # Ë§áÊï∞„ÅÆ„Éë„Çø„Éº„É≥„ÅßEXPLAINË¶ÅÁ¥Ñ„Éï„Ç°„Ç§„É´„ÇíÊ§úÁ¥¢Ôºàoptimized/original‰∏°ÊñπÂØæÂøúÔºâ
            optimized_files = glob.glob("output_explain_summary_optimized_*.md")
            original_files = glob.glob("output_explain_summary_original_*.md")
            all_explain_files = optimized_files + original_files
            
            if all_explain_files:
                # „Éï„Ç°„Ç§„É´‰ΩúÊàêÊôÇÂàª„ÅßÊúÄÊñ∞„ÅÆ„Éï„Ç°„Ç§„É´„ÇíÈÅ∏ÊäûÔºà„Çà„ÇäÁ¢∫ÂÆüÔºâ
                import os
                latest_explain_summary = max(all_explain_files, key=os.path.getctime)
                file_age = os.path.getctime(latest_explain_summary)
                
                print(f"üîç Found {len(all_explain_files)} EXPLAIN summary files:")
                for f in sorted(all_explain_files, key=os.path.getctime, reverse=True):
                    age = os.path.getctime(f)
                    status = "üìç SELECTED" if f == latest_explain_summary else "  "
                    print(f"   {status} {f} (created: {os.path.getctime(f)})")
                
                # „Éï„Ç°„Ç§„É´ÂÜÖÂÆπ„ÇíË™≠„ÅøËæº„Åø
                with open(latest_explain_summary, 'r', encoding='utf-8') as f:
                    explain_content = f.read()
                
                # Ëã±Ë™ûÁâà„Å´ÁøªË®≥
                explain_content_en = translate_explain_summary_to_english(explain_content)
                
                # „Éï„Ç°„Ç§„É´„Çø„Ç§„Éó„ÇíÂà§ÂÆöÔºàoptimized/originalÔºâ
                file_type = "Optimized" if "optimized" in latest_explain_summary else "Original"
                
                explain_summary_section = f"""
### üìã Current Query Explain Output ({file_type} Query)

> **Source File**: `{latest_explain_summary}`  
> **Analysis Type**: {file_type} query execution plan analysis

{explain_content_en}

"""
                print(f"‚úÖ EXPLAIN summary integrated: {latest_explain_summary} ({file_type})")
            else:
                print("‚ö†Ô∏è No EXPLAIN summary files found (searched: output_explain_summary_*.md)")
                # EXPLAINÂÆüË°å„ÅåÁÑ°Âäπ„Å™Â†¥Âêà„ÅÆË™¨Êòé„ÇíËøΩÂä†
                explain_summary_section = f"""
### üìã Current Query Explain Output

‚ö†Ô∏è **EXPLAIN analysis not available**

No EXPLAIN summary files were found. This could be due to:
- EXPLAIN_ENABLED setting is 'N' (disabled)
- EXPLAIN execution failed or was skipped
- Files haven't been generated yet for this query

To enable EXPLAIN analysis, set `EXPLAIN_ENABLED = 'Y'` before running the analysis.

"""
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading EXPLAIN summary: {str(e)}")
            explain_summary_section = f"""
### üìã Current Query Explain Output

‚ùå **Error loading EXPLAIN analysis**

An error occurred while loading EXPLAIN summary files: `{str(e)}`

Please check:
- File permissions and accessibility
- EXPLAIN_ENABLED setting
- Query execution status

"""

        report += f"""
## üöÄ 4. SQL Optimization Analysis Results

{optimization_process_details_en}### üéØ Optimization Strategy

{optimization_strategy_en}

### üí° Optimization Recommendations

{formatted_sql_content}

{explain_summary_section}### üîç 5. Performance Verification Results

{generate_performance_comparison_section(performance_comparison, language='en')}

### üìà 6. Expected Performance Improvement

#### üéØ Anticipated Improvements

"""
        
        # ÊúüÂæÖ„Åï„Çå„ÇãÊîπÂñÑÂäπÊûú„ÇíË®àÁÆóÔºàËã±Ë™ûÁâàÔºâ
        expected_improvements = []
        
        if bottleneck_indicators.get('has_spill', False):
            expected_improvements.append("**Memory Spill Resolution**: Up to 50-80% performance improvement expected")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            expected_improvements.append("**Shuffle Optimization**: 20-60% execution time reduction expected")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            expected_improvements.append("**Cache Efficiency**: 30-70% read time reduction expected")
        
        if not overall_metrics.get('photon_enabled', False):
            expected_improvements.append("**Photon Enablement**: 2-10x processing speed improvement expected")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.2:
            expected_improvements.append("**Filter Efficiency**: 40-90% data read volume reduction expected")
        
        if expected_improvements:
            for i, improvement in enumerate(expected_improvements, 1):
                report += f"{i}. {improvement}\n"
            
            # Á∑èÂêàÁöÑ„Å™ÊîπÂñÑÂäπÊûú
            total_time_ms = overall_metrics.get('total_time_ms', 0)
            if total_time_ms > 0:
                improvement_ratio = min(0.8, len(expected_improvements) * 0.15)  # ÊúÄÂ§ß80%ÊîπÂñÑ
                expected_time = total_time_ms * (1 - improvement_ratio)
                report += f"\n**Overall Improvement**: Execution time {total_time_ms:,}ms ‚Üí {expected_time:,.0f}ms (approx. {improvement_ratio*100:.0f}% improvement)\n"
        else:
            report += "Current query is already optimized. No significant improvements expected.\n"
        
        report += f"""

#### üîß Implementation Priority

1. **High Priority**: Photon enablement, Memory spill resolution
2. **Medium Priority**: Liquid Clustering, Data layout optimization
3. **Low Priority**: Statistics update, Cache strategy

{explain_section}

{explain_cost_section}

---

*Report generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    return report

def refine_report_with_llm(raw_report: str, query_id: str) -> str:
    """
    LLM„Çí‰Ωø„Å£„Å¶„É¨„Éù„Éº„Éà„ÇíÊé®Êï≤„Åó„ÄÅË™≠„Åø„ÇÑ„Åô„ÅÑÊúÄÁµÇ„É¨„Éù„Éº„Éà„ÇíÁîüÊàê
    
    Args:
        raw_report: ÂàùÊúüÁîüÊàê„Åï„Çå„Åü„É¨„Éù„Éº„Éà
        query_id: „ÇØ„Ç®„É™ID
        
    Returns:
        str: LLM„ÅßÊé®Êï≤„Åï„Çå„ÅüË™≠„Åø„ÇÑ„Åô„ÅÑ„É¨„Éù„Éº„Éà
    """
    
    print("ü§ñ Executing LLM-based report refinement...")
    
    # üö® „Éà„Éº„ÇØ„É≥Âà∂ÈôêÂØæÁ≠ñ: „É¨„Éù„Éº„Éà„Çµ„Ç§„Ç∫Âà∂Èôê
    MAX_REPORT_SIZE = 50000  # 50KBÂà∂Èôê
    original_size = len(raw_report)
    
    if original_size > MAX_REPORT_SIZE:
        print(f"‚ö†Ô∏è Report size too large: {original_size:,} characters ‚Üí truncated to {MAX_REPORT_SIZE:,} characters")
        # ÈáçË¶Å„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÂÑ™ÂÖàÁöÑ„Å´‰øùÊåÅ
        truncated_report = raw_report[:MAX_REPORT_SIZE]
        truncated_report += f"\n\n‚ö†Ô∏è „É¨„Éù„Éº„Éà„ÅåÂ§ß„Åç„Åô„Åé„Çã„Åü„ÇÅ„ÄÅ{MAX_REPORT_SIZE:,} ÊñáÂ≠ó„Å´Âàá„ÇäË©∞„ÇÅ„Çâ„Çå„Åæ„Åó„ÅüÔºàÂÖÉ„Çµ„Ç§„Ç∫: {original_size:,} ÊñáÂ≠óÔºâ"
        raw_report = truncated_report
    else:
        print(f"üìä Report size: {original_size:,} characters (executing refinement)")
    
    # Ë®ÄË™û„Å´Âøú„Åò„Å¶„Éó„É≠„É≥„Éó„Éà„ÇíÂàá„ÇäÊõø„Åà
    if OUTPUT_LANGUAGE == 'ja':
        refinement_prompt = f"""
ÊäÄË°ìÊñáÊõ∏„ÅÆÁ∑®ÈõÜËÄÖ„Å®„Åó„Å¶„ÄÅDatabricks SQL„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂàÜÊûê„É¨„Éù„Éº„Éà„Çí‰ª•‰∏ã„ÅÆ„É´„Éº„É´„Å´Âæì„Å£„Å¶Êé®Êï≤„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„ÄêÁµ∂ÂØæ„Å´ÂÆà„Çã„Åπ„ÅçË¶ãÂá∫„ÅóÊßãÈÄ†„Äë
```
# üìä SQLÊúÄÈÅ©Âåñ„É¨„Éù„Éº„Éà

## üîç 1. ÂàÜÊûê„Çµ„Éû„É™„Éº

### Áµ±Âêà„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂàÜÊûêË°®
‰∏ªË¶ÅË™≤È°å„Å®„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊåáÊ®ô„Çí‰ª•‰∏ã„ÅÆÁµ±ÂêàË°®ÂΩ¢Âºè„Åß„Åæ„Å®„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑÔºö

üîç ÂàÜÊûê„Çµ„Éû„É™„Éº
„ÇØ„Ç®„É™ÂÆüË°åÊôÇÈñì„ÅØ[X.X]Áßí„Å®[Ë©ï‰æ°]„Åß„Åô„Åå„ÄÅ‰ª•‰∏ã„ÅÆÊúÄÈÅ©Âåñ„Éù„Ç§„É≥„Éà„ÅåÁâπÂÆö„Åï„Çå„Åæ„Åó„ÅüÔºö

| È†ÖÁõÆ | ÁèæÂú®„ÅÆÁä∂Ê≥Å | Ë©ï‰æ° | ÂÑ™ÂÖàÂ∫¶ |
|------|-----------|------|--------|
| ÂÆüË°åÊôÇÈñì | [X.X]Áßí | ‚úÖ ËâØÂ•Ω / ‚ö†Ô∏è ÊîπÂñÑÂøÖË¶Å | - |
| „Éá„Éº„ÇøË™≠„ÅøÂèñ„ÇäÈáè | [X.XX]GB | ‚úÖ ËâØÂ•Ω / ‚ö†Ô∏è Â§ßÂÆπÈáè | - |
| PhotonÊúâÂäπÂåñ | „ÅØ„ÅÑ/„ÅÑ„ÅÑ„Åà | ‚úÖ ËâØÂ•Ω / ‚ùå Êú™ÊúâÂäπ | - |
| „Ç∑„É£„ÉÉ„Éï„É´Êìç‰Ωú | [N]Âõû | ‚úÖ ËâØÂ•Ω / ‚ö†Ô∏è Â§ö„ÅÑ | üö® È´ò / ‚ö†Ô∏è ‰∏≠ |
| „Çπ„Éî„É´Áô∫Áîü | „Å™„Åó/„ÅÇ„Çä | ‚úÖ ËâØÂ•Ω / ‚ùå ÂïèÈ°å | üö® È´ò / - |
| „Ç≠„É£„ÉÉ„Ç∑„É•ÂäπÁéá | [X.X]% | ‚úÖ ËâØÂ•Ω / ‚ö†Ô∏è ‰ΩéÂäπÁéá | ‚ö†Ô∏è ‰∏≠ |
| „Éï„Ç£„É´„ÇøÂäπÁéá | [X.X]% | ‚úÖ ËâØÂ•Ω / ‚ö†Ô∏è ‰ΩéÂäπÁéá | ‚ö†Ô∏è ‰∏≠ |
| „Éá„Éº„Çø„Çπ„Ç≠„É•„Éº | AQEÂØæÂøúÊ∏à / Êú™Ê§úÂá∫ | ‚úÖ ÂØæÂøúÊ∏à / ‚úÖ ËâØÂ•Ω | - |

## üìä 2. TOP10ÊôÇÈñìÊ∂àË≤ª„Éó„É≠„Çª„ÇπÂàÜÊûê

### ‚è±Ô∏è ÂÆüË°åÊôÇÈñì„É©„É≥„Ç≠„É≥„Ç∞

## üóÇÔ∏è 3. Liquid ClusteringÂàÜÊûêÁµêÊûú

### üìã Êé®Â•®„ÉÜ„Éº„Éñ„É´ÂàÜÊûê

## üöÄ 4. ÊúÄÈÅ©Âåñ„Åï„Çå„ÅüSQL„ÇØ„Ç®„É™

### üéØ ÊúÄÈÅ©Âåñ„Éó„É≠„Çª„ÇπË©≥Á¥∞
ÊúÄÈÅ©Âåñ„Éó„É≠„Çª„Çπ„ÅßÂÆüË°å„Åï„Çå„ÅüË©¶Ë°å„Å®„Åù„ÅÆÈÅ∏ÊäûÁêÜÁî±„Çí‰ª•‰∏ã„Å´Á§∫„Åó„Åæ„ÅôÔºö

**üìä ÊúÄÈÅ©ÂåñË©¶Ë°åÂ±•Ê≠¥:**
- Ë©¶Ë°åÂõûÊï∞: [total_attempts]ÂõûÂÆüË°å
- ÊúÄÁµÇÈÅ∏Êäû: Ë©¶Ë°å[selected_attempt_num]Áï™„ÅåÊúÄÈÅ©Ëß£„Å®„Åó„Å¶ÈÅ∏Êäû
- ÈÅ∏ÊäûÁêÜÁî±: [selection_reason]

**üèÜ ÈÅ∏Êäû„Åï„Çå„ÅüÊúÄÈÅ©Âåñ„ÅÆÂäπÊûú:**
- „Ç≥„Çπ„ÉàÂâäÊ∏õÁéá: [cost_improvement]% (EXPLAIN COSTÊØîËºÉ)
- „É°„É¢„É™ÂäπÁéáÊîπÂñÑ: [memory_improvement]% (Áµ±Ë®àÊØîËºÉ)

### üí° ÂÖ∑‰ΩìÁöÑ„Å™ÊúÄÈÅ©ÂåñÂÜÖÂÆπ„Å®„Ç≥„Çπ„ÉàÂäπÊûú
ÊúÄÈÅ©Âåñ„Åï„Çå„ÅüSQL„ÇØ„Ç®„É™„ÅÆÂâç„Å´„ÄÅ‰ª•‰∏ã„ÅÆÊÉÖÂ†±„ÇíÂøÖ„ÅöÂê´„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑÔºö

**üéØ ÈÅ©Áî®„Åï„Çå„ÅüÊúÄÈÅ©ÂåñÊâãÊ≥ï:**
„ÄêÈáçË¶Å„ÄëÊúÄÈÅ©Âåñ„Éó„É≠„Çª„ÇπË©≥Á¥∞„Çª„ÇØ„Ç∑„Éß„É≥„Åß„ÄåÂÖÉ„ÅÆ„ÇØ„Ç®„É™ÔºàÊúÄÈÅ©Âåñ„Å´„Çà„ÇäÊîπÂñÑ„Åï„Çå„Å™„Åã„Å£„Åü„Åü„ÇÅÔºâ„Äç„ÅåÈÅ∏Êäû„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥ÂêàÔºö
- ‚ö†Ô∏è ÊúÄÈÅ©ÂåñÊâãÊ≥ï„ÅØÈÅ©Áî®„Åï„Çå„Åæ„Åõ„Çì„Åß„Åó„ÅüÔºàÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Çí‰ΩøÁî®Ôºâ
- üìÑ ‰ΩøÁî®„Éï„Ç°„Ç§„É´: „Éó„É≠„Éï„Ç°„Ç§„É©„Éº„Éá„Éº„Çø„Åã„ÇâÊäΩÂá∫„Åï„Çå„ÅüÂÖÉ„ÅÆ„ÇØ„Ç®„É™
- üí° ÁêÜÁî±: ÊúÄÈÅ©ÂåñË©¶Ë°å„ÅßÊúâÂäπ„Å™ÊîπÂñÑ„ÅåÂæó„Çâ„Çå„Å™„Åã„Å£„Åü„Åü„ÇÅ

„Åù„Çå‰ª•Â§ñ„ÅÆÂ†¥Âêà„ÅÆ„Åø‰ª•‰∏ã„ÇíË®òËºâÔºö
- [ÂÆüÈöõ„ÅÆ„ÇØ„Ç®„É™Êõ∏„ÅçÊèõ„ÅàÂÜÖÂÆπ„ÇíÂÖ∑‰ΩìÁöÑ„Å´Ë¶ÅÁ¥Ñ]
- ‰æã: "JOINÈ†ÜÂ∫è„ÅÆÊúÄÈÅ©ÂåñÔºàÂ∞è„ÉÜ„Éº„Éñ„É´ÂÑ™ÂÖàÔºâ", "„Éï„Ç£„É´„ÇøÊù°‰ª∂„ÅÆÊó©ÊúüÈÅ©Áî®", "„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Éí„É≥„Éà„ÅÆËøΩÂä†"
- ‚ùå ÂÆüÊñΩ„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÊâãÊ≥ï„ÅØË®òËºâ„Åó„Å™„ÅÑÔºà‰æã: „Çπ„Éî„É´„ÅåÊ§úÂá∫„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÅØREPARTITIONÈÅ©Áî®„ÇíË®òËºâ„Åó„Å™„ÅÑÔºâ
- ‚ùå "Liquid Clustering implementation"Á≠â„ÅÆÊú™ÂÆüÊñΩ„ÅÆÂ§âÊõ¥„ÅØË®òËºâ„Åó„Å™„ÅÑ

**üí∞ EXPLAIN COST„Éô„Éº„Çπ„ÅÆÂäπÊûúÂàÜÊûê:**
„ÄêÈáçË¶Å„ÄëÂÖÉ„ÅÆ„ÇØ„Ç®„É™„ÅåÈÅ∏Êäû„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥ÂêàÔºö
- ‚ö†Ô∏è ÊúÄÈÅ©Âåñ„Å´„Çà„ÇãÊîπÂñÑ„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü
- üìä ÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Çí„Åù„ÅÆ„Åæ„Åæ‰ΩøÁî®„Åô„Çã„Åì„Å®„ÇíÊé®Â•®

„Åù„Çå‰ª•Â§ñ„ÅÆÂ†¥Âêà„ÅÆ„Åø‰ª•‰∏ã„ÇíË®òËºâÔºö
- „ÇØ„Ç®„É™ÂÆüË°å„Ç≥„Çπ„ÉàÂâäÊ∏õÁéá: [cost_ratio]ÂÄç (EXPLAIN COSTÊØîËºÉÁµêÊûú)
- „É°„É¢„É™‰ΩøÁî®ÈáèÂâäÊ∏õÁéá: [memory_ratio]ÂÄç (Áµ±Ë®àÊÉÖÂ†±„Éô„Éº„ÇπÊØîËºÉ)
- Êé®ÂÆö„Éá„Éº„ÇøÂá¶ÁêÜÂäπÁéá: [processing_efficiency]% („Çπ„Ç≠„É£„É≥„ÉªJOINÂäπÁéáÊîπÂñÑ)
```

„Äêüö® REPARTITION„Å´Èñ¢„Åô„ÇãÈáçË¶Å„Å™‰øÆÊ≠£ÊåáÁ§∫„Äë
- **„Çπ„Éî„É´„ÅåÊ§úÂá∫„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà**: „ÄåREPARTITION„ÅÆÈÅ©Áî®„Äç„ÇíÊé®Â•®ÊîπÂñÑ„Ç¢„ÇØ„Ç∑„Éß„É≥„Å´Âê´„ÇÅ„Å™„ÅÑ
- **ÂÆüÈöõ„Å´ÈÅ©Áî®„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÊúÄÈÅ©ÂåñÊâãÊ≥ï**: „ÄåÁ∑äÊÄ•ÂØæÂøú„Äç„ÇÑ„ÄåÊé®Â•®ÊîπÂñÑ„Ç¢„ÇØ„Ç∑„Éß„É≥„Äç„Å´Ë®òËºâ„Åó„Å™„ÅÑ
- **‰∫ãÂÆü„Éô„Éº„Çπ„ÅÆË®òËºâ**: ÂÆüÈöõ„Å´Ê§úÂá∫„Åï„Çå„ÅüÂïèÈ°å„Å®ÈÅ©Áî®„Åï„Çå„ÅüÂØæÁ≠ñ„ÅÆ„Åø„ÇíË®òËºâ

„Äêüí∞ „Ç≥„Çπ„ÉàÂäπÊûúÂàÜÊûê„Åß„ÅÆÂøÖÈ†à‰ΩøÁî®„Éá„Éº„Çø„Äë
- **performance_comparisonÁµêÊûú„ÇíÂøÖ„Åö‰ΩøÁî®**: cost_ratio„ÄÅmemory_ratioÁ≠â„ÅÆÂÆüÈöõ„ÅÆÊØîËºÉÂÄ§
- **ÂÆüË°åÊôÇÈñì‰∫àÊ∏¨„ÅØ‰ΩøÁî®Á¶ÅÊ≠¢**: ‰∏çÊ≠£Á¢∫„Å™„Åü„ÇÅË®òËºâ„Åó„Å™„ÅÑ
- **EXPLAIN COST„Éô„Éº„Çπ„ÅÆÊï∞ÂÄ§„ÅÆ„Åø**: ÊúÄÈÅ©Âåñ„Éó„É≠„Çª„Çπ‰∏≠„ÅÆÂÆüÈöõ„ÅÆË®àÁÆóÁµêÊûú„Çí‰ΩøÁî®

„ÄêÂé≥Ê†º„Å™Á¶ÅÊ≠¢‰∫ãÈ†Ö„Äë
- TOP10„ÇíÁµ∂ÂØæ„Å´TOP5„Å´Â§âÊõ¥„Åó„Å™„ÅÑ
- "=========="Á≠â„ÅÆÂå∫Âàá„ÇäÊñáÂ≠ó„ÇíÂâäÈô§Ôºà„Åü„Å†„ÅóÁµµÊñáÂ≠ó„Å´„Çà„ÇãË¶ñË¶öÁöÑË°®Á§∫„ÅØ‰øùÊåÅÔºâ
- Áï™Âè∑‰ªò„Åç„É™„Çπ„Éà„ÅßÂêå„ÅòÁï™Âè∑„ÇíÈáçË§á„Åï„Åõ„Å™„ÅÑ
- „É°„Éà„É™„ÇØ„ÇπÂÄ§„ÇÑÊäÄË°ìÊÉÖÂ†±„ÇíÂâäÈô§„Åó„Å™„ÅÑ
- ÂÆüÊñΩ„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÊúÄÈÅ©ÂåñÊâãÊ≥ï„Çí„ÄåÂÆüÊñΩÊ∏à„Åø„Äç„Å®„Åó„Å¶Ë®òËºâ„Åó„Å™„ÅÑ
- Âêå„Åò„Ç≥„Çπ„ÉàÊØî„ÇÑÂäπÊûúÊï∞ÂÄ§„ÇíË§áÊï∞ÂÄãÊâÄ„ÅßÈáçË§áË®òËºâ„Åó„Å™„ÅÑÔºàÊúÄÈÅ©Âåñ„Éó„É≠„Çª„ÇπË©≥Á¥∞„Åß‰∏ÄÂ∫¶Ë®òËºâ„Åô„Çå„Å∞ÂçÅÂàÜÔºâ

„Äêüö® ÈáçË¶Å„Å™ÊÉÖÂ†±‰øùÊåÅ„ÅÆÂøÖÈ†àË¶Å‰ª∂„Äë
- **ÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„ÉºÊÉÖÂ†±**: ÂêÑ„ÉÜ„Éº„Éñ„É´„ÅÆ„ÄåÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº: XX„ÄçÊÉÖÂ†±„ÅØÂøÖ„Åö‰øùÊåÅ
- **„Éï„Ç£„É´„ÇøÁéáÊÉÖÂ†±**: „Äå„Éï„Ç£„É´„ÇøÁéá: X.X% (Ë™≠„ÅøËæº„Åø: XX.XXGB, „Éó„É´„Éº„É≥: XX.XXGB)„ÄçÂΩ¢Âºè„ÅÆÊÉÖÂ†±„ÅØÂøÖ„Åö‰øùÊåÅ
- **„Éë„Éº„Çª„É≥„ÉÜ„Éº„Ç∏Ë®àÁÆó**: „Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûê„ÅÆ„Éë„Éº„Çª„É≥„ÉÜ„Éº„Ç∏ÔºàÂÖ®‰Ωì„ÅÆ‚óã‚óã%Ôºâ„ÅØÊ≠£Á¢∫„Å™ÂÄ§„Çí‰øùÊåÅ
- **Êé®Â•®vsÁèæÂú®„ÅÆÊØîËºÉ**: Êé®Â•®„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº„Å®ÁèæÂú®„ÅÆ„Ç≠„Éº„ÅÆÊØîËºÉÊÉÖÂ†±„ÅØÂâäÈô§Á¶ÅÊ≠¢
- **Êï∞ÂÄ§„É°„Éà„É™„ÇØ„Çπ**: ÂÆüË°åÊôÇÈñì„ÄÅ„Éá„Éº„ÇøË™≠„ÅøËæº„ÅøÈáè„ÄÅ„Çπ„Éî„É´ÈáèÁ≠â„ÅÆÊï∞ÂÄ§„Éá„Éº„Çø„ÅØÂâäÈô§Á¶ÅÊ≠¢
- **SQLÂÆüË£Ö‰æã**: ALTER TABLEÊñá„ÇÑCLUSTER BYÊßãÊñá„ÅÆÂÖ∑‰Ωì‰æã„ÅØÂâäÈô§Á¶ÅÊ≠¢

„ÄêÂá¶ÁêÜË¶Å‰ª∂„Äë
1. ‰∏äË®ò„ÅÆË¶ãÂá∫„ÅóÊßãÈÄ†„ÇíÂøÖ„Åö‰ΩøÁî®
2. ‰∏ªË¶ÅË™≤È°å„Å®„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊåáÊ®ô„ÇíÁµ±ÂêàË°®ÂΩ¢Âºè„Åß„Åæ„Å®„ÇÅ„Çã
3. ÂÆüÈöõ„Å´ÈÅ©Áî®„Åï„Çå„ÅüÊúÄÈÅ©ÂåñÊâãÊ≥ï„ÅÆ„Åø„ÇíË®òËºâÔºàÂÆüÊñΩ„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÊâãÊ≥ï„ÅØË®òËºâ„Åó„Å™„ÅÑÔºâ
4. ÂÖ∑‰ΩìÁöÑ„Å™„Ç≥„Çπ„ÉàÂäπÊûú„ÇíÊï∞ÂÄ§„ÅßÁ§∫„Åô
5. ÊäÄË°ìÊÉÖÂ†±„Å®„É°„Éà„É™„ÇØ„Çπ„ÇíÂÆåÂÖ®‰øùÊåÅÔºàÁâπ„Å´‰∏äË®ò„ÅÆÈáçË¶ÅÊÉÖÂ†±Ôºâ
6. TOP10Ë°®Á§∫„ÇíÁ∂≠ÊåÅ
7. ÁµµÊñáÂ≠ó„Å´„Çà„ÇãË¶ñË¶öÁöÑË°®Á§∫„Çí‰øùÊåÅÔºàüö® CRITICAL„ÄÅ‚ö†Ô∏è HIGH„ÄÅ‚úÖËâØÂ•ΩÁ≠âÔºâ
8. ‰∏çË¶Å„Å™Âå∫Âàá„ÇäÊñáÂ≠óÔºà========Á≠âÔºâ„ÅÆ„ÅøÂâäÈô§
9. ÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„ÉºÊÉÖÂ†±„Å®„Éï„Ç£„É´„ÇøÁéáÊÉÖÂ†±„ÅØÁµ∂ÂØæ„Å´‰øùÊåÅ

„ÄêÁèæÂú®„ÅÆ„É¨„Éù„Éº„Éà„Äë
```
{raw_report}
```

‰∏äË®ò„ÅÆË¶ãÂá∫„ÅóÊßãÈÄ†„Å´Âæì„Å£„Å¶Êé®Êï≤„Åó„ÄÅÊäÄË°ìÊÉÖÂ†±„ÇíÂÆåÂÖ®„Å´‰øùÊåÅ„Åó„Åü„É¨„Éù„Éº„Éà„ÇíÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
"""
    else:
        refinement_prompt = f"""
As a technical document editor, please refine the following Databricks SQL performance analysis report according to these rules.

„ÄêRequired Heading Structure„Äë
```
# üìä SQL Optimization Report

## üîç 1. Analysis Summary

### Integrated Performance Analysis Table
Merge major issues and performance indicators into the following integrated table format:

üîç Analysis Summary
Query execution time is [X.X] seconds, which is [evaluation], but the following optimization points were identified:

| Item | Current Status | Evaluation | Priority |
|------|---------------|------------|----------|
| Execution Time | [X.X]s | ‚úÖ Good / ‚ö†Ô∏è Needs Improvement | - |
| Data Read Volume | [X.XX]GB | ‚úÖ Good / ‚ö†Ô∏è Large Volume | - |
| Photon Enabled | Yes/No | ‚úÖ Good / ‚ùå Not Enabled | - |
| Shuffle Operations | [N] times | ‚úÖ Good / ‚ö†Ô∏è High | üö® High / ‚ö†Ô∏è Medium |
| Spill Occurrence | None/Present | ‚úÖ Good / ‚ùå Issues | üö® High / - |
| Cache Efficiency | [X.X]% | ‚úÖ Good / ‚ö†Ô∏è Low Efficiency | ‚ö†Ô∏è Medium |
| Filter Efficiency | [X.X]% | ‚úÖ Good / ‚ö†Ô∏è Low Efficiency | ‚ö†Ô∏è Medium |
| Data Skew | AQE Handled / Not Detected | ‚úÖ Handled / ‚úÖ Good | - |

## üìä 2. TOP10 Time-Consuming Processes Analysis

### ‚è±Ô∏è Execution Time Ranking

## üóÇÔ∏è 3. Liquid Clustering Analysis Results

### üìã Recommended Table Analysis

## üöÄ 4. Optimized SQL Query

### üéØ Optimization Process Details
The following shows the trials executed during the optimization process and the selection rationale:

**üìä Optimization Trial History:**
- Trial count: [total_attempts] attempts executed
- Final selection: Trial [selected_attempt_num] was chosen as the optimal solution
- Selection reason: [selection_reason]

**üèÜ Selected Optimization Effects:**
- Cost reduction rate: [cost_improvement]% (EXPLAIN COST comparison)
- Memory efficiency improvement: [memory_improvement]% (statistics comparison)

### üí° Specific Optimization Details and Cost Effects
Before the optimized SQL query, must include the following information:

**üéØ Applied Optimization Techniques:**
„ÄêImportant„ÄëIf "Original Query (no improvement achieved through optimization)" is selected in the Optimization Process Details section:
- ‚ö†Ô∏è No optimization techniques were applied (using original query)
- üìÑ Used file: Original query extracted from profiler data
- üí° Reason: Optimization trials did not yield effective improvements

Only for other cases, list the following:
- [Summarize actual query rewriting content specifically]
- Examples: "JOIN order optimization (small table first)", "Early filter condition application", "Index hint addition"
- ‚ùå Do not list techniques that were not implemented (e.g., do not mention REPARTITION application if no spill was detected)
- ‚ùå Do not mention unimplemented changes like "Liquid Clustering implementation"

**üí∞ EXPLAIN COST-Based Effect Analysis:**
„ÄêImportant„ÄëIf original query is selected:
- ‚ö†Ô∏è No improvement was achieved through optimization
- üìä Recommend using the original query as-is

Only for other cases, list the following:
- Query execution cost reduction: [cost_ratio]x (EXPLAIN COST comparison result)
- Memory usage reduction: [memory_ratio]x (statistics-based comparison)
- Estimated data processing efficiency: [processing_efficiency]% (scan/JOIN efficiency improvement)
```

„Äêüö® Critical REPARTITION Correction Instructions„Äë
- **When no spill is detected**: Do not include "REPARTITION application" in recommended improvement actions
- **Actually non-applied optimization techniques**: Do not list in "Emergency Response" or "Recommended Improvement Actions"
- **Fact-based reporting**: Only list actually detected problems and applied countermeasures

„Äêüí∞ Required Data for Cost Effect Analysis„Äë
- **Must use performance_comparison results**: cost_ratio, memory_ratio and other actual comparison values
- **Execution time prediction is prohibited**: Do not include due to inaccuracy
- **EXPLAIN COST-based numbers only**: Use actual calculation results from optimization process

„ÄêStrict Prohibitions„Äë
- Never change TOP10 to TOP5
- Remove separator characters like "==========" (but keep emoji visual displays)
- Do not duplicate numbered list items
- Do not delete metric values or technical information
- Do not report non-implemented optimization techniques as "implemented"
- Do not duplicate the same cost ratios or effect numbers in multiple sections (once in optimization process details is sufficient)

„Äêüö® Critical Information Preservation Requirements„Äë
- **Current clustering key information**: Must preserve each table's "Current clustering key: XX" information
- **Filter rate information**: Must preserve "Filter rate: X.X% (read: XX.XXGB, pruned: XX.XXGB)" format
- **Percentage calculations**: Preserve accurate percentage values in bottleneck analysis (XX% of total)
- **Recommended vs current comparison**: Do not delete comparison information between recommended and current clustering keys
- **Numerical metrics**: Do not delete execution time, data read volume, spill volume, etc.
- **SQL implementation examples**: Do not delete specific examples of ALTER TABLE and CLUSTER BY syntax

„ÄêProcessing Requirements„Äë
1. Must use the above heading structure
2. Merge major issues and performance indicators into integrated table format
3. List only actually applied optimization techniques (do not list non-implemented techniques)
4. Show specific cost effects with numerical values
5. Completely preserve technical information and metrics (especially the important information above)
6. Maintain TOP10 display
7. Keep emoji visual displays (üö® CRITICAL, ‚ö†Ô∏è HIGH, ‚úÖ Good, etc.)
8. Remove only unnecessary separator characters (======== etc.)
9. Absolutely preserve current clustering key information and filter rate information

„ÄêCurrent Report„Äë
```
{raw_report}
```

Please refine according to the above heading structure and output a report that completely preserves technical information.
"""
    
    try:
        provider = LLM_CONFIG.get("provider", "databricks")
        
        if provider == "databricks":
            refined_report = _call_databricks_llm(refinement_prompt)
        elif provider == "openai":
            refined_report = _call_openai_llm(refinement_prompt)
        elif provider == "azure_openai":
            refined_report = _call_azure_openai_llm(refinement_prompt)
        elif provider == "anthropic":
            refined_report = _call_anthropic_llm(refinement_prompt)
        else:
            raise ValueError(f"Unsupported LLM provider: {provider}")
        
        # üö® LLM„Ç®„É©„Éº„É¨„Çπ„Éù„É≥„Çπ„ÅÆÊ§úÂá∫ÔºàÁ≤æÂØÜÂåñÔºâ
        if isinstance(refined_report, str):
            # „Çà„ÇäÁ≤æÂØÜ„Å™„Ç®„É©„ÉºÊ§úÂá∫Ôºà„É¨„Éù„Éº„ÉàÂÜÖÂÆπ„ÅÆÁµµÊñáÂ≠ó„Å®Âå∫Âà•Ôºâ
            actual_error_indicators = [
                "API„Ç®„É©„Éº: „Çπ„ÉÜ„Éº„Çø„Çπ„Ç≥„Éº„Éâ",
                "Input is too long for requested model",
                "Bad Request",
                "„Çø„Ç§„É†„Ç¢„Ç¶„Éà„Ç®„É©„Éº:",
                "APIÂëº„Å≥Âá∫„Åó„Ç®„É©„Éº:",
                '„É¨„Çπ„Éù„É≥„Çπ: {"error_code":',
                "‚ùå API„Ç®„É©„Éº:",
                "‚ö†Ô∏è API„Ç®„É©„Éº:"
            ]
            
            # „Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÈñãÂßãÈÉ®ÂàÜ„Çí„ÉÅ„Çß„ÉÉ„ÇØÔºà„Çà„ÇäÂé≥ÂØÜÔºâ
            is_error_response = any(
                refined_report.strip().startswith(indicator) or 
                f"\n{indicator}" in refined_report[:500]  # ÂÖàÈ†≠500ÊñáÂ≠ó‰ª•ÂÜÖ„Åß„ÅÆ„Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏
                for indicator in actual_error_indicators
            )
            
            if is_error_response:
                print(f"‚ùå Error detected in LLM report refinement: {refined_report[:200]}...")
                print("üìÑ Returning original report")
                return raw_report
        
        # thinking_enabledÂØæÂøú
        if isinstance(refined_report, list):
            refined_report = format_thinking_response(refined_report)
        
        # signatureÊÉÖÂ†±„ÅÆÈô§Âéª
        import re
        signature_pattern = r"'signature':\s*'[A-Za-z0-9+/=]{100,}'"
        refined_report = re.sub(signature_pattern, "'signature': '[REMOVED]'", refined_report)
        
        print(f"‚úÖ LLM-based report refinement completed (Query ID: {query_id})")
        return refined_report
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error occurred during LLM-based report refinement: {str(e)}")
        print("üìÑ Returning original report")
        return raw_report

def validate_and_fix_sql_syntax(sql_query: str) -> str:
    """
    SQLÊßãÊñá„ÅÆÂü∫Êú¨„ÉÅ„Çß„ÉÉ„ÇØ„Å®‰øÆÊ≠£„ÇíË°å„ÅÜÔºàÊßãÊñá„Ç®„É©„ÉºÈò≤Ê≠¢Ôºâ
    
    ‰∏ªË¶Å„ÉÅ„Çß„ÉÉ„ÇØÈ†ÖÁõÆÔºö
    1. BROADCAST„Éí„É≥„Éà„ÅÆÈÖçÁΩÆ‰ΩçÁΩÆÊ§úË®º
    2. ÂÆåÂÖ®ÊÄß„ÉÅ„Çß„ÉÉ„ÇØÔºàSELECT„ÄÅFROM„ÄÅWHEREÁ≠â„ÅÆÂü∫Êú¨ÊßãÊñáÔºâ
    3. Âü∫Êú¨ÁöÑ„Å™ÊßãÊñá„Ç®„É©„Éº„ÅÆ‰øÆÊ≠£
    4. „Ç≥„É°„É≥„Éà„ÇÑ„Éó„É¨„Éº„Çπ„Éõ„É´„ÉÄ„Éº„ÅÆÈô§Âéª
    
    Args:
        sql_query: „ÉÅ„Çß„ÉÉ„ÇØÂØæË±°„ÅÆSQL„ÇØ„Ç®„É™
        
    Returns:
        str: ‰øÆÊ≠£„Åï„Çå„ÅüSQL„ÇØ„Ç®„É™
    """
    import re
    
    if not sql_query or not sql_query.strip():
        return ""
    
    # Âü∫Êú¨ÁöÑ„Å™„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó
    sql_query = sql_query.strip()
    
    # 1. BROADCAST„Éí„É≥„Éà„ÅÆÈÖçÁΩÆ‰ΩçÁΩÆ„ÉÅ„Çß„ÉÉ„ÇØ
    sql_query = fix_broadcast_hint_placement(sql_query)
    
    # 2. ‰∏çÂÆåÂÖ®„Å™SQLÊßãÊñá„ÅÆÊ§úÂá∫„Å®‰øÆÊ≠£
    sql_query = fix_incomplete_sql_syntax(sql_query)
    
    # 3. „Éó„É¨„Éº„Çπ„Éõ„É´„ÉÄ„Éº„ÇÑÁúÅÁï•Ë®òÂè∑„ÅÆÈô§Âéª
    sql_query = remove_sql_placeholders(sql_query)
    
    # 4. Âü∫Êú¨ÁöÑ„Å™ÊßãÊñá„Ç®„É©„Éº„ÅÆ‰øÆÊ≠£
    sql_query = fix_basic_syntax_errors(sql_query)
    
    return sql_query

def fix_broadcast_hint_placement(sql_query: str) -> str:
    """
    BROADCAST„Éí„É≥„Éà„ÅÆÈÖçÁΩÆ‰ΩçÁΩÆ„Çí‰øÆÊ≠£Ôºà„Çµ„Éñ„ÇØ„Ç®„É™ÂÜÖÈÉ®ÈÖçÁΩÆ„ÇíÁ¶ÅÊ≠¢Ôºâ
    
    ‰øÆÊ≠£ÂÜÖÂÆπÔºö
    - „Çµ„Éñ„ÇØ„Ç®„É™ÂÜÖÈÉ®„ÅÆBROADCAST„Éí„É≥„Éà„Çí„É°„Ç§„É≥„ÇØ„Ç®„É™„Å´ÁßªÂãï
    - FROMÂè•„ÄÅJOINÂè•„ÄÅWHEREÂè•ÂÜÖ„ÅÆ„Éí„É≥„Éà„ÇíÂâäÈô§
    - Ë§áÊï∞„ÅÆBROADCAST„Éí„É≥„Éà„ÇíÁµ±Âêà
    - DISTINCTÂè•„ÅÆ‰øùÊåÅ„ÇíÁ¢∫‰øù
    """
    import re
    
    # „Çµ„Éñ„ÇØ„Ç®„É™ÂÜÖÈÉ®„ÅÆBROADCAST„Éí„É≥„Éà„ÇíÊ§úÂá∫„Å®ÂâäÈô§
    # „Éë„Çø„Éº„É≥1: LEFT JOIN (SELECT /*+ BROADCAST(...) */ ... „ÅÆ„Éë„Çø„Éº„É≥
    subquery_broadcast_pattern = r'JOIN\s*\(\s*SELECT\s*/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
    sql_query = re.sub(subquery_broadcast_pattern, 'JOIN (\n  SELECT', sql_query, flags=re.IGNORECASE)
    
    # „Éë„Çø„Éº„É≥2: WITHÂè•„ÇÑ„Çµ„Éñ„ÇØ„Ç®„É™ÂÜÖÈÉ®„ÅÆBROADCAST„Éí„É≥„Éà
    cte_broadcast_pattern = r'(WITH\s+\w+\s+AS\s*\(\s*SELECT\s*)/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
    sql_query = re.sub(cte_broadcast_pattern, r'\1', sql_query, flags=re.IGNORECASE)
    
    # „Éë„Çø„Éº„É≥3: FROMÂè•ÂÜÖ„ÅÆBROADCAST„Éí„É≥„Éà
    from_broadcast_pattern = r'FROM\s+\w+\s*/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
    sql_query = re.sub(from_broadcast_pattern, 'FROM', sql_query, flags=re.IGNORECASE)
    
    # „Éë„Çø„Éº„É≥4: WHEREÂè•ÂÜÖ„ÅÆBROADCAST„Éí„É≥„Éà
    where_broadcast_pattern = r'WHERE\s*/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
    sql_query = re.sub(where_broadcast_pattern, 'WHERE', sql_query, flags=re.IGNORECASE)
    
    # DISTINCTÂè•„ÅÆÂ≠òÂú®Á¢∫Ë™çÔºàÂ§ßÊñáÂ≠óÂ∞èÊñáÂ≠ó„ÇíÂå∫Âà•„Åó„Å™„ÅÑÔºâ
    distinct_pattern = r'^\s*SELECT\s*(/\*\+[^*]*\*/)?\s*DISTINCT\b'
    has_distinct = bool(re.search(distinct_pattern, sql_query, re.IGNORECASE))
    
    # BROADCAST„Éí„É≥„Éà„Åå„É°„Ç§„É≥„ÇØ„Ç®„É™„ÅÆSELECTÁõ¥Âæå„Å´„ÅÇ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ
    main_select_pattern = r'^\s*SELECT\s*(/\*\+[^*]*\*/)?\s*(DISTINCT\s*)?'
    if not re.search(main_select_pattern, sql_query, re.IGNORECASE):
        # „É°„Ç§„É≥„ÇØ„Ç®„É™„ÅÆSELECTÁõ¥Âæå„Å´BROADCAST„Éí„É≥„Éà„Åå„Å™„ÅÑÂ†¥Âêà„ÅÆÂá¶ÁêÜ
        # ÂâäÈô§„Åï„Çå„ÅüBROADCAST„Éí„É≥„Éà„ÇíÂæ©ÂÖÉ„Åó„Å¶„É°„Ç§„É≥„ÇØ„Ç®„É™„Å´ÈÖçÁΩÆ
        broadcast_tables = extract_broadcast_tables_from_sql(sql_query)
        if broadcast_tables:
            broadcast_hint = f"/*+ BROADCAST({', '.join(broadcast_tables)}) */"
            if has_distinct:
                # DISTINCTÂè•„Åå„ÅÇ„ÇãÂ†¥ÂêàÔºöSELECT /*+ BROADCAST(...) */ DISTINCT „ÅÆÂΩ¢Âºè„Å´„Åô„Çã
                sql_query = re.sub(r'^\s*SELECT\s*', f'SELECT {broadcast_hint} ', sql_query, flags=re.IGNORECASE)
            else:
                # DISTINCTÂè•„Åå„Å™„ÅÑÂ†¥ÂêàÔºöÂæìÊù•„ÅÆÂΩ¢Âºè
                sql_query = re.sub(r'^\s*SELECT\s*', f'SELECT {broadcast_hint}\n  ', sql_query, flags=re.IGNORECASE)
    else:
        # Êó¢„Å´„Éí„É≥„Éà„Åå„ÅÇ„ÇãÂ†¥Âêà„ÄÅDISTINCTÂè•„ÅåÊ≠£„Åó„ÅÑ‰ΩçÁΩÆ„Å´„ÅÇ„Çã„ÅãÁ¢∫Ë™ç
        # ÈñìÈÅï„Å£„ÅüÈ†ÜÂ∫èÔºàSELECT DISTINCT /*+ BROADCAST(...) */ Ôºâ„Çí‰øÆÊ≠£
        wrong_order_pattern = r'^\s*SELECT\s*DISTINCT\s*(/\*\+[^*]*\*/)'
        if re.search(wrong_order_pattern, sql_query, re.IGNORECASE):
            # ÈñìÈÅï„Å£„ÅüÈ†ÜÂ∫è„Çí‰øÆÊ≠£ÔºöSELECT DISTINCT /*+ HINT */ ‚Üí SELECT /*+ HINT */ DISTINCT
            sql_query = re.sub(wrong_order_pattern, lambda m: f'SELECT {m.group(1)} DISTINCT', sql_query, flags=re.IGNORECASE)
    
    return sql_query


def fix_join_broadcast_hint_placement(sql_query: str) -> str:
    """
    JOINÂè•ÂÜÖ„ÅÆBROADCAST„Éí„É≥„ÉàÈÖçÁΩÆ„Ç®„É©„Éº„ÇíÂº∑Âà∂‰øÆÊ≠£ÔºàPARSE_SYNTAX_ERRORÂØæÁ≠ñÔºâ
    „É¶„Éº„Ç∂„ÉºÂ†±Âëä„ÅÆ„Ç®„É©„Éº„Ç±„Éº„ÇπÔºö join /*+ BROADCAST(i) */ item i ON ...
    """
    import re
    
    try:
        # JOINÂè•ÂÜÖ„ÅÆBROADCAST„Éí„É≥„Éà„ÇíÊ§úÂá∫„ÉªÊäΩÂá∫
        join_broadcast_pattern = r'JOIN\s+/\*\+\s*BROADCAST\(([^)]+)\)\s*\*/\s*(\w+)'
        join_broadcast_matches = re.findall(join_broadcast_pattern, sql_query, re.IGNORECASE | re.MULTILINE)
        
        if not join_broadcast_matches:
            # JOINÂè•ÂÜÖ„ÅÆBROADCAST„Éí„É≥„Éà„Åå„Å™„ÅÑÂ†¥Âêà„ÅØ„Åù„ÅÆ„Åæ„ÅæËøî„Åô
            return sql_query
        
        print(f"üîß Detected BROADCAST hints in JOIN clauses: {len(join_broadcast_matches)} instances")
        
        # ÊäΩÂá∫„Åï„Çå„ÅüBROADCASTÂØæË±°„ÉÜ„Éº„Éñ„É´Âêç/„Ç®„Ç§„É™„Ç¢„ÇπÂêç„ÇíÂèéÈõÜ
        broadcast_tables = []
        for table_name, table_alias in join_broadcast_matches:
            # „Ç´„É≥„ÉûÂå∫Âàá„Çä„ÅÆÂ†¥Âêà„ÇÇËÄÉÊÖÆ
            tables = [t.strip() for t in table_name.split(',')]
            broadcast_tables.extend(tables)
            # „Ç®„Ç§„É™„Ç¢„ÇπÂêç„ÇÇËøΩÂä†ÔºàÈáçË§áÂâäÈô§„ÅØÂæå„ÅßË°å„ÅÜÔºâ
            if table_alias.strip():
                broadcast_tables.append(table_alias.strip())
        
        # ÈáçË§áÂâäÈô§
        broadcast_tables = list(set(broadcast_tables))
        print(f"üìã BROADCAST targets: {', '.join(broadcast_tables)}")
        
        # JOINÂè•ÂÜÖ„ÅÆBROADCAST„Éí„É≥„Éà„ÇíÂâäÈô§
        fixed_query = re.sub(
            r'JOIN\s+/\*\+\s*BROADCAST\([^)]+\)\s*\*/\s*',
            'JOIN ',
            sql_query,
            flags=re.IGNORECASE | re.MULTILINE
        )
        
        # „É°„Ç§„É≥„ÇØ„Ç®„É™„ÅÆÊúÄÂàù„ÅÆSELECTÊñá„ÇíÊ§úÂá∫
        select_pattern = r'^(\s*SELECT)\s+'
        select_match = re.search(select_pattern, fixed_query, re.IGNORECASE | re.MULTILINE)
        
        if select_match:
            # Êó¢Â≠ò„ÅÆ„Éí„É≥„ÉàÂè•„Åå„ÅÇ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ
            existing_hint_pattern = r'^(\s*SELECT)\s+(/\*\+[^*]*\*/)\s+'
            existing_hint_match = re.search(existing_hint_pattern, fixed_query, re.IGNORECASE | re.MULTILINE)
            
            if existing_hint_match:
                # Êó¢Â≠ò„ÅÆ„Éí„É≥„ÉàÂè•„Å´BROADCAST„ÇíËøΩÂä†
                existing_hint = existing_hint_match.group(2)
                
                # Êó¢Â≠ò„ÅÆBROADCASTÊåáÂÆö„ÇíÁ¢∫Ë™ç
                existing_broadcast_pattern = r'BROADCAST\(([^)]+)\)'
                existing_broadcast_match = re.search(existing_broadcast_pattern, existing_hint, re.IGNORECASE)
                
                if existing_broadcast_match:
                    # Êó¢Â≠ò„ÅÆBROADCASTÊåáÂÆö„Å´ËøΩÂä†
                    existing_broadcast_tables = [t.strip() for t in existing_broadcast_match.group(1).split(',')]
                    all_broadcast_tables = list(set(existing_broadcast_tables + broadcast_tables))
                    new_broadcast = f"BROADCAST({', '.join(all_broadcast_tables)})"
                    new_hint = re.sub(
                        r'BROADCAST\([^)]+\)',
                        new_broadcast,
                        existing_hint,
                        flags=re.IGNORECASE
                    )
                else:
                    # Êó¢Â≠ò„ÅÆ„Éí„É≥„ÉàÂè•„Å´BROADCAST„ÇíËøΩÂä†
                    broadcast_hint = f"BROADCAST({', '.join(broadcast_tables)})"
                    # „Éí„É≥„ÉàÂè•„ÅÆÊú´Â∞æ„ÅÆ */ „ÅÆÂâç„Å´ËøΩÂä†
                    new_hint = existing_hint.replace('*/', f', {broadcast_hint} */')
                
                # „Éí„É≥„ÉàÂè•„ÇíÁΩÆÊèõ
                fixed_query = re.sub(
                    r'^(\s*SELECT)\s+(/\*\+[^*]*\*/)\s+',
                    f'{select_match.group(1)} {new_hint} ',
                    fixed_query,
                    flags=re.IGNORECASE | re.MULTILINE
                )
            else:
                # Êñ∞„Åó„Åè„Éí„É≥„ÉàÂè•„ÇíËøΩÂä†
                broadcast_hint = f"/*+ BROADCAST({', '.join(broadcast_tables)}) */"
                fixed_query = re.sub(
                    r'^(\s*SELECT)\s+',
                    f'{select_match.group(1)} {broadcast_hint} ',
                    fixed_query,
                    flags=re.IGNORECASE | re.MULTILINE
                )
            
            print(f"‚úÖ Completed moving BROADCAST hints to correct positions")
            return fixed_query
        else:
            print("‚ö†Ô∏è Main query SELECT statement not found, returning original query")
            return sql_query
            
    except Exception as e:
        print(f"‚ö†Ô∏è Error in JOIN BROADCAST placement correction: {str(e)}")
        print("üîÑ Returning original query")
        return sql_query


def enhance_error_correction_with_syntax_validation(corrected_query: str, original_query: str, error_info: str) -> str:
    """
    „Ç®„É©„Éº‰øÆÊ≠£Âæå„ÅÆ„ÇØ„Ç®„É™„ÇíÊ§úË®º„Åó„ÄÅPARSE_SYNTAX_ERROR„ÅåËß£Ê±∫„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÅØÂÖÉ„ÇØ„Ç®„É™„Å´„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
    """
    
    try:
        # ‰øÆÊ≠£„Åï„Çå„Åü„ÇØ„Ç®„É™„ÅÆÂæåÂá¶ÁêÜ
        print("üîß Executing post-processing of corrected query...")
        
        # JOINÂè•ÂÜÖ„ÅÆBROADCASTÈÖçÁΩÆ„ÅÆÂº∑Âà∂‰øÆÊ≠£
        final_query = fix_join_broadcast_hint_placement(corrected_query)
        
        # Âü∫Êú¨ÁöÑ„Å™ÊßãÊñá„ÉÅ„Çß„ÉÉ„ÇØ
        if "/*+" in error_info and "PARSE_SYNTAX_ERROR" in error_info:
            # PARSE_SYNTAX_ERROR„ÅÆÂ†¥Âêà„ÅØÁâπ„Å´Âé≥Ê†º„Å´„ÉÅ„Çß„ÉÉ„ÇØ
            
            # JOINÂè•ÂÜÖ„ÅÆBROADCAST„Éí„É≥„Éà„ÅåÊÆã„Å£„Å¶„ÅÑ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ
            import re
            join_broadcast_pattern = r'JOIN\s+/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
            if re.search(join_broadcast_pattern, final_query, re.IGNORECASE | re.MULTILINE):
                print("üö® BROADCAST hints still remain in JOIN clauses after correction, using original query")
                return f"""-- ‚ùå PARSE_SYNTAX_ERROR‰øÆÊ≠£Â§±Êïó„ÅÆ„Åü„ÇÅ„ÄÅÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Çí‰ΩøÁî®
-- üìã „Ç®„É©„ÉºÂÜÖÂÆπ: {error_info[:200]}
-- üí° Êé®Â•®: ÊâãÂãï„ÅßBROADCAST„Éí„É≥„Éà„ÅÆÈÖçÁΩÆ„Çí‰øÆÊ≠£„Åó„Å¶„Åè„Å†„Åï„ÅÑ

{original_query}"""
        
        print("‚úÖ Corrected query validation completed")
        return final_query
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error in post-correction validation: {str(e)}")
        print("üîÑ Using original query for safety")
        return f"""-- ‚ùå „Ç®„É©„Éº‰øÆÊ≠£Ê§úË®º‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„ÄÅÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Çí‰ΩøÁî®
-- üìã Ê§úË®º„Ç®„É©„Éº: {str(e)}
-- üìã ÂÖÉ„ÅÆ„Ç®„É©„Éº: {error_info[:200]}

{original_query}"""


def fallback_performance_evaluation(original_explain: str, optimized_explain: str) -> Dict[str, Any]:
    """
    EXPLAIN COSTÂ§±ÊïóÊôÇ„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπË©ï‰æ°
    EXPLAINÁµêÊûú„ÅÆ„Éó„É©„É≥Ë§áÈõëÂ∫¶„Å®PhotonÂà©Áî®Â∫¶„ÅßÁ∞°ÊòìÊØîËºÉ
    """
    
    try:
        import re
        
        # „Éó„É©„É≥Ë§áÈõëÂ∫¶„ÅÆË©ï‰æ°
        def analyze_plan_complexity(explain_text):
            metrics = {
                'join_count': 0,
                'scan_count': 0,
                'exchange_count': 0,
                'photon_ops': 0,
                'plan_depth': 0,
                'total_operations': 0
            }
            
            # JOINÊìç‰Ωú„Ç´„Ç¶„É≥„Éà
            metrics['join_count'] = len(re.findall(r'\bJoin\b|\bBroadcastHashJoin\b|\bSortMergeJoin\b', explain_text, re.IGNORECASE))
            
            # SCANÊìç‰Ωú„Ç´„Ç¶„É≥„Éà
            metrics['scan_count'] = len(re.findall(r'\bScan\b|\bFileScan\b|\bTableScan\b', explain_text, re.IGNORECASE))
            
            # ExchangeÊìç‰Ωú„Ç´„Ç¶„É≥„ÉàÔºàShuffleÔºâ
            metrics['exchange_count'] = len(re.findall(r'\bExchange\b|\bShuffle\b', explain_text, re.IGNORECASE))
            
            # PhotonÊìç‰Ωú„Ç´„Ç¶„É≥„Éà
            metrics['photon_ops'] = len(re.findall(r'\bPhoton\w*\b', explain_text, re.IGNORECASE))
            
            # „Éó„É©„É≥Ê∑±Â∫¶„ÅÆÊé®ÂÆöÔºà„Ç§„É≥„Éá„É≥„ÉàÊï∞„ÅÆÊúÄÂ§ßÂÄ§Ôºâ
            lines = explain_text.split('\n')
            max_indent = 0
            for line in lines:
                if line.strip():
                    indent_level = (len(line) - len(line.lstrip(' +'))) // 2
                    max_indent = max(max_indent, indent_level)
            metrics['plan_depth'] = max_indent
            
            # Á∑èÊìç‰ΩúÊï∞
            metrics['total_operations'] = metrics['join_count'] + metrics['scan_count'] + metrics['exchange_count']
            
            return metrics
        
        original_metrics = analyze_plan_complexity(original_explain)
        optimized_metrics = analyze_plan_complexity(optimized_explain)
        
        # ÊîπÂñÑ„Éù„Ç§„É≥„Éà„ÅÆË©ï‰æ°
        improvements = []
        concerns = []
        
        # JOINÂäπÁéáÂåñ
        if optimized_metrics['join_count'] < original_metrics['join_count']:
            improvements.append(f"JOINÂäπÁéáÂåñ: {original_metrics['join_count']} ‚Üí {optimized_metrics['join_count']}Êìç‰Ωú")
        elif optimized_metrics['join_count'] > original_metrics['join_count']:
            concerns.append(f"JOINÊìç‰ΩúÂ¢óÂä†: {original_metrics['join_count']} ‚Üí {optimized_metrics['join_count']}Êìç‰Ωú")
        
        # PhotonÊ¥ªÁî®Â∫¶
        if optimized_metrics['photon_ops'] > original_metrics['photon_ops']:
            improvements.append(f"PhotonÊ¥ªÁî®Êã°Â§ß: {original_metrics['photon_ops']} ‚Üí {optimized_metrics['photon_ops']}Êìç‰Ωú")
        elif optimized_metrics['photon_ops'] < original_metrics['photon_ops']:
            concerns.append(f"PhotonÊ¥ªÁî®Ê∏õÂ∞ë: {original_metrics['photon_ops']} ‚Üí {optimized_metrics['photon_ops']}Êìç‰Ωú")
        
        # Exchange/ShuffleÂäπÁéáÂåñ
        if optimized_metrics['exchange_count'] < original_metrics['exchange_count']:
            improvements.append(f"ShuffleÂâäÊ∏õ: {original_metrics['exchange_count']} ‚Üí {optimized_metrics['exchange_count']}Êìç‰Ωú")
        elif optimized_metrics['exchange_count'] > original_metrics['exchange_count']:
            concerns.append(f"ShuffleÂ¢óÂä†: {original_metrics['exchange_count']} ‚Üí {optimized_metrics['exchange_count']}Êìç‰Ωú")
        
        # „Éó„É©„É≥Ë§áÈõëÂ∫¶
        if optimized_metrics['plan_depth'] < original_metrics['plan_depth']:
            improvements.append(f"„Éó„É©„É≥Á∞°Á¥†Âåñ: Ê∑±Â∫¶{original_metrics['plan_depth']} ‚Üí {optimized_metrics['plan_depth']}")
        elif optimized_metrics['plan_depth'] > original_metrics['plan_depth']:
            concerns.append(f"„Éó„É©„É≥Ë§áÈõëÂåñ: Ê∑±Â∫¶{original_metrics['plan_depth']} ‚Üí {optimized_metrics['plan_depth']}")
        
        # Á∑èÂêàË©ï‰æ°
        improvement_score = len(improvements)
        concern_score = len(concerns)
        
        if improvement_score > concern_score:
            overall_status = "improvement_likely"
            recommendation = "use_optimized"
            summary = "‚úÖ ÂÆüË°å„Éó„É©„É≥ÂàÜÊûê„Å´„Çà„Çä„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊîπÂñÑ„ÅÆÂèØËÉΩÊÄß„ÅåÈ´ò„ÅÑ"
        elif concern_score > improvement_score:
            overall_status = "degradation_possible"
            recommendation = "use_original"
            summary = "‚ö†Ô∏è ÂÆüË°å„Éó„É©„É≥ÂàÜÊûê„Å´„Çà„Çä„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊÇ™Âåñ„ÅÆÂèØËÉΩÊÄß„ÅÇ„Çä"
        else:
            overall_status = "neutral"
            recommendation = "use_optimized"
            summary = "‚ûñ ÂÆüË°å„Éó„É©„É≥ÂàÜÊûê„Åß„ÅØÂ§ß„Åç„Å™Â§âÂåñ„Å™„ÅóÔºàÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™„ÇíÊé®Â•®Ôºâ"
        
        return {
            'evaluation_type': 'fallback_plan_analysis',
            'original_metrics': original_metrics,
            'optimized_metrics': optimized_metrics,
            'improvements': improvements,
            'concerns': concerns,
            'overall_status': overall_status,
            'recommendation': recommendation,
            'summary': summary,
            'confidence': 'medium',
            'details': improvements + concerns if improvements or concerns else ["ÂÆüË°å„Éó„É©„É≥„Å´Â§ß„Åç„Å™Â§âÂåñ„Å™„Åó"]
        }
        
    except Exception as e:
        return {
            'evaluation_type': 'fallback_error',
            'error': str(e),
            'overall_status': 'unknown',
            'recommendation': 'use_optimized',
            'summary': f"‚ö†Ô∏è „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØË©ï‰æ°„Åß„Ç®„É©„Éº: {str(e)}ÔºàÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™„ÇíÊé®Â•®Ôºâ",
            'confidence': 'low',
            'details': [f"Ë©ï‰æ°„Ç®„É©„Éº: {str(e)}", "‰øùÂÆàÁöÑ„Å´ÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™„ÇíÊé®Â•®"]
        }


def generate_fallback_performance_section(fallback_evaluation: Dict[str, Any], language: str = 'ja') -> str:
    """
    „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπË©ï‰æ°„ÅÆ„É¨„Éù„Éº„Éà„Çª„ÇØ„Ç∑„Éß„É≥ÁîüÊàê
    """
    
    if not fallback_evaluation:
        return ""
    
    if language == 'ja':
        section = f"""

### üîç 5. Á∞°Êòì„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπË©ï‰æ°ÁµêÊûúÔºàEXPLAIN COST‰ª£ÊõøÔºâ

**üìä Ë©ï‰æ°ÁµêÊûú**: {fallback_evaluation['summary']}

#### üéØ ÂÆüË°å„Éó„É©„É≥ÂàÜÊûê„Å´„Çà„ÇãË©ï‰æ°

**‰ø°È†ºÂ∫¶**: {fallback_evaluation['confidence'].upper()}ÔºàEXPLAINÁµêÊûú„Éô„Éº„ÇπÔºâ

**Êé®Â•®**: {'**ÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™„Çí‰ΩøÁî®**' if fallback_evaluation['recommendation'] == 'use_optimized' else '**ÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Çí‰ΩøÁî®**'}

#### üìã Ê§úÂá∫„Åï„Çå„ÅüÂ§âÂåñ

"""
        
        if fallback_evaluation.get('details'):
            for detail in fallback_evaluation['details']:
                section += f"- {detail}\n"
        else:
            section += "- ÂÆüË°å„Éó„É©„É≥„Å´Â§ß„Åç„Å™Â§âÂåñ„Å™„Åó\n"
        
        if fallback_evaluation.get('original_metrics') and fallback_evaluation.get('optimized_metrics'):
            orig = fallback_evaluation['original_metrics']
            opt = fallback_evaluation['optimized_metrics']
            
            section += f"""

#### üìä „Éó„É©„É≥Ë§áÈõëÂ∫¶ÊØîËºÉ

| È†ÖÁõÆ | ÂÖÉ„ÅÆ„ÇØ„Ç®„É™ | ÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™ | Â§âÂåñ |
|------|------------|-------------|------|
| JOINÊìç‰ΩúÊï∞ | {orig['join_count']} | {opt['join_count']} | {'‚úÖÊîπÂñÑ' if opt['join_count'] < orig['join_count'] else '‚ùåÂ¢óÂä†' if opt['join_count'] > orig['join_count'] else '‚ûñÂêåÁ≠â'} |
| PhotonÊìç‰ΩúÊï∞ | {orig['photon_ops']} | {opt['photon_ops']} | {'‚úÖÊîπÂñÑ' if opt['photon_ops'] > orig['photon_ops'] else '‚ùåÊ∏õÂ∞ë' if opt['photon_ops'] < orig['photon_ops'] else '‚ûñÂêåÁ≠â'} |
| ShuffleÊìç‰ΩúÊï∞ | {orig['exchange_count']} | {opt['exchange_count']} | {'‚úÖÊîπÂñÑ' if opt['exchange_count'] < orig['exchange_count'] else '‚ùåÂ¢óÂä†' if opt['exchange_count'] > orig['exchange_count'] else '‚ûñÂêåÁ≠â'} |
| „Éó„É©„É≥Ê∑±Â∫¶ | {orig['plan_depth']} | {opt['plan_depth']} | {'‚úÖÊîπÂñÑ' if opt['plan_depth'] < orig['plan_depth'] else '‚ùåÂ¢óÂä†' if opt['plan_depth'] > orig['plan_depth'] else '‚ûñÂêåÁ≠â'} |

"""
        
        section += f"""

#### ‚ö†Ô∏è Ë©ï‰æ°„ÅÆÂà∂Èôê‰∫ãÈ†Ö

- **EXPLAIN COSTÊú™ÂèñÂæó**: Ê≠£Á¢∫„Å™„Ç≥„Çπ„Éà„Éª„É°„É¢„É™‰ΩøÁî®ÈáèÊØîËºÉ‰∏çÂèØ
- **ÂÆüË°åÁµ±Ë®à‰∏çÊòé**: ÂÆüÈöõ„ÅÆÂÆüË°åÊôÇÈñì„ÇÑ„É™„ÇΩ„Éº„Çπ‰ΩøÁî®Èáè„ÅØ‰∏çÊòé
- **Êé®ÂÆö„Éô„Éº„Çπ**: ÂÆüË°å„Éó„É©„É≥ÊßãÈÄ†„Åã„Çâ„ÅÆÊé®ÂÆöË©ï‰æ°„ÅÆ„Åø
- **Êé®Â•®**: ÂèØËÉΩ„Åß„ÅÇ„Çå„Å∞ÂÆüÈöõ„ÅÆÂÆüË°å„ÉÜ„Çπ„Éà„ÅßÁ¢∫Ë™ç„Åô„Çã„Åì„Å®„ÇíÊé®Â•®

üí° **„Çà„ÇäÊ≠£Á¢∫„Å™Ë©ï‰æ°„ÅÆ„Åü„ÇÅ**: AMBIGUOUS_REFERENCEÁ≠â„ÅÆ„Ç®„É©„Éº„ÇíËß£Ê±∫„Åó„Å¶EXPLAIN COST„ÇíÂÆüË°å„Åô„Çã„Åì„Å®„ÇíÊé®Â•®
"""
        
    return section


def fix_common_ambiguous_references(sql_query: str) -> str:
    """
    „ÄêÂªÉÊ≠¢„ÄëÊ≠£Ë¶èË°®Áèæ„Å´„Çà„Çã‰øÆÊ≠£„ÅØÂªÉÊ≠¢ - LLM„Å´„Çà„ÇãÈ´òÂ∫¶„Å™‰øÆÊ≠£„Å´ÂÆåÂÖ®‰æùÂ≠ò
    """
    print("üö´ Regex-based pre-correction discontinued: Relying on advanced LLM-based correction")
    return sql_query


def fix_incomplete_sql_syntax(sql_query: str) -> str:
    """
    ‰∏çÂÆåÂÖ®„Å™SQLÊßãÊñá„ÅÆÊ§úÂá∫„Å®‰øÆÊ≠£
    """
    import re
    
    # Âü∫Êú¨ÁöÑ„Å™SQL„Ç≠„Éº„ÉØ„Éº„Éâ„ÅÆÂ≠òÂú®„ÉÅ„Çß„ÉÉ„ÇØ
    has_select = bool(re.search(r'\bSELECT\b', sql_query, re.IGNORECASE))
    has_from = bool(re.search(r'\bFROM\b', sql_query, re.IGNORECASE))
    
    # SELECT„Åå„Å™„ÅÑÂ†¥Âêà„ÅØÂü∫Êú¨ÁöÑ„Å™SQL„Åß„ÅØ„Å™„ÅÑÂèØËÉΩÊÄß„ÅåÈ´ò„ÅÑ
    if not has_select:
        return sql_query
    
    # FROM„Åå„Å™„ÅÑÂ†¥Âêà„ÅØ‰∏çÂÆåÂÖ®„Å™SQL„ÅÆÂèØËÉΩÊÄß
    if not has_from:
        # ‰∏çÂÆåÂÖ®„Å™SQL„ÅÆÂ†¥Âêà„ÅØ„Ç≥„É°„É≥„Éà„ÅßË≠¶Âëä„ÇíËøΩÂä†
        sql_query = f"-- ‚ö†Ô∏è ‰∏çÂÆåÂÖ®„Å™SQLÊßãÊñá„ÅåÊ§úÂá∫„Åï„Çå„Åæ„Åó„Åü„ÄÇÊâãÂãï„ÅßÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n{sql_query}"
    
    return sql_query

def remove_sql_placeholders(sql_query: str) -> str:
    """
    „Éó„É¨„Éº„Çπ„Éõ„É´„ÉÄ„Éº„ÇÑÁúÅÁï•Ë®òÂè∑„ÅÆÈô§ÂéªÔºàSQL„Éí„É≥„Éà„ÅØ‰øùÊåÅÔºâ
    """
    import re
    
    # ‰∏ÄËà¨ÁöÑ„Å™„Éó„É¨„Éº„Çπ„Éõ„É´„ÉÄ„Éº„Éë„Çø„Éº„É≥ÔºàSQL„Éí„É≥„Éà„ÅØÈô§Â§ñÔºâ
    placeholders = [
        r'\.\.\.',  # ÁúÅÁï•Ë®òÂè∑
        r'\[ÁúÅÁï•\]',  # ÁúÅÁï•Ë°®Ë®ò
        r'\[„Ç´„É©„É†Âêç\]',  # „Éó„É¨„Éº„Çπ„Éõ„É´„ÉÄ„Éº
        r'\[„ÉÜ„Éº„Éñ„É´Âêç\]',  # „Éó„É¨„Éº„Çπ„Éõ„É´„ÉÄ„Éº
        r'column1, column2, \.\.\.',  # „Ç´„É©„É†ÁúÅÁï•
        r'-- \.\.\.',  # „Ç≥„É°„É≥„ÉàÂÜÖ„ÅÆÁúÅÁï•
        r'column1, column2, \.\.\.',  # „Ç´„É©„É†ÁúÅÁï•„Éë„Çø„Éº„É≥
        r', \.\.\.',  # Êú´Â∞æ„ÅÆÁúÅÁï•Ë®òÂè∑
        r'ÂÆåÂÖ®„Å™SQL - „Åô„Åπ„Å¶„ÅÆ„Ç´„É©„É†.*?„ÇíÁúÅÁï•„Å™„Åó„ÅßË®òËø∞',  # ÊåáÁ§∫Êñá„ÅÆÈô§Âéª
        r'\[ÂÆåÂÖ®„Å™SQL.*?\]',  # ÂÆåÂÖ®„Å™SQLÊåáÁ§∫„ÅÆÈô§Âéª
    ]
    
    for pattern in placeholders:
        sql_query = re.sub(pattern, '', sql_query, flags=re.IGNORECASE)
    
    # SQL„Éí„É≥„Éà‰ª•Â§ñ„ÅÆË§áÊï∞Ë°å„Ç≥„É°„É≥„Éà„ÇíÈô§ÂéªÔºà„Éí„É≥„Éà„ÅØ‰øùÊåÅÔºâ
    # /*+ ... */ ÂΩ¢Âºè„ÅÆ„Éí„É≥„Éà„ÅØ‰øùÊåÅ„Åó„ÄÅ„Åù„ÅÆ‰ªñ„ÅÆ /* ... */ „Ç≥„É°„É≥„Éà„ÅÆ„ÅøÂâäÈô§
    sql_query = re.sub(r'/\*(?!\+).*?\*/', '', sql_query, flags=re.DOTALL)
    
    # ‰∏çÂÆåÂÖ®„Å™SQLÊåáÁ§∫„Ç≥„É°„É≥„Éà„ÇíÈô§Âéª
    instruction_comments = [
        r'-- üö® ÈáçË¶Å:.*',
        r'-- ‰æã:.*',
        r'-- Ë§áÊï∞„Éí„É≥„Éà‰æã.*',
        r'-- ÁÑ°Âäπ„Å™‰æã:.*',
        r'-- üö® REPARTITION„Éí„É≥„Éà.*',
    ]
    
    for pattern in instruction_comments:
        sql_query = re.sub(pattern, '', sql_query, flags=re.IGNORECASE)
    
    # Á©∫Ë°å„ÇíÊ≠£Ë¶èÂåñ
    sql_query = re.sub(r'\n\s*\n\s*\n+', '\n\n', sql_query)
    
    return sql_query.strip()

def fix_basic_syntax_errors(sql_query: str) -> str:
    """
    Âü∫Êú¨ÁöÑ„Å™ÊßãÊñá„Ç®„É©„Éº„ÅÆ‰øÆÊ≠£
    """
    import re
    
    # 1. NULL„É™„ÉÜ„É©„É´„ÅÆÂûã„Ç≠„É£„Çπ„Éà‰øÆÊ≠£ - „Ç≥„É°„É≥„Éà„Ç¢„Ç¶„ÉàÔºàÂÜóÈï∑CASTÁîüÊàê„ÅÆÂéüÂõ†Ôºâ
    # SELECT null as col01 ‚Üí SELECT cast(null as String) as col01
    # null_literal_pattern = r'\bnull\s+as\s+(\w+)'
    # sql_query = re.sub(null_literal_pattern, r'cast(null as String) as \1', sql_query, flags=re.IGNORECASE)
    
    # 2. ÈÄ£Á∂ö„Åô„Çã„Ç´„É≥„Éû„ÅÆ‰øÆÊ≠£
    sql_query = re.sub(r',\s*,', ',', sql_query)
    
    # 3. ‰∏çÊ≠£„Å™Á©∫ÁôΩ„ÅÆ‰øÆÊ≠£ÔºàË°åÂÜÖ„ÅÆÈÄ£Á∂ö„Åô„ÇãÁ©∫ÁôΩ„Çí1„Å§„Å´Ôºâ
    sql_query = re.sub(r'[ \t]+', ' ', sql_query)
    
    # 4. Ë°åÊú´„ÅÆ‰∏çË¶Å„Å™ÊñáÂ≠óÂâäÈô§
    sql_query = re.sub(r'[,;]\s*$', '', sql_query.strip())
    
    # 5. ‰∏çÂÆåÂÖ®„Å™SELECTÊñá„ÅÆ‰øÆÊ≠£
    # SELECT„ÅÆÂæå„Å´Áõ¥Êé•FROM„ÅåÊù•„ÇãÂ†¥Âêà„Çí‰øÆÊ≠£
    sql_query = re.sub(r'SELECT\s+FROM', 'SELECT *\nFROM', sql_query, flags=re.IGNORECASE)
    
    # 6. ‰∏çÂÆåÂÖ®„Å™JOINÂè•„ÅÆ‰øÆÊ≠£
    # JOIN„ÅÆÂæå„Å´ON„ÅåÊù•„Å™„ÅÑÂ†¥Âêà„ÅÆÂü∫Êú¨ÁöÑ„Å™‰øÆÊ≠£
    lines = sql_query.split('\n')
    fixed_lines = []
    
    for line in lines:
        line = line.strip()
        if line:
            # JOIN„ÅÆÂæå„Å´ON„Åå„Å™„ÅÑÂ†¥Âêà„ÅÆË≠¶Âëä„Ç≥„É°„É≥„ÉàËøΩÂä†
            if re.search(r'\bJOIN\s+\w+\s*$', line, re.IGNORECASE):
                fixed_lines.append(line)
                fixed_lines.append('  -- ‚ö†Ô∏è JOINÊù°‰ª∂ÔºàONÂè•Ôºâ„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ')
            else:
                fixed_lines.append(line)
    
    sql_query = '\n'.join(fixed_lines)
    
    # 7. Âü∫Êú¨ÁöÑ„Å™ÊßãÊñá„ÉÅ„Çß„ÉÉ„ÇØ
    sql_query = add_syntax_warnings(sql_query)
    
    return sql_query

def add_syntax_warnings(sql_query: str) -> str:
    """
    Âü∫Êú¨ÁöÑ„Å™ÊßãÊñá„ÉÅ„Çß„ÉÉ„ÇØ„Å®Ë≠¶Âëä„ÅÆËøΩÂä†
    """
    import re
    
    warnings = []
    
    # Âü∫Êú¨ÁöÑ„Å™SQL„Ç≠„Éº„ÉØ„Éº„Éâ„ÅÆÂ≠òÂú®„ÉÅ„Çß„ÉÉ„ÇØ
    has_select = bool(re.search(r'\bSELECT\b', sql_query, re.IGNORECASE))
    has_from = bool(re.search(r'\bFROM\b', sql_query, re.IGNORECASE))
    
    # JOIN„Åå„ÅÇ„Çã„ÅåON„Åå„Å™„ÅÑÂ†¥Âêà
    joins = re.findall(r'\b(LEFT|RIGHT|INNER|OUTER)?\s*JOIN\s+\w+', sql_query, re.IGNORECASE)
    ons = re.findall(r'\bON\b', sql_query, re.IGNORECASE)
    
    if len(joins) > len(ons):
        warnings.append('-- ‚ö†Ô∏è JOINÂè•„ÅÆÊï∞„Å´ÂØæ„Åó„Å¶ONÂè•„Åå‰∏çË∂≥„Åó„Å¶„ÅÑ„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô')
    
    # WITHÂè•„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅÆÂü∫Êú¨„ÉÅ„Çß„ÉÉ„ÇØ
    if re.search(r'\bWITH\s+\w+\s+AS\s*\(', sql_query, re.IGNORECASE):
        if not re.search(r'\)\s*SELECT\b', sql_query, re.IGNORECASE):
            warnings.append('-- ‚ö†Ô∏è WITHÂè•„ÅÆÂæå„ÅÆ„É°„Ç§„É≥SELECTÊñá„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ')
    
    # Ë≠¶Âëä„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØÂÖàÈ†≠„Å´ËøΩÂä†
    if warnings:
        sql_query = '\n'.join(warnings) + '\n\n' + sql_query
    
    return sql_query

def extract_broadcast_tables_from_sql(sql_query: str) -> list:
    """
    SQL„ÇØ„Ç®„É™„Åã„ÇâBROADCAST„Åï„Çå„Çã„Åπ„Åç„ÉÜ„Éº„Éñ„É´Âêç„ÇíÊäΩÂá∫
    """
    import re
    
    # ÂâäÈô§„Åï„Çå„ÅüBROADCAST„Éí„É≥„Éà„Åã„Çâ„ÉÜ„Éº„Éñ„É´Âêç„ÇíÊäΩÂá∫
    broadcast_pattern = r'BROADCAST\(([^)]+)\)'
    matches = re.findall(broadcast_pattern, sql_query, re.IGNORECASE)
    
    tables = []
    for match in matches:
        # „Ç´„É≥„Éû„ÅßÂå∫Âàá„Çâ„Çå„Åü„ÉÜ„Éº„Éñ„É´Âêç„ÇíÂàÜÂâ≤
        table_names = [name.strip() for name in match.split(',')]
        tables.extend(table_names)
    
    return list(set(tables))  # ÈáçË§á„ÇíÈô§Âéª

def validate_final_sql_syntax(sql_query: str) -> bool:
    """
    ÊúÄÁµÇÁöÑ„Å™SQLÊßãÊñá„ÉÅ„Çß„ÉÉ„ÇØÔºà‰øùÂ≠òÂâç„ÅÆÁ¢∫Ë™çÔºâ
    
    Returns:
        bool: ÊßãÊñá„ÅåÊ≠£„Åó„ÅÑ„Å®Âà§ÂÆö„Åï„Çå„ÅüÂ†¥ÂêàTrue„ÄÅÂïèÈ°å„Åå„ÅÇ„ÇãÂ†¥ÂêàFalse
    """
    import re
    
    if not sql_query or not sql_query.strip():
        return False
    
    # Âü∫Êú¨ÁöÑ„Å™SQL„Ç≠„Éº„ÉØ„Éº„Éâ„ÅÆÂ≠òÂú®„ÉÅ„Çß„ÉÉ„ÇØ
    has_select = bool(re.search(r'\bSELECT\b', sql_query, re.IGNORECASE))
    
    # SELECT„Åå„Å™„ÅÑÂ†¥Âêà„ÅØ‰∏çÊ≠£
    if not has_select:
        return False
    
    # Êòé„Çâ„Åã„Å´‰∏çÂÆåÂÖ®„Å™„Éë„Çø„Éº„É≥„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ
    incomplete_patterns = [
        r'\.\.\.',  # ÁúÅÁï•Ë®òÂè∑
        r'\[ÁúÅÁï•\]',  # ÁúÅÁï•Ë°®Ë®ò
        r'\[„Ç´„É©„É†Âêç\]',  # „Éó„É¨„Éº„Çπ„Éõ„É´„ÉÄ„Éº
        r'\[„ÉÜ„Éº„Éñ„É´Âêç\]',  # „Éó„É¨„Éº„Çπ„Éõ„É´„ÉÄ„Éº
        r'column1, column2, \.\.\.',  # „Ç´„É©„É†ÁúÅÁï•
        r'ÂÆåÂÖ®„Å™SQL.*?„Çí.*?Ë®òËø∞',  # ÊåáÁ§∫Êñá
    ]
    
    for pattern in incomplete_patterns:
        if re.search(pattern, sql_query, re.IGNORECASE):
            return False
    
    # BROADCAST„Éí„É≥„ÉàÈÖçÁΩÆ„ÅÆÂü∫Êú¨„ÉÅ„Çß„ÉÉ„ÇØ
    broadcast_hints = re.findall(r'/\*\+\s*BROADCAST\([^)]+\)\s*\*/', sql_query, re.IGNORECASE)
    if broadcast_hints:
        # BROADCAST„Éí„É≥„Éà„Åå„Çµ„Éñ„ÇØ„Ç®„É™ÂÜÖÈÉ®„Å´„ÅÇ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ
        subquery_broadcast = re.search(r'JOIN\s*\(\s*SELECT\s*/\*\+\s*BROADCAST', sql_query, re.IGNORECASE)
        if subquery_broadcast:
            return False
    
    # Âü∫Êú¨ÁöÑ„Å™ÊßãÊñá„Ç®„É©„Éº„ÉÅ„Çß„ÉÉ„ÇØ
    # ÈÄ£Á∂ö„Åô„Çã„Ç´„É≥„Éû
    if re.search(r',\s*,', sql_query):
        return False
    
    # ‰∏çÊ≠£„Å™Á©∫ÁôΩ„Éë„Çø„Éº„É≥
    if re.search(r'\s{5,}', sql_query):  # 5ÂÄã‰ª•‰∏ä„ÅÆÈÄ£Á∂ö„Åô„ÇãÁ©∫ÁôΩ
        return False
    
    return True

def save_optimized_sql_files(original_query: str, optimized_result: str, metrics: Dict[str, Any], analysis_result: str = "", llm_response: str = "", performance_comparison: Dict[str, Any] = None, best_attempt_number: int = None, optimization_attempts: list = None) -> Dict[str, str]:
    """
    ÊúÄÈÅ©Âåñ„Åï„Çå„ÅüSQL„ÇØ„Ç®„É™„ÇíÂÆüË°åÂèØËÉΩ„Å™ÂΩ¢„Åß„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò
    
    ÁâπÂæ¥:
    - SQL„Éï„Ç°„Ç§„É´„ÅÆÊú´Â∞æ„Å´Ëá™Âãï„Åß„Çª„Éü„Ç≥„É≠„É≥(;)„Çí‰ªò‰∏é
    - „Åù„ÅÆ„Åæ„ÅæDatabricks Notebook„ÅßÂÆüË°åÂèØËÉΩ
    - %sql „Éû„Ç∏„ÉÉ„ÇØ„Ç≥„Éû„É≥„Éâ„Åß„ÇÇÁõ¥Êé•ÂÆüË°åÂèØËÉΩ
    - LLM„Å´„Çà„Çã„É¨„Éù„Éº„ÉàÊé®Êï≤„ÅßË™≠„Åø„ÇÑ„Åô„ÅÑÊúÄÁµÇ„É¨„Éù„Éº„Éà„ÇíÁîüÊàê
    """
    
    import re
    from datetime import datetime
    
    # thinking_enabled: True„ÅÆÂ†¥Âêà„Å´optimized_result„Åå„É™„Çπ„Éà„Å´„Å™„Çã„Åì„Å®„Åå„ÅÇ„Çã„Åü„ÇÅÂØæÂøú
    optimized_result_for_file = optimized_result
    optimized_result_main_content = optimized_result
    
    if isinstance(optimized_result, list):
        # Convert to human-readable format for file saving
        optimized_result_for_file = format_thinking_response(optimized_result)
        # SQLÊäΩÂá∫Áî®„ÅØ‰∏ªË¶Å„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅÆ„Åø„Çí‰ΩøÁî®
        optimized_result_main_content = extract_main_content_from_thinking_response(optimized_result)
    
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    query_id = metrics.get('query_info', {}).get('query_id', 'unknown')
    
    # „Ç™„É™„Ç∏„Éä„É´„ÇØ„Ç®„É™„Éï„Ç°„Ç§„É´„ÅÆ‰øùÂ≠ò„ÅØÈô§Â§ñÔºà‰∏çË¶ÅÔºâ
    original_filename = None
    
    # ÊúÄÈÅ©Âåñ„Åï„Çå„Åü„ÇØ„Ç®„É™„ÅÆÊäΩÂá∫„Å®‰øùÂ≠ò
    optimized_filename = f"output_optimized_query_{timestamp}.sql"
    
    # ÊúÄÈÅ©ÂåñÁµêÊûú„Åã„ÇâSQL„Ç≥„Éº„Éâ„ÇíÊäΩÂá∫Ôºà‰∏ªË¶Å„Ç≥„É≥„ÉÜ„É≥„ÉÑ„Åã„ÇâÊäΩÂá∫Ôºâ - ÊîπÂñÑÁâà
    sql_pattern = r'```sql\s*(.*?)\s*```'
    sql_matches = re.findall(sql_pattern, optimized_result_main_content, re.DOTALL | re.IGNORECASE)
    
    optimized_sql = ""
    if sql_matches:
        # ÊúÄ„ÇÇÈï∑„ÅÑSQL„Éñ„É≠„ÉÉ„ÇØ„Çí‰ΩøÁî®ÔºàÂÆåÂÖ®ÊÄß„ÇíÂÑ™ÂÖàÔºâ
        optimized_sql = max(sql_matches, key=len).strip()
    else:
        # SQL„Éñ„É≠„ÉÉ„ÇØ„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅSQLÈñ¢ÈÄ£„ÅÆË°å„ÇíÊäΩÂá∫ÔºàÊîπÂñÑÁâàÔºâ
        lines = optimized_result_main_content.split('\n')
        sql_lines = []
        in_sql_section = False
        
        for line in lines:
            line_stripped = line.strip()
            
            # SQL„ÅÆÈñãÂßã„ÇíÊ§úÂá∫
            if any(keyword in line.upper() for keyword in ['SELECT', 'FROM', 'WHERE', 'WITH', 'CREATE', 'INSERT', 'UPDATE', 'DELETE']):
                in_sql_section = True
            
            if in_sql_section:
                # SQL„ÅÆÁµÇ‰∫Ü„ÇíÊ§úÂá∫Ôºà„Éû„Éº„ÇØ„ÉÄ„Ç¶„É≥„Çª„ÇØ„Ç∑„Éß„É≥„ÇÑ„É¨„Éù„Éº„Éà„Çª„ÇØ„Ç∑„Éß„É≥Ôºâ
                if (line_stripped.startswith('#') or 
                    line_stripped.startswith('*') or 
                    line_stripped.startswith('##') or
                    line_stripped.startswith('**') or
                    line_stripped.startswith('---') or
                    line_stripped.startswith('===') or
                    'ÊîπÂñÑ„Éù„Ç§„É≥„Éà' in line_stripped or
                    'ÊúüÂæÖÂäπÊûú' in line_stripped or
                    'BROADCASTÈÅ©Áî®Ê†πÊã†' in line_stripped):
                    in_sql_section = False
                else:
                    # Á©∫Ë°å„ÇÑÊúâÂäπ„Å™SQLË°å„ÇíËøΩÂä†
                    sql_lines.append(line)
        
        optimized_sql = '\n'.join(sql_lines).strip()
    
    # SQLÊßãÊñá„ÅÆÂü∫Êú¨„ÉÅ„Çß„ÉÉ„ÇØÔºàÂÆåÂÖ®ÊÄßÁ¢∫Ë™çÔºâ
    if optimized_sql:
        optimized_sql = validate_and_fix_sql_syntax(optimized_sql)
    
    # ÊúÄÈÅ©Âåñ„Åï„Çå„Åü„ÇØ„Ç®„É™„Éï„Ç°„Ç§„É´„ÅÆ‰øùÂ≠òÔºà„Ç®„É©„Éº„Éè„É≥„Éâ„É™„É≥„Ç∞Âº∑ÂåñÔºâ
    try:
        with open(optimized_filename, 'w', encoding='utf-8') as f:
            f.write(f"-- ÊúÄÈÅ©Âåñ„Åï„Çå„ÅüSQL„ÇØ„Ç®„É™\n")
            f.write(f"-- ÂÖÉ„ÇØ„Ç®„É™ID: {query_id}\n")
            f.write(f"-- ÊúÄÈÅ©ÂåñÊó•ÊôÇ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"-- „Éï„Ç°„Ç§„É´: {optimized_filename}\n\n")
            
            
            # üéØ CATALOG/DATABASEË®≠ÂÆö„ÅÆËá™ÂãïËøΩÂä†
            catalog_name = globals().get("CATALOG", "tpcds")
            database_name = globals().get("DATABASE", "tpcds_sf1000_delta_lc")
            
            f.write(f"-- üóÇÔ∏è „Ç´„Çø„É≠„Ç∞„Éª„Çπ„Ç≠„Éº„ÉûË®≠ÂÆöÔºàËá™ÂãïËøΩÂä†Ôºâ\n")
            f.write(f"USE CATALOG {catalog_name};\n")
            f.write(f"USE SCHEMA {database_name};\n\n")
                
            if optimized_sql:
                # SQL„ÅÆÊú´Â∞æ„Å´„Çª„Éü„Ç≥„É≠„É≥„ÇíÁ¢∫ÂÆü„Å´ËøΩÂä†
                optimized_sql_clean = optimized_sql.strip()
                if optimized_sql_clean and not optimized_sql_clean.endswith(';'):
                    optimized_sql_clean += ';'
                
                # ÊúÄÁµÇÁöÑ„Å™ÊßãÊñá„ÉÅ„Çß„ÉÉ„ÇØ
                if validate_final_sql_syntax(optimized_sql_clean):
                    f.write(optimized_sql_clean)
                else:
                    f.write("-- ‚ö†Ô∏è ÊßãÊñá„Ç®„É©„Éº„ÅåÊ§úÂá∫„Åï„Çå„Åæ„Åó„Åü„ÄÇÊâãÂãï„ÅßÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n")
                    f.write(f"-- ÂÖÉ„ÅÆSQL:\n{optimized_sql_clean}\n")
                    f.write("-- ‰ª•‰∏ã„ÅØÊúÄÈÅ©ÂåñÂàÜÊûê„ÅÆÂÖ®ÁµêÊûú„Åß„Åô:\n\n")
                    f.write(f"/*\n{optimized_result_main_content}\n*/")
            else:
                f.write("-- ‚ö†Ô∏è SQL„Ç≥„Éº„Éâ„ÅÆËá™ÂãïÊäΩÂá∫„Å´Â§±Êïó„Åó„Åæ„Åó„Åü\n")
                f.write("-- ‰ª•‰∏ã„ÅØÊúÄÈÅ©ÂåñÂàÜÊûê„ÅÆÂÖ®ÁµêÊûú„Åß„Åô:\n\n")
                f.write(f"/*\n{optimized_result_main_content}\n*/")
    except Exception as e:
        print(f"‚ö†Ô∏è Error occurred during SQL file saving: {str(e)}")
        # Generate basic file on error
        with open(optimized_filename, 'w', encoding='utf-8') as f:
            f.write(f"-- ‚ö†Ô∏è Error occurred during SQL file saving: {str(e)}\n")
            f.write(f"-- Optimization result:\n{optimized_result_main_content}\n")
    
    # Save analysis report file (readable report refined by LLM)
    # Generate filename based on OUTPUT_LANGUAGE setting
    language_suffix = 'en' if OUTPUT_LANGUAGE == 'en' else 'jp'
    report_filename = f"output_optimization_report_{language_suffix}_{timestamp}.md"
    
    print("ü§ñ Executing LLM report refinement...")
    
    # üöÄ Load content of actually saved SQL file and use for report
    try:
        with open(optimized_filename, 'r', encoding='utf-8') as f:
            actual_sql_content = f.read()
        
        # Use actual SQL file content for report (guaranteed to work)
        print(f"‚úÖ Loaded SQL file content for report generation: {optimized_filename}")
        report_data = actual_sql_content
        
    except Exception as e:
        print(f"‚ö†Ô∏è SQL file loading failed, using initial response: {str(e)}")
        # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: ÂàùÂõû„É¨„Çπ„Éù„É≥„Çπ„Çí‰ΩøÁî®
        report_data = llm_response if llm_response else optimized_result
    
    initial_report = generate_comprehensive_optimization_report(
        query_id, report_data, metrics, analysis_result, performance_comparison, best_attempt_number, optimization_attempts
    )
    
    # LLM„Åß„É¨„Éù„Éº„Éà„ÇíÊé®Êï≤ÔºàË©≥Á¥∞„Å™ÊäÄË°ìÊÉÖÂ†±„Çí‰øùÊåÅÔºâ
    refined_report = refine_report_with_llm(initial_report, query_id)
    
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(refined_report)
    
    print(f"‚úÖ Report file saving completed: {report_filename}")
    
    # Output file results (independent TOP10 files removed and integrated into optimization report)
    result = {
        'optimized_file': optimized_filename,
        'report_file': report_filename
    }
    
    return result

def demonstrate_execution_plan_size_extraction():
    """
    ÂÆüË°å„Éó„É©„É≥„Åã„Çâ„ÅÆ„Çµ„Ç§„Ç∫Êé®ÂÆöÊ©üËÉΩ„ÅÆ„Éá„É¢„É≥„Çπ„Éà„É¨„Éº„Ç∑„Éß„É≥
    """
    print("üß™ Demo of table size estimation feature from execution plan")
    print("-" * 50)
    
    # „Çµ„É≥„Éó„É´„ÅÆ„Éó„É≠„Éï„Ç°„Ç§„É©„Éº„Éá„Éº„ÇøÊßãÈÄ†
    sample_profiler_data = {
        "executionPlan": {
            "physicalPlan": {
                "nodes": [
                    {
                        "nodeName": "Scan Delta orders",
                        "id": "1",
                        "metrics": {
                            "estimatedSizeInBytes": 10485760,  # 10MB
                            "numFiles": 5,
                            "numPartitions": 2
                        },
                        "output": "[order_id#123, customer_id#124, amount#125] orders",
                        "details": "Table: catalog.database.orders"
                    },
                    {
                        "nodeName": "Scan Delta customers",
                        "id": "2", 
                        "metrics": {
                            "estimatedSizeInBytes": 52428800,  # 50MB
                            "numFiles": 10,
                            "numPartitions": 4
                        },
                        "output": "[customer_id#126, name#127, region#128] customers"
                    }
                ]
            }
        }
    }
    
    print("üìä Sample execution plan:")
    print("  ‚Ä¢ orders table: estimatedSizeInBytes = 10,485,760 (10MB)")
    print("  ‚Ä¢ customers table: estimatedSizeInBytes = 52,428,800 (50MB)")
    print("")
    
    # „ÉÜ„Éº„Éñ„É´„Çµ„Ç§„Ç∫Êé®ÂÆö„ÅÆÂÆüË°å
    table_size_estimates = extract_table_size_estimates_from_plan(sample_profiler_data)
    
    print("üîç Extracted table size estimations:")
    if table_size_estimates:
        for table_name, size_info in table_size_estimates.items():
            print(f"  üìã {table_name}:")
            print(f"    - Size: {size_info['estimated_size_mb']:.1f}MB")
            print(f"    - Confidence: {size_info['confidence']}")
            print(f"    - Source: {size_info['source']}")
            if 'num_files' in size_info:
                print(f"    - File count: {size_info['num_files']}")
            if 'num_partitions' in size_info:
                print(f"    - Partition count: {size_info['num_partitions']}")
            print("")
    else:
        print("  ‚ö†Ô∏è Table size estimation information could not be extracted")
    
    print("üí° Impact on BROADCAST analysis:")
    if table_size_estimates:
        for table_name, size_info in table_size_estimates.items():
            size_mb = size_info['estimated_size_mb']
            if size_mb <= 30:
                print(f"  ‚úÖ {table_name}: {size_mb:.1f}MB ‚â§ 30MB ‚Üí BROADCAST recommended")
            else:
                print(f"  ‚ùå {table_name}: {size_mb:.1f}MB > 30MB ‚Üí BROADCAST not recommended")
    
    print("")
    print("üéØ Comparison with conventional estimation methods:")
    print("  üìà Conventional: Metrics-based indirect estimation (estimation accuracy: medium)")
    print("  ‚ùå New feature: Utilizing estimatedSizeInBytes from execution plan (disabled due to unavailability)")
    print("  ‚ÑπÔ∏è Current: Adopting conservative estimation with 3.0x compression ratio")
    
    return {}

print("‚úÖ Function definition completed: SQL optimization related functions (execution plan size estimation support)")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üöÄ Original Query Extraction
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Extraction of original query from profiler data
# MAGIC - Detailed display of extracted query (up to 64KB)
# MAGIC - Fallback processing (sample query configuration)

# COMMAND ----------

# üöÄ SQL„ÇØ„Ç®„É™ÊúÄÈÅ©Âåñ„ÅÆÂÆüË°å
print("\n" + "üöÄ" * 20)
print("üîß „ÄêSQL Query Optimization Execution„Äë")
print("üöÄ" * 20)

# 1. „Ç™„É™„Ç∏„Éä„É´„ÇØ„Ç®„É™„ÅÆÊäΩÂá∫
print("\nüìã Step 1: Extract Original Query")
print("-" * 40)

original_query = extract_original_query_from_profiler_data(profiler_data)

if original_query:
    print(f"‚úÖ Original query extracted ({len(original_query)} characters)")
    print(f"üîç Query preview:")
    # 64KB (65536ÊñáÂ≠ó) „Åæ„ÅßË°®Á§∫
    max_display_chars = 65536
    if len(original_query) > max_display_chars:
        preview = original_query[:max_display_chars] + f"\n... (ÊÆã„Çä {len(original_query) - max_display_chars} ÊñáÂ≠ó„ÅØÁúÅÁï•)"
    else:
        preview = original_query
    print(f"   {preview}")
else:
    print("‚ö†Ô∏è Original query not found")
    print("   Please set the query manually")
    
    # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: „Çµ„É≥„Éó„É´„ÇØ„Ç®„É™„ÇíË®≠ÂÆö
    original_query = """
    -- „Çµ„É≥„Éó„É´„ÇØ„Ç®„É™ÔºàÂÆüÈöõ„ÅÆ„ÇØ„Ç®„É™„Å´ÁΩÆ„ÅçÊèõ„Åà„Å¶„Åè„Å†„Åï„ÅÑÔºâ
    SELECT 
        customer_id,
        SUM(order_amount) as total_amount,
        COUNT(*) as order_count
    FROM orders 
    WHERE order_date >= '2023-01-01'
    GROUP BY customer_id
    ORDER BY total_amount DESC
    LIMIT 100
    """
    print(f"üìù Sample query has been set")

# üìÅ „Ç™„É™„Ç∏„Éä„É´„ÇØ„Ç®„É™„Çí„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò
print("\nüìÅ Saving original query to file")
print("-" * 40)

from datetime import datetime

# „Çø„Ç§„É†„Çπ„Çø„É≥„Éó‰ªò„Åç„Éï„Ç°„Ç§„É´Âêç„ÇíÁîüÊàê
timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
original_query_filename = f"output_original_query_{timestamp}.sql"

try:
    # „Ç´„Çø„É≠„Ç∞„Å®„Éá„Éº„Çø„Éô„Éº„ÇπË®≠ÂÆö„ÅÆÂèñÂæó
    catalog_name = globals().get('CATALOG', 'tpcds')
    database_name = globals().get('DATABASE', 'tpcds_sf1000_delta_lc')
    
    with open(original_query_filename, 'w', encoding='utf-8') as f:
        f.write(f"-- üìã „Ç™„É™„Ç∏„Éä„É´„ÇØ„Ç®„É™Ôºà„Éó„É≠„Éï„Ç°„Ç§„É©„Éº„Éá„Éº„Çø„Åã„ÇâÊäΩÂá∫Ôºâ\n")
        f.write(f"-- ÊäΩÂá∫Êó•ÊôÇ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"-- „Éï„Ç°„Ç§„É´: {original_query_filename}\n")
        f.write(f"-- „ÇØ„Ç®„É™ÊñáÂ≠óÊï∞: {len(original_query):,}\n\n")
        
        # „Ç´„Çø„É≠„Ç∞„Éª„Çπ„Ç≠„Éº„ÉûË®≠ÂÆö„ÅÆËøΩÂä†
        f.write(f"-- üóÇÔ∏è „Ç´„Çø„É≠„Ç∞„Éª„Çπ„Ç≠„Éº„ÉûË®≠ÂÆöÔºàËá™ÂãïËøΩÂä†Ôºâ\n")
        f.write(f"USE CATALOG {catalog_name};\n")
        f.write(f"USE SCHEMA {database_name};\n\n")
        
        # „Ç™„É™„Ç∏„Éä„É´„ÇØ„Ç®„É™„ÅÆÊõ∏„ÅçËæº„Åø
        f.write(f"-- üîç „Ç™„É™„Ç∏„Éä„É´„ÇØ„Ç®„É™\n")
        f.write(original_query)
        
        # „Éï„Ç°„Ç§„É´Êú´Â∞æ„Å´ÊîπË°å„ÇíËøΩÂä†
        if not original_query.endswith('\n'):
            f.write('\n')
    
    print(f"‚úÖ Original query saved: {original_query_filename}")
    print(f"üìä Saved query character count: {len(original_query):,}")
    print(f"üíæ File path: ./{original_query_filename}")
    print("üìå This file is retained as final output regardless of DEBUG_ENABLED setting")
    
except Exception as e:
    print(f"‚ùå Failed to save original query file: {str(e)}")
    print("‚ö†Ô∏è Processing continues, but original query file was not created")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîç SQL Optimization Execution
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Retrieve original query extracted in Cell 43
# MAGIC - Generate and execute EXPLAIN statements in Databricks
# MAGIC - Output execution plan details to files
# MAGIC - Error handling and result verification

# COMMAND ----------

def extract_select_from_ctas(query: str) -> str:
    """
    CREATE TABLE AS SELECT (CTAS) „ÇØ„Ç®„É™„Åã„ÇâAS‰ª•Èôç„ÅÆÈÉ®ÂàÜ„ÅÆ„Åø„ÇíÊäΩÂá∫
    
    ÂØæÂøú„Éë„Çø„Éº„É≥:
    - CREATE TABLE ... AS SELECT ...
    - CREATE OR REPLACE TABLE ... AS SELECT ...
    - CREATE TABLE ... AS WITH ... SELECT ...
    - AS „ÅÆÂæå„Çç„Å´Êã¨Âºß„Åå„Å™„ÅÑÂ†¥Âêà
    - Ë§áÊï∞Ë°å„Å´„Åæ„Åü„Åå„ÇãÂ†¥Âêà
    - „ÉÜ„Éº„Éñ„É´ÂÆöÁæ©„ÅÆË§áÈõë„Å™„Éë„Çø„Éº„É≥ÔºàUSING„ÄÅPARTITIONED BY„ÄÅTBLPROPERTIESÁ≠âÔºâ
    
    Args:
        query: ÂÖÉ„ÅÆ„ÇØ„Ç®„É™
    
    Returns:
        str: AS‰ª•Èôç„ÅÆÈÉ®ÂàÜ„ÅÆ„Åø„ÅÆ„ÇØ„Ç®„É™„ÄÅ„Åæ„Åü„ÅØCTAS„Åß„Å™„ÅÑÂ†¥Âêà„ÅØÂÖÉ„ÅÆ„ÇØ„Ç®„É™
    """
    import re
    
    # „ÇØ„Ç®„É™„ÇíÊ≠£Ë¶èÂåñÔºàÊîπË°å„ÉªÁ©∫ÁôΩ„ÇíÁµ±‰∏ÄÔºâ
    normalized_query = re.sub(r'\s+', ' ', query.strip())
    
    # CTAS „Éë„Çø„Éº„É≥„ÅÆÊ§úÂá∫ÔºàÂåÖÊã¨ÁöÑ„Å™„Éë„Çø„Éº„É≥Ôºâ
    # CREATE [OR REPLACE] TABLE ... AS ... „ÅÆÂΩ¢Âºè„ÇíÊ§úÂá∫
    # AS„Ç≠„Éº„ÉØ„Éº„Éâ„ÅÆ‰ΩçÁΩÆ„ÇíÊ≠£Á¢∫„Å´ÁâπÂÆö„Åô„Çã
    
    # CREATE [OR REPLACE] TABLEÈÉ®ÂàÜ„ÅÆÊ§úÂá∫
    create_patterns = [
        r'CREATE\s+OR\s+REPLACE\s+TABLE',
        r'CREATE\s+TABLE'
    ]
    
    for create_pattern in create_patterns:
        # CREATE TABLEÈÉ®ÂàÜ„ÇíÊ§úÂá∫
        create_match = re.search(create_pattern, normalized_query, re.IGNORECASE)
        if create_match:
            # CREATE TABLE‰ª•Èôç„ÅÆÈÉ®ÂàÜ„ÇíÂèñÂæó
            after_create = normalized_query[create_match.end():].strip()
            
            # AS „Ç≠„Éº„ÉØ„Éº„Éâ„ÅÆ‰ΩçÁΩÆ„ÇíÊ§úÁ¥¢ÔºàÂ§ßÊñáÂ≠óÂ∞èÊñáÂ≠ó„ÇíÂå∫Âà•„Åó„Å™„ÅÑÔºâ
            # AS „ÅØÂçòË™ûÂ¢ÉÁïå„ÅßÂå∫Âàá„Çâ„Çå„Å¶„ÅÑ„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã
            as_pattern = r'\bAS\b'
            as_match = re.search(as_pattern, after_create, re.IGNORECASE)
            
            if as_match:
                # AS‰ª•Èôç„ÅÆÈÉ®ÂàÜ„ÇíÂèñÂæó
                as_part = after_create[as_match.end():].strip()
                
                if as_part:
                    print(f"‚úÖ CTAS detected: Using part after AS for EXPLAIN statement")
                    print(f"üìä Original query length: {len(query):,} characters")
                    print(f"üìä Part after AS length: {len(as_part):,} characters")
                    
                    # WITHÂè•„ÅßÂßã„Åæ„ÇãÂ†¥Âêà„ÇÑSELECTÂè•„ÅßÂßã„Åæ„ÇãÂ†¥Âêà„ÇíÂà§ÂÆö
                    if as_part.upper().startswith('WITH'):
                        print("üìã Detected query starting with WITH clause")
                    elif as_part.upper().startswith('SELECT'):
                        print("üìã Detected query starting with SELECT clause")
                    else:
                        print("üìã Detected other query format")
                    
                    return as_part
    
    print("üìã Regular query: Use as is for EXPLAIN statement")
    return query

def generate_improved_query_for_performance_degradation(original_query: str, analysis_result: str, metrics: Dict[str, Any], degradation_analysis: Dict[str, Any], previous_optimized_query: str = "") -> str:
    """
    LLM optimization function specifically for performance degradation
    Apply specific improvement measures based on degradation cause analysis
    
    Args:
        original_query: Original query
        analysis_result: Bottleneck analysis result
        metrics: Metrics information
        degradation_analysis: Degradation cause analysis result
        previous_optimized_query: Previous optimized query
    """
    
    # ÊÇ™ÂåñÂàÜÊûê„ÅÆË©≥Á¥∞ÊÉÖÂ†±„ÇíÊäΩÂá∫
    primary_cause = degradation_analysis.get('primary_cause', 'unknown')
    cost_ratio = degradation_analysis.get('analysis_details', {}).get('cost_ratio', 1.0)
    specific_issues = degradation_analysis.get('specific_issues', [])
    fix_instructions = degradation_analysis.get('fix_instructions', [])
    confidence_level = degradation_analysis.get('confidence_level', 'low')
    
    # ÂâçÂõû„ÇØ„Ç®„É™„ÅÆÂàÜÊûê„Çª„ÇØ„Ç∑„Éß„É≥
    previous_query_section = ""
    if previous_optimized_query:
        previous_query_section = f"""

„Äêüö® ÂâçÂõû„ÅÆÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™Ôºà„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊÇ™ÂåñÔºâ„Äë
```sql
{previous_optimized_query}
```

**‚ùå Ê§úÂá∫„Åï„Çå„ÅüÂïèÈ°åÁÇπ:**
- ÂÆüË°å„Ç≥„Çπ„ÉàÊØî: {cost_ratio:.2f}ÂÄç„ÅÆÊÇ™Âåñ
- ‰∏ªË¶ÅÂéüÂõ†: {primary_cause}
- ÂÖ∑‰ΩìÁöÑÂïèÈ°å: {', '.join(specific_issues)}
"""

    # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊÇ™Âåñ‰øÆÊ≠£„Å´ÁâπÂåñ„Åó„Åü„Éó„É≠„É≥„Éó„Éà
    performance_improvement_prompt = f"""
„ÅÇ„Å™„Åü„ÅØDatabricks„ÅÆSQL„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊúÄÈÅ©Âåñ„ÅÆÂ∞ÇÈñÄÂÆ∂„Åß„Åô„ÄÇ

ÂâçÂõû„ÅÆÊúÄÈÅ©Âåñ„Åß„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊÇ™Âåñ„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü„ÄÇÊÇ™ÂåñÂéüÂõ†ÂàÜÊûê„Å´Âü∫„Å•„ÅÑ„Å¶ **Ê†πÊú¨ÁöÑ„Å™ÊîπÂñÑ** „ÇíË°å„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„Äêüìä „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊÇ™Âåñ„ÅÆË©≥Á¥∞ÂàÜÊûê„Äë
- **ÊÇ™ÂåñÁéá**: {cost_ratio:.2f}ÂÄçÔºà{(cost_ratio-1)*100:.1f}%Â¢óÂä†Ôºâ
- **‰∏ªË¶ÅÂéüÂõ†**: {primary_cause}
- **‰ø°È†ºÂ∫¶**: {confidence_level}
- **ÂÖ∑‰ΩìÁöÑÂïèÈ°å**: {', '.join(specific_issues)}

„ÄêÂÖÉ„ÅÆÂàÜÊûêÂØæË±°„ÇØ„Ç®„É™„Äë
```sql
{original_query}
```
{previous_query_section}

„Äêüîß ÊÇ™ÂåñÂéüÂõ†Âà•„ÅÆÂÖ∑‰ΩìÁöÑ‰øÆÊ≠£ÊåáÁ§∫„Äë
{chr(10).join(f"- {instruction}" for instruction in fix_instructions)}

„ÄêüéØ „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊîπÂñÑ„ÅÆÈáçË¶Å„Å™ÊñπÈáù„Äë

1. **üö® ÈÅéÂâ∞ÊúÄÈÅ©Âåñ„ÅÆÊòØÊ≠£**:
           - JOINÈ†ÜÂ∫è„ÅÆÂäπÁéáÂåñ
           - ÂäπÁéáÁöÑ„Åß„Å™„ÅÑJOINÈ†ÜÂ∫è„ÅÆË¶ãÁõ¥„Åó
   - ÂäπÊûúÁöÑ„Åß„Å™„ÅÑ„Éí„É≥„Éà„ÅØÁ©çÊ•µÁöÑ„Å´ÂâäÈô§

2. **‚ö° JOINÂäπÁéáÂåñ**:
   - JOINÊìç‰ΩúÊï∞„ÅÆÂ§ßÂπÖ„Å™Â¢óÂä†„ÇíÈÅø„Åë„Çã
   - ÂÖÉ„ÅÆJOINÈ†ÜÂ∫è„ÇíÂ∞äÈáç
   - ‰∏çË¶Å„Å™„Çµ„Éñ„ÇØ„Ç®„É™Âåñ„Å´„Çà„ÇãJOINÈáçË§á„ÇíÈò≤„Åê

3. **üéØ „Éá„Éº„Çø„Çµ„Ç§„Ç∫ÊúÄÈÅ©Âåñ**:
   - „Éï„Ç£„É´„Çø„Éº„Éó„ÉÉ„Ç∑„É•„ÉÄ„Ç¶„É≥„ÇíÊúÄÂ§ßÂåñ
   - Êó©Êúü„ÅÆË°åÊï∞ÂâäÊ∏õ„ÇíÈáçË¶ñ
   - ‰∏≠ÈñìÁµêÊûú„ÅÆ„Çµ„Ç§„Ç∫„ÇíÊúÄÂ∞èÂåñ

4. **üìä Áµ±Ë®àÊÉÖÂ†±„Å´Âü∫„Å•„ÅèÂà§Êñ≠**:
   - Â∞è„ÉÜ„Éº„Éñ„É´Ôºà<30MBÔºâ„ÅÆ„ÅøBROADCASTÈÅ©Áî®
   - „É°„É¢„É™ÂäπÁéá„ÇíÈáçË¶ñ„Åó„ÅüJOINÊà¶Áï•
   - „Çπ„Éî„É´Áô∫Áîü„ÅÆÊúÄÂ∞èÂåñ

„ÄêüîÑ ÊîπÂñÑ„ÇØ„Ç®„É™ÁîüÊàê„ÅÆÊåáÈáù„Äë

**A. ‰øùÂÆàÁöÑ„Ç¢„Éó„É≠„Éº„ÉÅÔºàÊé®Â•®Ôºâ:**
- ÂÖÉ„ÇØ„Ç®„É™„ÅÆÊßãÈÄ†„ÇíÊúÄÂ§ßÈôê‰øùÊåÅ
- Á¢∫ÂÆü„Å´ÂäπÊûúÁöÑ„Å™ÊúÄÈÅ©Âåñ„ÅÆ„ÅøÈÅ©Áî®
- „É™„Çπ„ÇØ„ÅÆÈ´ò„ÅÑÂ§âÊõ¥„ÅØÈÅø„Åë„Çã

**B. ÊÆµÈöéÁöÑÊîπÂñÑ:**
- ÊúÄ„ÇÇÂïèÈ°å„Å®„Å™„Å£„Å¶„ÅÑ„ÇãÁÆáÊâÄ„ÅÆ„Åø‰øÆÊ≠£
- ‰∏ÄÂ∫¶„Å´Â§ö„Åè„ÅÆÂ§âÊõ¥„ÇíÂä†„Åà„Å™„ÅÑ
- Ê∏¨ÂÆöÂèØËÉΩ„Å™ÊîπÂñÑ„ÇíÈáçË¶ñ

**C. „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÊà¶Áï•:**
- ‰∏çÁ¢∫ÂÆü„Å™ÊúÄÈÅ©Âåñ„ÅØÂâäÈô§
- ÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Å´Ëøë„ÅÑÂΩ¢„Åß„ÅÆËªΩÂæÆ„Å™ÊîπÂñÑ

„ÄêÈáçË¶Å„Å™Âà∂Á¥Ñ„Äë
- „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊÇ™Âåñ„ÅÆ‰∏ªË¶ÅÂéüÂõ†„ÇíÁ¢∫ÂÆü„Å´Ëß£Ê±∫
- ÂÖÉ„ÇØ„Ç®„É™„Çà„ÇäÁ¢∫ÂÆü„Å´È´òÈÄü„Å™„ÇØ„Ç®„É™„ÇíÁîüÊàê
- Ê©üËÉΩÊÄß„Çí‰∏ÄÂàáÊêç„Å™„Çè„Å™„ÅÑ
- ÂÆåÂÖ®„ÅßÂÆüË°åÂèØËÉΩ„Å™SQL„ÅÆ„ÅøÂá∫Âäõ

„ÄêÂá∫ÂäõÂΩ¢Âºè„Äë
## üöÄ „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊîπÂñÑSQL

**ÊîπÂñÑ„Åó„ÅüÂÜÖÂÆπ**:
- [ÂÖ∑‰ΩìÁöÑ„Å™ÊÇ™ÂåñÂéüÂõ†„ÅÆ‰øÆÊ≠£]
- [ÂâäÈô§/Â§âÊõ¥„Åó„ÅüÊúÄÈÅ©ÂåñË¶ÅÁ¥†]
- [Êñ∞„Åü„Å´ÈÅ©Áî®„Åó„ÅüÊîπÂñÑÁ≠ñ]

```sql
[ÂÆåÂÖ®„Å™SQL - „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊîπÂñÑÊ∏à„Åø]
```

## ÊîπÂñÑË©≥Á¥∞
[ÊÇ™ÂåñÂéüÂõ†„ÅÆËß£Ê±∫ÊñπÊ≥ï„Å®ÊúüÂæÖ„Åï„Çå„ÇãÊÄßËÉΩÊîπÂñÑ„ÅÆË™¨Êòé]
"""

    # Ë®≠ÂÆö„Åï„Çå„ÅüLLM„Éó„É≠„Éê„Ç§„ÉÄ„Éº„Çí‰ΩøÁî®
    provider = LLM_CONFIG["provider"]
    
    try:
        if provider == "databricks":
            improved_result = _call_databricks_llm(performance_improvement_prompt)
        elif provider == "openai":
            improved_result = _call_openai_llm(performance_improvement_prompt)
        elif provider == "azure_openai":
            improved_result = _call_azure_openai_llm(performance_improvement_prompt)
        elif provider == "anthropic":
            improved_result = _call_anthropic_llm(performance_improvement_prompt)
        else:
            error_msg = "‚ö†Ô∏è Configured LLM provider is not recognized"
            print(f"‚ùå LLM performance improvement error: {error_msg}")
            return f"LLM_ERROR: {error_msg}"
        
        # LLM„É¨„Çπ„Éù„É≥„Çπ„ÅÆ„Ç®„É©„Éº„ÉÅ„Çß„ÉÉ„ÇØ
        if isinstance(improved_result, str):
            error_indicators = [
                "API„Ç®„É©„Éº:",
                "Input is too long", 
                "Bad Request",
                "‚ùå",
                "‚ö†Ô∏è",
                "„Çø„Ç§„É†„Ç¢„Ç¶„Éà„Ç®„É©„Éº:",
                "APIÂëº„Å≥Âá∫„Åó„Ç®„É©„Éº:",
            ]
            
            for indicator in error_indicators:
                if indicator in improved_result:
                    print(f"‚ùå Error detected in LLM performance improvement: {indicator}")
                    return f"LLM_ERROR: {improved_result}"
        
        return improved_result
        
    except Exception as e:
        error_msg = f"„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊîπÂñÑÂá¶ÁêÜ„Åß„Ç®„É©„Éº: {str(e)}"
        print(f"‚ùå {error_msg}")
        return f"LLM_ERROR: {error_msg}"


def generate_optimized_query_with_error_feedback(original_query: str, analysis_result: str, metrics: Dict[str, Any], error_info: str = "", previous_optimized_query: str = "") -> str:
    """
    Execute SQL optimization by LLM including error information
    Use prompts specialized for error correction
    
    Args:
        original_query: Original query
        analysis_result: Bottleneck analysis result
        metrics: Metrics information
        error_info: Error information
        previous_optimized_query: Initial optimized query (for hint retention)
    """
    
    # ÂàùÂõûÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™„ÅÆÊÉÖÂ†±„ÇíÂê´„ÇÅ„Çã
    previous_query_section = ""
    if previous_optimized_query:
        previous_query_section = f"""

„ÄêüöÄ ÂàùÂõûÁîüÊàê„Åï„Çå„ÅüÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™Ôºà„Ç®„É©„ÉºÁô∫ÁîüÔºâ„Äë
```sql
{previous_optimized_query}
```

**‚ö†Ô∏è ÈáçË¶Å**: ‰∏äË®ò„ÅÆÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™„Å´Âê´„Åæ„Çå„Çã‰ª•‰∏ã„ÅÆË¶ÅÁ¥†„ÅØÂøÖ„Åö‰øùÊåÅ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö
- **REPARTITION„Éí„É≥„Éà**: `/*+ REPARTITION(Êï∞ÂÄ§, „Ç´„É©„É†Âêç) */`
- **„Åù„ÅÆ‰ªñ„ÅÆÊúÄÈÅ©Âåñ„Éí„É≥„Éà**: COALESCE„ÄÅCACHEÁ≠â
- **ÊúÄÈÅ©ÂåñÊâãÊ≥ï**: CTEÊßãÈÄ†„ÄÅÁµêÂêàÈ†ÜÂ∫è„ÄÅ„Éï„Ç£„É´„Çø„Éº„Éó„ÉÉ„Ç∑„É•„ÉÄ„Ç¶„É≥Á≠â
- **„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊîπÂñÑÁ≠ñ**: „Çπ„Éî„É´ÂØæÁ≠ñ„ÄÅ‰∏¶ÂàóÂ∫¶ÊîπÂñÑÁ≠â

**üéØ „Ç®„É©„Éº‰øÆÊ≠£„ÅÆÊñπÈáù**: 
- „Ç®„É©„ÉºÁÆáÊâÄ„ÅÆ„Åø„Çí‰øÆÊ≠£„Åó„ÄÅÊúÄÈÅ©ÂåñË¶ÅÁ¥†„ÅØÂÖ®„Å¶‰øùÊåÅ
- „Éí„É≥„ÉàÂè•„ÅÆÈÖçÁΩÆ„É´„Éº„É´„ÅØÂé≥ÂÆàÔºàREPARTITION„ÅØ„É°„Ç§„É≥„ÇØ„Ç®„É™SELECTÁõ¥ÂæåÁ≠âÔºâ
"""

    # üö® NEW: „Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏Ëß£Êûê„Å´„Çà„ÇãË©≥Á¥∞‰øÆÊ≠£ÊåáÁ§∫ÁîüÊàê
    def generate_specific_error_guidance(error_message: str) -> str:
        """Generate detailed correction instructions based on specific error messages"""
        guidance = ""
        
        if "AMBIGUOUS_REFERENCE" in error_message.upper():
            # AMBIGUOUS_REFERENCE„Ç®„É©„Éº„ÅÆÂÖ∑‰ΩìÁöÑÂØæÂá¶
            import re
            ambiguous_column_match = re.search(r'Reference `([^`]+)` is ambiguous', error_message)
            if ambiguous_column_match:
                ambiguous_column = ambiguous_column_match.group(1)
                guidance += f"""
üéØ **AMBIGUOUS_REFERENCE Â∞ÇÁî®‰øÆÊ≠£ÊåáÁ§∫**: 
- **ÂïèÈ°å**: „Ç´„É©„É† `{ambiguous_column}` „ÅåË§áÊï∞„ÉÜ„Éº„Éñ„É´„Å´Â≠òÂú®
- **ÂøÖÈ†à‰øÆÊ≠£**: ÂÖ®„Å¶„ÅÆ `{ambiguous_column}` ÂèÇÁÖß„Å´„ÉÜ„Éº„Éñ„É´„Ç®„Ç§„É™„Ç¢„Çπ„ÇíÊòéÁ§∫
- **‰øÆÊ≠£‰æã**: `{ambiguous_column}` ‚Üí `table_alias.{ambiguous_column}`
- **ÈáçË¶Å**: WHEREÂè•„ÄÅSELECTÂè•„ÄÅJOINÂè•ÂÖ®„Å¶„ÅßÊòéÁ§∫ÁöÑ‰øÆÈ£æ„ÅåÂøÖË¶Å
"""
            
        if "UNRESOLVED_COLUMN" in error_message.upper():
            # UNRESOLVED_COLUMN„Ç®„É©„Éº„ÅÆÂÖ∑‰ΩìÁöÑÂØæÂá¶
            import re
            unresolved_match = re.search(r'column.*`([^`]+)`', error_message)
            if unresolved_match:
                unresolved_column = unresolved_match.group(1)
                guidance += f"""
üéØ **UNRESOLVED_COLUMN Â∞ÇÁî®‰øÆÊ≠£ÊåáÁ§∫**:
- **ÂïèÈ°å**: „Ç´„É©„É† `{unresolved_column}` „ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑ
- **Á¢∫Ë™ç‰∫ãÈ†Ö**: „ÉÜ„Éº„Éñ„É´„Ç®„Ç§„É™„Ç¢„Çπ„ÄÅ„Çπ„Éö„É´„Éü„Çπ„ÄÅ„Çπ„Ç≥„Éº„Éó
- **‰øÆÊ≠£‰æã**: Ê≠£„Åó„ÅÑ„ÉÜ„Éº„Éñ„É´‰øÆÈ£æ„ÄÅÂ≠òÂú®„Åô„Çã„Ç´„É©„É†Âêç„Å∏„ÅÆÂ§âÊõ¥
"""
        
        if "PARSE_SYNTAX_ERROR" in error_message.upper():
            guidance += f"""
üéØ **PARSE_SYNTAX_ERROR Â∞ÇÁî®‰øÆÊ≠£ÊåáÁ§∫**:
- **ÈáçË¶Å**: ÊßãÊñá„Ç®„É©„ÉºÊúÄÂÑ™ÂÖà‰øÆÊ≠£Ôºà„Ç´„É≥„ÉûÊäú„Åë„ÄÅ„Ç®„Ç§„É™„Ç¢„ÇπÈáçË§áÁ≠âÔºâ
- **Á¢∫Ë™ç**: SELECTÂè•„ÅÆ„Ç´„É≥„Éû„ÄÅFROMÂè•„ÅÆÊßãÊñá„ÄÅ„Ç®„Ç§„É™„Ç¢„ÇπÂÆöÁæ©
"""
            
        return guidance
    
    specific_guidance = generate_specific_error_guidance(error_info)

    error_feedback_prompt = f"""
„ÅÇ„Å™„Åü„ÅØDatabricks„ÅÆSQL„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊúÄÈÅ©Âåñ„Å®„Ç®„É©„Éº‰øÆÊ≠£„ÅÆÂ∞ÇÈñÄÂÆ∂„Åß„Åô„ÄÇ

‰ª•‰∏ã„ÅÆÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™„ÅßEXPLAINÂÆüË°åÊôÇ„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü„ÄÇ**ÊúÄÈÅ©ÂåñË¶ÅÁ¥†„Çí‰øùÊåÅ„Åó„Å™„Åå„Çâ**„Ç®„É©„ÉºÊÉÖÂ†±„ÇíÂü∫„Å´‰øÆÊ≠£„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„Äêüö® Áô∫Áîü„Åó„Åü„Ç®„É©„ÉºÊÉÖÂ†±„Äë
{error_info}
{specific_guidance}

„ÄêÂÖÉ„ÅÆÂàÜÊûêÂØæË±°„ÇØ„Ç®„É™„Äë
```sql
{original_query}
```
{previous_query_section}
„ÄêË©≥Á¥∞„Å™„Éú„Éà„É´„Éç„ÉÉ„ÇØÂàÜÊûêÁµêÊûú„Äë
{analysis_result}

„Äêüîß „Ç®„É©„Éº‰øÆÊ≠£„ÅÆÈáçË¶Å„Å™ÊåáÈáù„Äë
1. **üöÄ ÊúÄÈÅ©ÂåñË¶ÅÁ¥†„ÅÆÁµ∂ÂØæ‰øùÊåÅÔºàÊúÄÈáçË¶ÅÔºâ**:
   - **ÂàùÂõûÁîüÊàê„Åï„Çå„ÅüJOINÈ†ÜÂ∫èÊúÄÈÅ©Âåñ„ÇíÂøÖ„Åö‰øùÊåÅ**
   - **ÂàùÂõûÁîüÊàê„Åï„Çå„ÅüREPARTITION„Éí„É≥„Éà„ÇíÂøÖ„Åö‰øùÊåÅ**: `/*+ REPARTITION(Êï∞ÂÄ§, „Ç´„É©„É†) */`
   - **„Åù„ÅÆ‰ªñ„ÅÆÊúÄÈÅ©Âåñ„Éí„É≥„Éà„ÇÇÂÖ®„Å¶‰øùÊåÅ**: COALESCE„ÄÅCACHEÁ≠â
   - **CTEÊßãÈÄ†„ÇÑÁµêÂêàÈ†ÜÂ∫è„Å™„Å©„ÅÆÊúÄÈÅ©ÂåñË®≠Ë®à„ÇíÁ∂≠ÊåÅ**
   - **„Çπ„Éî„É´ÂØæÁ≠ñ„ÇÑ„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊîπÂñÑÁ≠ñ„Çí‰øùÊåÅ**

2. **üö® Ëá¥ÂëΩÁöÑÊßãÊñá„Ç®„É©„Éº„ÅÆÊúÄÂÑ™ÂÖà‰øÆÊ≠£**:

   **A. „Ç´„É≥„ÉûÊäú„Åë„Ç®„É©„Éº (PARSE_SYNTAX_ERROR)**:
   - ‚ùå `i.i_item_sk ss.ss_item_sk` ‚Üí ‚úÖ `i.i_item_sk, ss.ss_item_sk`
   - ‚ùå `SELECT col1 col2 FROM` ‚Üí ‚úÖ `SELECT col1, col2 FROM`
   - **SELECTÂè•ÂÜÖ„Åß„ÅÆ„Ç´„É≥„ÉûÊäú„Åë„ÇíÊúÄÂÑ™ÂÖà„Åß‰øÆÊ≠£**

   **B. ‰∫åÈáç„Éª‰∏âÈáç„Ç®„Ç§„É™„Ç¢„Çπ„Ç®„É©„Éº**:
   - ‚ùå `iss.i.i_brand_id` ‚Üí ‚úÖ `iss.i_brand_id` „Åæ„Åü„ÅØ `i.i_brand_id`
   - ‚ùå `ss.ss.ss_item_sk` ‚Üí ‚úÖ `ss.ss_item_sk`
   - **‰∏Ä„Å§„ÅÆ„ÉÜ„Éº„Éñ„É´„Å´ÂØæ„Åô„ÇãÈáçË§á„Ç®„Ç§„É™„Ç¢„ÇπÂèÇÁÖß„Çí‰øÆÊ≠£**

   **C. Â≠òÂú®„Åó„Å™„ÅÑ„ÉÜ„Éº„Éñ„É´/„Ç´„É©„É†ÂèÇÁÖß**:
   - ‚ùå `this_year.i.i_brand_id` ‚Üí ‚úÖ `this_year.i_brand_id`
   - **„Çµ„Éñ„ÇØ„Ç®„É™„Ç®„Ç§„É™„Ç¢„Çπ„Å®ÂÜÖÈÉ®„ÉÜ„Éº„Éñ„É´„Ç®„Ç§„É™„Ç¢„Çπ„ÅÆÊ∑∑Âêå„Çí‰øÆÊ≠£**

   **D. FROMÂè•ÊßãÊñá„Ç®„É©„Éº**:
   - ‚ùå `FROM table1, (SELECT ...) x WHERE` ‚Üí ‚úÖ ÈÅ©Âàá„Å™JOINÊßãÊñá„Å´Â§âÊèõ
   - **Âè§„ÅÑ„Ç´„É≥„ÉûÁµêÂêà„ÇíÊòéÁ§∫ÁöÑJOINÊßãÊñá„Å´Â§âÊèõ**

3. **üîç AMBIGUOUS_REFERENCE „Ç®„É©„Éº„ÅÆ‰øÆÊ≠£**: 
   - **ÂÖ®„Å¶„ÅÆ„Ç´„É©„É†ÂèÇÁÖß„Åß„ÉÜ„Éº„Éñ„É´Âêç„Åæ„Åü„ÅØ„Ç®„Ç§„É™„Ç¢„ÇπÂêç„ÇíÊòéÁ§∫ÁöÑ„Å´ÊåáÂÆö**
   - ‰æã: `ss_item_sk` ‚Üí `store_sales.ss_item_sk` „Åæ„Åü„ÅØ `ss.ss_item_sk`
   - **„Çµ„Éñ„ÇØ„Ç®„É™„Å®„É°„Ç§„É≥„ÇØ„Ç®„É™„ÅßÂêåÂêç„Ç´„É©„É†„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØÁâπ„Å´Ê≥®ÊÑè**

4. **„ÉÜ„Éº„Éñ„É´„Ç®„Ç§„É™„Ç¢„Çπ„ÅÆ‰∏ÄË≤´‰ΩøÁî®**: 
   - ÂÖ®„Å¶„ÅÆ„ÉÜ„Éº„Éñ„É´„Å´Áü≠„ÅÑ„Ç®„Ç§„É™„Ç¢„ÇπÂêç„Çí‰ªò‰∏éÔºà‰æã: store_sales ‚Üí ss, item ‚Üí iÔºâ
   - „ÇØ„Ç®„É™ÂÖ®‰Ωì„Åß‰∏ÄË≤´„Åó„Å¶„Ç®„Ç§„É™„Ç¢„ÇπÂêç„Çí‰ΩøÁî®
   - „Çµ„Éñ„ÇØ„Ç®„É™ÂÜÖ„Åß„ÇÇÂêå„Åò„Ç®„Ç§„É™„Ç¢„ÇπÂêç‰ΩìÁ≥ª„ÇíÁ∂≠ÊåÅ

5. **„Åù„ÅÆ‰ªñ„ÅÆÊßãÊñá„Ç®„É©„Éº‰øÆÊ≠£**: 
   - **ÂûãÂ§âÊèõ„Ç®„É©„Éº**: ‰∏çÈÅ©Âàá„Å™„Ç≠„É£„Çπ„Éà‰øÆÊ≠£
   - **„Éí„É≥„ÉàÂè•„Ç®„É©„Éº**: ÊßãÊñá„Å´Âêà„Çè„Åõ„ÅüÈÖçÁΩÆ‰øÆÊ≠£
   - **Ê®©Èôê„Ç®„É©„Éº**: ‰ª£Êõø„Ç¢„ÇØ„Çª„ÇπÊñπÊ≥ïÊèêÊ°à

„Äêüö® BROADCAST„Éí„É≥„ÉàÈÖçÁΩÆ„ÅÆÂé≥Ê†º„Å™„É´„Éº„É´ - „Ç®„É©„Éº‰øÆÊ≠£Áâà„Äë
**‚úÖ Ê≠£„Åó„ÅÑÈÖçÁΩÆÔºàÂøÖÈ†àÔºâ:**
```sql
-- ‚úÖ Ê≠£„Åó„ÅÑ: „É°„Ç§„É≥„ÇØ„Ç®„É™„ÅÆSELECTÁõ¥Âæå„ÅÆ„Åø
SELECT /*+ BROADCAST(i, d) */
  ss.ss_item_sk, i.i_brand_id, d.d_year
FROM store_sales ss
  JOIN item i ON ss.ss_item_sk = i.i_item_sk
  JOIN date_dim d ON ss.ss_sold_date_sk = d.d_date_sk
```

**‚ùå Áµ∂ÂØæ„Å´Á¶ÅÊ≠¢„Åï„Çå„ÇãÈÖçÁΩÆÔºàÊßãÊñá„Ç®„É©„Éº„ÅÆÂéüÂõ†Ôºâ:**
```sql
-- ‚ùå ÈñìÈÅï„ÅÑ: JOINÂè•ÂÜÖ„Å∏„ÅÆÈÖçÁΩÆÔºàPARSE_SYNTAX_ERRORÁô∫ÁîüÔºâ
FROM store_sales ss
  JOIN /*+ BROADCAST(i) */ item i ON ss.ss_item_sk = i.i_item_sk  -- „Åì„Çå„ÅåÊßãÊñá„Ç®„É©„Éº
  JOIN /*+ BROADCAST(d) */ date_dim d ON ss.ss_sold_date_sk = d.d_date_sk  -- „Åì„Çå„ÇÇÊßãÊñá„Ç®„É©„Éº

-- ‚ùå ÈñìÈÅï„ÅÑ: „Çµ„Éñ„ÇØ„Ç®„É™ÂÜÖ„Å∏„ÅÆÈÖçÁΩÆ
SELECT ... FROM (
  SELECT /*+ BROADCAST(i) */ ...  -- „Çµ„Éñ„ÇØ„Ç®„É™ÂÜÖ„ÅØÁÑ°Âäπ
  FROM ...
)

-- ‚ùå ÈñìÈÅï„ÅÑ: FROMÂè•ÂÜÖ„Å∏„ÅÆÈÖçÁΩÆ
FROM /*+ BROADCAST(i) */ item i  -- FROMÂè•ÂÜÖ„ÅØÊßãÊñá„Ç®„É©„Éº
```

**üîß PARSE_SYNTAX_ERROR‰øÆÊ≠£„ÅÆÂÖ∑‰ΩìÁöÑÊâãÈ†Ü:**
1. **JOINÂè•ÂÜÖ„ÅÆBROADCAST„Éí„É≥„Éà„ÇíÂÖ®„Å¶ÂâäÈô§**
2. **„É°„Ç§„É≥„ÇØ„Ç®„É™„ÅÆÊúÄÂàù„ÅÆSELECTÁõ¥Âæå„Å´ÂÖ®„Å¶„ÅÆBROADCAST„Éí„É≥„Éà„ÇíÁµ±Âêà**
3. **„ÉÜ„Éº„Éñ„É´Âêç/„Ç®„Ç§„É™„Ç¢„ÇπÂêç„ÇíÊ≠£Á¢∫„Å´ÊåáÂÆö**

**üìù ÂÖ∑‰ΩìÁöÑ‰øÆÊ≠£‰æãÔºàPARSE_SYNTAX_ERRORÂØæÂøúÔºâ:**

‚ùå **‰øÆÊ≠£ÂâçÔºà„Ç®„É©„ÉºÁô∫ÁîüÔºâ:**
```sql
SELECT ss.ss_item_sk, i.i_brand_id
FROM store_sales ss
  JOIN /*+ BROADCAST(i) */ item i ON ss.ss_item_sk = i.i_item_sk  -- PARSE_SYNTAX_ERROR
  JOIN /*+ BROADCAST(d) */ date_dim d ON ss.ss_sold_date_sk = d.d_date_sk  -- PARSE_SYNTAX_ERROR
```

‚úÖ **‰øÆÊ≠£ÂæåÔºàÊ≠£Â∏∏Ôºâ:**
```sql
SELECT /*+ BROADCAST(i, d) */ ss.ss_item_sk, i.i_brand_id
FROM store_sales ss
  JOIN item i ON ss.ss_item_sk = i.i_item_sk
  JOIN date_dim d ON ss.ss_sold_date_sk = d.d_date_sk
```

**üö® „Ç®„É©„Éº‰øÆÊ≠£„ÅÆÊúÄÈáçË¶Å„É´„Éº„É´:**
- **JOINÂè•ÂÜÖ„ÅÆ`/*+ BROADCAST(...) */`„ÅØÂç≥Â∫ß„Å´ÂâäÈô§**
- **ÂâäÈô§„Åó„ÅüBROADCASTÂØæË±°„Çí„É°„Ç§„É≥SELECTÁõ¥Âæå„Å´ÁßªÂãï**
- **Ë§áÊï∞„ÅÆBROADCASTÂØæË±°„ÅØ„Ç´„É≥„ÉûÂå∫Âàá„Çä„ÅßÁµ±Âêà: `/*+ BROADCAST(table1, table2, table3) */`**

„Äêüö® REPARTITION„Éí„É≥„ÉàÈÖçÁΩÆ„ÅÆÂé≥Ê†º„Å™„É´„Éº„É´ - „Ç®„É©„Éº‰øÆÊ≠£Áâà„Äë
- **„Çµ„Éñ„ÇØ„Ç®„É™ÂÜÖÈÉ®„ÅÆSELECTÊñáÁõ¥Âæå„Å´ÈÖçÁΩÆ**
- **„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥Êï∞„Å®„Ç´„É©„É†Âêç„ÅØÂøÖÈ†à**: `/*+ REPARTITION(200, column_name) */`
- **„Çπ„Éî„É´Ê§úÂá∫ÊôÇ„ÅÆ„ÅøÈÅ©Áî®**

„ÄêÈáçË¶Å„Å™Âà∂Á¥Ñ - „Ç®„É©„Éº‰øÆÊ≠£Áâà„Äë
- ÊßãÊñá„Ç®„É©„Éº„ÇíÁµ∂ÂØæ„Å´Áô∫Áîü„Åï„Åõ„Å™„ÅÑÂÆåÂÖ®„Å™SQL„ÇØ„Ç®„É™„ÇíÁîüÊàê
- „Åô„Åπ„Å¶„ÅÆ„Ç´„É©„É†Âêç„ÄÅ„ÉÜ„Éº„Éñ„É´Âêç„ÇíÂÆåÂÖ®„Å´Ë®òËø∞
- „Éó„É¨„Éº„Çπ„Éõ„É´„ÉÄ„ÉºÔºà...„ÄÅ[ÁúÅÁï•]Ôºâ„ÅØ‰∏ÄÂàá‰ΩøÁî®Á¶ÅÊ≠¢
- ÂÖÉ„ÅÆ„ÇØ„Ç®„É™„ÅÆDISTINCTÂè•„ÅØÂøÖ„Åö‰øùÊåÅ
- ÂÆüÈöõ„Å´ÂÆüË°å„Åß„Åç„ÇãÂÆåÂÖ®„Å™SQL„ÇØ„Ç®„É™„ÅÆ„Åø„ÇíÂá∫Âäõ

„ÄêÂá∫ÂäõÂΩ¢Âºè„Äë
## üîß „Ç®„É©„Éº‰øÆÊ≠£Ê∏à„ÅøÊúÄÈÅ©ÂåñSQL

**‰øÆÊ≠£„Åó„ÅüÂÜÖÂÆπ**:
- [ÂÖ∑‰ΩìÁöÑ„Å™„Ç®„É©„Éº‰øÆÊ≠£ÁÆáÊâÄ]

**‰øùÊåÅ„Åó„ÅüÊúÄÈÅ©ÂåñË¶ÅÁ¥†**:
- [‰øùÊåÅ„Åï„Çå„ÅüREPARTITION„Éí„É≥„Éà]
- [‰øùÊåÅ„Åï„Çå„ÅüJOINÈ†ÜÂ∫èÊúÄÈÅ©Âåñ]
- [‰øùÊåÅ„Åï„Çå„Åü„Åù„ÅÆ‰ªñ„ÅÆÊúÄÈÅ©ÂåñÊâãÊ≥ï]

```sql
[ÂÆåÂÖ®„Å™SQL - „Ç®„É©„Éº‰øÆÊ≠£Ê∏à„Åø„ÄÅÊúÄÈÅ©ÂåñË¶ÅÁ¥†‰øùÊåÅ]
```

## ‰øÆÊ≠£Ë©≥Á¥∞
[„Ç®„É©„Éº„ÅÆÂéüÂõ†„Å®‰øÆÊ≠£ÊñπÊ≥ï„ÄÅ„Åä„Çà„Å≥ÊúÄÈÅ©ÂåñË¶ÅÁ¥†‰øùÊåÅ„ÅÆË™¨Êòé]
"""

    # Ë®≠ÂÆö„Åï„Çå„ÅüLLM„Éó„É≠„Éê„Ç§„ÉÄ„Éº„Çí‰ΩøÁî®
    provider = LLM_CONFIG["provider"]
    
    try:
        if provider == "databricks":
            optimized_result = _call_databricks_llm(error_feedback_prompt)
        elif provider == "openai":
            optimized_result = _call_openai_llm(error_feedback_prompt)
        elif provider == "azure_openai":
            optimized_result = _call_azure_openai_llm(error_feedback_prompt)
        elif provider == "anthropic":
            optimized_result = _call_anthropic_llm(error_feedback_prompt)
        else:
            error_msg = "‚ö†Ô∏è Ë®≠ÂÆö„Åï„Çå„ÅüLLM„Éó„É≠„Éê„Ç§„ÉÄ„Éº„ÅåË™çË≠ò„Åß„Åç„Åæ„Åõ„Çì"
            print(f"‚ùå LLM error correction error: {error_msg}")
            return f"LLM_ERROR: {error_msg}"
        
        # LLM„É¨„Çπ„Éù„É≥„Çπ„ÅÆ„Ç®„É©„Éº„ÉÅ„Çß„ÉÉ„ÇØÔºàÈáçË¶ÅÔºâ
        if isinstance(optimized_result, str):
            # API„Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÊ§úÂá∫
            error_indicators = [
                 "API„Ç®„É©„Éº:",
                 "Input is too long",
                 "Bad Request",
                 "‚ùå",
                 "‚ö†Ô∏è",
                 "„Çø„Ç§„É†„Ç¢„Ç¶„Éà„Ç®„É©„Éº:",
                 "APIÂëº„Å≥Âá∫„Åó„Ç®„É©„Éº:",
                 "„É¨„Çπ„Éù„É≥„Çπ:",
                 '{"error_code":'
             ]
            
            # „Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏„Åã„Å©„ÅÜ„Åã„Çí„ÉÅ„Çß„ÉÉ„ÇØ
            is_error_response = any(indicator in optimized_result for indicator in error_indicators)
            
            if is_error_response:
                print(f"‚ùå Error occurred in LLM error correction API call: {optimized_result[:200]}...")
                return f"LLM_ERROR: {optimized_result}"
        
        # üîß ‰øÆÊ≠£Âæå„ÅÆ„ÇØ„Ç®„É™„Å´ÂØæ„Åó„Å¶„Éó„É≠„Ç∞„É©„Éû„ÉÜ„Ç£„ÉÉ„ÇØÂæåÂá¶ÁêÜ„ÇíÈÅ©Áî®
        if isinstance(optimized_result, str) and not optimized_result.startswith("LLM_ERROR:"):
            print("üîß Executing query validation and post-processing after error correction")
            final_corrected_query = enhance_error_correction_with_syntax_validation(optimized_result, original_query, error_info)
            return final_corrected_query
        
        return optimized_result
        
    except Exception as e:
        error_msg = f"‚ö†Ô∏è „Ç®„É©„Éº‰øÆÊ≠£SQLÁîüÊàê‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {str(e)}"
        print(f"‚ùå LLM error correction exception error: {error_msg}")
        return f"LLM_ERROR: {error_msg}"


def compare_query_performance(original_explain_cost: str, optimized_explain_cost: str) -> Dict[str, Any]:
    """
    Compare EXPLAIN COST results to detect performance degradation
    
    Args:
        original_explain_cost: EXPLAIN COST result of original query
        optimized_explain_cost: EXPLAIN COST result of optimized query
        
    Returns:
        Dict: Performance comparison results and recommendations
    """
    comparison_result = {
        'is_optimization_beneficial': True,
        'performance_degradation_detected': False,
        'significant_improvement_detected': False,  # üö® ÊòéÁ¢∫„Å™ÊîπÂñÑÊ§úÂá∫„Éï„É©„Ç∞ËøΩÂä†Ôºà1%‰ª•‰∏äÔºâ
        'substantial_improvement_detected': False,  # üöÄ Â§ßÂπÖÊîπÂñÑÊ§úÂá∫„Éï„É©„Ç∞ËøΩÂä†Ôºà10%‰ª•‰∏äÔºâ
        'total_cost_ratio': 1.0,
        'memory_usage_ratio': 1.0,
        'scan_cost_ratio': 1.0,
        'join_cost_ratio': 1.0,
        'recommendation': 'use_optimized',
        'details': []
    }
    
    try:
        import re
        
        # üö® EXPLAIN COSTÂÜÖÂÆπ„ÅÆÂ¶•ÂΩìÊÄß„ÉÅ„Çß„ÉÉ„ÇØ
        def validate_explain_cost_content(explain_cost_text, query_type):
            """EXPLAIN COSTÂÜÖÂÆπ„ÅåÊ≠£Â∏∏„Åã„ÉÅ„Çß„ÉÉ„ÇØ"""
            if len(explain_cost_text) < 1000:
                return False, f"{query_type} EXPLAIN COST content too short ({len(explain_cost_text)} chars)"
            
            if 'ExplainCommand' in explain_cost_text:
                return False, f"{query_type} EXPLAIN COST contains ExplainCommand (invalid result)"
            
            if '== Optimized Logical Plan ==' not in explain_cost_text:
                return False, f"{query_type} EXPLAIN COST missing expected structure"
                
            return True, "Valid"
        
        # ÂÖÉ„ÇØ„Ç®„É™„Å®„ÅÆÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™„ÅÆEXPLAIN COSTÂ¶•ÂΩìÊÄß„ÉÅ„Çß„ÉÉ„ÇØ
        original_valid, original_error = validate_explain_cost_content(original_explain_cost, "Original")
        optimized_valid, optimized_error = validate_explain_cost_content(optimized_explain_cost, "Optimized")
        
        if not original_valid:
            comparison_result['performance_degradation_detected'] = True
            comparison_result['is_optimization_beneficial'] = False
            comparison_result['recommendation'] = 'use_original'
            comparison_result['details'] = [f"‚ùå {original_error}"]
            return comparison_result
            
        if not optimized_valid:
            comparison_result['performance_degradation_detected'] = True
            comparison_result['is_optimization_beneficial'] = False
            comparison_result['recommendation'] = 'use_original'
            comparison_result['details'] = [f"‚ùå {optimized_error} - reverting to original query"]
            return comparison_result
        
        # „Ç≥„Çπ„ÉàÊÉÖÂ†±„ÇíÊäΩÂá∫„Åô„ÇãÈñ¢Êï∞
        def extract_cost_metrics(explain_cost_text):
            metrics = {
                'total_size_bytes': 0,
                'total_rows': 0,
                'scan_operations': 0,
                'join_operations': 0,
                'memory_estimates': 0,
                'shuffle_partitions': 0
            }
            
            # „Çµ„Ç§„Ç∫„Å®„É°„É¢„É™‰ΩøÁî®Èáè„ÇíÊäΩÂá∫
            size_patterns = [
                r'size_bytes["\s]*[:=]\s*([0-9.]+)',
                r'sizeInBytes["\s]*[:=]\s*([0-9.]+)',
                r'(\d+\.?\d*)\s*[KMG]?iB',
                r'(\d+\.?\d*)\s*[KMG]?B'
            ]
            
            for pattern in size_patterns:
                matches = re.findall(pattern, explain_cost_text, re.IGNORECASE)
                for match in matches:
                    try:
                        size_val = float(match)
                        metrics['total_size_bytes'] += size_val
                    except:
                        continue
            
            # Ë°åÊï∞„ÇíÊäΩÂá∫
            row_patterns = [
                r'rows["\s]*[:=]\s*([0-9]+)',
                r'numRows["\s]*[:=]\s*([0-9]+)'
            ]
            
            for pattern in row_patterns:
                matches = re.findall(pattern, explain_cost_text, re.IGNORECASE)
                for match in matches:
                    try:
                        metrics['total_rows'] += int(match)
                    except:
                        continue
            
            # „Çπ„Ç≠„É£„É≥„ÉªJOINÊìç‰ΩúÊï∞„Çí„Ç´„Ç¶„É≥„Éà
            metrics['scan_operations'] = len(re.findall(r'Scan|FileScan|TableScan', explain_cost_text, re.IGNORECASE))
            metrics['join_operations'] = len(re.findall(r'Join|HashJoin|SortMergeJoin', explain_cost_text, re.IGNORECASE))
            
            # „Ç∑„É£„ÉÉ„Éï„É´„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥Êï∞
            shuffle_matches = re.findall(r'partitions?["\s]*[:=]\s*([0-9]+)', explain_cost_text, re.IGNORECASE)
            for match in shuffle_matches:
                try:
                    metrics['shuffle_partitions'] += int(match)
                except:
                    continue
                    
            return metrics
        
        # ÂÖÉ„ÇØ„Ç®„É™„Å®ÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™„ÅÆ„É°„Éà„É™„ÇØ„ÇπÊäΩÂá∫
        original_metrics = extract_cost_metrics(original_explain_cost)
        optimized_metrics = extract_cost_metrics(optimized_explain_cost)
        
        # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉÔºàÂÄ§„Åå0„ÅÆÂ†¥Âêà„ÅØ1„Å®„Åó„Å¶Ë®àÁÆóÔºâ
        if original_metrics['total_size_bytes'] > 0:
            comparison_result['total_cost_ratio'] = optimized_metrics['total_size_bytes'] / original_metrics['total_size_bytes']
        
        if original_metrics['total_rows'] > 0:
            comparison_result['memory_usage_ratio'] = optimized_metrics['total_rows'] / original_metrics['total_rows']
        
        # üö® Âé≥Ê†º„Å™Âà§ÂÆöÈñæÂÄ§Ôºà„É¶„Éº„Ç∂„ÉºË¶ÅÊ±ÇÔºö‰øùÂÆàÁöÑ„Ç¢„Éó„É≠„Éº„ÉÅÔºâ
        COST_DEGRADATION_THRESHOLD = 1.01   # 1%‰ª•‰∏ä„ÅÆ„Ç≥„Çπ„ÉàÂ¢óÂä†„ÅßÂÖÉ„ÇØ„Ç®„É™Êé®Â•®ÔºàÂé≥Ê†ºÂåñÔºâ
        MEMORY_DEGRADATION_THRESHOLD = 1.01 # 1%‰ª•‰∏ä„ÅÆ„É°„É¢„É™Â¢óÂä†„ÅßÂÖÉ„ÇØ„Ç®„É™Êé®Â•®ÔºàÂé≥Ê†ºÂåñÔºâ
        COST_IMPROVEMENT_THRESHOLD = 0.99   # 1%‰ª•‰∏ä„ÅÆÂâäÊ∏õ„ÅßÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™Êé®Â•®ÔºàÂé≥Ê†ºÂåñÔºâ
        MEMORY_IMPROVEMENT_THRESHOLD = 0.99 # 1%‰ª•‰∏ä„ÅÆÂâäÊ∏õ„ÅßÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™Êé®Â•®ÔºàÂé≥Ê†ºÂåñÔºâ
        
        # üöÄ Â§ßÂπÖÊîπÂñÑ„ÅÆÂà§ÂÆöÈñæÂÄ§Ôºà„É¶„Éº„Ç∂„ÉºË¶ÅÊ±ÇÔºö10%‰ª•‰∏äÊîπÂñÑ„ÅßË©¶Ë°åÁµÇ‰∫ÜÔºâ
        SUBSTANTIAL_COST_IMPROVEMENT_THRESHOLD = 0.9   # 10%‰ª•‰∏ä„ÅÆ„Ç≥„Çπ„ÉàÂâäÊ∏õ„ÅßÂ§ßÂπÖÊîπÂñÑË™çÂÆö
        SUBSTANTIAL_MEMORY_IMPROVEMENT_THRESHOLD = 0.9 # 10%‰ª•‰∏ä„ÅÆ„É°„É¢„É™ÂâäÊ∏õ„ÅßÂ§ßÂπÖÊîπÂñÑË™çÂÆö
        
        # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊÇ™ÂåñÊ§úÂá∫Ôºà„Éû„Éº„Ç∏„É≥„Å™„Åó„ÅßÊòéÁ¢∫„Å™Âà§ÂÆöÔºâ
        degradation_factors = []
        
        # üéØ ÊòéÁ¢∫„Å™ÊÇ™ÂåñÂà§ÂÆöÔºàÂ¢ÉÁïåÂÄ§„ÅÆÊõñÊòß„Åï„ÇíÊéíÈô§Ôºâ
        if comparison_result['total_cost_ratio'] > COST_DEGRADATION_THRESHOLD:
            degradation_factors.append(f"Total execution cost degradation: {comparison_result['total_cost_ratio']:.2f}x (threshold: {COST_DEGRADATION_THRESHOLD:.2f})")
            
        if comparison_result['memory_usage_ratio'] > MEMORY_DEGRADATION_THRESHOLD:
            degradation_factors.append(f"Memory usage degradation: {comparison_result['memory_usage_ratio']:.2f}x (threshold: {MEMORY_DEGRADATION_THRESHOLD:.2f})")
        
        # Check for significant JOIN operations count increase
        if (optimized_metrics['join_operations'] > original_metrics['join_operations'] * 1.5):
            degradation_factors.append(f"JOIN operations count increase: {original_metrics['join_operations']} ‚Üí {optimized_metrics['join_operations']}")
        
        # ÊÇ™ÂåñÂà§ÂÆö
        if degradation_factors:
            comparison_result['performance_degradation_detected'] = True
            comparison_result['is_optimization_beneficial'] = False
            comparison_result['recommendation'] = 'use_original'
            comparison_result['details'] = degradation_factors
        else:
            # ÊÇ™Âåñ„Åß„ÅØ„Å™„ÅÑ„Åå„ÄÅÊîπÂñÑ/ÂêåÁ≠â„ÅÆË©≥Á¥∞Âà§ÂÆö
            performance_factors = []
            
            # üö® Âé≥Ê†º„Å™Ë©≥Á¥∞Âà§ÂÆöÔºà„É¶„Éº„Ç∂„ÉºË¶ÅÊ±ÇÔºö‰øùÂÆàÁöÑ„Ç¢„Éó„É≠„Éº„ÉÅÔºâ
            # ÂÆüË°å„Ç≥„Çπ„Éà„ÅÆË©≥Á¥∞Âà§ÂÆö
            if comparison_result['total_cost_ratio'] < COST_IMPROVEMENT_THRESHOLD:
                performance_factors.append(f"Execution cost improvement: {(1-comparison_result['total_cost_ratio'])*100:.1f}% reduction")
            elif comparison_result['total_cost_ratio'] > COST_DEGRADATION_THRESHOLD:  # 1%‰ª•‰∏ä„ÅÆÂ¢óÂä†„ÅßÂç≥Â∫ß„Å´ÊÇ™ÂåñÂà§ÂÆö
                cost_increase_pct = (comparison_result['total_cost_ratio']-1)*100
                performance_factors.append(f"Execution cost increase: {cost_increase_pct:.1f}% increase (original query recommended)")
            else:
                performance_factors.append(f"Execution cost equivalent: {comparison_result['total_cost_ratio']:.2f}x (no change)")
                
            # „É°„É¢„É™‰ΩøÁî®Èáè„ÅÆË©≥Á¥∞Âà§ÂÆö
            if comparison_result['memory_usage_ratio'] < MEMORY_IMPROVEMENT_THRESHOLD:
                performance_factors.append(f"Memory usage improvement: {(1-comparison_result['memory_usage_ratio'])*100:.1f}% reduction")
            elif comparison_result['memory_usage_ratio'] > MEMORY_DEGRADATION_THRESHOLD:  # 1%‰ª•‰∏ä„ÅÆÂ¢óÂä†„ÅßÂç≥Â∫ß„Å´ÊÇ™ÂåñÂà§ÂÆö
                memory_increase_pct = (comparison_result['memory_usage_ratio']-1)*100
                performance_factors.append(f"Memory usage increase: {memory_increase_pct:.1f}% increase (original query recommended)")
            else:
                performance_factors.append(f"Memory usage equivalent: {comparison_result['memory_usage_ratio']:.2f}x (no change)")
            
            # JOINÂäπÁéáÂåñ„ÉÅ„Çß„ÉÉ„ÇØ
            if optimized_metrics['join_operations'] < original_metrics['join_operations']:
                performance_factors.append(f"JOIN optimization: {original_metrics['join_operations']} ‚Üí {optimized_metrics['join_operations']} operations")
            elif optimized_metrics['join_operations'] > original_metrics['join_operations']:
                performance_factors.append(f"JOIN operations increase: {original_metrics['join_operations']} ‚Üí {optimized_metrics['join_operations']} operations (minor)")
            
            # üö® Âé≥Ê†º„Å™Á∑èÂêàÂà§ÂÆöÔºà„É¶„Éº„Ç∂„ÉºË¶ÅÊ±ÇÔºöÊòéÁ¢∫„Å™ÊîπÂñÑ„ÅÆ„ÅøÊàêÂäüÔºâ
            has_improvement = any("improvement" in factor for factor in performance_factors)
            has_cost_increase = any("cost increase" in factor for factor in performance_factors)
            has_memory_increase = any("memory increase" in factor for factor in performance_factors)
            
            # üö® ÊòéÁ¢∫„Å™ÊîπÂñÑÊ§úÂá∫Ôºà1%‰ª•‰∏ä„ÅÆÊîπÂñÑ„ÅÆ„ÅøÔºâ
            has_significant_improvement = (
                comparison_result['total_cost_ratio'] < COST_IMPROVEMENT_THRESHOLD or
                comparison_result['memory_usage_ratio'] < MEMORY_IMPROVEMENT_THRESHOLD
            )
            
            # üöÄ Â§ßÂπÖÊîπÂñÑÊ§úÂá∫Ôºà10%‰ª•‰∏ä„ÅÆÊîπÂñÑÔºâ
            has_substantial_improvement = (
                comparison_result['total_cost_ratio'] < SUBSTANTIAL_COST_IMPROVEMENT_THRESHOLD or
                comparison_result['memory_usage_ratio'] < SUBSTANTIAL_MEMORY_IMPROVEMENT_THRESHOLD
            )
            
            # üö® Âé≥Ê†ºÂà§ÂÆöÔºö1%‰ª•‰∏ä„ÅÆÂ¢óÂä†„Åß„ÇÇÂÖÉ„ÇØ„Ç®„É™Êé®Â•®
            if has_cost_increase or has_memory_increase:
                performance_factors.insert(0, "‚ùå Performance degradation detected (original query recommended)")
                # üö® Â¢óÂä†Ê§úÂá∫ÊôÇ„ÅØÊé®Â•®„ÇÇÂÖÉ„ÇØ„Ç®„É™„Å´Â§âÊõ¥
                comparison_result['performance_degradation_detected'] = True
                comparison_result['is_optimization_beneficial'] = False  
                comparison_result['recommendation'] = 'use_original'
                comparison_result['significant_improvement_detected'] = False
                comparison_result['substantial_improvement_detected'] = False
            elif has_substantial_improvement:
                # üöÄ Â§ßÂπÖÊîπÂñÑÔºà10%‰ª•‰∏äÔºâ„ÇíÊ§úÂá∫
                cost_reduction = (1 - comparison_result['total_cost_ratio']) * 100
                memory_reduction = (1 - comparison_result['memory_usage_ratio']) * 100
                max_reduction = max(cost_reduction, memory_reduction)
                performance_factors.insert(0, f"üöÄ Significant performance improvement confirmed (max {max_reduction:.1f}% reduction - optimized query recommended)")
                comparison_result['significant_improvement_detected'] = True
                comparison_result['substantial_improvement_detected'] = True
            elif has_significant_improvement:
                performance_factors.insert(0, "‚úÖ Clear performance improvement confirmed (optimized query recommended)")
                comparison_result['significant_improvement_detected'] = True
                comparison_result['substantial_improvement_detected'] = False
            else:
                performance_factors.insert(0, "‚ûñ Performance equivalent (no clear improvement)")
                comparison_result['significant_improvement_detected'] = False
                comparison_result['substantial_improvement_detected'] = False
            
            comparison_result['details'] = performance_factors
        
    except Exception as e:
        # „Ç®„É©„ÉºÊôÇ„ÅØÂÆâÂÖ®ÂÅ¥„Å´ÂÄí„Åó„Å¶ÂÖÉ„ÇØ„Ç®„É™„ÇíÊé®Â•®
        comparison_result['performance_degradation_detected'] = True
        comparison_result['is_optimization_beneficial'] = False
        comparison_result['recommendation'] = 'use_original'
        comparison_result['details'] = [f"„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉ„Ç®„É©„Éº„ÅÆ„Åü„ÇÅÂÖÉ„ÇØ„Ç®„É™‰ΩøÁî®: {str(e)}"]
    
    return comparison_result


def analyze_degradation_causes(performance_comparison: Dict[str, Any], original_explain_cost: str = "", optimized_explain_cost: str = "") -> Dict[str, str]:
    """
    Analyze causes of performance degradation and generate correction instructions
    """
    degradation_analysis = {
        'primary_cause': 'unknown',
        'specific_issues': [],
        'fix_instructions': [],
        'confidence_level': 'low',
        'analysis_details': {}
    }
    
    try:
        if not performance_comparison or not performance_comparison.get('performance_degradation_detected'):
            degradation_analysis['primary_cause'] = 'no_degradation'
            return degradation_analysis
        
        details = performance_comparison.get('details', [])
        cost_ratio = performance_comparison.get('total_cost_ratio', 1.0)
        memory_ratio = performance_comparison.get('memory_usage_ratio', 1.0)
        
        # üîç „Ç≥„Çπ„ÉàÊÇ™Âåñ„ÅÆÊ∑±ÂàªÂ∫¶ÂàÜÊûê
        if cost_ratio > 1.5:  # 50%‰ª•‰∏ä„ÅÆÊÇ™Âåñ
            degradation_analysis['confidence_level'] = 'high'
            severity = 'critical'
        elif cost_ratio > 1.3:  # 30%‰ª•‰∏ä„ÅÆÊÇ™Âåñ
            degradation_analysis['confidence_level'] = 'medium'
            severity = 'significant'
        else:
            degradation_analysis['confidence_level'] = 'low'
            severity = 'minor'
        
        degradation_analysis['analysis_details']['cost_degradation_severity'] = severity
        degradation_analysis['analysis_details']['cost_ratio'] = cost_ratio
        degradation_analysis['analysis_details']['memory_ratio'] = memory_ratio
        
        # üéØ ‰∏ªË¶ÅÂéüÂõ†„ÅÆÁâπÂÆö„Å®JOINÊìç‰ΩúÊï∞ÂàÜÊûê
        for detail in details:
            detail_str = str(detail).lower()
            
            # Detect significant JOIN operations count increase
            if 'join operations count increase' in detail_str or 'join' in detail_str:
                degradation_analysis['primary_cause'] = 'excessive_joins'
                degradation_analysis['specific_issues'].append('Significant JOIN operations count increase')
                
                # JOINÊï∞„ÅÆÂÖ∑‰ΩìÁöÑ„Å™Â¢óÂä†„ÇíËß£Êûê
                import re
                join_match = re.search(r'(\d+)\s*‚Üí\s*(\d+)', detail_str)
                if join_match:
                    original_joins = int(join_match.group(1))
                    optimized_joins = int(join_match.group(2))
                    join_increase_ratio = optimized_joins / original_joins if original_joins > 0 else float('inf')
                    
                    degradation_analysis['analysis_details']['original_joins'] = original_joins
                    degradation_analysis['analysis_details']['optimized_joins'] = optimized_joins
                    degradation_analysis['analysis_details']['join_increase_ratio'] = join_increase_ratio
                    
                    if join_increase_ratio > 1.5:  # 50%‰ª•‰∏ä„ÅÆJOINÂ¢óÂä†
                        degradation_analysis['fix_instructions'].extend([
                            "JOINÈ†ÜÂ∫è„ÅÆÂäπÁéáÂåñ„ÇíÊ§úË®é„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                            "ÂÖÉ„ÅÆJOINÈ†ÜÂ∫è„ÇíÂ∞äÈáç„Åó„ÄÅÂ§ßÂπÖ„Å™ÊßãÈÄ†Â§âÊõ¥„ÇíÈÅø„Åë„Å¶„Åè„Å†„Åï„ÅÑ",
                            "‰∏çË¶Å„Å™„Çµ„Éñ„ÇØ„Ç®„É™Âåñ„Å´„Çà„ÇãJOINÈáçË§á„ÇíÈò≤„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ",
                            "CTEÂ±ïÈñã„Å´„Çà„ÇãJOINÂ¢óÂä†„ÇíÈÅø„Åë„ÄÅÂÖÉ„ÅÆÊßãÈÄ†„Çí‰øùÊåÅ„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
                        ])
                
            # Total execution cost degradation
            elif 'total execution cost degradation' in detail_str or 'cost' in detail_str:
                if degradation_analysis['primary_cause'] == 'unknown':
                    degradation_analysis['primary_cause'] = 'cost_increase'
                degradation_analysis['specific_issues'].append('Total execution cost degradation')
                degradation_analysis['fix_instructions'].extend([
                    "Â∞è„ÉÜ„Éº„Éñ„É´„ÇíÂäπÁéáÁöÑ„Å´JOIN„ÅßÂá¶ÁêÜ„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                                         "Â§ß„Åç„Å™„ÉÜ„Éº„Éñ„É´„ÅÆJOINÈ†ÜÂ∫è„ÇíÊúÄÈÅ©Âåñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "REPARTITION„Éí„É≥„Éà„ÅÆÈÖçÁΩÆ‰ΩçÁΩÆ„ÇíË¶ãÁõ¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
                ])
            
            # Memory usage degradation
            elif 'memory usage degradation' in detail_str or 'memory' in detail_str:
                if degradation_analysis['primary_cause'] == 'unknown':
                    degradation_analysis['primary_cause'] = 'memory_increase'
                degradation_analysis['specific_issues'].append('Memory usage degradation')
                degradation_analysis['fix_instructions'].extend([
                    "Â§ß„Åç„Å™„ÉÜ„Éº„Éñ„É´„ÅÆBROADCASTÈÅ©Áî®„ÇíÂâäÈô§„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "„É°„É¢„É™ÂäπÁéáÁöÑ„Å™JOINÊà¶Áï•„ÇíÈÅ∏Êäû„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "‰∏≠ÈñìÁµêÊûú„ÅÆ„Çµ„Ç§„Ç∫„ÇíÂâäÊ∏õ„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
                ])
        
        # üîç EXPLAIN COSTÂàÜÊûê„Å´„Çà„ÇãË©≥Á¥∞ÂéüÂõ†ÁâπÂÆöÔºàÂà©Áî®ÂèØËÉΩ„Å™Â†¥ÂêàÔºâ
        if original_explain_cost and optimized_explain_cost:
            cost_analysis = analyze_explain_cost_differences(original_explain_cost, optimized_explain_cost)
            degradation_analysis['analysis_details']['explain_cost_analysis'] = cost_analysis
            
            # BROADCASTÈñ¢ÈÄ£„ÅÆÂïèÈ°åÊ§úÂá∫
            if cost_analysis.get('broadcast_issues'):
                degradation_analysis['fix_instructions'].extend([
                    "Ê§úÂá∫„Åï„Çå„ÅüBROADCASTÂïèÈ°å„Çí‰øÆÊ≠£„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "ÈÅ©Âàá„Å™„Çµ„Ç§„Ç∫„ÅÆ„ÉÜ„Éº„Éñ„É´„ÅÆ„ÅøBROADCASTÂØæË±°„Å®„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
                ])
        
        # ÂéüÂõ†„ÅåÁâπÂÆö„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
        if degradation_analysis['primary_cause'] == 'unknown':
            degradation_analysis['primary_cause'] = 'optimization_backfire'
            degradation_analysis['fix_instructions'].extend([
                "ÊúÄÈÅ©Âåñ„Ç¢„Éó„É≠„Éº„ÉÅ„Çí‰øùÂÆàÁöÑ„Å´Â§âÊõ¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                "ÂÖÉ„ÅÆ„ÇØ„Ç®„É™ÊßãÈÄ†„Çí„Çà„ÇäÂ§ö„Åè‰øùÊåÅ„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                "„Éí„É≥„ÉàÂè•„ÅÆÈÅ©Áî®„ÇíÊúÄÂ∞èÈôê„Å´Êäë„Åà„Å¶„Åè„Å†„Åï„ÅÑ"
            ])
        
        # ÈáçË§á„Åô„Çã‰øÆÊ≠£ÊåáÁ§∫„ÇíÂâäÈô§
        degradation_analysis['fix_instructions'] = list(set(degradation_analysis['fix_instructions']))
        
    except Exception as e:
        degradation_analysis['primary_cause'] = 'analysis_error'
        degradation_analysis['specific_issues'] = [f"ÂàÜÊûê„Ç®„É©„Éº: {str(e)}"]
        degradation_analysis['fix_instructions'] = [
            "‰øùÂÆàÁöÑ„Å™ÊúÄÈÅ©Âåñ„Ç¢„Éó„É≠„Éº„ÉÅ„Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
            "ÂÖÉ„ÅÆ„ÇØ„Ç®„É™ÊßãÈÄ†„ÇíÊúÄÂ§ßÈôê‰øùÊåÅ„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
        ]
    
    return degradation_analysis


def analyze_explain_cost_differences(original_cost: str, optimized_cost: str) -> Dict[str, Any]:
    """
    Identify degradation causes through differential analysis of EXPLAIN COST results
    """
    analysis = {
        'broadcast_issues': False,
        'join_strategy_changes': [],
        'size_estimation_problems': [],
        'plan_structure_changes': []
    }
    
    try:
        import re
        
        # BROADCASTÈñ¢ÈÄ£„ÅÆÂïèÈ°åÊ§úÂá∫
        original_broadcasts = len(re.findall(r'broadcast', original_cost.lower()))
        optimized_broadcasts = len(re.findall(r'broadcast', optimized_cost.lower()))
        
        if optimized_broadcasts > original_broadcasts * 2:  # BROADCAST‰ΩøÁî®Èáè„Åå2ÂÄç‰ª•‰∏äÂ¢óÂä†
            analysis['broadcast_issues'] = True
            analysis['join_strategy_changes'].append(f"BROADCAST‰ΩøÁî®„ÅåÂ§ßÂπÖÂ¢óÂä†: {original_broadcasts} ‚Üí {optimized_broadcasts}")
        
        # JOINÊà¶Áï•„ÅÆÂ§âÂåñÊ§úÂá∫
        original_join_types = set(re.findall(r'(\w+)Join', original_cost))
        optimized_join_types = set(re.findall(r'(\w+)Join', optimized_cost))
        
        if optimized_join_types != original_join_types:
            analysis['join_strategy_changes'].append(f"JOINÊà¶Áï•Â§âÂåñ: {original_join_types} ‚Üí {optimized_join_types}")
        
        # „Éó„É©„É≥ÊßãÈÄ†„ÅÆË§áÈõëÂåñÊ§úÂá∫
        original_plan_depth = original_cost.count('+-')
        optimized_plan_depth = optimized_cost.count('+-')
        
        if optimized_plan_depth > original_plan_depth * 1.3:  # „Éó„É©„É≥Ê∑±Â∫¶„Åå30%‰ª•‰∏äÂ¢óÂä†
            analysis['plan_structure_changes'].append(f"ÂÆüË°å„Éó„É©„É≥Ë§áÈõëÂåñ: Ê∑±Â∫¶ {original_plan_depth} ‚Üí {optimized_plan_depth}")
        
    except Exception as e:
        analysis['analysis_error'] = str(e)
    
    return analysis


def execute_iterative_optimization_with_degradation_analysis(original_query: str, analysis_result: str, metrics: Dict[str, Any], max_optimization_attempts: int = 3) -> Dict[str, Any]:
    """
    Iterative optimization and performance degradation analysis
    Attempt re-optimization up to 3 times by analyzing degradation causes, use original query if no improvement
    """
    from datetime import datetime
    
    print(f"\nüöÄ Starting iterative optimization process (maximum {max_optimization_attempts} improvement attempts)")
    print("üéØ Goal: Achieve 10%+ cost reduction | Select best result when maximum attempts reached")
    print("=" * 70)
    
    optimization_attempts = []
    original_query_for_explain = original_query  # ÂÖÉ„ÇØ„Ç®„É™„ÅÆ‰øùÊåÅ
    
    # üöÄ „Éô„Çπ„ÉàÁµêÊûúËøΩË∑°Ôºà„É¶„Éº„Ç∂„ÉºË¶ÅÊ±ÇÔºöÊúÄÂ§ßË©¶Ë°åÂõûÊï∞Âà∞ÈÅîÊôÇ„ÅØÊúÄ„ÇÇËâØ„ÅÑÁµêÊûú„ÇíÈÅ∏ÊäûÔºâ
    best_result = {
        'attempt_num': 0,
        'query': original_query,
        'cost_ratio': 1.0,
        'memory_ratio': 1.0,
        'performance_comparison': None,
        'optimized_result': '',
        'status': 'baseline'
    }
    
    # üöÄ „Ç™„É™„Ç∏„Éä„É´„ÇØ„Ç®„É™„ÅÆEXPLAINÁµêÊûú„Ç≠„É£„ÉÉ„Ç∑„É•ÔºàÈáçË§áÂÆüË°åÈò≤Ê≠¢Ôºâ
    original_explain_cost_result = None
    corrected_original_query = globals().get('original_query_corrected', original_query)
    
    for attempt_num in range(1, max_optimization_attempts + 1):
        print(f"\nüîÑ Optimization attempt {attempt_num}/{max_optimization_attempts}")
        print("-" * 50)
        
        # ÂâçÂõû„ÅÆË©¶Ë°åÁµêÊûú„Å´Âü∫„Å•„Åè‰øÆÊ≠£ÊåáÁ§∫„ÇíÁîüÊàê
        fix_instructions = ""
        if attempt_num > 1 and optimization_attempts:
            previous_attempt = optimization_attempts[-1]
            if previous_attempt.get('degradation_analysis'):
                degradation_analysis = previous_attempt['degradation_analysis']
                fix_instructions = "\n".join([
                    f"„ÄêÂâçÂõû„ÅÆÊÇ™ÂåñÂéüÂõ†: {degradation_analysis['primary_cause']}„Äë",
                    f"„Äê‰ø°È†ºÂ∫¶: {degradation_analysis['confidence_level']}„Äë",
                    "„Äê‰øÆÊ≠£ÊåáÁ§∫„Äë"
                ] + degradation_analysis['fix_instructions'])
                
                print(f"üîß Degradation cause analysis result: {degradation_analysis['primary_cause']}")
                print(f"üìä Confidence level: {degradation_analysis['confidence_level']}")
                print(f"üí° Fix instructions: {len(degradation_analysis['fix_instructions'])} items")
        
        # ÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™ÁîüÊàêÔºàÂàùÂõû or ‰øÆÊ≠£ÁâàÔºâ
        if attempt_num == 1:
            print("ü§ñ Initial optimization query generation")
            optimized_query = generate_optimized_query_with_llm(original_query, analysis_result, metrics)
            # üêõ DEBUG: ÂàùÂõûË©¶Ë°å„ÇØ„Ç®„É™„Çí‰øùÂ≠ò
            if isinstance(optimized_query, str) and not optimized_query.startswith("LLM_ERROR:"):
                save_debug_query_trial(optimized_query, attempt_num, "initial")
        else:
            print(f"üîß Corrected optimization query generation (attempt {attempt_num})")
            # üö® ‰øÆÊ≠£: „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊÇ™ÂåñÂ∞ÇÁî®Èñ¢Êï∞„Çí‰ΩøÁî®
            previous_attempt = optimization_attempts[-1] if optimization_attempts else {}
            degradation_analysis = previous_attempt.get('degradation_analysis', {})
            optimized_query = generate_improved_query_for_performance_degradation(
                original_query, 
                analysis_result, 
                metrics, 
                degradation_analysis, 
                previous_attempt.get('optimized_query', '')
            )
            # üêõ DEBUG: „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊîπÂñÑË©¶Ë°å„ÇØ„Ç®„É™„Çí‰øùÂ≠ò
            if isinstance(optimized_query, str) and not optimized_query.startswith("LLM_ERROR:"):
                degradation_cause = degradation_analysis.get('primary_cause', '„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊÇ™Âåñ')
                save_debug_query_trial(optimized_query, attempt_num, "performance_improvement", 
                                     error_info=f"ÂâçÂõûÊÇ™ÂåñÂéüÂõ†: {degradation_cause}")
        
        # LLM„Ç®„É©„Éº„ÉÅ„Çß„ÉÉ„ÇØ
        if isinstance(optimized_query, str) and optimized_query.startswith("LLM_ERROR:"):
            print(f"‚ùå LLM error occurred in optimization attempt {attempt_num}")
            optimization_attempts.append({
                'attempt': attempt_num,
                'status': 'llm_error',
                'error': optimized_query[10:],
                'optimized_query': None
            })
            continue
        
        # „ÇØ„Ç®„É™ÊäΩÂá∫
        if isinstance(optimized_query, list):
            optimized_query_str = extract_main_content_from_thinking_response(optimized_query)
        else:
            optimized_query_str = str(optimized_query)
        
        extracted_sql = extract_sql_from_llm_response(optimized_query_str)
        current_query = extracted_sql if extracted_sql else original_query
        
        # EXPLAINÂÆüË°å„Å®ÊßãÊñá„ÉÅ„Çß„ÉÉ„ÇØ
        explain_result = execute_explain_with_retry_logic(current_query, analysis_result, metrics, max_retries=MAX_RETRIES)
        
        if explain_result['final_status'] != 'success':
            print(f"‚ö†Ô∏è Attempt {attempt_num}: EXPLAIN execution failed")
            optimization_attempts.append({
                'attempt': attempt_num,
                'status': 'explain_failed',
                'error': explain_result.get('error_details', 'Unknown error'),
                'optimized_query': current_query
            })
            continue
        
        # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉÂÆüË°å
        print(f"üîç Attempt {attempt_num}: Executing performance degradation detection")
        
        # üéØ „Ç≠„É£„ÉÉ„Ç∑„É•„Åï„Çå„ÅüÂÖÉ„ÇØ„Ç®„É™„Çí‰ΩøÁî®ÔºàÈáçË§áÂá¶ÁêÜÈò≤Ê≠¢Ôºâ
        if corrected_original_query != original_query:
            print("üíæ Using cached original query: Preventing duplicate processing")
        
        # üöÄ ÂÖÉ„ÇØ„Ç®„É™„ÅÆEXPLAIN COSTÂèñÂæóÔºàÂàùÂõû„ÅÆ„ÅøÂÆüË°å„ÄÅ‰ª•Èôç„ÅØ„Ç≠„É£„ÉÉ„Ç∑„É•‰ΩøÁî®Ôºâ
        if original_explain_cost_result is None:
            print(f"üîÑ Attempt {attempt_num}: Executing EXPLAIN COST for original query (first time only)")
            original_explain_cost_result = execute_explain_and_save_to_file(corrected_original_query, "original_performance_check")
            # „Ç∞„É≠„Éº„Éê„É´„Ç≠„É£„ÉÉ„Ç∑„É•„Å´‰øùÂ≠ò
            globals()['cached_original_explain_cost_result'] = original_explain_cost_result
        else:
            print(f"üíæ Attempt {attempt_num}: Using cached EXPLAIN COST result for original query (avoiding duplicate execution)")
            # „Ç≠„É£„ÉÉ„Ç∑„É•„Åã„ÇâÂæ©ÂÖÉ
            original_explain_cost_result = globals().get('cached_original_explain_cost_result', original_explain_cost_result)
        
        # ÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™„ÅÆEXPLAIN COSTÂèñÂæó
        optimized_explain_cost_result = execute_explain_and_save_to_file(current_query, f"optimized_attempt_{attempt_num}")
        
        performance_comparison = None
        degradation_analysis = None
        
        # üîç EXPLAIN COST„Ç®„É©„Éº„Éè„É≥„Éâ„É™„É≥„Ç∞„ÅÆÊîπÂñÑ
        original_cost_success = ('explain_cost_file' in original_explain_cost_result and 
                                'error_file' not in original_explain_cost_result)
        optimized_cost_success = ('explain_cost_file' in optimized_explain_cost_result and 
                                 'error_file' not in optimized_explain_cost_result)
        
        # üö® Á∑äÊÄ•„Éá„Éê„ÉÉ„Ç∞: EXPLAIN COSTÊàêÂäü/Â§±Êïó„ÅÆË©≥Á¥∞Ë°®Á§∫
        print(f"üîç EXPLAIN COST success determination:")
        print(f"   üìä Original query: {'‚úÖ Success' if original_cost_success else '‚ùå Failed'}")
        if not original_cost_success:
            print(f"      ‚Ä¢ explain_cost_file exists: {'explain_cost_file' in original_explain_cost_result}")
            print(f"      ‚Ä¢ error_file exists: {'error_file' in original_explain_cost_result}")
            print(f"      ‚Ä¢ Return keys: {list(original_explain_cost_result.keys())}")
        print(f"   üîß Optimized query: {'‚úÖ Success' if optimized_cost_success else '‚ùå Failed'}")
        if not optimized_cost_success:
            print(f"      ‚Ä¢ explain_cost_file exists: {'explain_cost_file' in optimized_explain_cost_result}")
            print(f"      ‚Ä¢ error_file exists: {'error_file' in optimized_explain_cost_result}")
            print(f"      ‚Ä¢ Return keys: {list(optimized_explain_cost_result.keys())}")
        
        if not original_cost_success:
            print("‚ö†Ô∏è Original query EXPLAIN COST execution failed: Skipping performance comparison")
            if 'error_file' in original_explain_cost_result:
                print(f"üìÑ Error details: {original_explain_cost_result['error_file']}")
        
        if not optimized_cost_success:
            print("‚ö†Ô∏è Optimized query EXPLAIN COST execution failed: Attempting error correction")
            if 'error_file' in optimized_explain_cost_result:
                print(f"üìÑ Error details: {optimized_explain_cost_result['error_file']}")
                
                # üö® CRITICAL FIX: „Ç®„É©„ÉºÊ§úÂá∫ÊôÇ„ÅØÂç≥Â∫ß„Å´LLM‰øÆÊ≠£„ÇíÂÆüË°å
                print("üîß Executing LLM-based error correction...")
                error_message = optimized_explain_cost_result.get('error_message', 'Unknown error')
                
                # „Ç®„É©„Éº‰øÆÊ≠£„ÅÆ„Åü„ÇÅ„ÅÆLLMÂëº„Å≥Âá∫„Åó
                corrected_query = generate_optimized_query_with_error_feedback(
                    original_query,
                    analysis_result, 
                    metrics,
                    error_message,
                    current_query  # ÁèæÂú®„ÅÆ„ÇØ„Ç®„É™Ôºà„Éí„É≥„Éà‰ªò„ÅçÔºâ„ÇíÊ∏°„Åô
                )
                
                # üêõ DEBUG: „Ç®„É©„Éº‰øÆÊ≠£„ÇØ„Ç®„É™„Çí‰øùÂ≠ò
                if isinstance(corrected_query, str) and not corrected_query.startswith("LLM_ERROR:"):
                    save_debug_query_trial(corrected_query, attempt_num, "error_correction", 
                                         error_info=f"‰øÆÊ≠£ÂØæË±°„Ç®„É©„Éº: {error_message[:100]}")
                
                # LLM„Ç®„É©„Éº„ÉÅ„Çß„ÉÉ„ÇØ
                if isinstance(corrected_query, str) and corrected_query.startswith("LLM_ERROR:"):
                    print("‚ùå Error occurred in LLM correction: Executing fallback evaluation")
                else:
                    # thinking_enabledÂØæÂøú
                    if isinstance(corrected_query, list):
                        corrected_query_str = extract_main_content_from_thinking_response(corrected_query)
                    else:
                        corrected_query_str = str(corrected_query)
                    
                    # SQL„ÇØ„Ç®„É™ÈÉ®ÂàÜ„ÅÆ„Åø„ÇíÊäΩÂá∫
                    extracted_sql = extract_sql_from_llm_response(corrected_query_str)
                    if extracted_sql:
                        current_query = extracted_sql
                        print("‚úÖ LLM-based error correction completed, re-evaluating with corrected query")
                        
                        # ‰øÆÊ≠£„ÇØ„Ç®„É™„ÅßÂÜçÂ∫¶EXPLAINÂÆüË°å
                        optimized_explain_cost_result = execute_explain_and_save_to_file(current_query, f"optimized_attempt_{attempt_num}_corrected")
                        optimized_cost_success = ('explain_cost_file' in optimized_explain_cost_result and 
                                                'error_file' not in optimized_explain_cost_result)
                        
                        if optimized_cost_success:
                            print("üéØ Corrected query EXPLAIN execution successful!")
                        else:
                            print("‚ö†Ô∏è Error occurred even with corrected query: Executing fallback evaluation")
                    else:
                        print("‚ùå Failed to extract SQL query: Executing fallback evaluation")
            
            # „Ç®„É©„Éº‰øÆÊ≠£Âæå„ÇÇ„Ç®„É©„Éº„ÅÆÂ†¥Âêà„ÄÅ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØË©ï‰æ°„ÇíÂÆüË°å
            if not optimized_cost_success:
                print("üîÑ Executing fallback evaluation")
        
        # üö® Á∑äÊÄ•‰øÆÊ≠£: EXPLAIN COSTÂ§±ÊïóÊôÇ„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπË©ï‰æ°
        if not (original_cost_success and optimized_cost_success):
            print("üîÑ Fallback: Executing simple performance evaluation using EXPLAIN results")
            
            # EXPLAINÁµêÊûú„ÅåÂà©Áî®ÂèØËÉΩ„Å™Â†¥Âêà„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØË©ï‰æ°
            original_explain_success = ('explain_file' in original_explain_cost_result and 
                                       'error_file' not in original_explain_cost_result)
            optimized_explain_success = ('explain_file' in optimized_explain_cost_result and 
                                        'error_file' not in optimized_explain_cost_result)
            
            if original_explain_success and optimized_explain_success:
                try:
                    # EXPLAINÁµêÊûú„ÇíË™≠„ÅøËæº„Åø
                    with open(original_explain_cost_result['explain_file'], 'r', encoding='utf-8') as f:
                        original_explain_content = f.read()
                    
                    with open(optimized_explain_cost_result['explain_file'], 'r', encoding='utf-8') as f:
                        optimized_explain_content = f.read()
                    
                    # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØË©ï‰æ°ÂÆüË°å
                    fallback_evaluation = fallback_performance_evaluation(original_explain_content, optimized_explain_content)
                    
                    print(f"üìä Fallback evaluation result: {fallback_evaluation['summary']}")
                    print(f"   - Recommendation: {fallback_evaluation['recommendation']}")
                    print(f"   - Confidence: {fallback_evaluation['confidence']}")
                    
                    for detail in fallback_evaluation['details']:
                        print(f"   - {detail}")
                    
                    # performance_comparison„ÅÆ‰ª£Êõø„Å®„Åó„Å¶‰ΩøÁî®
                    performance_comparison = {
                        'is_optimization_beneficial': fallback_evaluation['recommendation'] == 'use_optimized',
                        'performance_degradation_detected': fallback_evaluation['overall_status'] == 'degradation_possible',
                        'significant_improvement_detected': fallback_evaluation['overall_status'] == 'clear_improvement',  # üö® „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØË©ï‰æ°„Åß„ÇÇÊòéÁ¢∫ÊîπÂñÑÊ§úÂá∫
                        'recommendation': fallback_evaluation['recommendation'],
                        'evaluation_type': 'fallback_plan_analysis',
                        'details': fallback_evaluation['details'],
                        'fallback_evaluation': fallback_evaluation,
                        'total_cost_ratio': 1.0,  # EXPLAIN COST„Å™„Åó„ÅÆ„Åü„ÇÅÊú™Áü•
                        'memory_usage_ratio': 1.0  # EXPLAIN COST„Å™„Åó„ÅÆ„Åü„ÇÅÊú™Áü•
                    }
                    
                    print("‚úÖ Fallback performance evaluation completed")
                    
                    # üö® „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØË©ï‰æ°„Åß„ÇÇÂé≥Ê†ºÂà§ÂÆöÈÅ©Áî®
                    if not performance_comparison.get('significant_improvement_detected', False):
                        if performance_comparison['performance_degradation_detected']:
                            print(f"üö® Attempt {attempt_num}: Possibility of degradation in fallback evaluation")
                            status_reason = "fallback_degradation_detected"
                        else:
                            print(f"‚ö†Ô∏è Attempt {attempt_num}: Clear improvement not confirmed in fallback evaluation")
                            status_reason = "fallback_insufficient_improvement"
                        
                        optimization_attempts.append({
                            'attempt': attempt_num,
                            'status': status_reason,
                            'optimized_query': current_query,
                            'performance_comparison': performance_comparison,
                            'cost_ratio': performance_comparison['total_cost_ratio'],
                            'memory_ratio': performance_comparison['memory_usage_ratio']
                        })
                        
                        # üöÄ „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØË©ï‰æ°„Åß„ÇÇ„Éô„Çπ„ÉàÁµêÊûúËøΩË∑°
                        current_cost_ratio = performance_comparison.get('total_cost_ratio', 1.0)
                        current_memory_ratio = performance_comparison.get('memory_usage_ratio', 1.0)
                        
                        # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØË©ï‰æ°„Åß„ÅØÊîπÂñÑ„ÅÆÂ†¥Âêà„ÅÆ„Åø„Éô„Çπ„ÉàÊõ¥Êñ∞Ôºà‰∏çÁ¢∫ÂÆüÊÄß„ÇíËÄÉÊÖÆÔºâ
                        if performance_comparison.get('significant_improvement_detected', False):
                            is_better_than_best = (
                                current_cost_ratio < best_result['cost_ratio'] or 
                                (current_cost_ratio == best_result['cost_ratio'] and current_memory_ratio < best_result['memory_ratio'])
                            )
                            
                            if is_better_than_best:
                                print(f"üèÜ Attempt {attempt_num}: New best result in fallback evaluation!")
                                best_result.update({
                                    'attempt_num': attempt_num,
                                    'query': current_query,
                                    'cost_ratio': current_cost_ratio,
                                    'memory_ratio': current_memory_ratio,
                                    'performance_comparison': performance_comparison,
                                    'optimized_result': optimized_query_str,
                                    'status': 'fallback_improved'
                                })
                        
                        optimization_attempts.append({
                            'attempt': attempt_num,
                            'status': status_reason,
                            'optimized_query': current_query,
                            'performance_comparison': performance_comparison,
                            'cost_ratio': current_cost_ratio,
                            'memory_ratio': current_memory_ratio
                        })
                        
                        # üöÄ „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØË©ï‰æ°„Åß„ÅØÂ§ßÂπÖÊîπÂñÑÂà§ÂÆö„ÅåÂõ∞Èõ£„Å™„Åü„ÇÅ„ÄÅË©¶Ë°åÁ∂ôÁ∂ö
                        if attempt_num < max_optimization_attempts:
                            print(f"üîÑ Aiming for more reliable improvement in attempt {attempt_num + 1} (fallback evaluation)")
                            continue
                        else:
                            print(f"‚è∞ Maximum attempts ({max_optimization_attempts}) reached ‚Üí Selecting best result")
                            break
                    
                except Exception as e:
                    print(f"‚ùå Error in fallback evaluation as well: {str(e)}")
                    print(f"   üìä Error details: {type(e).__name__}")
                    if hasattr(e, '__traceback__'):
                        import traceback
                        print(f"   üìÑ Stack trace: {traceback.format_exc()}")
                    performance_comparison = None
            else:
                print("‚ùå EXPLAIN results also insufficient, performance evaluation impossible")
                performance_comparison = None
        
        # üö® Á∑äÊÄ•‰øÆÊ≠£: „É≠„Ç∏„ÉÉ„ÇØÈ†ÜÂ∫è„Çí‰øÆÊ≠£ÔºàEXPLAIN COSTÊàêÂäüÂà§ÂÆö„ÇíÂÖà„Å´ÂÆüË°åÔºâ
        if (original_cost_success and optimized_cost_success):
            
            try:
                print(f"üéØ Both EXPLAIN COST successful ‚Üí Executing performance comparison")
                
                # EXPLAIN COSTÂÜÖÂÆπ„ÇíË™≠„ÅøËæº„Åø
                with open(original_explain_cost_result['explain_cost_file'], 'r', encoding='utf-8') as f:
                    original_cost_content = f.read()
                
                with open(optimized_explain_cost_result['explain_cost_file'], 'r', encoding='utf-8') as f:
                    optimized_cost_content = f.read()
                
                print(f"   üìä Original query COST content length: {len(original_cost_content)} characters")
                print(f"   üîß Optimized query COST content length: {len(optimized_cost_content)} characters")
                
                # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉÂÆüË°å
                print(f"üîç Executing compare_query_performance...")
                performance_comparison = compare_query_performance(original_cost_content, optimized_cost_content)
                print(f"‚úÖ compare_query_performance completed: {performance_comparison is not None}")
                
                if performance_comparison:
                    print(f"   üìä significant_improvement_detected: {performance_comparison.get('significant_improvement_detected', 'UNKNOWN')}")
                    print(f"   üìä performance_degradation_detected: {performance_comparison.get('performance_degradation_detected', 'UNKNOWN')}")
                    print(f"   üìä is_optimization_beneficial: {performance_comparison.get('is_optimization_beneficial', 'UNKNOWN')}")
                else:
                    print(f"‚ùå performance_comparison is None!")
                
                # üöÄ „Éô„Çπ„ÉàÁµêÊûúÊõ¥Êñ∞Âà§ÂÆöÔºà„É¶„Éº„Ç∂„ÉºË¶ÅÊ±ÇÔºöÂ∏∏„Å´ÊúÄËâØÁµêÊûú„ÇíËøΩË∑°Ôºâ
                current_cost_ratio = performance_comparison['total_cost_ratio']
                current_memory_ratio = performance_comparison['memory_usage_ratio']
                
                # ÁèæÂú®„ÅÆÁµêÊûú„Åå„Éô„Çπ„Éà„Çí‰∏äÂõû„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØÔºà„Ç≥„Çπ„ÉàÊØîÁéá„Åå‰Ωé„ÅÑ„Åª„Å©ËâØ„ÅÑÔºâ
                is_better_than_best = (
                    current_cost_ratio < best_result['cost_ratio'] or 
                    (current_cost_ratio == best_result['cost_ratio'] and current_memory_ratio < best_result['memory_ratio'])
                )
                
                if is_better_than_best:
                    print(f"üèÜ Attempt {attempt_num}: New best result recorded!")
                    print(f"   üìä Cost ratio: {best_result['cost_ratio']:.3f} ‚Üí {current_cost_ratio:.3f}")
                    print(f"   üíæ Memory ratio: {best_result['memory_ratio']:.3f} ‚Üí {current_memory_ratio:.3f}")
                    best_result.update({
                        'attempt_num': attempt_num,
                        'query': current_query,
                        'cost_ratio': current_cost_ratio,
                        'memory_ratio': current_memory_ratio,
                        'performance_comparison': performance_comparison,
                        'optimized_result': optimized_query_str,
                        'status': 'improved'
                    })
                
                # üöÄ Â§ßÂπÖÊîπÂñÑÔºà10%‰ª•‰∏äÔºâÈÅîÊàê„ÅßÂç≥Â∫ß„Å´ÁµÇ‰∫Ü
                if performance_comparison.get('substantial_improvement_detected', False):
                    print(f"üöÄ Attempt {attempt_num}: Significant improvement achieved (10%+ reduction)! Optimization completed immediately")
                    optimization_attempts.append({
                        'attempt': attempt_num,
                        'status': 'substantial_success',
                        'optimized_query': current_query,
                        'performance_comparison': performance_comparison,
                        'cost_ratio': current_cost_ratio,
                        'memory_ratio': current_memory_ratio
                    })
                    
                    return {
                        'final_status': 'optimization_success',
                        'final_query': current_query,
                        'successful_attempt': attempt_num,
                        'total_attempts': attempt_num,
                        'optimization_attempts': optimization_attempts,
                        'performance_comparison': performance_comparison,
                        'optimized_result': optimized_query_str,
                        'saved_files': None,  # „É°„Ç§„É≥Âá¶ÁêÜ„Åß‰øùÂ≠ò
                        'achievement_type': 'substantial_improvement'
                    }
                
                # üöÄ ÊîπÂñÑ„ÅØ„ÅÇ„Çã„ÅåÂ§ßÂπÖ„Åß„Å™„ÅÑÂ†¥Âêà„ÅÆÂà§ÂÆö
                elif performance_comparison.get('significant_improvement_detected', False):
                    print(f"‚úÖ Attempt {attempt_num}: Improvement confirmed (target 10% not reached)")
                    status_reason = "partial_improvement"
                else:
                    # ÊîπÂñÑ„Å™„Åó„Åæ„Åü„ÅØÊÇ™Âåñ„ÅÆÂ†¥Âêà
                    if performance_comparison['performance_degradation_detected']:
                        print(f"üö® Attempt {attempt_num}: Performance increase detected")
                        status_reason = "performance_degraded"
                    else:
                        print(f"‚ö†Ô∏è Attempt {attempt_num}: Clear improvement cannot be confirmed")
                        status_reason = "insufficient_improvement"
                
                # ÊÇ™ÂåñÂéüÂõ†ÂàÜÊûêÔºàÊîπÂñÑ‰∏çË∂≥„ÅÆÂ†¥Âêà„ÇÇÂÆüË°åÔºâ
                degradation_analysis = analyze_degradation_causes(performance_comparison, original_cost_content, optimized_cost_content)
                
                print(f"   Details: {', '.join(performance_comparison.get('details', []))}")
                
                optimization_attempts.append({
                    'attempt': attempt_num,
                    'status': status_reason,
                    'optimized_query': current_query,
                    'performance_comparison': performance_comparison,
                    'degradation_analysis': degradation_analysis,
                    'cost_ratio': current_cost_ratio,
                    'memory_ratio': current_memory_ratio
                })
                
                # üöÄ Êñ∞Âà§ÂÆö: Â§ßÂπÖÊîπÂñÑÔºà10%‰ª•‰∏äÔºâ„Åß„Å™„ÅÑÈôê„ÇäË©¶Ë°åÁ∂ôÁ∂ö
                if attempt_num < max_optimization_attempts:
                    print(f"üîÑ Aiming for significant improvement (10%+ reduction) in attempt {attempt_num + 1}")
                    continue
                else:
                    print(f"‚è∞ Maximum attempts ({max_optimization_attempts}) reached ‚Üí Selecting best result")
                    break
            
            except Exception as e:
                print(f"‚ùå Attempt {attempt_num}: Error in performance comparison: {str(e)}")
                print(f"   üìä Error type: {type(e).__name__}")
                if hasattr(e, '__traceback__'):
                    import traceback
                    print(f"   üìÑ Stack trace: {traceback.format_exc()}")
                print(f"üö® This error is the cause of 'Performance evaluation impossible'!")
                optimization_attempts.append({
                    'attempt': attempt_num,
                    'status': 'comparison_error',
                    'error': str(e),
                    'optimized_query': current_query
                })
                continue
        
        # üö® Á∑äÊÄ•‰øÆÊ≠£: „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπË©ï‰æ°„ÅåÂÆåÂÖ®„Å´Â§±Êïó„Åó„ÅüÂ†¥Âêà„ÅÆ„Éè„É≥„Éâ„É™„É≥„Ç∞Ôºà„É≠„Ç∏„ÉÉ„ÇØÈ†ÜÂ∫è‰øÆÊ≠£ÂæåÔºâ
        elif performance_comparison is None:
            print(f"üö® Attempt {attempt_num}: Performance evaluation impossible, proceeding to next attempt")
            
            optimization_attempts.append({
                'attempt': attempt_num,
                'status': 'performance_evaluation_failed',
                'optimized_query': current_query,
                'performance_comparison': None,
                'error': 'EXPLAIN COSTÂÆüË°åÂ§±Êïó„Åæ„Åü„ÅØ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØË©ï‰æ°Â§±Êïó',
                'cost_ratio': None,
                'memory_ratio': None
            })
            
            # ÊúÄÂæå„ÅÆË©¶Ë°å„Åß„Å™„ÅÑÂ†¥Âêà„ÅØÊ¨°„ÅÆÊîπÂñÑ„ÇíË©¶Ë°å
            if attempt_num < max_optimization_attempts:
                print(f"üîÑ Will retry performance evaluation in attempt {attempt_num + 1}")
                continue
            else:
                print(f"‚ùå Maximum attempts ({max_optimization_attempts}) reached, using original query")
                break
        
        else:
            print(f"‚ö†Ô∏è Attempt {attempt_num}: EXPLAIN COST acquisition failed, using syntactically normal optimized query")
            optimization_attempts.append({
                'attempt': attempt_num,
                'status': 'explain_cost_failed',
                'optimized_query': current_query,
                'note': 'EXPLAIN COST comparison skipped due to execution failure'
            })
            
            # üö® ‰øÆÊ≠£: EXPLAIN COST„ÅåÂèñÂæó„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÇÇÈáçË§á‰øùÂ≠ò„ÇíÈò≤Ê≠¢
            # saved_files = save_optimized_sql_files(...)  # ‚Üê ÈáçË§áÈò≤Ê≠¢„ÅÆ„Åü„ÇÅ„Ç≥„É°„É≥„Éà„Ç¢„Ç¶„Éà
            
            return {
                'final_status': 'partial_success',
                'final_query': current_query,
                'successful_attempt': attempt_num,
                'total_attempts': attempt_num,
                'optimization_attempts': optimization_attempts,
                'optimized_result': optimized_query_str,  # üîß „É°„Ç§„É≥Âá¶ÁêÜ„Åß„ÅÆ‰øùÂ≠òÁî®„Å´ËøΩÂä†
                'saved_files': None,  # üîß „É°„Ç§„É≥Âá¶ÁêÜ„Åß‰øùÂ≠ò„Åô„Çã„Åü„ÇÅNone
                'note': 'Performance comparison unavailable but query is syntactically valid'
            }
    
    # üöÄ ÊúÄÂ§ßË©¶Ë°åÂõûÊï∞Âà∞ÈÅîÔºö„Éô„Çπ„ÉàÁµêÊûú„ÇíÊúÄÁµÇ„ÇØ„Ç®„É™„Å®„Åó„Å¶ÈÅ∏Êäû
    print(f"\n‚è∞ All {max_optimization_attempts} optimization attempts completed")
    print("üèÜ Selecting best result as final query")
    print("=" * 60)
    
    # üìä ÊúÄÈÅ©ÂåñË©¶Ë°åÁµêÊûú„Çµ„Éû„É™„ÉºË°®Á§∫
    print(f"\nüìä Optimization attempt details: {len(optimization_attempts)} times")
    for i, attempt in enumerate(optimization_attempts, 1):
        status_symbol = {
            'llm_error': '‚ùå',
            'explain_failed': 'üö´', 
            'insufficient_improvement': '‚ùì',
            'substantial_success': 'üèÜ',
            'performance_degraded': '‚¨áÔ∏è',
            'comparison_error': 'üí•'
        }.get(attempt['status'], '‚ùì')
        
        status_details = ""
        if 'cost_ratio' in attempt:
            cost_ratio = attempt['cost_ratio']
            status_details = f"üí∞ Cost ratio: {cost_ratio:.2f}x"
        
        print(f"   {status_symbol} Attempt {i}: {attempt['status']}")
        if status_details:
            print(f"      {status_details}")
    
    print("=" * 60)
    
    # „Éô„Çπ„ÉàÁµêÊûú„ÅÆË©≥Á¥∞Ë°®Á§∫
    if best_result['attempt_num'] > 0:
        print(f"ü•á FINAL SELECTION: Attempt {best_result['attempt_num']} has been chosen as the optimized query")
        print(f"   üìä Cost ratio: {best_result['cost_ratio']:.3f} (Improvement: {(1-best_result['cost_ratio'])*100:.1f}%)")
        print(f"   üíæ Memory ratio: {best_result['memory_ratio']:.3f} (Improvement: {(1-best_result['memory_ratio'])*100:.1f}%)")
        print(f"   üéØ Selection reason: Best cost performance among all attempts")
        
        final_query = best_result['query']
        final_optimized_result = best_result['optimized_result']
        final_performance_comparison = best_result['performance_comparison']
        final_status = 'optimization_success'
        achievement_type = 'best_of_trials'
        
        print(f"‚úÖ CONFIRMED: Using Attempt {best_result['attempt_num']} optimized query for final report")
        
    else:
        print(f"‚ö†Ô∏è Using original query due to errors or evaluation failures in all attempts")
        
        # Ë©¶Ë°åÁµêÊûú„Çµ„Éû„É™„Éº
        failure_summary = []
        for attempt in optimization_attempts:
            if attempt['status'] == 'performance_degraded':
                failure_summary.append(f"Ë©¶Ë°å{attempt['attempt']}: {attempt.get('degradation_analysis', {}).get('primary_cause', 'unknown')} („Ç≥„Çπ„ÉàÊØî: {attempt.get('cost_ratio', 'N/A')})")
            elif attempt['status'] == 'llm_error':
                failure_summary.append(f"Ë©¶Ë°å{attempt['attempt']}: LLM„Ç®„É©„Éº")
            elif attempt['status'] == 'explain_failed':
                failure_summary.append(f"Ë©¶Ë°å{attempt['attempt']}: EXPLAINÂÆüË°åÂ§±Êïó")
            elif attempt['status'] == 'comparison_error':
                failure_summary.append(f"Ë©¶Ë°å{attempt['attempt']}: „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉ„Ç®„É©„Éº")
            else:
                failure_summary.append(f"Ë©¶Ë°å{attempt['attempt']}: {attempt['status']}")
        
        failure_report = f"""# ‚ö†Ô∏è ÂÖ®ÊúÄÈÅ©ÂåñË©¶Ë°åÂÆå‰∫Ü„ÅÆ„Åü„ÇÅ„ÄÅÂÖÉ„ÇØ„Ç®„É™„Çí‰ΩøÁî®

## ÊúÄÈÅ©ÂåñË©¶Ë°åÁµêÊûú

{chr(10).join(failure_summary) if failure_summary else "ÂÖ®„Å¶„ÅÆË©¶Ë°å„Åß„Ç®„É©„Éº„ÅåÁô∫Áîü"}

## ÊúÄÁµÇÂà§Êñ≠

{max_optimization_attempts}Âõû„ÅÆÊúÄÈÅ©ÂåñË©¶Ë°å„ÇíÂÆüË°å„Åó„Åæ„Åó„Åü„Åå„ÄÅ10%‰ª•‰∏ä„ÅÆÂ§ßÂπÖÊîπÂñÑ„Å´„ÅØÂà∞ÈÅî„Åõ„Åö„ÄÅ
„Éô„Çπ„ÉàÁµêÊûú„ÇÇÂÖÉ„ÇØ„Ç®„É™„Çí‰∏äÂõû„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ

## ÂÖÉ„ÅÆ„ÇØ„Ç®„É™

```sql
{original_query}
```

## Êé®Â•®‰∫ãÈ†Ö

- „Éá„Éº„ÇøÈáè„ÇÑ„ÉÜ„Éº„Éñ„É´Áµ±Ë®àÊÉÖÂ†±„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ
- „Çà„ÇäË©≥Á¥∞„Å™EXPLAINÊÉÖÂ†±„ÇíÂèñÂæó„Åó„Å¶ÊâãÂãïÊúÄÈÅ©Âåñ„ÇíÊ§úË®é„Åó„Å¶„Åè„Å†„Åï„ÅÑ  
- Liquid Clustering„ÇÑ„ÉÜ„Éº„Éñ„É´Áµ±Ë®à„ÅÆÊõ¥Êñ∞„ÇíÊ§úË®é„Åó„Å¶„Åè„Å†„Åï„ÅÑ
"""
        
        final_query = original_query
        final_optimized_result = failure_report
        final_performance_comparison = None
        final_status = 'optimization_failed'
        achievement_type = 'no_improvement'
    
    return {
        'final_status': final_status,
        'final_query': final_query,
        'total_attempts': len(optimization_attempts),
        'optimization_attempts': optimization_attempts,
        'performance_comparison': final_performance_comparison,
        'optimized_result': final_optimized_result,
        'saved_files': None,  # „É°„Ç§„É≥Âá¶ÁêÜ„Åß‰øùÂ≠ò
        'best_result': best_result,
        'achievement_type': achievement_type,
        'fallback_reason': f'Best result from {max_optimization_attempts} attempts selected' if best_result['attempt_num'] > 0 else 'All attempts failed or degraded'
    }


def execute_explain_with_retry_logic(original_query: str, analysis_result: str, metrics: Dict[str, Any], max_retries: int = 3) -> Dict[str, Any]:
    """
    EXPLAIN execution and error correction retry logic (syntax errors only)
    Attempts automatic correction up to max_retries times, uses original query on failure
    """
    from datetime import datetime
    
    print(f"\nüîÑ EXPLAIN execution and automatic error correction (max {max_retries} attempts)")
    print("=" * 60)
    
    # Initial optimization query generation
    print("ü§ñ Step 1: Initial optimization query generation")
    optimized_query = generate_optimized_query_with_llm(original_query, analysis_result, metrics)
    
    # üêõ DEBUG: Âçò‰ΩìÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™„Çí‰øùÂ≠òÔºàÂèçÂæ©ÊúÄÈÅ©Âåñ‰ª•Â§ñ„ÅÆ„Éë„ÇπÔºâ
    if isinstance(optimized_query, str) and not optimized_query.startswith("LLM_ERROR:"):
        save_debug_query_trial(optimized_query, 1, "single_optimization", query_id="direct_path")
    
    # LLM„Ç®„É©„Éº„ÉÅ„Çß„ÉÉ„ÇØÔºàÈáçË¶ÅÔºâ
    if isinstance(optimized_query, str) and optimized_query.startswith("LLM_ERROR:"):
        print("‚ùå Error occurred in LLM optimization, using original query")
        print(f"üîß Error details: {optimized_query[10:]}")  # Remove "LLM_ERROR:"
        
        # „Ç®„É©„ÉºÊôÇ„ÅØÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Çí‰ΩøÁî®„Åó„Å¶Âç≥Â∫ß„Å´„Éï„Ç°„Ç§„É´ÁîüÊàê
        fallback_result = save_optimized_sql_files(
            original_query,
            f"# ‚ùå LLMÊúÄÈÅ©Âåñ„Åß„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åü„Åü„ÇÅ„ÄÅÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Çí‰ΩøÁî®\n\n## „Ç®„É©„ÉºË©≥Á¥∞\n{optimized_query[10:]}\n\n## ÂÖÉ„ÅÆ„ÇØ„Ç®„É™\n```sql\n{original_query}\n```",
            metrics,
            analysis_result,
            "",  # llm_response
            None  # performance_comparison
        )
        
        return {
            'final_status': 'llm_error',
            'final_query': original_query,
            'total_attempts': 0,
            'all_attempts': [],
            'explain_result': None,
            'optimized_result': optimized_query,
            'error_details': optimized_query[10:]
        }
    
    # thinking_enabledÂØæÂøú: „É™„Çπ„ÉàÂΩ¢Âºè„ÅÆÂ†¥Âêà„ÅØ„É°„Ç§„É≥„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÇíÊäΩÂá∫
    if isinstance(optimized_query, list):
        optimized_query_str = extract_main_content_from_thinking_response(optimized_query)
    else:
        optimized_query_str = str(optimized_query)
    
    # SQL„ÇØ„Ç®„É™ÈÉ®ÂàÜ„ÅÆ„Åø„ÇíÊäΩÂá∫
    extracted_sql = extract_sql_from_llm_response(optimized_query_str)
    current_query = extracted_sql if extracted_sql else original_query
    
    retry_count = 0
    all_attempts = []  # ÂÖ®Ë©¶Ë°å„ÅÆË®òÈå≤
    
    while retry_count <= max_retries:
        attempt_num = retry_count + 1
        print(f"\nüîç Attempt {attempt_num}/{max_retries + 1}: EXPLAIN execution")
        
        # EXPLAINÂÆüË°åÔºàÊúÄÈÅ©ÂåñÂæå„ÇØ„Ç®„É™Ôºâ
        explain_result = execute_explain_and_save_to_file(current_query, "optimized")
        
        # ÊàêÂäüÊôÇ„ÅÆÂá¶ÁêÜ
        if 'explain_file' in explain_result and 'error_file' not in explain_result:
            print(f"‚úÖ Succeeded in attempt {attempt_num}!")
            
            # üö® ‰øÆÊ≠£Ôºö„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉ„ÅØÂèçÂæ©ÊúÄÈÅ©ÂåñÈñ¢Êï∞„Åß‰∏ÄÂÖÉÂåñ
            # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉ„Çí„Åì„Åì„ÅßÂÆüË°å„Åô„Çã„Å®‰∫åÈáçÂÆüË°å„Å´„Å™„Çã„Åü„ÇÅÂâäÈô§
            
            # üö® ‰øÆÊ≠£Ôºö‰ª•‰∏ã„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉ„ÇíÁÑ°ÂäπÂåñÔºà‰∫åÈáçÂÆüË°åÈò≤Ê≠¢Ôºâ
            # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉ„ÅØ execute_iterative_optimization_with_degradation_analysis „Åß‰∏ÄÂÖÉÂåñ
            
            # üîß ÊßãÊñá„ÉÅ„Çß„ÉÉ„ÇØÊàêÂäü„ÅÆ„Åü„ÇÅ„ÄÅÂç≥Â∫ß„Å´ success „Çπ„ÉÜ„Éº„Çø„Çπ„ÅßÊ¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó„Å∏
            performance_comparison = None  # ÂèçÂæ©ÊúÄÈÅ©Âåñ„ÅßË®≠ÂÆö„Åï„Çå„Çã
            
            if False:  # üö® „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉ„Éñ„É≠„ÉÉ„ÇØ„ÇíÁÑ°ÂäπÂåñ
                
                try:
                    # EXPLAIN COST„Éï„Ç°„Ç§„É´ÂÜÖÂÆπ„ÇíË™≠„ÅøËæº„Åø
                    with open(original_explain_cost_result['explain_cost_file'], 'r', encoding='utf-8') as f:
                        original_cost_content = f.read()
                    
                    with open(optimized_explain_cost_result['explain_cost_file'], 'r', encoding='utf-8') as f:
                        optimized_cost_content = f.read()
                    
                    # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉÂÆüË°å
                    performance_comparison = compare_query_performance(original_cost_content, optimized_cost_content)
                    
                    print(f"üìä Performance comparison results:")
                    print(f"   - Execution cost ratio: {performance_comparison['total_cost_ratio']:.2f}x")
                    print(f"   - Memory usage ratio: {performance_comparison['memory_usage_ratio']:.2f}x")
                    print(f"   - Recommendation: {performance_comparison['recommendation']}")
                    
                    for detail in performance_comparison['details']:
                        print(f"   - {detail}")
                    
                    # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊÇ™Âåñ„ÅåÊ§úÂá∫„Åï„Çå„ÅüÂ†¥Âêà
                    if performance_comparison['performance_degradation_detected']:
                        print("üö® Performance degradation detected! Using original query")
                        
                        # ÂÖÉ„ÇØ„Ç®„É™„Åß„ÅÆ„Éï„Ç°„Ç§„É´ÁîüÊàêÔºà„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊÇ™ÂåñÈò≤Ê≠¢Ôºâ
                        fallback_result = save_optimized_sql_files(
                            original_query,
                            f"# üö® „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊÇ™ÂåñÊ§úÂá∫„ÅÆ„Åü„ÇÅÂÖÉ„ÇØ„Ç®„É™„Çí‰ΩøÁî®\n\n## ÊÇ™ÂåñË¶ÅÂõ†\n{'; '.join(performance_comparison['details'])}\n\n## „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉÁµêÊûú\n- ÂÆüË°å„Ç≥„Çπ„ÉàÊØî: {performance_comparison['total_cost_ratio']:.2f}ÂÄç\n- „É°„É¢„É™‰ΩøÁî®ÊØî: {performance_comparison['memory_usage_ratio']:.2f}ÂÄç\n\n## ÂÖÉ„ÅÆ„ÇØ„Ç®„É™ÔºàÊúÄÈÅ©ÂåñÂâçÔºâ\n```sql\n{original_query}\n```",
                            metrics,
                            analysis_result,
                            "",  # llm_response
                            performance_comparison  # üîç Ë©≥Á¥∞„Å™„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉÁµêÊûú„ÇíÂê´„ÇÅ„Çã
                        )
                        
                        return {
                            'final_status': 'performance_degradation_detected',
                            'final_query': original_query,
                            'total_attempts': attempt_num,
                            'all_attempts': all_attempts,
                            'explain_result': original_explain_cost_result,
                            'optimized_result': optimized_query,
                            'performance_comparison': performance_comparison,
                            'fallback_reason': 'performance_degradation'
                        }
                    
                    else:
                        print("‚úÖ Performance improvement confirmed. Using optimized query")
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Error occurred in performance comparison: {str(e)}")
                    print("üîÑ Using original query for safety")
                    
                    # „Ç®„É©„ÉºÊôÇ„ÇÇÂÆâÂÖ®ÂÅ¥„Å´ÂÄí„Åó„Å¶ÂÖÉ„ÇØ„Ç®„É™„Çí‰ΩøÁî®
                    fallback_result = save_optimized_sql_files(
                        original_query,
                        f"# ‚ö†Ô∏è „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉ„Ç®„É©„Éº„ÅÆ„Åü„ÇÅÂÆâÂÖ®ÊÄß„ÇíÂÑ™ÂÖà„Åó„Å¶ÂÖÉ„ÇØ„Ç®„É™„Çí‰ΩøÁî®\n\n## „Ç®„É©„ÉºË©≥Á¥∞\n{str(e)}\n\n## ÂÖÉ„ÅÆ„ÇØ„Ç®„É™\n```sql\n{original_query}\n```",
                        metrics,
                        analysis_result,
                        "",  # llm_response
                        None  # performance_comparison
                    )
                    
                    return {
                        'final_status': 'performance_comparison_error',
                        'final_query': original_query,
                        'total_attempts': attempt_num,
                        'all_attempts': all_attempts,
                        'explain_result': explain_result,
                        'optimized_result': optimized_query,
                        'fallback_reason': 'performance_comparison_error',
                        'error_details': str(e)
                    }
            
            # üö® ‰øÆÊ≠£ÔºöelseÈÉ®ÂàÜ„ÇÇÁÑ°ÂäπÂåñÔºà‰∫åÈáçÂÆüË°åÈò≤Ê≠¢Ôºâ
            # else:
            #     print("‚ö†Ô∏è Skipping performance comparison due to EXPLAIN COST acquisition failure")
#     print("üîÑ Using syntactically valid optimized query")
            
            # ÊàêÂäüË®òÈå≤
            attempt_record = {
                'attempt': attempt_num,
                'status': 'success',
                'query': current_query,
                'explain_file': explain_result.get('explain_file'),
                'plan_lines': explain_result.get('plan_lines', 0),
                'performance_comparison': performance_comparison
            }
            all_attempts.append(attempt_record)
            
            # ÊúÄÁµÇÁµêÊûúÔºà„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊÇ™Âåñ„Å™„Åó„ÅÆÂ†¥ÂêàÔºâ
            return {
                'final_status': 'success',
                'final_query': current_query,
                'total_attempts': attempt_num,
                'all_attempts': all_attempts,
                'explain_result': explain_result,
                'optimized_result': optimized_query,  # ÂÖÉ„ÅÆÂÆåÂÖ®„Å™„É¨„Çπ„Éù„É≥„Çπ
                'performance_comparison': performance_comparison
            }
        
        # „Ç®„É©„ÉºÊôÇ„ÅÆÂá¶ÁêÜ
        elif 'error_file' in explain_result:
            error_message = explain_result.get('error_message', 'Unknown error')
            print(f"‚ùå Error occurred in attempt {attempt_num}: {error_message}")
            
            # „Ç®„É©„ÉºË®òÈå≤
            attempt_record = {
                'attempt': attempt_num,
                'status': 'error',
                'query': current_query,
                'error_message': error_message,
                'error_file': explain_result.get('error_file')
            }
            all_attempts.append(attempt_record)
            
            # ÊúÄÂ§ßË©¶Ë°åÂõûÊï∞„Å´ÈÅî„Åó„ÅüÂ†¥Âêà
            if retry_count >= max_retries:
                print(f"üö® Maximum number of attempts ({max_retries}) reached")
                print("üìã Using original working query")
                
                # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: ÂÖÉ„ÇØ„Ç®„É™„Åß„ÅÆ„Éï„Ç°„Ç§„É´ÁîüÊàê
                fallback_result = save_optimized_sql_files(
                    original_query, 
                    f"# üö® ÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™„ÅÆEXPLAINÂÆüË°å„Åå{max_retries}Âõû„Å®„ÇÇÂ§±Êïó„Åó„Åü„Åü„ÇÅ„ÄÅÂÖÉ„ÇØ„Ç®„É™„Çí‰ΩøÁî®\n\n## ÊúÄÂæå„ÅÆ„Ç®„É©„ÉºÊÉÖÂ†±\n{error_message}\n\n## ÂÖÉ„ÅÆ„ÇØ„Ç®„É™\n```sql\n{original_query}\n```",
                    metrics,
                    analysis_result,
                    "",  # llm_response
                    None  # performance_comparison
                )
                
                # Â§±ÊïóÊôÇ„ÅÆ„É≠„Ç∞Ë®òÈå≤
                timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                log_filename = f"output_optimization_failure_log_{timestamp}.txt"
                
                try:
                    with open(log_filename, 'w', encoding='utf-8') as f:
                        f.write(f"# ÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™ÁîüÊàêÂ§±Êïó„É≠„Ç∞\n")
                        f.write(f"ÂÆüË°åÊó•ÊôÇ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                        f.write(f"ÊúÄÂ§ßË©¶Ë°åÂõûÊï∞: {max_retries}Âõû\n")
                        f.write(f"ÊúÄÁµÇÁµêÊûú: ÂÖÉ„ÇØ„Ç®„É™„Çí‰ΩøÁî®\n\n")
                        
                        f.write("=" * 80 + "\n")
                        f.write("ÂÖ®Ë©¶Ë°å„ÅÆË©≥Á¥∞Ë®òÈå≤:\n")
                        f.write("=" * 80 + "\n\n")
                        
                        for attempt in all_attempts:
                            f.write(f"„ÄêË©¶Ë°å {attempt['attempt']}„Äë\n")
                            f.write(f"„Çπ„ÉÜ„Éº„Çø„Çπ: {attempt['status']}\n")
                            if attempt['status'] == 'error':
                                f.write(f"„Ç®„É©„Éº: {attempt['error_message']}\n")
                                f.write(f"„Ç®„É©„Éº„Éï„Ç°„Ç§„É´: {attempt.get('error_file', 'N/A')}\n")
                            f.write(f"‰ΩøÁî®„ÇØ„Ç®„É™Èï∑: {len(attempt['query'])} ÊñáÂ≠ó\n\n")
                        
                        f.write("=" * 80 + "\n")
                        f.write("ÂÖÉ„ÅÆ„ÇØ„Ç®„É™Ôºà„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ‰ΩøÁî®Ôºâ:\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(original_query)
                    
                    print(f"üìÑ Saved failure log: {log_filename}")
                    
                except Exception as log_error:
                                            print(f"‚ùå Failed to save failure log as well: {str(log_error)}")
                
                return {
                    'final_status': 'fallback_to_original',
                    'final_query': original_query,
                    'total_attempts': attempt_num,
                    'all_attempts': all_attempts,
                    'fallback_files': fallback_result,
                    'failure_log': log_filename
                }
            
            # ÂÜçË©¶Ë°å„Åô„ÇãÂ†¥Âêà„ÅÆ„Ç®„É©„Éº‰øÆÊ≠£
            retry_count += 1
            print(f"üîß Correcting error for attempt {retry_count + 1}...")
            
            # „Ç®„É©„ÉºÊÉÖÂ†±„ÇíÂê´„ÇÅ„Å¶ÂÜçÁîüÊàêÔºàÂàùÂõûÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™„ÇÇÊ∏°„ÅôÔºâ
            corrected_query = generate_optimized_query_with_error_feedback(
                original_query, 
                analysis_result, 
                metrics, 
                error_message,
                current_query  # üöÄ ÂàùÂõûÊúÄÈÅ©Âåñ„ÇØ„Ç®„É™Ôºà„Éí„É≥„Éà‰ªò„ÅçÔºâ„ÇíÊ∏°„Åô
            )
            
            # üêõ DEBUG: ÂÜçË©¶Ë°åÊôÇ„ÅÆ„Ç®„É©„Éº‰øÆÊ≠£„ÇØ„Ç®„É™„Çí‰øùÂ≠ò
            if isinstance(corrected_query, str) and not corrected_query.startswith("LLM_ERROR:"):
                save_debug_query_trial(corrected_query, retry_count + 1, "retry_error_correction", 
                                     query_id=f"retry_{retry_count + 1}", 
                                     error_info=f"ÂÜçË©¶Ë°å{retry_count + 1}„ÅÆ„Ç®„É©„Éº‰øÆÊ≠£: {error_message[:100]}")
            
            # LLM„Ç®„É©„Éº„ÉÅ„Çß„ÉÉ„ÇØÔºà„Ç®„É©„Éº‰øÆÊ≠£ÊôÇÔºâ
            if isinstance(corrected_query, str) and corrected_query.startswith("LLM_ERROR:"):
                print("‚ùå LLM error occurred even in error correction, using original query")
                print(f"üîß Error details: {corrected_query[10:]}")  # Remove "LLM_ERROR:"
                
                # Â§±ÊïóË®òÈå≤
                attempt_record = {
                    'attempt': retry_count + 1,
                    'status': 'llm_error_correction_failed',
                    'query': current_query,
                    'error_message': f"„Ç®„É©„Éº‰øÆÊ≠£ÊôÇLLM„Ç®„É©„Éº: {corrected_query[10:]}",
                    'error_file': None
                }
                all_attempts.append(attempt_record)
                
                # ÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Çí‰ΩøÁî®„Åó„Å¶„Éï„Ç°„Ç§„É´ÁîüÊàê
                fallback_result = save_optimized_sql_files(
                    original_query,
                    f"# ‚ùå „Ç®„É©„Éº‰øÆÊ≠£ÊôÇ„ÇÇLLM„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åü„Åü„ÇÅ„ÄÅÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Çí‰ΩøÁî®\n\n## „Ç®„É©„Éº‰øÆÊ≠£ÊôÇ„ÅÆ„Ç®„É©„ÉºË©≥Á¥∞\n{corrected_query[10:]}\n\n## ÂÖÉ„ÅÆ„ÇØ„Ç®„É™\n```sql\n{original_query}\n```",
                    metrics,
                    analysis_result,
                    "",  # llm_response
                    None  # performance_comparison
                )
                
                return {
                    'final_status': 'llm_error_correction_failed',
                    'final_query': original_query,
                    'total_attempts': retry_count + 1,
                    'all_attempts': all_attempts,
                    'explain_result': None,
                    'optimized_result': corrected_query,
                    'error_details': corrected_query[10:]
                }
            
            # thinking_enabledÂØæÂøú
            if isinstance(corrected_query, list):
                corrected_query_str = extract_main_content_from_thinking_response(corrected_query)
            else:
                corrected_query_str = str(corrected_query)
            
            # SQL„ÇØ„Ç®„É™ÈÉ®ÂàÜ„ÅÆ„Åø„ÇíÊäΩÂá∫
            extracted_sql = extract_sql_from_llm_response(corrected_query_str)
            current_query = extracted_sql if extracted_sql else current_query
            
            print(f"‚úÖ Generated error correction query ({len(current_query)} characters)")
    
    # „Åì„Åì„Å´„ÅØÂà∞ÈÅî„Åó„Å™„ÅÑ„ÅØ„Åö„Å†„Åå„ÄÅÂÆâÂÖ®„ÅÆ„Åü„ÇÅ
    return {
        'final_status': 'unexpected_error',
        'final_query': original_query,
        'total_attempts': retry_count + 1,
        'all_attempts': all_attempts
    }


def extract_sql_from_llm_response(llm_response: str) -> str:
    """
    Extract only SQL query part from LLM response
    """
    import re
    
    # SQL„Ç≥„Éº„Éâ„Éñ„É≠„ÉÉ„ÇØ„ÇíÊ§úÁ¥¢Ôºà```sql ... ```Ôºâ
    sql_pattern = r'```sql\s*(.*?)\s*```'
    matches = re.findall(sql_pattern, llm_response, re.DOTALL | re.IGNORECASE)
    
    if matches:
        # ÊúÄÈï∑„ÅÆSQL„Éñ„É≠„ÉÉ„ÇØ„ÇíÈÅ∏Êäû
        sql_query = max(matches, key=len).strip()
        return sql_query
    
    # SQL„Ç≥„Éº„Éâ„Éñ„É≠„ÉÉ„ÇØ„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑÂ†¥Âêà„ÄÅÂà•„ÅÆ„Éë„Çø„Éº„É≥„ÇíË©¶Ë°å
    # ```„ÅÆ„Åø„ÅÆ„Ç≥„Éº„Éâ„Éñ„É≠„ÉÉ„ÇØ
    code_pattern = r'```\s*(.*?)\s*```'
    matches = re.findall(code_pattern, llm_response, re.DOTALL)
    
    for match in matches:
        match = match.strip()
        # SQL„Ç≠„Éº„ÉØ„Éº„Éâ„ÅßÂßã„Åæ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ
        if re.match(r'^(SELECT|WITH|CREATE|INSERT|UPDATE|DELETE|EXPLAIN)', match, re.IGNORECASE):
            return match
    
    # „Éë„Çø„Éº„É≥„Éû„ÉÉ„ÉÅ„Åó„Å™„ÅÑÂ†¥Âêà„ÅØÂÖÉ„ÅÆ„É¨„Çπ„Éù„É≥„Çπ„Çí„Åù„ÅÆ„Åæ„ÅæËøî„Åô
    return llm_response.strip()


def execute_explain_and_save_to_file(original_query: str, query_type: str = "original") -> Dict[str, str]:
    """
    Execute EXPLAIN and EXPLAIN COST statements for queries and save results to file based on EXPLAIN_ENABLED setting
    For CTAS, extract only the SELECT part and pass it to EXPLAIN statement
    
    Args:
        original_query: Query to execute EXPLAIN on
        query_type: "original" or "optimized" to identify filename
    """
    from datetime import datetime
    import os
    
    if not original_query or not original_query.strip():
        print("‚ùå Query is empty")
        return {}
    
    # EXPLAIN_ENABLEDË®≠ÂÆö„ÇíÁ¢∫Ë™ç
    explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
    debug_enabled = globals().get('DEBUG_ENABLED', 'N')
    
    # „Éï„Ç°„Ç§„É´Âêç„ÅÆÁîüÊàêÔºàEXPLAIN_ENABLED=Y„ÅÆÂ†¥Âêà„ÅÆ„ÅøÔºâ
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    if explain_enabled.upper() == 'Y':
        explain_filename = f"output_explain_{query_type}_{timestamp}.txt"
        explain_cost_filename = f"output_explain_cost_{query_type}_{timestamp}.txt"
    else:
        explain_filename = None
        explain_cost_filename = None
    
    # CTAS„ÅÆÂ†¥Âêà„ÅØSELECTÈÉ®ÂàÜ„ÅÆ„Åø„ÇíÊäΩÂá∫
    query_for_explain = extract_select_from_ctas(original_query)
    
    # EXPLAINÊñá„Å®EXPLAIN COSTÊñá„ÅÆÁîüÊàê
    explain_query = f"EXPLAIN {query_for_explain}"
    explain_cost_query = f"EXPLAIN COST {query_for_explain}"
    
    # „Ç´„Çø„É≠„Ç∞„Å®„Éá„Éº„Çø„Éô„Éº„Çπ„ÅÆË®≠ÂÆö„ÇíÂèñÂæó
    catalog = globals().get('CATALOG', 'main')
    database = globals().get('DATABASE', 'default')
    
    print(f"üìÇ Using catalog: {catalog}")
    print(f"üóÇÔ∏è Using database: {database}")
    
    # „Ç´„Çø„É≠„Ç∞„Å®„Éá„Éº„Çø„Éô„Éº„Çπ„ÇíË®≠ÂÆö
    try:
        spark.sql(f"USE CATALOG {catalog}")
        spark.sql(f"USE DATABASE {database}")
    except Exception as e:
        print(f"‚ö†Ô∏è Catalog/database configuration error: {str(e)}")
    
    # EXPLAINÊñá„Å®EXPLAIN COSTÊñá„ÅÆÂÆüË°å
    try:
        print("üîÑ Executing EXPLAIN and EXPLAIN COST statements...")
        
        # 1. ÈÄöÂ∏∏„ÅÆEXPLAINÂÆüË°å
        print("   üìä Executing EXPLAIN...")
        explain_result_spark = spark.sql(explain_query)
        explain_result = explain_result_spark.collect()
        
        # EXPLAINÁµêÊûú„ÅÆÂÜÖÂÆπ„Çí„ÉÅ„Çß„ÉÉ„ÇØ
        explain_content = ""
        for row in explain_result:
            explain_content += str(row[0]) + "\n"
        
        # 2. EXPLAIN COSTÂÆüË°å
        print("   üí∞ Executing EXPLAIN COST...")
        explain_cost_result_spark = spark.sql(explain_cost_query)
        explain_cost_result = explain_cost_result_spark.collect()
        
        # EXPLAIN COSTÁµêÊûú„ÅÆÂÜÖÂÆπ„Çí„ÉÅ„Çß„ÉÉ„ÇØ
        explain_cost_content = ""
        for row in explain_cost_result:
            explain_cost_content += str(row[0]) + "\n"
        
        # üö® Á∑äÊÄ•‰øÆÊ≠£: „Ç®„É©„Éº„Éë„Çø„Éº„É≥„ÇíÂé≥ÂØÜÂåñÔºàË™§Ê§úÂá∫Èò≤Ê≠¢Ôºâ
        retryable_error_patterns = [
            "Error occurred during query planning",
            "error occurred during query planning", 
            "Query planning failed",
            "query planning failed",
            "Plan optimization failed",
            "plan optimization failed",
            "Failed to plan query",
            "failed to plan query",
            "Analysis exception",
            "analysis exception",
            "AMBIGUOUS_REFERENCE",
            "ambiguous_reference",
            "[AMBIGUOUS_REFERENCE]",
            # "Reference",  # üö® Èô§Âéª: ÈÅéÂ∫¶„Å´‰∏ÄËà¨ÁöÑ„ÄÅÊ≠£Â∏∏ÁµêÊûú„ÇÇË™§Ê§úÂá∫
            "reference is ambiguous",  # „Çà„ÇäÂÖ∑‰ΩìÁöÑ„Å™„Éë„Çø„Éº„É≥„Å´Â§âÊõ¥
            # "is ambiguous",  # üö® Èô§Âéª: ÈÅéÂ∫¶„Å´‰∏ÄËà¨ÁöÑ
            "ambiguous reference",  # „Çà„ÇäÂÖ∑‰ΩìÁöÑ„Å™„Éë„Çø„Éº„É≥„Å´Â§âÊõ¥
            # "Ambiguous",  # üö® Èô§Âéª: ÈÅéÂ∫¶„Å´‰∏ÄËà¨ÁöÑ
            "ParseException",
            "SemanticException", 
            "AnalysisException",
            "Syntax error",
            "syntax error",
            "PARSE_SYNTAX_ERROR",
            "INVALID_IDENTIFIER",
            "TABLE_OR_VIEW_NOT_FOUND",
            "COLUMN_NOT_FOUND",
            "UNRESOLVED_COLUMN",
            "[UNRESOLVED_COLUMN",
            "UNRESOLVED_COLUMN.WITH_SUGGESTION",
            "[UNRESOLVED_COLUMN.WITH_SUGGESTION]"
        ]
        
        # üö® ÈáçË¶Å: EXPLAINÁµêÊûú„Å®EXPLAIN COSTÁµêÊûú„ÅÆ‰∏°Êñπ„Çí„Ç®„É©„Éº„ÉÅ„Çß„ÉÉ„ÇØ
        detected_error = None
        error_source = None
        
        # üö® Á∑äÊÄ•„Éá„Éê„ÉÉ„Ç∞: „Ç®„É©„ÉºÊ§úÂá∫„Éó„É≠„Çª„Çπ„ÅÆË©≥Á¥∞Ë°®Á§∫
        print(f"üîç Executing error pattern detection (patterns: {len(retryable_error_patterns)})")
        print(f"   üìä EXPLAIN content length: {len(explain_content)} characters")
        print(f"   üí∞ EXPLAIN COST content length: {len(explain_cost_content)} characters")
        
        # 1. EXPLAINÁµêÊûú„ÅÆ„Ç®„É©„Éº„ÉÅ„Çß„ÉÉ„ÇØ
        for pattern in retryable_error_patterns:
            if pattern in explain_content.lower():
                detected_error = pattern
                error_source = "EXPLAIN"
                print(f"‚ùå Error pattern detected in EXPLAIN result: '{pattern}'")
                break
        
        # 2. EXPLAIN COSTÁµêÊûú„ÅÆ„Ç®„É©„Éº„ÉÅ„Çß„ÉÉ„ÇØÔºàEXPLAIN„Åß„Ç®„É©„Éº„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑÂ†¥Âêà„ÅÆ„ÅøÔºâ
        if not detected_error:
            for pattern in retryable_error_patterns:
                if pattern in explain_cost_content.lower():
                    detected_error = pattern
                    error_source = "EXPLAIN COST"
                    print(f"‚ùå Error pattern detected in EXPLAIN COST result: '{pattern}'")
                    break
        
        if not detected_error:
            print("‚úÖ No error patterns detected: Processing as normal result")
        
        if detected_error:
            # „Ç®„É©„Éº„ÅåÊ§úÂá∫„Åï„Çå„ÅüÂ†¥Âêà„ÅØ„Ç®„É©„Éº„Å®„Åó„Å¶Âá¶ÁêÜ
            print(f"‚ùå Error detected in {error_source} result: {detected_error}")
            
            # ÁµêÊûú„ÅÆ„Éó„É¨„Éì„É•„ÉºË°®Á§∫Ôºà„Ç®„É©„ÉºÁî®Ôºâ
            print(f"\nüìã {error_source} result preview:")
            print("-" * 50)
            if error_source == "EXPLAIN":
                preview_lines = min(10, len(explain_result))
                for i, row in enumerate(explain_result[:preview_lines]):
                    print(f"{i+1:2d}: {str(row[0])[:100]}...")
            else:
                preview_lines = min(10, len(explain_cost_result))
                for i, row in enumerate(explain_cost_result[:preview_lines]):
                    print(f"{i+1:2d}: {str(row[0])[:100]}...")
            
            # „Ç®„É©„Éº„Éï„Ç°„Ç§„É´„ÅÆ‰øùÂ≠òÔºàEXPLAIN_ENABLED=Y„ÅÆÂ†¥Âêà„ÅÆ„ÅøÔºâ
            error_filename = None
            error_cost_filename = None
            if explain_enabled.upper() == 'Y':
                # EXPLAINÁµêÊûú„Ç®„É©„Éº„Éï„Ç°„Ç§„É´
                error_filename = f"output_explain_error_{query_type}_{timestamp}.txt"
                with open(error_filename, 'w', encoding='utf-8') as f:
                    f.write(f"# EXPLAINÂÆüË°å„Ç®„É©„Éº ({query_type}„ÇØ„Ç®„É™)\n")
                    f.write(f"ÂÆüË°åÊó•ÊôÇ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"„ÇØ„Ç®„É™„Çø„Ç§„Éó: {query_type}\n")
                    f.write(f"„Ç®„É©„ÉºÊ§úÂá∫ÂÖÉ: {error_source}\n")
                    f.write(f"Ê§úÂá∫„Ç®„É©„Éº„Éë„Çø„Éº„É≥: {detected_error}\n")
                    f.write(f"„ÇØ„Ç®„É™ÊñáÂ≠óÊï∞: {len(original_query):,}\n")
                    f.write("\n" + "=" * 80 + "\n")
                    f.write("EXPLAIN ÁµêÊûú:\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(explain_content)
                    f.write("\n" + "=" * 80 + "\n")
                    f.write("EXPLAIN COST ÁµêÊûú:\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(explain_cost_content)
                
                print(f"üìÑ Saved error details: {error_filename}")
                if error_source == "EXPLAIN" and len(explain_result) > preview_lines:
                    print(f"... (Remaining {len(explain_result) - preview_lines} lines, see {error_filename})")
                elif error_source == "EXPLAIN COST" and len(explain_cost_result) > preview_lines:
                    print(f"... (Remaining {len(explain_cost_result) - preview_lines} lines, see {error_filename})")
            else:
                print("üí° Error file not saved because EXPLAIN_ENABLED=N")
                if error_source == "EXPLAIN" and len(explain_result) > preview_lines:
                    print(f"... (Remaining {len(explain_result) - preview_lines} lines)")
                elif error_source == "EXPLAIN COST" and len(explain_cost_result) > preview_lines:
                    print(f"... (Remaining {len(explain_cost_result) - preview_lines} lines)")
            
            print("-" * 50)
            
            result_dict = {
                'error_message': explain_content.strip() if error_source == "EXPLAIN" else explain_cost_content.strip(),
                'detected_pattern': detected_error,
                'error_source': error_source
            }
            if error_filename:
                result_dict['error_file'] = error_filename
            
            return result_dict
        
        # „Ç®„É©„Éº„ÅåÊ§úÂá∫„Åï„Çå„Å™„Åã„Å£„ÅüÂ†¥Âêà„ÅØÊàêÂäü„Å®„Åó„Å¶Âá¶ÁêÜ
        print(f"‚úÖ EXPLAIN & EXPLAIN COST execution successful")
        print(f"üìä EXPLAIN execution plan lines: {len(explain_result):,}")
        print(f"üí∞ EXPLAIN COST statistics lines: {len(explain_cost_result):,}")
        
        # ÁµêÊûú„ÅÆ„Éó„É¨„Éì„É•„ÉºË°®Á§∫
        print("\nüìã EXPLAIN results preview:")
        print("-" * 50)
        preview_lines = min(10, len(explain_result))
        for i, row in enumerate(explain_result[:preview_lines]):
            print(f"{i+1:2d}: {str(row[0])[:100]}...")
        
        print("\nüí∞ EXPLAIN COST results preview:")
        print("-" * 50)
        cost_preview_lines = min(10, len(explain_cost_result))
        for i, row in enumerate(explain_cost_result[:cost_preview_lines]):
            print(f"{i+1:2d}: {str(row[0])[:100]}...")
        
        # ÁµêÊûú„Çí„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠òÔºàEXPLAIN_ENABLED=Y„ÅÆÂ†¥Âêà„ÅÆ„ÅøÔºâ
        if explain_enabled.upper() == 'Y' and explain_filename and explain_cost_filename:
            # EXPLAINÁµêÊûú„Éï„Ç°„Ç§„É´
            with open(explain_filename, 'w', encoding='utf-8') as f:
                f.write(f"# EXPLAINÂÆüË°åÁµêÊûú ({query_type}„ÇØ„Ç®„É™)\n")
                f.write(f"ÂÆüË°åÊó•ÊôÇ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"„ÇØ„Ç®„É™„Çø„Ç§„Éó: {query_type}\n")
                f.write(f"„ÇØ„Ç®„É™ÊñáÂ≠óÊï∞: {len(original_query):,}\n")
                f.write("\n" + "=" * 80 + "\n")
                f.write("EXPLAINÁµêÊûú:\n")
                f.write("=" * 80 + "\n\n")
                f.write(explain_content)
            
            # EXPLAIN COSTÁµêÊûú„Éï„Ç°„Ç§„É´
            with open(explain_cost_filename, 'w', encoding='utf-8') as f:
                f.write(f"# EXPLAIN COSTÂÆüË°åÁµêÊûú ({query_type}„ÇØ„Ç®„É™)\n")
                f.write(f"ÂÆüË°åÊó•ÊôÇ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"„ÇØ„Ç®„É™„Çø„Ç§„Éó: {query_type}\n")
                f.write(f"„ÇØ„Ç®„É™ÊñáÂ≠óÊï∞: {len(original_query):,}\n")
                f.write("\n" + "=" * 80 + "\n")
                f.write("EXPLAIN COSTÁµêÊûúÔºàÁµ±Ë®àÊÉÖÂ†±‰ªò„ÅçÔºâ:\n")
                f.write("=" * 80 + "\n\n")
                f.write(explain_cost_content)
            
            print(f"üìÑ Saved EXPLAIN results: {explain_filename}")
            print(f"üí∞ Saved EXPLAIN COST results: {explain_cost_filename}")
            if len(explain_result) > preview_lines:
                print(f"... (Remaining {len(explain_result) - preview_lines} lines, see {explain_filename})")
            if len(explain_cost_result) > cost_preview_lines:
                print(f"... (Remaining {len(explain_cost_result) - cost_preview_lines} lines, see {explain_cost_filename})")
        else:
            print("üí° EXPLAIN result files not saved because EXPLAIN_ENABLED=N")
            if len(explain_result) > preview_lines:
                print(f"... (Remaining {len(explain_result) - preview_lines} lines)")
            if len(explain_cost_result) > cost_preview_lines:
                print(f"... (Remaining {len(explain_cost_result) - cost_preview_lines} lines)")
        
        print("-" * 50)
        
        result_dict = {
            'plan_lines': len(explain_result),
            'cost_lines': len(explain_cost_result)
        }
        if explain_filename and explain_enabled.upper() == 'Y':
            result_dict['explain_file'] = explain_filename
        if explain_cost_filename and explain_enabled.upper() == 'Y':
            result_dict['explain_cost_file'] = explain_cost_filename
        
        return result_dict
        
    except Exception as e:
        error_message = str(e)
        print(f"‚ùå Failed to execute EXPLAIN or EXPLAIN COST statement: {error_message}")
        
        # Áúü„ÅÆËá¥ÂëΩÁöÑ„Ç®„É©„ÉºÔºà„É™„Éà„É©„Ç§‰∏çÂèØËÉΩ„Å™„Ç®„É©„ÉºÔºâ„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ
        truly_fatal_errors = [
            "Permission denied",
            "Access denied", 
            "Insufficient privileges",
            "Database not found",
            "Catalog not found",
            "Spark session is not active",
            "java.lang.OutOfMemoryError",
            "Driver killed",
            "Connection refused"
        ]
        
        # ÂÜçË©¶Ë°åÂèØËÉΩ„Å™„Ç®„É©„ÉºÔºàLLM„Åß‰øÆÊ≠£ÂèØËÉΩÔºâ
        retryable_error_patterns = [
            "Error occurred during query planning",
            "error occurred during query planning", 
            "Query planning failed",
            "query planning failed",
            "Plan optimization failed",
            "plan optimization failed",
            "Failed to plan query",
            "failed to plan query",
            "Analysis exception",
            "analysis exception",
            "AMBIGUOUS_REFERENCE",
            "ambiguous_reference",
            "[AMBIGUOUS_REFERENCE]",
            "Reference",
            "is ambiguous",
            "Ambiguous",
            "ParseException",
            "SemanticException",
            "AnalysisException",
            "Syntax error",
            "syntax error",
            "PARSE_SYNTAX_ERROR",
            "INVALID_IDENTIFIER",
            "TABLE_OR_VIEW_NOT_FOUND",
            "COLUMN_NOT_FOUND",
            "UNRESOLVED_COLUMN",
            "[UNRESOLVED_COLUMN",
            "UNRESOLVED_COLUMN.WITH_SUGGESTION",
            "[UNRESOLVED_COLUMN.WITH_SUGGESTION]"
        ]
        
        # Áúü„ÅÆËá¥ÂëΩÁöÑ„Ç®„É©„Éº„Åã„ÉÅ„Çß„ÉÉ„ÇØ
        is_truly_fatal = any(pattern in error_message.lower() for pattern in truly_fatal_errors)
        
        # ÂÜçË©¶Ë°åÂèØËÉΩ„Ç®„É©„Éº„Åã„ÉÅ„Çß„ÉÉ„ÇØ
        is_retryable = any(pattern in error_message.lower() for pattern in retryable_error_patterns)
        
        if is_truly_fatal:
            print(f"üö® FATAL: Unrecoverable error occurred")
            print(f"üö® Error details: {error_message}")
            print(f"üö® Terminating processing.")
            
            # „Ç®„É©„Éº„Éï„Ç°„Ç§„É´„ÅÆ‰øùÂ≠òÔºàEXPLAIN_ENABLED=Y„ÅÆÂ†¥Âêà„ÅÆ„ÅøÔºâ
            if explain_enabled.upper() == 'Y':
                error_filename = f"output_explain_fatal_error_{query_type}_{timestamp}.txt"
                try:
                    with open(error_filename, 'w', encoding='utf-8') as f:
                        f.write(f"# FATAL EXPLAINÂÆüË°å„Ç®„É©„Éº (ÂõûÂæ©‰∏çÂèØËÉΩ, {query_type}„ÇØ„Ç®„É™)\n")
                        f.write(f"ÂÆüË°åÊó•ÊôÇ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                        f.write(f"„ÇØ„Ç®„É™„Çø„Ç§„Éó: {query_type}\n")
                        f.write(f"„Ç®„É©„ÉºÂÜÖÂÆπ: {error_message}\n")
                        f.write(f"„Ç®„É©„Éº„Çø„Ç§„Éó: FATAL - Unrecoverable Error\n")
                        f.write("\n" + "=" * 80 + "\n")
                        f.write("ÂÆüË°å„Åó„Çà„ÅÜ„Å®„Åó„ÅüEXPLAINÊñá:\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(explain_query)
                        f.write("\n\n" + "=" * 80 + "\n")
                        f.write("ÂÆüË°å„Åó„Çà„ÅÜ„Å®„Åó„ÅüEXPLAIN COSTÊñá:\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(explain_cost_query)
                    
                    print(f"üìÑ Saved Fatal error details: {error_filename}")
                    
                except Exception as file_error:
                    print(f"‚ùå Failed to save Fatal error file: {str(file_error)}")
            else:
                print("üí° Fatal error file not saved because EXPLAIN_ENABLED=N")
            
            # „Éó„É≠„Ç∞„É©„É†„ÇíÁµÇ‰∫Ü
            import sys
            sys.exit(1)
        
        elif is_retryable:
            print(f"üîÑ Detected retryable error: {error_message}")
            print(f"üí° This error is a candidate for LLM automatic correction")
        
        # ÈùûËá¥ÂëΩÁöÑ„Å™„Ç®„É©„Éº„ÅÆÂ†¥Âêà„ÅÆÂá¶ÁêÜ
        error_filename = None
        if explain_enabled.upper() == 'Y':
            error_filename = f"output_explain_error_{query_type}_{timestamp}.txt"
            try:
                with open(error_filename, 'w', encoding='utf-8') as f:
                    f.write(f"# EXPLAINÂÆüË°å„Ç®„É©„Éº ({query_type}„ÇØ„Ç®„É™)\n")
                    f.write(f"ÂÆüË°åÊó•ÊôÇ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"„ÇØ„Ç®„É™„Çø„Ç§„Éó: {query_type}\n")
                    f.write(f"„Ç®„É©„ÉºÂÜÖÂÆπ: {error_message}\n")
                    f.write("\n" + "=" * 80 + "\n")
                    f.write("ÂÆüË°å„Åó„Çà„ÅÜ„Å®„Åó„ÅüEXPLAINÊñá:\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(explain_query)
                    f.write("\n\n" + "=" * 80 + "\n")
                    f.write("ÂÆüË°å„Åó„Çà„ÅÜ„Å®„Åó„ÅüEXPLAIN COSTÊñá:\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(explain_cost_query)
                
                print(f"üìÑ Saved error details: {error_filename}")
                
            except Exception as file_error:
                print(f"‚ùå Failed to save error file: {str(file_error)}")
        else:
            print("üí° Error file not saved because EXPLAIN_ENABLED=N")
        
        result_dict = {
            'error_message': error_message
        }
        if error_filename:
            result_dict['error_file'] = error_filename
        
        return result_dict

# EXPLAINÊñáÂÆüË°å„ÅÆÂÆüË°å
print("\nüîç EXPLAIN statement execution processing")
print("-" * 40)

# „Çª„É´43„ÅßÊäΩÂá∫„Åó„Åü„Ç™„É™„Ç∏„Éä„É´„ÇØ„Ç®„É™„ÅåÂ§âÊï∞„Å´ÊÆã„Å£„Å¶„ÅÑ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ
try:
    # original_query„ÅåÊó¢„Å´ÂÆöÁæ©„Åï„Çå„Å¶„ÅÑ„Çã„ÅãÁ¢∫Ë™ç
    original_query_for_explain = original_query
    print(f"‚úÖ Retrieved original query ({len(original_query_for_explain)} characters)")
    
except NameError:
    print("‚ö†Ô∏è Original query variable not found in current session")
    print("   Attempting automatic extraction from profiler data...")
    
    # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: „Éó„É≠„Éï„Ç°„Ç§„É©„Éº„Éá„Éº„Çø„Åã„ÇâÂÜçÊäΩÂá∫
    try:
        print("üîÑ Extracting original query from profiler data...")
        original_query_for_explain = extract_original_query_from_profiler_data(profiler_data)
        
        if original_query_for_explain and original_query_for_explain.strip():
            print(f"‚úÖ Extraction successful ({len(original_query_for_explain)} characters)")
            print(f"üîç Query preview: {original_query_for_explain[:200]}{'...' if len(original_query_for_explain) > 200 else ''}")
        else:
            print("‚ö†Ô∏è Query extraction from profiler data returned empty result")
            print("   Using default sample query for demonstration")
            # „Éá„Éï„Ç©„É´„Éà„Çµ„É≥„Éó„É´„ÇØ„Ç®„É™„ÇíÊèê‰æõ
            original_query_for_explain = """
            -- Sample query for demonstration (replace with actual query)
            SELECT 
                ss_customer_sk,
                ss_item_sk,
                SUM(ss_sales_price) as total_sales,
                COUNT(*) as transaction_count
            FROM store_sales 
            WHERE ss_sold_date_sk >= 2450815
            GROUP BY ss_customer_sk, ss_item_sk
            ORDER BY total_sales DESC
            LIMIT 100
            """
            print(f"üìù Default query has been set ({len(original_query_for_explain)} characters)")
            
    except Exception as e:
        print(f"‚ùå Error during extraction: {str(e)}")
        print("   Using default sample query for demonstration")
        # „Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„ÅüÂ†¥Âêà„ÇÇ„Éá„Éï„Ç©„É´„Éà„ÇØ„Ç®„É™„ÇíË®≠ÂÆö
        original_query_for_explain = """
        -- Sample query for demonstration (replace with actual query)
        SELECT 
            ss_customer_sk,
            ss_item_sk,
            SUM(ss_sales_price) as total_sales,
            COUNT(*) as transaction_count
        FROM store_sales 
        WHERE ss_sold_date_sk >= 2450815
        GROUP BY ss_customer_sk, ss_item_sk
        ORDER BY total_sales DESC
        LIMIT 100
        """
        print(f"üìù Default query has been set ({len(original_query_for_explain)} characters)")

# EXPLAINÂÆüË°å„Éï„É©„Ç∞„ÅÆÁ¢∫Ë™ç
explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
print(f"üîç EXPLAIN execution setting: {explain_enabled}")

if explain_enabled.upper() != 'Y':
    print("‚ö†Ô∏è EXPLAIN execution is disabled")
    print("   To execute EXPLAIN statements, set EXPLAIN_ENABLED = 'Y' in the first cell")
elif original_query_for_explain and original_query_for_explain.strip():
    print("\nüöÄ Integrated SQL Optimization & EXPLAIN Execution (with automatic error correction)")
    
    # SparkÁí∞Â¢É„ÅÆÁ¢∫Ë™ç
    try:
        spark_version = spark.version
        print(f"üìä Spark environment: {spark_version}")
    except Exception as e:
        print(f"‚ùå Failed to check Spark environment: {str(e)}")
        print("   Please execute in Databricks environment")
        spark = None
    
    if spark:
        # Áµ±ÂêàÂá¶ÁêÜ: ÂàÜÊûêÁµêÊûú„ÅåÂøÖË¶Å„Å™„ÅÆ„ÅßÁ¢∫Ë™ç
        try:
            # analysis_result„ÅåÂÆöÁæ©„Åï„Çå„Å¶„ÅÑ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ
            if 'analysis_result' in globals():
                current_analysis_result = analysis_result
            else:
                print("‚ö†Ô∏è Analysis results not found. Executing simple analysis...")
                current_analysis_result = "ÂàÜÊûêÁµêÊûú„ÅåÂà©Áî®„Åß„Åç„Å™„ÅÑ„Åü„ÇÅ„ÄÅÂü∫Êú¨ÁöÑ„Å™ÊúÄÈÅ©Âåñ„ÅÆ„ÅøÂÆüË°å"
            
            # extracted_metrics„ÅåÂÆöÁæ©„Åï„Çå„Å¶„ÅÑ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ  
            if 'extracted_metrics' in globals():
                current_metrics = extracted_metrics
            else:
                print("‚ö†Ô∏è Metrics not found. Executing with empty metrics...")
                current_metrics = {}
            
            # thinking_enabledÂØæÂøú
            if isinstance(current_analysis_result, list):
                analysis_result_str = extract_main_content_from_thinking_response(current_analysis_result)
            else:
                analysis_result_str = str(current_analysis_result)
            
            # üîç Step 1: Original query EXPLAIN execution (with pre-correction)
            print("\nüìã Step 1: Original query EXPLAIN execution (Photon compatibility analysis)")
            print("-" * 60)
            
            # üéØ Save the original query as-is (relying completely on LLM correction)
            print("üìã Using original query as-is: Relying on advanced LLM correction")
            original_query_validated = original_query_for_explain
            
            # üéØ ÂÖÉ„ÇØ„Ç®„É™„Çí„Ç∞„É≠„Éº„Éê„É´Â§âÊï∞„Å®„Åó„Å¶‰øùÂ≠òÔºàÈáçË§áÂá¶ÁêÜÈò≤Ê≠¢Ôºâ
            globals()['original_query_corrected'] = original_query_validated
            print("üíæ Caching original query: Preventing duplicate processing")
            
            original_explain_result = execute_explain_and_save_to_file(original_query_for_explain, "original")
            
            # üöÄ EXPLAINÁµêÊûú„Çí„Ç∞„É≠„Éº„Éê„É´„Ç≠„É£„ÉÉ„Ç∑„É•„Å´‰øùÂ≠òÔºàÈáçË§áÂÆüË°åÈò≤Ê≠¢Ôºâ
            if original_explain_result and 'error_file' not in original_explain_result:
                globals()['cached_main_original_explain_result'] = original_explain_result
                print("üíæ Caching main EXPLAIN results: Preventing duplicate processing")
            
            # üö® ÂÖÉ„ÇØ„Ç®„É™„Åß„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„ÅüÂ†¥Âêà„ÅÆLLM‰øÆÊ≠£
            if 'error_file' in original_explain_result:
                print(f"üö® Detected syntax error in original query: {original_explain_result.get('error_file', 'unknown')}")
                print("ü§ñ Executing LLM-based original query correction...")
                
                # „Ç®„É©„ÉºÂÜÖÂÆπ„ÇíË™≠„ÅøËæº„Åø
                error_message = ""
                if 'error_file' in original_explain_result:
                    try:
                        with open(original_explain_result['error_file'], 'r', encoding='utf-8') as f:
                            error_message = f.read()
                    except:
                        error_message = "„Ç®„É©„Éº„Éï„Ç°„Ç§„É´Ë™≠„ÅøËæº„ÅøÂ§±Êïó"
                
                # LLM„Å´„Çà„ÇãÂÖÉ„ÇØ„Ç®„É™‰øÆÊ≠£
                corrected_original_query = generate_optimized_query_with_error_feedback(
                    original_query_for_explain,
                    "ÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Å´ÊßãÊñá„Ç®„É©„Éº„ÅåÊ§úÂá∫„Åï„Çå„Åæ„Åó„Åü„ÄÇ‰øÆÊ≠£„ÅåÂøÖË¶Å„Åß„Åô„ÄÇ",
                    current_metrics,
                    error_message,
                    ""  # previous_optimized_query„ÅØÁ©∫
                )
                
                # üêõ DEBUG: ÂÖÉ„ÇØ„Ç®„É™„ÅÆ„Ç®„É©„Éº‰øÆÊ≠£ÁµêÊûú„Çí‰øùÂ≠ò
                if isinstance(corrected_original_query, str) and not corrected_original_query.startswith("LLM_ERROR:"):
                    save_debug_query_trial(corrected_original_query, 0, "original_query_correction", 
                                         query_id="original_corrected", 
                                         error_info=f"ÂÖÉ„ÇØ„Ç®„É™ÊßãÊñá„Ç®„É©„Éº‰øÆÊ≠£: {error_message[:100] if error_message else 'unknown error'}")
                
                # ‰øÆÊ≠£ÁµêÊûú„Çí„ÉÅ„Çß„ÉÉ„ÇØ
                if isinstance(corrected_original_query, str) and not corrected_original_query.startswith("LLM_ERROR:"):
                    print("‚úÖ LLM-based original query correction completed")
                    
                    # ‰øÆÊ≠£„Åï„Çå„Åü„ÇØ„Ç®„É™„Åã„ÇâSQL„ÇíÊäΩÂá∫
                    if isinstance(corrected_original_query, list):
                        corrected_query_str = extract_main_content_from_thinking_response(corrected_original_query)
                    else:
                        corrected_query_str = str(corrected_original_query)
                    
                    extracted_sql = extract_sql_from_llm_response(corrected_query_str)
                    if extracted_sql:
                        original_query_for_explain = extracted_sql
                        print("üîÑ Re-executing EXPLAIN with corrected query")
                        
                        # ‰øÆÊ≠£„Åï„Çå„Åü„ÇØ„Ç®„É™„ÅßÂÜçÂ∫¶EXPLAINÂÆüË°å
                        original_explain_result = execute_explain_and_save_to_file(original_query_for_explain, "original_corrected")
                        
                        # üöÄ ‰øÆÊ≠£ÂæåEXPLAINÁµêÊûú„ÇÇ„Ç≠„É£„ÉÉ„Ç∑„É•„Å´‰øùÂ≠òÔºàÈáçË§áÂÆüË°åÈò≤Ê≠¢Ôºâ
                        if original_explain_result and 'error_file' not in original_explain_result:
                            globals()['cached_corrected_original_explain_result'] = original_explain_result
                            print("üíæ Caching corrected EXPLAIN results: Preventing duplicate processing")
                        
                        # „Ç∞„É≠„Éº„Éê„É´„Ç≠„É£„ÉÉ„Ç∑„É•„ÇÇÊõ¥Êñ∞
                        globals()['original_query_corrected'] = original_query_for_explain
                        print("üíæ Updating cache with corrected original query")
                    else:
                        print("‚ùå Failed to extract SQL from corrected query")
                else:
                    print("‚ùå LLM-based original query correction failed")
            
            if 'explain_file' in original_explain_result:
                print(f"‚úÖ Saved original query EXPLAIN result: {original_explain_result['explain_file']}")
            if 'plan_lines' in original_explain_result:
                print(f"üìä Original query execution plan lines: {original_explain_result['plan_lines']:,}")
            
            # üöÄ Step 2: New iterative optimization process: up to 3 improvement attempts with degradation cause analysis
            print("\nüìã Step 2: Iterative LLM optimization & performance degradation analysis (max 3 improvement attempts)")
            print("-" * 60)
            max_optimization_attempts = globals().get('MAX_OPTIMIZATION_ATTEMPTS', 3)
            retry_result = execute_iterative_optimization_with_degradation_analysis(
                original_query_for_explain, 
                analysis_result_str, 
                current_metrics, 
                max_optimization_attempts=max_optimization_attempts
            )            
            # ÁµêÊûú„ÅÆË°®Á§∫
            print(f"\nüìä Final result: {retry_result['final_status']}")
            print(f"üîÑ Total attempts: {retry_result['total_attempts']}")
            
            # ÂèçÂæ©ÊúÄÈÅ©Âåñ„ÅÆË©¶Ë°åË©≥Á¥∞Ë°®Á§∫
            if 'optimization_attempts' in retry_result:
                attempts = retry_result['optimization_attempts']
                print(f"üìà Optimization attempt details: {len(attempts)} times")
                for attempt in attempts:
                    status_icon = {
                        'success': '‚úÖ',
                        'performance_degraded': 'üö®',
                        'llm_error': '‚ùå',
                        'explain_failed': '‚ö†Ô∏è',
                        'comparison_error': 'üîß'
                    }.get(attempt['status'], '‚ùì')
                    print(f"   {status_icon} Attempt {attempt['attempt']}: {attempt['status']}")
                    if 'cost_ratio' in attempt and attempt['cost_ratio'] is not None:
                        print(f"      üí∞ Cost ratio: {attempt['cost_ratio']:.2f}x")
            
            if retry_result['final_status'] in ['optimization_success', 'partial_success']:
                print("‚úÖ Successfully executed EXPLAIN for optimized query!")
                
                # ÊàêÂäüÊôÇ„ÅÆ„Éï„Ç°„Ç§„É´ÊÉÖÂ†±Ë°®Á§∫
                explain_result = retry_result.get('explain_result', {})
                if explain_result:
                    print("\nüìÅ Generated files:")
                    if 'explain_file' in explain_result:
                        print(f"   üìÑ EXPLAIN results: {explain_result['explain_file']}")
                    if 'plan_lines' in explain_result:
                        print(f"   üìä Execution plan lines: {explain_result['plan_lines']:,}")
                
                # ÊúÄÈÅ©Âåñ„Åï„Çå„Åü„ÇØ„Ç®„É™„ÅÆ‰øùÂ≠ò
                optimized_result = retry_result.get('optimized_result', '')
                final_query = retry_result.get('final_query', original_query_for_explain)
                
                # File saving: final_query (successful query) to SQL file, optimized_result (original LLM response) to report
                performance_comparison = retry_result.get('performance_comparison')
                best_attempt_number = retry_result.get('best_result', {}).get('attempt_num')  # üéØ „Éô„Çπ„ÉàË©¶Ë°åÁï™Âè∑„ÇíÂèñÂæó
                optimization_attempts = retry_result.get('optimization_attempts', [])  # üéØ ÊúÄÈÅ©ÂåñË©¶Ë°åË©≥Á¥∞„ÇíÂèñÂæó
                saved_files = save_optimized_sql_files(
                    original_query_for_explain,
                    final_query,  # üöÄ ÊàêÂäü„Åó„Åü„ÇØ„Ç®„É™Ôºà„Éí„É≥„Éà‰ªò„ÅçÔºâ„Çí‰øùÂ≠ò
                    current_metrics,
                    analysis_result_str,
                    optimized_result,  # üìä ÂÖÉ„ÅÆLLM„É¨„Çπ„Éù„É≥„ÇπÔºà„É¨„Éù„Éº„ÉàÁî®Ôºâ
                    performance_comparison,  # üîç „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉÁµêÊûú
                    best_attempt_number,  # üéØ „Éô„Çπ„ÉàË©¶Ë°åÁï™Âè∑Ôºà„É¨„Éù„Éº„ÉàÁî®Ôºâ
                    optimization_attempts  # üéØ ÊúÄÈÅ©ÂåñË©¶Ë°åË©≥Á¥∞Ôºà„É¨„Éù„Éº„ÉàÁî®Ôºâ
                )
                
                print("\nüìÅ Optimization files:")
                for file_type, filename in saved_files.items():
                    print(f"   üìÑ {file_type}: {filename}")
                    
            elif retry_result['final_status'] == 'optimization_failed':
                print("üö® Using original query due to failure or degradation in all optimization attempts")
                fallback_reason = retry_result.get('fallback_reason', 'Unknown reason')
                print(f"üîß Failure reason: {fallback_reason}")
                
                # Â§±ÊïóË©≥Á¥∞„ÅÆË°®Á§∫
                if 'optimization_attempts' in retry_result:
                    attempts = retry_result['optimization_attempts']
                    degraded_count = sum(1 for a in attempts if a['status'] == 'performance_degraded')
                    error_count = sum(1 for a in attempts if a['status'] in ['llm_error', 'explain_failed'])
                    
                    if degraded_count > 0:
                        print(f"üìä Performance degradation: {degraded_count} times")
                    if error_count > 0:
                        print(f"‚ùå Errors occurred: {error_count} times")
                
                print("üí° Recommendations:")
                print("   - Consider updating table statistics")
                print("   - Consider manual optimization with more detailed EXPLAIN information")
                print("   - Please check data volume and query complexity")
                
                # üöÄ Â§±ÊïóÊôÇ„Åß„ÇÇ„É¨„Éù„Éº„ÉàÁîüÊàê„ÇíÂÆüË°åÔºà„É¶„Éº„Ç∂„ÉºË¶ÅÊ±Ç„Å´„Çà„ÇãËøΩÂä†Ôºâ
                print("\nü§ñ Generating final report even though optimization failed...")
                fallback_query = retry_result.get('final_query', original_query_for_explain)
                fallback_result = retry_result.get('optimized_result', 'Optimization failed')
                optimization_attempts = retry_result.get('optimization_attempts', [])
                best_attempt_number = retry_result.get('best_result', {}).get('attempt_num', 1)
                
                saved_files = save_optimized_sql_files(
                    original_query_for_explain,
                    fallback_query,  # ÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Åæ„Åü„ÅØÊúÄÂæå„Å´ÊàêÂäü„Åó„Åü„ÇØ„Ç®„É™
                    current_metrics,
                    analysis_result_str,
                    fallback_result,  # Â§±ÊïóÊÉÖÂ†±„ÇíÂê´„ÇÄ„É¨„Éù„Éº„Éà
                    None,  # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉ„ÅØÂ§±Êïó
                    best_attempt_number,  # ÊúÄÈÅ©ÂåñË©¶Ë°åÁï™Âè∑
                    optimization_attempts  # ÊúÄÈÅ©ÂåñË©¶Ë°åË©≥Á¥∞
                )
                
                print("\nüìÅ Generated files (failure case):")
                for file_type, filename in saved_files.items():
                    print(f"   üìÑ {file_type}: {filename}")
            
            elif retry_result['final_status'] == 'fallback_to_original':
                print("‚ö†Ô∏è Using original query due to persistent errors in optimized query")
            
            elif retry_result['final_status'] == 'llm_error':
                print("‚ùå Using original query due to LLM API call error")
                error_details = retry_result.get('error_details', 'Unknown error')
                print(f"üîß LLM error details: {error_details[:200]}...")
                print("üí° Solution: Reduce input data size or adjust LLM settings")
            
            elif retry_result['final_status'] == 'llm_error_correction_failed':
                print("‚ùå Using original query due to LLM error even during error correction")
                error_details = retry_result.get('error_details', 'Unknown error')
                print(f"üîß LLM error details: {error_details[:200]}...")
                print("üí° Solution: Execute manual SQL optimization or retry with simpler query")
                
                # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÊôÇ„ÅÆ„Éï„Ç°„Ç§„É´ÊÉÖÂ†±Ë°®Á§∫
                fallback_files = retry_result.get('fallback_files', {})
                failure_log = retry_result.get('failure_log', '')
                
                print("\nüìÅ Generated files:")
                for file_type, filename in fallback_files.items():
                    print(f"   üìÑ {file_type}: {filename}")
                if failure_log:
                    print(f"   üìÑ Failure log: {failure_log}")
                    
            # ÂÖ®Ë©¶Ë°å„ÅÆË©≥Á¥∞Ë°®Á§∫
            print("\nüìã Attempt details:")
            for attempt in retry_result.get('all_attempts', []):
                status_icon = "‚úÖ" if attempt['status'] == 'success' else "‚ùå"
                print(f"   {status_icon} Attempt {attempt['attempt']}: {attempt['status']}")
                if attempt['status'] == 'error':
                    print(f"      Error: {attempt['error_message'][:100]}...")
                    
        except Exception as e:
            print(f"‚ùå Error occurred during integrated processing: {str(e)}")
            print("üö® Emergency error details:")
            import traceback
            traceback.print_exc()
            print("   Emergency fallback: Executing basic analysis and minimal file generation...")
            
            try:
                # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: ÂæìÊù•„ÅÆEXPLAINÂÆüË°åÔºà„Ç™„É™„Ç∏„Éä„É´„ÇØ„Ç®„É™Ôºâ
                # üöÄ Êó¢Â≠ò„ÅÆ„Ç≠„É£„ÉÉ„Ç∑„É•„Åï„Çå„ÅüEXPLAINÁµêÊûú„Çí„ÉÅ„Çß„ÉÉ„ÇØÔºàÈáçË§áÂÆüË°åÈò≤Ê≠¢Ôºâ
                cached_original_result = globals().get('cached_original_explain_cost_result')
                if cached_original_result and 'explain_file' in cached_original_result:
                    print("üíæ Using cached EXPLAIN results for fallback processing (avoiding duplicate execution)")
                    explain_results = cached_original_result
                else:
                    print("üîÑ Executing EXPLAIN for original query (fallback processing)")
                    explain_results = execute_explain_and_save_to_file(original_query_for_explain, "original")
                
                if explain_results:
                    print("\nüìÅ EXPLAIN results:")
                    for file_type, filename in explain_results.items():
                        if file_type == 'explain_file':
                            print(f"   üìÑ EXPLAIN results: {filename}")
                        elif file_type == 'error_file':
                            print(f"   üìÑ Error log: {filename}")
                        elif file_type == 'plan_lines':
                            print(f"   üìä Execution plan lines: {filename}")
                        elif file_type == 'error_message':
                            print(f"   ‚ùå Error message: {filename}")
                
                # üö® Á∑äÊÄ•‰øÆÊ≠£: „Ç®„É©„ÉºÊôÇ„Åß„ÇÇ„É¨„Éù„Éº„Éà„Éï„Ç°„Ç§„É´„ÇíÂº∑Âà∂ÁîüÊàê
                print("üö® Executing emergency report generation...")
                emergency_saved_files = save_optimized_sql_files(
                    original_query_for_explain,
                    original_query_for_explain,  # ÊúÄÈÅ©ÂåñÂ§±ÊïóÊôÇ„ÅØÂÖÉ„ÇØ„Ç®„É™„Çí‰ΩøÁî®
                    current_metrics if 'current_metrics' in locals() else {},
                    "Á∑äÊÄ•„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: Áµ±ÂêàÂá¶ÁêÜ„Åß„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åü„Åü„ÇÅ„ÄÅÂü∫Êú¨ÂàÜÊûê„ÅÆ„ÅøÂÆüË°å",
                    f"Á∑äÊÄ•„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÂá¶ÁêÜ\n\n„Ç®„É©„ÉºË©≥Á¥∞:\n{str(e)}\n\nÂÖÉ„ÇØ„Ç®„É™„Çí„Åù„ÅÆ„Åæ„Åæ‰ΩøÁî®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ",
                    None  # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉÁµêÊûú„Å™„Åó
                )
                
                print("\nüìÅ Emergency generated files:")
                for file_type, filename in emergency_saved_files.items():
                    print(f"   üìÑ {file_type}: {filename}")
                    
            except Exception as emergency_error:
                print(f"üö® Error even in emergency fallback processing: {str(emergency_error)}")
                print("‚ö†Ô∏è Please verify query manually")
        
        print("\n‚úÖ Integrated SQL optimization processing completed")
        
    else:
        print("‚ùå EXPLAIN statements cannot be executed because Spark environment is not available")
        print("   Please execute in Databricks environment")
        
else:
    print("‚ùå No executable original query available")
    print("   Note: Original query extraction from profiler data was unsuccessful")

print()



# COMMAND ----------

# MAGIC %md
# MAGIC ## üìù Report Formatting Process
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Search and load optimization report files
# MAGIC - Refine and improve report content using LLM
# MAGIC - Save refinement results and generate final report

# COMMAND ----------
# 
# üìù „É¨„Éù„Éº„ÉàÊé®Êï≤Âá¶ÁêÜÔºàÁµ±ÂêàÂá¶ÁêÜÁî®Ôºâ
print("\nüìù Report refinement processing")
print("-" * 40)
# 
def find_latest_report_file() -> str:
    """Find the latest report file"""
    import os
    import glob
    
    # ÁèæÂú®„ÅÆ„Éá„Ç£„É¨„ÇØ„Éà„É™„Åß„É¨„Éù„Éº„Éà„Éï„Ç°„Ç§„É´„ÇíÊ§úÁ¥¢ (Ë®ÄË™ûÂà•ÂØæÂøú)
    language_suffix = 'en' if OUTPUT_LANGUAGE == 'en' else 'jp'
    pattern = f"output_optimization_report_{language_suffix}_*.md"
    report_files = glob.glob(pattern)
    
    if not report_files:
        return None
    
    # ÊúÄÊñ∞„ÅÆ„Éï„Ç°„Ç§„É´„ÇíÂèñÂæóÔºà„Çø„Ç§„É†„Çπ„Çø„É≥„ÉóÈ†ÜÔºâ
    latest_file = max(report_files, key=os.path.getctime)
    return latest_file
# 
def refine_report_content_with_llm(report_content: str) -> str:
    """Refine report using LLM"""
    
    # LLM„Éó„É≠„Éê„Ç§„ÉÄ„Éº„ÅÆË®≠ÂÆöÁ¢∫Ë™ç
    if not LLM_CONFIG or not LLM_CONFIG.get('provider'):
        print("‚ùå LLM provider is not configured")
        return report_content
    
    # üö® „Éà„Éº„ÇØ„É≥Âà∂ÈôêÂØæÁ≠ñ: „É¨„Éù„Éº„Éà„Çµ„Ç§„Ç∫Âà∂Èôê
    MAX_CONTENT_SIZE = 50000  # 50KBÂà∂Èôê
    original_size = len(report_content)
    
    if original_size > MAX_CONTENT_SIZE:
        print(f"‚ö†Ô∏è Report size too large: {original_size:,} characters ‚Üí truncated to {MAX_CONTENT_SIZE:,} characters")
        # ÈáçË¶Å„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÂÑ™ÂÖàÁöÑ„Å´‰øùÊåÅ
        truncated_content = report_content[:MAX_CONTENT_SIZE]
        truncated_content += f"\n\n‚ö†Ô∏è „É¨„Éù„Éº„Éà„ÅåÂ§ß„Åç„Åô„Åé„Çã„Åü„ÇÅ„ÄÅ{MAX_CONTENT_SIZE:,} ÊñáÂ≠ó„Å´Âàá„ÇäË©∞„ÇÅ„Çâ„Çå„Åæ„Åó„ÅüÔºàÂÖÉ„Çµ„Ç§„Ç∫: {original_size:,} ÊñáÂ≠óÔºâ"
        report_content = truncated_content
    else:
        print(f"üìä Report size: {original_size:,} characters (executing refinement)")
    
    # PhotonÂà©Áî®Áéá„ÅÆÊäΩÂá∫„Å®Ë©ï‰æ°Âà§ÂÆö
    import re
    photon_pattern = r'Âà©Áî®Áéá[Ôºö:]\s*(\d+(?:\.\d+)?)%'
    photon_match = re.search(photon_pattern, report_content)
    
    photon_evaluation_instruction = ""
    if photon_match:
        photon_utilization = float(photon_match.group(1))
        if OUTPUT_LANGUAGE == 'ja':
            if photon_utilization <= 80:
                photon_evaluation_instruction = """
„ÄêPhotonÂà©Áî®ÁéáË©ï‰æ°ÊåáÁ§∫„Äë
- PhotonÂà©Áî®Áéá„Åå80%‰ª•‰∏ã„ÅÆÂ†¥Âêà„ÅØ„ÄåË¶ÅÊîπÂñÑ„Äç„Åæ„Åü„ÅØ„Äå‰∏çËâØ„Äç„ÅÆË©ï‰æ°„ÇíÊòéÁ¢∫„Å´Ë°®Á§∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ
- 80%‰ª•‰∏ã„ÅÆÂ†¥Âêà„ÅØ„ÄÅÊîπÂñÑ„ÅÆÂøÖË¶ÅÊÄß„ÇíÂº∑Ë™ø„Åó„ÄÅÂÖ∑‰ΩìÁöÑ„Å™ÊîπÂñÑ„Ç¢„ÇØ„Ç∑„Éß„É≥„ÇíÊèêÁ§∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ
- Ë©ï‰æ°‰æã: „ÄåPhotonÂà©Áî®Áéá: XX% (Ë©ï‰æ°: Ë¶ÅÊîπÂñÑ)„Äç
"""
            else:
                photon_evaluation_instruction = """
„ÄêPhotonÂà©Áî®ÁéáË©ï‰æ°ÊåáÁ§∫„Äë
- PhotonÂà©Áî®Áéá„Åå80%‰ª•‰∏ä„ÅÆÂ†¥Âêà„ÅØ„ÄåËâØÂ•Ω„Äç„ÅÆË©ï‰æ°„ÇíË°®Á§∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ
- Ë©ï‰æ°‰æã: „ÄåPhotonÂà©Áî®Áéá: XX% (Ë©ï‰æ°: ËâØÂ•Ω)„Äç
"""
        else:
            if photon_utilization <= 80:
                photon_evaluation_instruction = """
„ÄêPhoton Utilization Rate Evaluation Instructions„Äë
- If Photon utilization rate is 80% or below, clearly display "Needs Improvement" or "Poor" evaluation
- For 80% or below, emphasize the need for improvement and provide specific improvement actions
- Example: "Photon Utilization Rate: XX% (Evaluation: Needs Improvement)"
"""
            else:
                photon_evaluation_instruction = """
„ÄêPhoton Utilization Rate Evaluation Instructions„Äë
- If Photon utilization rate is 80% or above, display "Good" evaluation
- Example: "Photon Utilization Rate: XX% (Evaluation: Good)"
"""
    
    # Ë®ÄË™û„Å´Âøú„Åò„Å¶Êé®Êï≤„Éó„É≠„É≥„Éó„Éà„ÇíÂàá„ÇäÊõø„Åà
    if OUTPUT_LANGUAGE == 'ja':
        refinement_prompt = f"""„ÅÇ„Å™„Åü„ÅØÊäÄË°ìÊñáÊõ∏Á∑®ÈõÜËÄÖ„Åß„Åô„ÄÇ‰ª•‰∏ã„ÅÆDatabricks SQL „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂàÜÊûê„É¨„Éù„Éº„Éà„ÇíË™≠„Åø„ÇÑ„Åô„ÅèÁ∞°ÊΩî„Å´Êé®Êï≤„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„ÄêÊé®Êï≤Ë¶Å‰ª∂„Äë
1. ÂÖ®‰ΩìÊßãÊàê„ÇíÊï¥ÁêÜ„Åó„ÄÅË´ñÁêÜÁöÑ„Å´ÊÉÖÂ†±„ÇíÈÖçÁΩÆ
2. ÂÜóÈï∑„Å™Ë°®Áèæ„ÇíÂâäÈô§„Åó„ÄÅÁ∞°ÊΩî„ÅßÁêÜËß£„Åó„ÇÑ„Åô„ÅÑË°®Áèæ„Å´‰øÆÊ≠£
3. ÈáçË¶Å„Å™ÊÉÖÂ†±„ÅåÂüã„ÇÇ„Çå„Å™„ÅÑ„Çà„ÅÜÈÅ©Âàá„Å™Ë¶ãÂá∫„Åó„É¨„Éô„É´„ÅßÊßãÈÄ†Âåñ
4. ÊäÄË°ìÁî®Ë™û„Çí‰øùÊåÅ„Åó„Å§„Å§„ÄÅÁêÜËß£„Åó„ÇÑ„Åô„ÅÑË™¨Êòé„ÇíËøΩÂä†
5. Êï∞ÂÄ§„Éá„Éº„Çø„Å®„É°„Éà„É™„ÇØ„Çπ„Çí‰øùÊåÅ
6. ÂÆüÁî®ÁöÑ„Å™Êé®Â•®‰∫ãÈ†Ö„ÇíÊòéÁ¢∫„Å´ÊèêÁ§∫

„Äêüö® ÂâäÈô§„Éª‰øÆÊ≠£„Åó„Å¶„ÅØ„ÅÑ„Åë„Å™„ÅÑÈáçË¶ÅÊÉÖÂ†±„Äë
- **ÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„ÉºÊÉÖÂ†±**: "ÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº: XX" „Åæ„Åü„ÅØ "Ë®≠ÂÆö„Å™„Åó" „ÅÆË°®Á§∫
- **„Éï„Ç£„É´„ÇøÁéáÊÉÖÂ†±**: "„Éï„Ç£„É´„ÇøÁéá: X.X% (Ë™≠„ÅøËæº„Åø: XX.XXGB, „Éó„É´„Éº„É≥: XX.XXGB)" „ÅÆÂΩ¢Âºè
- **„Éë„Éº„Çª„É≥„ÉÜ„Éº„Ç∏Ë®àÁÆó**: ÂêÑ„Éó„É≠„Çª„Çπ„ÅÆ "ÂÖ®‰Ωì„ÅÆXX%" Ë°®Á§∫Ôºà‰∏¶ÂàóÂÆüË°å„ÇíËÄÉÊÖÆ„Åó„ÅüÊ≠£Á¢∫„Å™Ë®àÁÆóÔºâ
- **Êé®Â•®vsÁèæÂú®„ÅÆÊØîËºÉÂàÜÊûê**: Êé®Â•®„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„Éº„Å®ÁèæÂú®„ÅÆ„Ç≠„Éº„ÅÆÊØîËºÉÊÉÖÂ†±
- **ÂÖ∑‰ΩìÁöÑ„Å™Êï∞ÂÄ§„É°„Éà„É™„ÇØ„Çπ**: ÂÆüË°åÊôÇÈñì„ÄÅ„Éá„Éº„ÇøË™≠„ÅøËæº„ÅøÈáè„ÄÅ„Çπ„Éî„É´Èáè„ÄÅÂà©Áî®ÁéáÁ≠â
- **SQLÂÆüË£Ö‰æã**: ALTER TABLEÊßãÊñá„ÄÅCLUSTER BYÊñá„ÄÅ„Éí„É≥„ÉàÂè•Á≠â„ÅÆÂÖ∑‰Ωì‰æã
- **„ÉÜ„Éº„Éñ„É´Âà•Ë©≥Á¥∞ÊÉÖÂ†±**: ÂêÑ„ÉÜ„Éº„Éñ„É´„ÅÆ„Éé„Éº„ÉâÊÉÖÂ†±„ÄÅ„Éï„Ç£„É´„ÇøÂäπÁéá„ÄÅÊé®Â•®‰∫ãÈ†Ö

{photon_evaluation_instruction}

„ÄêÁèæÂú®„ÅÆ„É¨„Éù„Éº„ÉàÂÜÖÂÆπ„Äë
{report_content}

„ÄêÂá∫ÂäõË¶Å‰ª∂„Äë
- „Éû„Éº„ÇØ„ÉÄ„Ç¶„É≥ÂΩ¢Âºè„ÅßÊé®Êï≤„Åï„Çå„Åü„É¨„Éù„Éº„Éà„ÇíÂá∫Âäõ
- ÊäÄË°ìÊÉÖÂ†±„Çí‰øùÊåÅ„Åó„Å§„Å§ÂèØË™≠ÊÄß„ÇíÂêë‰∏ä
- ÈáçË¶Å„Éù„Ç§„É≥„Éà„ÅÆÂº∑Ë™ø„Å®Ë°åÂãïË®àÁîª„ÅÆÊòéÁ¢∫Âåñ
- PhotonÂà©Áî®ÁéáË©ï‰æ°„ÅÆÊòéÁ¢∫„Å™Ë°®Á§∫
- **ÂøÖÈ†à**: ÁèæÂú®„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Ç≠„ÉºÊÉÖÂ†±„Å®„Éï„Ç£„É´„ÇøÁéáÊÉÖÂ†±„ÅÆÂÆåÂÖ®‰øùÊåÅ
- **ÂøÖÈ†à**: „Éë„Éº„Çª„É≥„ÉÜ„Éº„Ç∏Ë®àÁÆó„Åß„ÅØÂÖÉ„ÅÆÊ≠£Á¢∫„Å™Êï∞ÂÄ§„Çí‰ΩøÁî®
- **ÂøÖÈ†à**: „ÉÜ„Éº„Éñ„É´Âà•Ë©≥Á¥∞ÂàÜÊûêÊÉÖÂ†±ÔºàÁèæÂú®„Ç≠„Éº„ÄÅÊé®Â•®„Ç≠„Éº„ÄÅ„Éï„Ç£„É´„ÇøÁéáÔºâ„ÇíÂâäÈô§„Åó„Å™„ÅÑ
- **ÂøÖÈ†à**: SQLÂÆüË£Ö‰æãÔºàALTER TABLE„ÄÅCLUSTER BYÁ≠âÔºâ„ÇíÂÆåÂÖ®„Å™ÂΩ¢„Åß‰øùÊåÅ
"""
    else:
        refinement_prompt = f"""You are a technical document editor. Please refine the following Databricks SQL performance analysis report to make it readable and concise.

„ÄêRefinement Requirements„Äë
1. Organize the overall structure and arrange information logically
2. Remove redundant expressions and modify to concise, understandable expressions
3. Structure with appropriate heading levels so important information doesn't get buried
4. Keep technical terms while adding understandable explanations
5. Preserve numerical data and metrics
6. Clearly present practical recommendations

„Äêüö® Critical Information That Must NOT Be Deleted or Modified„Äë
- **Current clustering key information**: Display "Current clustering key: XX" or "Not configured"
- **Filter rate information**: Format "Filter rate: X.X% (read: XX.XXGB, pruned: XX.XXGB)"
- **Percentage calculations**: Display "XX% of total" for each process (accurate calculations considering parallel execution)
- **Recommended vs current comparison analysis**: Comparison information between recommended clustering keys and current keys
- **Specific numerical metrics**: Execution time, data read volume, spill volume, utilization rates, etc.
- **SQL implementation examples**: Specific examples of ALTER TABLE syntax, CLUSTER BY statements, hint clauses, etc.
- **Table-specific detailed information**: Node information, filter efficiency, and recommendations for each table

{photon_evaluation_instruction}

„ÄêCurrent Report Content„Äë
{report_content}

„ÄêOutput Requirements„Äë
- Output refined report in markdown format
- Maintain technical information while improving readability
- Emphasize important points and clarify action plans
- Clearly display Photon utilization rate evaluation
- **Required**: Completely preserve current clustering key information and filter rate information
- **Required**: Use original accurate numerical values for percentage calculations
- **Required**: Do not delete detailed analysis information by table (current key, recommended key, filter rate)
- **Required**: Preserve SQL implementation examples (ALTER TABLE, CLUSTER BY, etc.) in complete form
"""
    
    try:
        # Ë®≠ÂÆö„Åï„Çå„ÅüLLM„Éó„É≠„Éê„Ç§„ÉÄ„Éº„Å´Âü∫„Å•„ÅÑ„Å¶Êé®Êï≤„ÇíÂÆüË°å
        provider = LLM_CONFIG.get('provider', 'databricks')
        
        if provider == 'databricks':
            refined_content = _call_databricks_llm(refinement_prompt)
        elif provider == 'openai':
            refined_content = _call_openai_llm(refinement_prompt)
        elif provider == 'azure_openai':
            refined_content = _call_azure_openai_llm(refinement_prompt)
        elif provider == 'anthropic':
            refined_content = _call_anthropic_llm(refinement_prompt)
        else:
            print(f"‚ùå Unsupported LLM provider: {provider}")
            return report_content
        
        # üö® LLM„Ç®„É©„Éº„É¨„Çπ„Éù„É≥„Çπ„ÅÆÊ§úÂá∫ÔºàÁ≤æÂØÜÂåñÔºâ
        if isinstance(refined_content, str):
            # „Çà„ÇäÁ≤æÂØÜ„Å™„Ç®„É©„ÉºÊ§úÂá∫Ôºà„É¨„Éù„Éº„ÉàÂÜÖÂÆπ„ÅÆÁµµÊñáÂ≠ó„Å®Âå∫Âà•Ôºâ
            actual_error_indicators = [
                "API„Ç®„É©„Éº: „Çπ„ÉÜ„Éº„Çø„Çπ„Ç≥„Éº„Éâ",
                "Input is too long for requested model",
                "Bad Request",
                "„Çø„Ç§„É†„Ç¢„Ç¶„Éà„Ç®„É©„Éº:",
                "APIÂëº„Å≥Âá∫„Åó„Ç®„É©„Éº:",
                '„É¨„Çπ„Éù„É≥„Çπ: {"error_code":',
                "‚ùå API„Ç®„É©„Éº:",
                "‚ö†Ô∏è API„Ç®„É©„Éº:"
            ]
            
            # „Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÈñãÂßãÈÉ®ÂàÜ„Çí„ÉÅ„Çß„ÉÉ„ÇØÔºà„Çà„ÇäÂé≥ÂØÜÔºâ
            is_error_response = any(
                refined_content.strip().startswith(indicator) or 
                f"\n{indicator}" in refined_content[:500]  # ÂÖàÈ†≠500ÊñáÂ≠ó‰ª•ÂÜÖ„Åß„ÅÆ„Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏
                for indicator in actual_error_indicators
            )
            
            if is_error_response:
                print(f"‚ùå Error detected in LLM report refinement: {refined_content[:200]}...")
                print("üìÑ Returning original report")
                return report_content
        
        # thinking_enabledÂØæÂøú: ÁµêÊûú„Åå„É™„Çπ„Éà„ÅÆÂ†¥Âêà„ÅÆÂá¶ÁêÜ
        if isinstance(refined_content, list):
            refined_content = format_thinking_response(refined_content)
        
        print(f"‚úÖ LLM-based report refinement completed (Cell 46 independent processing)")
        return refined_content
        
    except Exception as e:
        print(f"‚ùå Error occurred during LLM-based report refinement: {str(e)}")
        return report_content
# 
def save_refined_report(refined_content: str, original_filename: str) -> str:
    """Save refined report"""
    from datetime import datetime
    
    # ÊúÄÁµÇ„É¨„Éù„Éº„Éà„ÅÆ„Éï„Ç°„Ç§„É´Âêç„ÇíÁîüÊàêÔºàË®ÄË™ûÂà•ÂØæÂøúÔºâ
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    language_suffix = 'en' if OUTPUT_LANGUAGE == 'en' else 'jp'
    refined_filename = f"output_final_report_{language_suffix}_{timestamp}.md"
    
    try:
        with open(refined_filename, 'w', encoding='utf-8') as f:
            f.write(refined_content)
        
        print(f"‚úÖ Saved final report: {refined_filename}")
        return refined_filename
        
    except Exception as e:
        print(f"‚ùå Error during refined report saving: {str(e)}")
        return None
# 
def finalize_report_files(original_filename: str, refined_filename: str) -> str:
    """Execute file processing based on DEBUG_ENABLED setting"""
    import os
    
    # DEBUG_ENABLEDË®≠ÂÆö„ÇíÁ¢∫Ë™ç
    debug_enabled = globals().get('DEBUG_ENABLED', 'N')
    
    try:
        if debug_enabled.upper() == 'Y':
            # DEBUG_ENABLED=Y: ÂÖÉ„ÅÆ„Éï„Ç°„Ç§„É´„ÇíÂêçÁß∞Â§âÊõ¥„Åó„Å¶‰øùÊåÅ
            if os.path.exists(original_filename):
                # „Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó„Éï„Ç°„Ç§„É´Âêç„ÇíÁîüÊàêÔºàÂÖÉ„Éï„Ç°„Ç§„É´Âêç„Å´ _raw „ÇíËøΩÂä†Ôºâ
                backup_filename = original_filename.replace('.md', '_raw.md')
                
                os.rename(original_filename, backup_filename)
                print(f"üìÅ Preserving original file: {original_filename} ‚Üí {backup_filename}")
            else:
                print(f"‚ö†Ô∏è Original file not found: {original_filename}")
        else:
            # DEBUG_ENABLED=N: ÂÖÉ„ÅÆ„Éï„Ç°„Ç§„É´„ÇíÂâäÈô§
            if os.path.exists(original_filename):
                os.remove(original_filename)
                print(f"üóëÔ∏è Deleted original file: {original_filename}")
        
        # ÊúÄÁµÇ„É¨„Éù„Éº„Éà„Éï„Ç°„Ç§„É´Ôºàoutput_final_report_*Ôºâ„ÅØ„É™„Éç„Éº„É†„Åõ„Åö„Åù„ÅÆ„Åæ„Åæ‰øùÊåÅ
        if os.path.exists(refined_filename):
            print(f"‚úÖ Preserving final report file: {refined_filename}")
            return refined_filename
        else:
            print(f"‚ùå Refined version file not found: {refined_filename}")
            return None
            
    except Exception as e:
        print(f"‚ùå Error during file operations: {str(e)}")
        return None
# 
# 
# „É°„Ç§„É≥Âá¶ÁêÜ
try:
    # ÊúÄÊñ∞„ÅÆ„É¨„Éù„Éº„Éà„Éï„Ç°„Ç§„É´„ÇíÊ§úÁ¥¢
    latest_report = find_latest_report_file()
    
    if not latest_report:
        print("‚ùå Report file not found")
        print("‚ö†Ô∏è No analysis report files were found in the current directory")
        print()
        print("üîç Detailed troubleshooting:")
        print("1. Please confirm that the main analysis processing completed normally")
        print("2. Please check if any error messages are displayed in previous cells")
        print("3. Please check if variables like current_analysis_result and extracted_metrics are defined")
        print("4. Emergency fallback processing may have been executed")
        print("5. You may need to re-run the main analysis cells to generate reports")
        
        # Èñ¢ÈÄ£„Éï„Ç°„Ç§„É´„ÅÆÂ≠òÂú®„ÉÅ„Çß„ÉÉ„ÇØ
        import glob
        sql_files = glob.glob("output_optimized_query_*.sql")
        original_files = glob.glob("output_original_query_*.sql")
        all_reports = glob.glob("output_optimization_report*.md")
        
        # ÁèæÂú®„ÅÆË®ÄË™ûË®≠ÂÆö„Å´ÂØæÂøú„Åô„Çã„É¨„Éù„Éº„Éà„Éï„Ç°„Ç§„É´
        language_suffix = 'en' if OUTPUT_LANGUAGE == 'en' else 'jp'
        current_lang_reports = glob.glob(f"output_optimization_report_{language_suffix}_*.md")
        
        print(f"\nüìÅ Current file status:")
        print(f"   üìÑ Optimized query files: {len(sql_files)} files")
        print(f"   üìÑ Original query files: {len(original_files)} files")
        print(f"   üìÑ Report files ({language_suffix.upper()}): {len(current_lang_reports)} files")
        print(f"   üìÑ Report files (total): {len(all_reports)} files")
        
        if all_reports:
            print(f"   üìã Detected report files:")
            for report in all_reports:
                print(f"      - {report}")
            print("   ‚ö†Ô∏è Files exist but not detected by find_latest_report_file()")
            print("   üí° Please check filenames manually - possible pattern matching issue")
        
        if not sql_files and not original_files:
            print("   üö® Important: Cell 43 processing may not have been executed at all")
            print("   üìã Solution: Re-execute Cell 43 from the beginning")
    else:
        print(f"üìÑ Target report file: {latest_report}")
        
        # „É¨„Éù„Éº„Éà„Éï„Ç°„Ç§„É´„ÅÆÂÜÖÂÆπ„ÇíË™≠„ÅøËæº„Åø
        with open(latest_report, 'r', encoding='utf-8') as f:
            original_content = f.read()
        
        print(f"üìä Original report size: {len(original_content):,} characters")
        
        # üö® ÈáçË§áÊé®Êï≤Èò≤Ê≠¢: Êó¢„Å´Êé®Êï≤Ê∏à„Åø„Åã„ÉÅ„Çß„ÉÉ„ÇØ
        refinement_indicators = [
            "üìä **ÊúÄÈÅ©Âåñ„É¨„Éù„Éº„Éà**",  # Êé®Êï≤Âæå„ÅÆÂÖ∏ÂûãÁöÑ„Å™„Éò„ÉÉ„ÉÄ„Éº
            "üöÄ **„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊîπÂñÑÁµêÊûú**",  # Êé®Êï≤Âæå„ÅÆÂÖ∏ÂûãÁöÑ„Å™„Çª„ÇØ„Ç∑„Éß„É≥
            "‚úÖ **Êé®Â•®‰∫ãÈ†Ö**",  # Êé®Êï≤Âæå„ÅÆ„Éï„Ç©„Éº„Éû„ÉÉ„Éà
            "LLM„Å´„Çà„ÇãÊé®Êï≤„ÇíÂÆüË°å‰∏≠",  # Êé®Êï≤„Éó„É≠„Çª„Çπ‰∏≠„Å´Âê´„Åæ„Çå„Çã„É°„ÉÉ„Çª„Éº„Ç∏
            "Êé®Êï≤Áâà„É¨„Éù„Éº„Éà:",  # Êé®Êï≤Ê∏à„Åø„Éï„Ç°„Ç§„É´„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏
        ]
        
        already_refined = any(indicator in original_content for indicator in refinement_indicators)
        
        if already_refined:
            print(f"‚úÖ Report already refined (avoiding duplicate processing): {latest_report}")
            print("üìã Using refined report as is")
            refined_content = original_content
        else:
            print(f"ü§ñ Executing LLM-based refinement (target: {latest_report})...")
            refined_content = refine_report_content_with_llm(original_content)
        
        if refined_content != original_content:
            print(f"üìä Post-refinement size: {len(refined_content):,} characters")
            
            # Êé®Êï≤„Åï„Çå„Åü„É¨„Éù„Éº„Éà„Çí‰øùÂ≠ò
            refined_filename = save_refined_report(refined_content, latest_report)
            
            if refined_filename:
                print(f"üìÑ Refined report: {refined_filename}")
                
                # „Éï„Ç°„Ç§„É´„Çµ„Ç§„Ç∫„ÅÆÁ¢∫Ë™ç
                import os
                if os.path.exists(refined_filename):
                    file_size = os.path.getsize(refined_filename)
                    print(f"üìÅ Refined file size: {file_size:,} bytes")
                
                # ÂÖÉ„ÅÆ„Éï„Ç°„Ç§„É´„ÇíÂâäÈô§„Åó„ÄÅÊé®Êï≤Áâà„Éï„Ç°„Ç§„É´„ÇíÂÖÉ„ÅÆ„Éï„Ç°„Ç§„É´Âêç„Å´„É™„Éç„Éº„É†
                final_filename = finalize_report_files(latest_report, refined_filename)
                
                if final_filename:
                    print(f"üìÑ Final report file: {final_filename}")
                    
                    # ÊúÄÁµÇ„Éï„Ç°„Ç§„É´„Çµ„Ç§„Ç∫„ÅÆÁ¢∫Ë™ç
                    if os.path.exists(final_filename):
                        final_file_size = os.path.getsize(final_filename)
                        print(f"üìÅ Final file size: {final_file_size:,} bytes")
                
                print(f"‚úÖ Report refinement processing completed: {final_filename}")
                
                # Êé®Êï≤„ÅÆÁµêÊûú„ÇíË°®Á§∫ÔºàÊúÄÂàù„ÅÆ1000ÊñáÂ≠óÔºâ
                print("\nüìã Refinement result preview:")
                print("-" * 50)
                preview = refined_content[:1000]
                print(preview)
                if len(refined_content) > 1000:
                    print(f"\n... (remaining {len(refined_content) - 1000} characters see {final_filename or latest_report})")
                print("-" * 50)
            else:
                print("‚ùå Failed to save refined report")
        else:
            print("üìã Report is already in optimal state (refinement processing skipped)")
            print("‚úÖ Using existing report file as is")
            
            # Êó¢„Å´Êé®Êï≤Ê∏à„Åø„ÅÆÂ†¥Âêà„ÇÇ„Éó„É¨„Éì„É•„Éº„ÇíË°®Á§∫
            print("\nüìã Report content preview:")
            print("-" * 50)
            preview = refined_content[:1000]
            print(preview)
            if len(refined_content) > 1000:
                print(f"\n... (remaining {len(refined_content) - 1000} characters see {latest_report})")
            print("-" * 50)
            
except Exception as e:
    print(f"‚ùå Error occurred during report refinement processing: {str(e)}")
    import traceback
    traceback.print_exc()
# 
print()
# 
# # üßπ ‰∏≠Èñì„Éï„Ç°„Ç§„É´„ÅÆÂâäÈô§Âá¶ÁêÜÔºàDEBUG_ENABLED„Éï„É©„Ç∞„Å´Âü∫„Å•„ÅèÔºâ
debug_enabled = globals().get('DEBUG_ENABLED', 'N')
explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')

if debug_enabled.upper() == 'Y':
    print("\nüêõ Debug mode enabled: Preserving intermediate files")
    print("-" * 40)
    print("üí° All intermediate files are preserved because DEBUG_ENABLED=Y")
    print("üìÅ The following files are preserved:")
    
    import glob
    import os
    
    # ‰øùÊåÅ„Åï„Çå„Çã„Éï„Ç°„Ç§„É´‰∏ÄË¶ß„ÇíË°®Á§∫
    if explain_enabled.upper() == 'Y':
        original_files = glob.glob("output_explain_original_*.txt")
        optimized_files = glob.glob("output_explain_optimized_*.txt")
        cost_original_files = glob.glob("output_explain_cost_original_*.txt")
        cost_optimized_files = glob.glob("output_explain_cost_optimized_*.txt")
        error_files = glob.glob("output_explain_error_*.txt")
        all_files = original_files + optimized_files + cost_original_files + cost_optimized_files + error_files
        
        if all_files:
            print(f"   üîç EXPLAIN result files:")
            print(f"      üìä EXPLAIN: Original {len(original_files)} files, Post-optimization {len(optimized_files)} files")
            print(f"      üí∞ EXPLAIN COST: Original {len(cost_original_files)} files, Post-optimization {len(cost_optimized_files)} files")
            print(f"      ‚ùå Errors: {len(error_files)} files")
            for file_path in all_files[:3]:  # ÊúÄÂ§ß3ÂÄã„Åæ„ÅßË°®Á§∫
                print(f"      üìÑ {file_path}")
            if len(all_files) > 3:
                print(f"      ... and {len(all_files) - 3} other files")
    
    print("‚úÖ Debug mode: Skipped file deletion processing")
else:
    print("\nüßπ Intermediate file deletion processing")
    print("-" * 40)
    print("üí° Deleting intermediate files because DEBUG_ENABLED=N")
    language_suffix = 'en' if OUTPUT_LANGUAGE == 'en' else 'jp'
    print(f"üìÅ Files to be kept: output_original_query_*.sql, output_optimization_report_{language_suffix}_*.md, output_optimized_query_*.sql")
    
    import glob
    import os
    
    if explain_enabled.upper() == 'Y':
        # EXPLAINÁµêÊûú„Éï„Ç°„Ç§„É´„Å®„Ç®„É©„Éº„Éï„Ç°„Ç§„É´„ÇíÊ§úÁ¥¢ÔºàÊñ∞„Éë„Çø„Éº„É≥ + Êóß„Éë„Çø„Éº„É≥Ôºâ
        original_files = glob.glob("output_explain_original_*.txt")
        optimized_files = glob.glob("output_explain_optimized_*.txt")
        cost_original_files = glob.glob("output_explain_cost_original_*.txt")
        cost_optimized_files = glob.glob("output_explain_cost_optimized_*.txt")
        error_original_files = glob.glob("output_explain_error_original_*.txt")
        error_optimized_files = glob.glob("output_explain_error_optimized_*.txt")
        
        # Êóß„Éë„Çø„Éº„É≥„ÅÆ„Éï„Ç°„Ç§„É´„ÇÇÂâäÈô§ÂØæË±°„Å´Âê´„ÇÅ„ÇãÔºà‰∏ã‰Ωç‰∫íÊèõÊÄßÔºâ
        old_explain_files = glob.glob("output_explain_plan_*.txt")
        old_error_files = glob.glob("output_explain_error_*.txt")
        
        # üö® Êñ∞Ë¶èËøΩÂä†: DEBUGÁî®„ÅÆÂÆåÂÖ®ÊÉÖÂ†±„Éï„Ç°„Ç§„É´„ÇÇÂâäÈô§ÂØæË±°„Å´Âê´„ÇÅ„Çã
        full_plan_files = glob.glob("output_physical_plan_full_*.txt")
        full_stats_files = glob.glob("output_explain_cost_statistics_full_*.txt")
        extracted_stats_files = glob.glob("output_explain_cost_statistics_extracted_*.json")
        structured_plan_files = glob.glob("output_physical_plan_structured_*.json")
        structured_cost_files = glob.glob("output_explain_cost_structured_*.json")
        
        all_temp_files = (original_files + optimized_files + cost_original_files + cost_optimized_files + 
                         error_original_files + error_optimized_files + old_explain_files + old_error_files +
                         full_plan_files + full_stats_files + extracted_stats_files + 
                         structured_plan_files + structured_cost_files)
        
        explain_files = original_files + optimized_files + old_explain_files
        cost_files = cost_original_files + cost_optimized_files
        error_files = error_original_files + error_optimized_files + old_error_files
        debug_files = full_plan_files + full_stats_files + extracted_stats_files + structured_plan_files + structured_cost_files
        
        if all_temp_files:
            print(f"üìÅ Files to be deleted:")
            print(f"   üìä EXPLAIN results: {len(explain_files)} files")
            print(f"   üí∞ EXPLAIN COST results: {len(cost_files)} files")
            print(f"   ‚ùå Error files: {len(error_files)} files")
            print(f"   üîß DEBUG complete information: {len(debug_files)} files")
            print("üí° Note: These files should not have been created because DEBUG_ENABLED=N")
            
            # üîß Â§âÊï∞„ÅÆÂàùÊúüÂåñ„Çí„Çà„ÇäÂÆâÂÖ®„Å´ÂÆüË°å
            deleted_count = 0
            for file_path in all_temp_files:
                try:
                    os.remove(file_path)
                    print(f"‚úÖ Deletion completed: {file_path}")
                    deleted_count += 1
                except Exception as e:
                    print(f"‚ùå Deletion failed: {file_path} - {str(e)}")
            
            print(f"üóëÔ∏è Deletion completed: {deleted_count}/{len(all_temp_files)} files")
            print("üí° EXPLAIN/EXPLAIN COST results and error files deleted as they were already used by LLM optimization processing")
        else:
            print("üìÅ No EXPLAIN/EXPLAIN COST results or error files found for deletion")
    else:
        print("‚ö†Ô∏è Skipped EXPLAIN result file deletion processing because EXPLAIN execution is disabled")

print()

print("üéâ All processing completed!")
print("üìÅ Please check the generated files and utilize the analysis results.")
