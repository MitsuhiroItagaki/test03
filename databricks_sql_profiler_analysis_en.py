# Databricks notebook source
# MAGIC %md
# MAGIC # Databricks SQL Profiler Analysis Tool
# MAGIC
# MAGIC This notebook reads Databricks SQL profiler JSON log files and extracts metrics necessary for bottleneck identification and improvement recommendations.
# MAGIC
# MAGIC ## Feature Overview
# MAGIC
# MAGIC 1. **SQL Profiler JSON File Loading**
# MAGIC    - Analysis of profiler logs output by Databricks
# MAGIC    - Extraction of execution plan metrics stored in the `graphs` key
# MAGIC
# MAGIC 2. **Key Metrics Extraction**
# MAGIC    - Query basic information (ID, status, execution time, etc.)
# MAGIC    - Overall performance (execution time, data volume, cache efficiency, etc.)
# MAGIC    - Stage and node detailed metrics
# MAGIC    - Bottleneck indicator calculation
# MAGIC
# MAGIC 3. **AI-powered Bottleneck Analysis**
# MAGIC    - Configurable LLM endpoints (Databricks, OpenAI, Azure OpenAI, Anthropic)
# MAGIC    - Bottleneck identification from extracted metrics
# MAGIC    - Specific improvement recommendations
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC **Prerequisites:**
# MAGIC - LLM endpoint configuration (Databricks Model Serving or external API)
# MAGIC - Required API key setup
# MAGIC - SQL profiler JSON file preparation (DBFS or FileStore)
# MAGIC
# MAGIC ---

# COMMAND ----------

# MAGIC %md
# MAGIC # ğŸ”§ Configuration & Setup Section
# MAGIC
# MAGIC **This section performs basic tool configuration**
# MAGIC
# MAGIC ğŸ“‹ **Configuration Contents:**
# MAGIC - Analysis target file specification
# MAGIC - LLM endpoint configuration
# MAGIC - Analysis function definitions
# MAGIC
# MAGIC âš ï¸ **Important:** Execute all cells in this section before running the main processing

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“ Analysis Target File Configuration
# MAGIC
# MAGIC **First, specify the SQL profiler JSON file to be analyzed.**
# MAGIC
# MAGIC This cell performs the following configurations:
# MAGIC - ğŸ“‚ SQL profiler JSON file path configuration
# MAGIC - ğŸ“‹ Examples of supported file path formats
# MAGIC - âš™ï¸ Basic environment configuration

# COMMAND ----------

# ğŸ“ SQL Profiler JSON File Path Configuration
# 
# Please change the JSON_FILE_PATH below to your actual file path:

# Notebook environment file path configuration (please select from the following options)

# Option 1: Pre-tuning plan file (recommended)
JSON_FILE_PATH = '/Workspace/Shared/AutoSQLTuning/Query2.json'

# Option 2: To use other JSON files, uncomment and edit the following
# JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/nophoton.json'
# JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/POC1.json'
# JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/your_file.json'

# Command line environment (optional)
import sys
if len(sys.argv) > 1 and not sys.argv[1].startswith('-'):
    # Use only when command line argument is not a flag (doesn't start with -)
    JSON_FILE_PATH = sys.argv[1]

# ğŸŒ Output language setting (OUTPUT_LANGUAGE: 'ja' = Japanese, 'en' = English)
OUTPUT_LANGUAGE = 'en'

# ğŸ” EXPLAIN statement execution setting (EXPLAIN_ENABLED: 'Y' = execute, 'N' = do not execute)
EXPLAIN_ENABLED = 'Y'

# ğŸ› Debug mode setting (DEBUG_ENABLED: 'Y' = keep intermediate files, 'N' = keep final files only)
DEBUG_ENABLED = 'Y'

# COMMAND ----------

def save_debug_query_trial(query: str, attempt_num: int, trial_type: str, query_id: str = None, error_info: str = None) -> str:
    """
    Save queries under optimization attempt by attempt when DEBUG_ENABLED=Y
    
    Args:
        query: Generated query
        attempt_num: Trial number (1, 2, 3, ...)
        trial_type: Trial type ('initial', 'performance_improvement', 'error_correction')
        query_id: Query ID (optional)
        error_info: Error information (optional)
    
    Returns:
        Saved file path (empty string if not saved)
    """
    debug_enabled = globals().get('DEBUG_ENABLED', 'N')
    if debug_enabled.upper() != 'Y':
        return ""
    
    try:
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        
        # Generate query ID from trial number if not specified
        if not query_id:
            query_id = f"trial_{attempt_num}"
        
        # Generate filename: debug_trial_{attempt_num}_{trial_type}_{timestamp}.sql
        filename = f"debug_trial_{attempt_num:02d}_{trial_type}_{timestamp}.sql"
        
        # Prepare metadata information
        metadata_header = f"""-- ğŸ› DEBUG: Optimization trial query (DEBUG_ENABLED=Y)
-- ğŸ“‹ Trial number: {attempt_num}
-- ğŸ¯ Trial type: {trial_type}
-- ğŸ• Generated time: {timestamp}
-- ğŸ” Query ID: {query_id}
"""
        
        # Add error information if available
        if error_info:
            metadata_header += f"""-- âš ï¸  Error information: {error_info[:200]}{'...' if len(error_info) > 200 else ''}
"""
        
        metadata_header += f"""-- ğŸ“„ Generated file: {filename}
-- ================================================

"""
        
        # File saving
        full_content = metadata_header + query
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        print(f"ğŸ› DEBUG save completed: {filename} (attempt {attempt_num}: {trial_type})")
        return filename
        
    except Exception as e:
        print(f"âš ï¸ DEBUG save error: {str(e)}")
        return ""

# ğŸ§  Structured extraction settings (STRUCTURED_EXTRACTION_ENABLED: 'Y' = use structured extraction, 'N' = use traditional truncation)
# Controls the processing method for Physical Plan and EXPLAIN COST
# - 'Y': Structured extraction of important information only (recommended: high precision & high efficiency)
# - 'N': Traditional truncation based on character limits (for fallback)
STRUCTURED_EXTRACTION_ENABLED = 'Y'

# ğŸ”„ Maximum retry count settings for automatic error correction (MAX_RETRIES: default 2 times)
# Number of retries when EXPLAIN execution of LLM-generated optimized queries encounters errors
# - 1st attempt: EXPLAIN execution with initial generated query
# - 2nd attempt and beyond: Re-input error information to LLM to generate corrected query and re-execute
# - When maximum attempts reached: Use original working query for file generation
MAX_RETRIES = 3

# ğŸš€ Iterative optimization maximum attempt count settings (MAX_OPTIMIZATION_ATTEMPTS: default 3 times)
# Number of improvement attempts when performance degradation is detected
# - 1st attempt: Initial optimization query generation and performance verification
# - 2nd attempt and beyond: Corrected query generation and verification based on degradation cause analysis
# - When maximum attempts reached: Use original query
# Note: This is a separate parameter from syntax error correction (MAX_RETRIES)
MAX_OPTIMIZATION_ATTEMPTS = 3

# ğŸ—‚ï¸ Catalog and database configuration (used when executing EXPLAIN statements)
CATALOG = 'tpcds'
DATABASE = 'tpcds_sf1000_delta_lc'

# ğŸ’¡ Usage examples:
# OUTPUT_LANGUAGE = 'ja'  # Output files in Japanese
# OUTPUT_LANGUAGE = 'en'  # Output files in English

# ğŸŒ Multilingual message dictionary
MESSAGES = {
    'ja': {
        'bottleneck_title': 'Databricks SQL Profiler Bottleneck Analysis Results',
        'query_id': 'Query ID',
        'analysis_time': 'Analysis Date/Time',
        'execution_time': 'Execution Time',
        'sql_optimization_report': 'SQL Optimization Report',
        'optimization_time': 'Optimization Date/Time',
        'original_file': 'Original File',
        'optimized_file': 'Optimized File',
        'optimization_analysis': 'Optimization Analysis Results',
        'performance_metrics': 'Performance Metrics Reference Information',
        'read_data': 'Data Read',
        'spill': 'Spill',
        'top10_processes': 'TOP10 Most Time-Consuming Processes'
    },
    'en': {
        'bottleneck_title': 'Databricks SQL Profiler Bottleneck Analysis Results',
        'query_id': 'Query ID',
        'analysis_time': 'Analysis Time',
        'execution_time': 'Execution Time',
        'sql_optimization_report': 'SQL Optimization Report',
        'optimization_time': 'Optimization Time',
        'original_file': 'Original File',
        'optimized_file': 'Optimized File',
        'optimization_analysis': 'Optimization Analysis Results',
        'performance_metrics': 'Performance Metrics Reference',
        'read_data': 'Data Read',
        'spill': 'Spill',
        'top10_processes': 'Top 10 Most Time-Consuming Processes'
    }
}

def get_message(key: str) -> str:
    """Get multilingual message"""
    return MESSAGES.get(OUTPUT_LANGUAGE, MESSAGES['ja']).get(key, key)

# ğŸ“‹ Supported file path format examples:
# Unity Catalog Volumes:
# JSON_FILE_PATH = '/Volumes/catalog/schema/volume/profiler.json'
# 
# FileStore (recommended):
# JSON_FILE_PATH = '/FileStore/shared_uploads/your_username/profiler_log.json'
# 
# DBFS:
# JSON_FILE_PATH = '/dbfs/FileStore/shared_uploads/your_username/profiler_log.json'
# 
# DBFS URI:
# JSON_FILE_PATH = 'dbfs:/FileStore/shared_uploads/your_username/profiler_log.json'

print("ğŸ“ ã€Analysis Target File Configuration Completedã€‘")
print("=" * 50)
print(f"ğŸ“„ Target file: {JSON_FILE_PATH}")
print("=" * 50)

# âš™ï¸ Basic environment configuration
import json
try:
    import pandas as pd
except ImportError:
    print("Warning: pandas is not installed, some features may not work")
    pd = None
from typing import Dict, List, Any, Optional
from datetime import datetime

print("âœ… Basic library import completed")
print("ğŸš€ Please proceed to the next cell")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¤– LLM Endpoint Configuration
# MAGIC
# MAGIC This cell performs the following configurations:
# MAGIC - LLM provider selection (Databricks/OpenAI/Azure/Anthropic)
# MAGIC - Connection settings for each provider
# MAGIC - Required library imports

# COMMAND ----------

# ğŸ¤– LLM Endpoint Configuration
LLM_CONFIG = {
    # Endpoint type: 'databricks', 'openai', 'azure_openai', 'anthropic'
    "provider": "databricks",
    
    # Databricks Model Serving configuration (high-speed execution priority)
    "databricks": {
        "endpoint_name": "databricks-claude-3-7-sonnet",  # Model Serving endpoint name
        "max_tokens": 131072,  # 128K tokens (Claude 3.7 Sonnet maximum limit)
        "temperature": 0.0,    # For deterministic output (0.1â†’0.0)
        # "thinking_enabled": False,  # Extended thinking mode (default: disabled - high-speed execution priority) - Claude 3 Sonnet only
        # "thinking_budget_tokens": 65536  # Thinking token budget 64K tokens (used only when enabled) - Claude 3 Sonnet only
    },
    
    # OpenAI configuration (optimized for complete SQL generation)
    "openai": {
        "api_key": "",  # OpenAI API key (can also use environment variable OPENAI_API_KEY)
        "model": "gpt-4o",  # gpt-4o, gpt-4-turbo, gpt-3.5-turbo
        "max_tokens": 16000,  # Maximum within OpenAI limits
        "temperature": 0.0    # For deterministic output (0.1â†’0.0)
    },
    
    # Azure OpenAI configuration (optimized for complete SQL generation)
    "azure_openai": {
        "api_key": "",  # Azure OpenAI API key (can also use environment variable AZURE_OPENAI_API_KEY)
        "endpoint": "",  # https://your-resource.openai.azure.com/
        "deployment_name": "",  # Deployment name
        "api_version": "2024-02-01",
        "max_tokens": 16000,  # Maximum within Azure OpenAI limits
        "temperature": 0.0    # For deterministic output (0.1â†’0.0)
    },
    
    # Anthropic configuration (optimized for complete SQL generation)
    "anthropic": {
        "api_key": "",  # Anthropic API key (can also use environment variable ANTHROPIC_API_KEY)
        "model": "claude-3-5-sonnet-20241022",  # claude-3-5-sonnet-20241022, claude-3-opus-20240229
        "max_tokens": 16000,  # Maximum within Anthropic limits
        "temperature": 0.0    # For deterministic output (0.1â†’0.0)
    }
}

print("ğŸ¤– LLM endpoint configuration completed")
print(f"ğŸ¤– LLM Provider: {LLM_CONFIG['provider']}")

if LLM_CONFIG['provider'] == 'databricks':
    print(f"ğŸ”— Databricks endpoint: {LLM_CONFIG['databricks']['endpoint_name']}")
    thinking_status = "Enabled" if LLM_CONFIG['databricks'].get('thinking_enabled', False) else "Disabled"
    thinking_budget = LLM_CONFIG['databricks'].get('thinking_budget_tokens', 65536)
    max_tokens = LLM_CONFIG['databricks'].get('max_tokens', 131072)
    print(f"ğŸ§  Extended thinking mode: {thinking_status} (budget: {thinking_budget:,} tokens)")
    print(f"ğŸ“Š Maximum tokens: {max_tokens:,} tokens ({max_tokens//1024}K)")
    if not LLM_CONFIG['databricks'].get('thinking_enabled', False):
        print("âš¡ Fast execution mode: Skip thinking process for rapid result generation")
elif LLM_CONFIG['provider'] == 'openai':
    print(f"ğŸ”— OpenAI model: {LLM_CONFIG['openai']['model']}")
elif LLM_CONFIG['provider'] == 'azure_openai':
    print(f"ğŸ”— Azure OpenAI deployment: {LLM_CONFIG['azure_openai']['deployment_name']}")
elif LLM_CONFIG['provider'] == 'anthropic':
    print(f"ğŸ”— Anthropic model: {LLM_CONFIG['anthropic']['model']}")

print()
print("ğŸ’¡ LLM provider switching examples:")
print('   LLM_CONFIG["provider"] = "openai"      # Switch to OpenAI GPT-4')
print('   LLM_CONFIG["provider"] = "anthropic"   # Switch to Anthropic Claude')
print('   LLM_CONFIG["provider"] = "azure_openai" # Switch to Azure OpenAI')
print()
print("ğŸ§  Databricks extended thinking mode configuration examples:")
print('   LLM_CONFIG["databricks"]["thinking_enabled"] = False  # Disable extended thinking mode (default, fast execution)')
print('   LLM_CONFIG["databricks"]["thinking_enabled"] = True   # Enable extended thinking mode (detailed analysis only)')
print('   LLM_CONFIG["databricks"]["thinking_budget_tokens"] = 65536  # Thinking token budget (64K)')
print('   LLM_CONFIG["databricks"]["max_tokens"] = 131072  # Maximum tokens (128K)')
print()

# Import necessary libraries
try:
    import requests
except ImportError:
    print("Warning: requests is not installed, some features may not work")
    requests = None
import os
try:
    from pyspark.sql import SparkSession
except ImportError:
    print("Warning: pyspark is not installed")
    SparkSession = None
    print("âœ… Spark Version: Not available")

# Safely retrieve Databricks Runtime information
try:
    if spark is not None:
        runtime_version = spark.conf.get('spark.databricks.clusterUsageTags.sparkVersion')
    print(f"âœ… Databricks Runtime: {runtime_version}")
except Exception:
    try:
        # Retrieve DBR information using alternative method
        dbr_version = spark.conf.get('spark.databricks.clusterUsageTags.clusterName', 'Unknown')
        print(f"âœ… Databricks Cluster: {dbr_version}")
    except Exception:
        print("âœ… Databricks Environment: Skipped configuration information retrieval")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“‚ SQL Profiler JSON File Loading Function
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - SQL profiler JSON file loading
# MAGIC - Automatic detection of DBFS/FileStore/local paths
# MAGIC - File size and data information display

# COMMAND ----------

def load_profiler_json(file_path: str) -> Dict[str, Any]:
    """
    Load SQL profiler JSON file
    
    Args:
        file_path: JSON file path (DBFS or local path)
        
    Returns:
        Dict: Parsed JSON data
    """
    try:
        # Handle DBFS paths appropriately
        if file_path.startswith('/dbfs/'):
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        elif file_path.startswith('dbfs:/'):
            # Convert dbfs: prefix to /dbfs/
            local_path = file_path.replace('dbfs:', '/dbfs')
            with open(local_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        elif file_path.startswith('/FileStore/'):
            # Convert FileStore path to /dbfs/FileStore/
            local_path = '/dbfs' + file_path
            with open(local_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        else:
            # Local file
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        
        print(f"âœ… Successfully loaded JSON file: {file_path}")
        print(f"ğŸ“Š Data size: {len(str(data)):,} characters")
        return data
    except Exception as e:
        print(f"âŒ File loading error: {str(e)}")
        return {}

print("âœ… Function definition completed: load_profiler_json")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“Š Performance Metrics Extraction Function
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - Metrics extraction from SQL profiler data
# MAGIC - Query basic information retrieval
# MAGIC - Overall/stage/node-level performance indicator calculation
# MAGIC - Spill detection and bottleneck indicator analysis

# COMMAND ----------

def detect_data_format(profiler_data: Dict[str, Any]) -> str:
    """
    Detect JSON data format
    """
    # SQL profiler format detection
    if 'graphs' in profiler_data and isinstance(profiler_data['graphs'], list):
        if len(profiler_data['graphs']) > 0:
            return 'sql_profiler'
    
    # SQL query summary format detection (test2.json format)
    if 'query' in profiler_data and 'planMetadatas' in profiler_data:
        query_data = profiler_data.get('query', {})
        if 'metrics' in query_data:
            return 'sql_query_summary'
    
    return 'unknown'

def extract_performance_metrics_from_query_summary(profiler_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extract basic metrics from Databricks SQL query summary format JSON
    (supports test2.json format)
    """
    try:
        query_data = profiler_data.get('query', {})
        metrics_data = query_data.get('metrics', {})
        
        if not metrics_data:
            print("âš ï¸ No metrics data found")
            return {}
        
        print(f"âœ… Detected SQL query summary format metrics")
        print(f"   - Execution time: {metrics_data.get('totalTimeMs', 0):,} ms")
        print(f"   - Data read: {metrics_data.get('readBytes', 0) / 1024 / 1024 / 1024:.2f} GB")
        print(f"   - Rows processed: {metrics_data.get('rowsReadCount', 0):,} rows")
        
        # Extract basic metrics
        overall_metrics = {
            'total_time_ms': metrics_data.get('totalTimeMs', 0),
            'execution_time_ms': metrics_data.get('executionTimeMs', 0),
            'compilation_time_ms': metrics_data.get('compilationTimeMs', 0),
            'read_bytes': metrics_data.get('readBytes', 0),
            'read_remote_bytes': metrics_data.get('readRemoteBytes', 0),
            'read_cache_bytes': metrics_data.get('readCacheBytes', 0),
            'spill_to_disk_bytes': metrics_data.get('spillToDiskBytes', 0),
            'rows_produced_count': metrics_data.get('rowsProducedCount', 0),
            'rows_read_count': metrics_data.get('rowsReadCount', 0),
            'read_files_count': metrics_data.get('readFilesCount', 0),
            'read_partitions_count': metrics_data.get('readPartitionsCount', 0),
            'photon_total_time_ms': metrics_data.get('photonTotalTimeMs', 0),
            'task_total_time_ms': metrics_data.get('taskTotalTimeMs', 0),
            'network_sent_bytes': metrics_data.get('networkSentBytes', 0),
            'photon_enabled': metrics_data.get('photonTotalTimeMs', 0) > 0,
            'photon_utilization_ratio': 0
        }
        
        # Calculate Photon utilization rate
        if overall_metrics['task_total_time_ms'] > 0:
            overall_metrics['photon_utilization_ratio'] = min(
                overall_metrics['photon_total_time_ms'] / overall_metrics['task_total_time_ms'], 1.0
            )
        
        # Calculate cache hit rate
        cache_hit_ratio = 0
        if overall_metrics['read_bytes'] > 0:
            cache_hit_ratio = overall_metrics['read_cache_bytes'] / overall_metrics['read_bytes']
        
        # Calculate bottleneck indicators
        bottleneck_indicators = {
            'spill_bytes': overall_metrics['spill_to_disk_bytes'],
            'has_spill': overall_metrics['spill_to_disk_bytes'] > 0,
            'cache_hit_ratio': cache_hit_ratio,
            'has_cache_miss': cache_hit_ratio < 0.8,
            'photon_efficiency': overall_metrics['photon_utilization_ratio'],
            'has_shuffle_bottleneck': False,  # Cannot determine due to lack of detailed information
            'remote_read_ratio': 0,
            'has_memory_pressure': overall_metrics['spill_to_disk_bytes'] > 0,
            'max_task_duration_ratio': 1.0,  # Unknown
            'has_data_skew': False  # Cannot determine due to lack of detailed information
        }
        
        # Calculate remote read ratio
        if overall_metrics['read_bytes'] > 0:
            bottleneck_indicators['remote_read_ratio'] = overall_metrics['read_remote_bytes'] / overall_metrics['read_bytes']
        
        # Extract query information
        query_info = {
            'query_id': query_data.get('id', ''),
            'query_text': query_data.get('queryText', '')[:300] + "..." if len(query_data.get('queryText', '')) > 300 else query_data.get('queryText', ''),
            'status': query_data.get('status', ''),
            'query_start_time': query_data.get('queryStartTimeMs', 0),
            'query_end_time': query_data.get('queryEndTimeMs', 0),
            'spark_ui_url': query_data.get('sparkUiUrl', ''),
            'endpoint_id': query_data.get('endpointId', ''),
            'user': query_data.get('user', {}).get('displayName', ''),
            'statement_type': query_data.get('statementType', ''),
            'plans_state': query_data.get('plansState', '')
        }
        
        # Calculate detailed performance insights (recalculated later with node_metrics)
        performance_insights = calculate_performance_insights_from_metrics(overall_metrics, None)
        
        # Pseudo node metrics (generated from summary information)
        summary_node = {
            'node_id': 'summary_node',
            'name': f'Query Execution Summary ({query_data.get("statementType", "SQL")})',
            'tag': 'QUERY_SUMMARY',
            'key_metrics': {
                'durationMs': overall_metrics['total_time_ms'],
                'rowsNum': overall_metrics['rows_read_count'],
                'peakMemoryBytes': 0,  # Unknown
                'throughputMBps': performance_insights['parallelization']['throughput_mb_per_second'],
                'dataSelectivity': performance_insights['data_efficiency']['data_selectivity'],
                'cacheHitRatio': performance_insights['cache_efficiency']['cache_hit_ratio']
            },
            'detailed_metrics': {
                'Total Time': {'value': overall_metrics['total_time_ms'], 'display_name': 'Total Time'},
                'Read Bytes': {'value': overall_metrics['read_bytes'], 'display_name': 'Read Bytes'},
                'Spill Bytes': {'value': overall_metrics['spill_to_disk_bytes'], 'display_name': 'Spill to Disk'},
                'Photon Time': {'value': overall_metrics['photon_total_time_ms'], 'display_name': 'Photon Time'},
                'Rows Read': {'value': overall_metrics['rows_read_count'], 'display_name': 'Rows Read Count'},
                'Cache Hit Ratio': {'value': performance_insights['cache_efficiency']['cache_hit_ratio'], 'display_name': 'Cache Hit Ratio'},
                'Filter Rate': {'value': performance_insights['data_efficiency']['data_selectivity'], 'display_name': 'Filter Rate'},
                'Throughput': {'value': performance_insights['parallelization']['throughput_mb_per_second'], 'display_name': 'Throughput (MB/s)'}
            },
            'graph_index': 0,
            'performance_insights': performance_insights
        }
        
        # Recalculate performance_insights with complete metrics
        complete_metrics = {
            'overall_metrics': overall_metrics,
            'node_metrics': [summary_node]
        }
        performance_insights = calculate_performance_insights_from_metrics(overall_metrics, complete_metrics)
        
        return {
            'data_format': 'sql_query_summary',
            'query_info': query_info,
            'overall_metrics': overall_metrics,
            'bottleneck_indicators': bottleneck_indicators,
            'node_metrics': [summary_node],
            'stage_metrics': [],  # No detailed stage information
            'liquid_clustering_analysis': {},  # To be added later
            'raw_profiler_data': profiler_data,
            'performance_insights': performance_insights,  # Add detailed performance insights
            'analysis_capabilities': [
                'Metrics-based bottleneck analysis (cache efficiency, filter rate, Photon efficiency)',
                'Resource usage analysis (spill, parallelization efficiency, throughput)',
                'Performance metrics calculation (file efficiency, partition efficiency)',
                'Potential bottleneck identification (metrics-based)'
            ],
            'analysis_limitations': [
                'Detailed execution plan information (nodes, edges) is not available',
                'Stage-level metrics are not available', 
                'BROADCAST analysis is limited to basic estimation only',
                'Liquid Clustering analysis provides general recommendations only',
                'Data skew detection is based on average-value estimation only',
                'Detailed query structure analysis is not performed (metrics-focused approach)'
            ]
        }
        
    except Exception as e:
        print(f"âš ï¸ Error extracting SQL query summary format metrics: {str(e)}")
        return {}

def extract_performance_metrics(profiler_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extract bottleneck analysis metrics from SQL profiler data (supports multiple formats)
    """
    # Detect data format
    data_format = detect_data_format(profiler_data)
    
    print(f"ğŸ” Detected data format: {data_format}")
    
    if data_format == 'sql_query_summary':
        print("ğŸ“Š Processing as Databricks SQL query summary format...")
        result = extract_performance_metrics_from_query_summary(profiler_data)
        if result:
            # Add Liquid Clustering analysis (with limitations)
            try:
                result["liquid_clustering_analysis"] = analyze_liquid_clustering_opportunities(profiler_data, result)
            except Exception as e:
                print(f"âš ï¸ Skipping Liquid Clustering analysis: {str(e)}")
                result["liquid_clustering_analysis"] = {}
        return result
    elif data_format == 'sql_profiler':
        print("ğŸ“Š Processing as SQL profiler detailed format...")
        # Continue processing existing SQL profiler format
        pass
    else:
        print(f"âš ï¸ Unknown data format: {data_format}")
        return {}
    
    # Processing existing SQL profiler format
    metrics = {
        "query_info": {},
        "overall_metrics": {},
        "stage_metrics": [],
        "node_metrics": [],
        "bottleneck_indicators": {},
        "liquid_clustering_analysis": {},
        "raw_profiler_data": profiler_data  # Save raw data for plan analysis
    }
    
    # Basic query information
    if 'query' in profiler_data:
        query = profiler_data['query']
        metrics["query_info"] = {
            "query_id": query.get('id', ''),
            "status": query.get('status', ''),
            "query_start_time": query.get('queryStartTimeMs', 0),
            "query_end_time": query.get('queryEndTimeMs', 0),
            "user": query.get('user', {}).get('displayName', ''),
            "query_text": query.get('queryText', '')[:300] + "..." if len(query.get('queryText', '')) > 300 else query.get('queryText', '')
        }
        
        # Overall metrics
        if 'metrics' in query:
            query_metrics = query['metrics']
            metrics["overall_metrics"] = {
                "total_time_ms": query_metrics.get('totalTimeMs', 0),
                "compilation_time_ms": query_metrics.get('compilationTimeMs', 0),
                "execution_time_ms": query_metrics.get('executionTimeMs', 0),
                "read_bytes": query_metrics.get('readBytes', 0),
                "read_remote_bytes": query_metrics.get('readRemoteBytes', 0),
                "read_cache_bytes": query_metrics.get('readCacheBytes', 0),
                "rows_produced_count": query_metrics.get('rowsProducedCount', 0),
                "rows_read_count": query_metrics.get('rowsReadCount', 0),
                "spill_to_disk_bytes": query_metrics.get('spillToDiskBytes', 0),
                "read_files_count": query_metrics.get('readFilesCount', 0),
                "task_total_time_ms": query_metrics.get('taskTotalTimeMs', 0),
                "photon_total_time_ms": query_metrics.get('photonTotalTimeMs', 0),
                # Photon usage analysis (Photon execution time / total task time)
                "photon_enabled": query_metrics.get('photonTotalTimeMs', 0) > 0,
                "photon_utilization_ratio": min(query_metrics.get('photonTotalTimeMs', 0) / max(query_metrics.get('taskTotalTimeMs', 1), 1), 1.0)
            }
    
    # Extract stage and node metrics from graph data (supports multiple graphs)
    if 'graphs' in profiler_data and profiler_data['graphs']:
        # Analyze all graphs
        for graph_index, graph in enumerate(profiler_data['graphs']):
            print(f"ğŸ” Analyzing graph {graph_index}...")
            
            # Stage data
            if 'stageData' in graph:
                for stage in graph['stageData']:
                    stage_metric = {
                        "stage_id": stage.get('stageId', ''),
                        "status": stage.get('status', ''),
                        "duration_ms": stage.get('keyMetrics', {}).get('durationMs', 0),
                        "num_tasks": stage.get('numTasks', 0),
                        "num_failed_tasks": stage.get('numFailedTasks', 0),
                        "num_complete_tasks": stage.get('numCompleteTasks', 0),
                        "start_time_ms": stage.get('startTimeMs', 0),
                        "end_time_ms": stage.get('endTimeMs', 0),
                        "graph_index": graph_index  # Record which graph this originates from
                    }
                    metrics["stage_metrics"].append(stage_metric)
            
            # Node data (important ones only)
            if 'nodes' in graph:
                for node in graph['nodes']:
                    if not node.get('hidden', False):
                        # Use keyMetrics as-is (durationMs is already in milliseconds)
                        key_metrics = node.get('keyMetrics', {})
                        
                        node_metric = {
                            "node_id": node.get('id', ''),
                            "name": node.get('name', ''),
                            "tag": node.get('tag', ''),
                            "key_metrics": key_metrics,  # Unit-converted key_metrics
                            "metrics": node.get('metrics', []),  # Retain original metrics array
                            "metadata": node.get('metadata', []),  # Add metadata
                            "graph_index": graph_index  # Record which graph this originates from
                        }
                        
                        # Extract only important metrics in detail (added spill-related keywords, label support)
                        detailed_metrics = {}
                        for metric in node.get('metrics', []):
                            metric_key = metric.get('key', '')
                            metric_label = metric.get('label', '')
                            
                            # Check keywords in both key and label
                            key_keywords = ['TIME', 'MEMORY', 'ROWS', 'BYTES', 'DURATION', 'PEAK', 'CUMULATIVE', 'EXCLUSIVE', 
                                           'SPILL', 'DISK', 'PRESSURE', 'SINK']
                            
                            # Extract when metric_key or metric_label contains important keywords
                            is_important_metric = (
                                any(keyword in metric_key.upper() for keyword in key_keywords) or
                                any(keyword in metric_label.upper() for keyword in key_keywords)
                            )
                            
                            if is_important_metric:
                                                                  # Use label as metric name if valid, otherwise use key
                                metric_name = metric_label if metric_label and metric_label != 'UNKNOWN_KEY' else metric_key
                                detailed_metrics[metric_name] = {
                                    'value': metric.get('value', 0),
                                    'label': metric_label,
                                    'type': metric.get('metricType', ''),
                                    'original_key': metric_key,  # Save original key name
                                    'display_name': metric_name  # Display name
                                }
                        node_metric['detailed_metrics'] = detailed_metrics
                        metrics["node_metrics"].append(node_metric)
    
    # Calculate bottleneck indicators
    metrics["bottleneck_indicators"] = calculate_bottleneck_indicators(metrics)
    
    # Liquid Clustering analysis
    metrics["liquid_clustering_analysis"] = analyze_liquid_clustering_opportunities(profiler_data, metrics)
    
    return metrics

print("âœ… Function definition completed: extract_performance_metrics")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ·ï¸ Node Name Analysis & Enhancement Function
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - Concretization of generic node names (Whole Stage Codegen, etc.)
# MAGIC - Related node search and optimal processing name selection
# MAGIC - Addition of Photon information and table information
# MAGIC - Semantic improvement of processing names

# COMMAND ----------

def get_meaningful_node_name(node: Dict[str, Any], extracted_metrics: Dict[str, Any]) -> str:
    """
    Function to get more meaningful node names
    Convert generic names (such as Whole Stage Codegen) to specific process names
    """
    original_name = node.get('name', '')
    node_id = node.get('node_id', node.get('id', ''))
    node_tag = node.get('tag', '')
    
    # Get detailed information from metadata
    metadata = node.get('metadata', [])
    metadata_info = {}
    for meta in metadata:
        key = meta.get('key', '')
        value = meta.get('value', '')
        label = meta.get('label', '')
        if value:
            metadata_info[key] = value
    
    # 1. Replace generic names with specific names
    if 'whole stage codegen' in original_name.lower():
        # Heuristic to infer more specific process names
        
        # Infer relevance based on node ID (adjacent IDs)
        node_id_num = None
        try:
            node_id_num = int(node_id) if node_id else None
        except:
            pass
        
        if node_id_num:
            # Look for specific processes with nearby IDs in the same file
            all_nodes = extracted_metrics.get('node_metrics', [])
            nearby_specific_nodes = []
            
            for other_node in all_nodes:
                other_id = other_node.get('node_id', '')
                other_name = other_node.get('name', '')
                
                try:
                    other_id_num = int(other_id) if other_id else None
                    if other_id_num and abs(other_id_num - node_id_num) <= 10:  # Within 10 nearby
                        if is_specific_process_name(other_name):
                            nearby_specific_nodes.append(other_name)
                except:
                    continue
            
            # Select the most specific process name
            if nearby_specific_nodes:
                specific_name = get_most_specific_process_name_from_list(nearby_specific_nodes)
                if specific_name and specific_name != original_name:
                    return f"{specific_name} (Whole Stage Codegen)"
        
        # Fallback: Extract more specific information from tag
        if 'CODEGEN' in node_tag:
            # Check child tag information from metadata
            child_tag = metadata_info.get('CHILD_TAG', '')
            if child_tag and child_tag != 'Child':
                return f"Whole Stage Codegen ({child_tag})"
    
    # 2. Reflect more specific tag information in node name
    tag_to_name_mapping = {
        'PHOTON_SHUFFLE_EXCHANGE_SINK_EXEC': 'Photon Shuffle Exchange',
        'PHOTON_GROUPING_AGG_EXEC': 'Photon Grouping Aggregate', 
        'UNKNOWN_DATA_SOURCE_SCAN_EXEC': 'Data Source Scan',
        'HASH_AGGREGATE_EXEC': 'Hash Aggregate',
        'WHOLE_STAGE_CODEGEN_EXEC': 'Whole Stage Codegen'
    }
    
    if node_tag in tag_to_name_mapping:
        mapped_name = tag_to_name_mapping[node_tag]
        if mapped_name != original_name and mapped_name != 'Whole Stage Codegen':
            # Use tag if it's more specific
            enhanced_name = mapped_name
        else:
            enhanced_name = original_name
    else:
        enhanced_name = original_name
    
    # 3. Add processing details from metadata
    
    # Add database and table information (enhanced version)
    table_name = None
    
    # Extract table name from multiple metadata keys (prioritize full path)
    for key_candidate in ['SCAN_TABLE', 'SCAN_IDENTIFIER', 'TABLE_NAME', 'RELATION', 'SCAN_RELATION']:
        if key_candidate in metadata_info:
            extracted_table = metadata_info[key_candidate]
            # Use as-is for full path (catalog.schema.table)
            if isinstance(extracted_table, str) and extracted_table.count('.') >= 2:
                table_name = extracted_table
                break
            elif isinstance(extracted_table, str) and extracted_table.count('.') == 1:
                # Use as-is for schema.table format as well
                table_name = extracted_table
                break
            elif not table_name:  # Set only if table name has not been found yet
                table_name = extracted_table
    
    # If table name cannot be extracted from metadata, infer from node name
    if not table_name and ('scan' in enhanced_name.lower() or 'data source' in enhanced_name.lower()):
        # Infer table name from node name
        import re
        
        # Format like "Scan tpcds.tpcds_sf1000_delta_lc.customer"
        table_patterns = [
            r'[Ss]can\s+([a-zA-Z_][a-zA-Z0-9_.]*[a-zA-Z0-9_])',
            r'[Tt]able\s+([a-zA-Z_][a-zA-Z0-9_.]*[a-zA-Z0-9_])',
            r'([a-zA-Z_][a-zA-Z0-9_]*\.)+([a-zA-Z_][a-zA-Z0-9_]*)',
        ]
        
        for pattern in table_patterns:
            match = re.search(pattern, original_name)
            if match:
                if '.' in match.group(0):
                    # Use full path for full table name (catalog.schema.table)
                    table_name = match.group(0)
                else:
                    table_name = match.group(1) if match.lastindex and match.lastindex >= 1 else match.group(0)
                break
    
            # Search for table name from metadata values field as well
    if not table_name:
        for meta in metadata:
            values = meta.get('values', [])
            if values:
                for value in values:
                    if isinstance(value, str) and '.' in value and len(value.split('.')) >= 2:
                        # For "catalog.schema.table" format
                        parts = value.split('.')
                        if len(parts) >= 2 and not any(part.isdigit() for part in parts[-2:]):
                            # Use full path (catalog.schema.table)
                            if len(parts) >= 3:
                                table_name = '.'.join(parts)  # Full path
                            else:
                                table_name = value  # Use as is
                            break
                if table_name:
                    break
    
    # Display table name for Data Source Scan
    if table_name and ('scan' in enhanced_name.lower() or 'data source' in enhanced_name.lower()):
        # Relax limits for full path display (up to 60 characters)
        if len(table_name) > 60:
            # Abbreviate middle part for catalog.schema.table format
            parts = table_name.split('.')
            if len(parts) >= 3:
                table_name = f"{parts[0]}.*.{parts[-1]}"
            else:
                table_name = table_name[:57] + "..."
        enhanced_name = f"Data Source Scan ({table_name})"
    elif 'scan' in enhanced_name.lower() and 'data source' in enhanced_name.lower():
        # Use clearer name even when table name is not found
        enhanced_name = "Data Source Scan"
    
    # Add Photon information
    if 'IS_PHOTON' in metadata_info and metadata_info['IS_PHOTON'] == 'true':
        if not enhanced_name.startswith('Photon'):
            enhanced_name = f"Photon {enhanced_name}"
    
    return enhanced_name

def find_related_specific_nodes(target_node_id: str, nodes: list, edges: list) -> list:
    """Search for specific processing nodes related to the specified node"""
    
    # Identify related nodes from edges
    related_node_ids = set()
    
    # Directly connected nodes
    for edge in edges:
        from_id = edge.get('fromId', '')
        to_id = edge.get('toId', '')
        
        if from_id == target_node_id:
            related_node_ids.add(to_id)
        elif to_id == target_node_id:
            related_node_ids.add(from_id)
    
    # Get details of related nodes
    related_nodes = []
    for node in nodes:
        node_id = node.get('id', '')
        if node_id in related_node_ids:
            node_name = node.get('name', '')
            # Select only nodes with specific process names
            if is_specific_process_name(node_name):
                related_nodes.append(node)
    
    return related_nodes

def is_specific_process_name(name: str) -> bool:
    """Determine if it's a specific processing name"""
    specific_keywords = [
        'columnar to row', 'row to columnar', 'filter', 'project', 'join',
        'aggregate', 'sort', 'exchange', 'broadcast', 'scan', 'union'
    ]
    
    generic_keywords = [
        'whole stage codegen', 'stage', 'query', 'result'
    ]
    
    name_lower = name.lower()
    
            # When containing specific keywords
    for keyword in specific_keywords:
        if keyword in name_lower:
            return True
    
            # Exclude cases with only generic keywords
    for keyword in generic_keywords:
        if keyword in name_lower and len(name_lower.split()) <= 3:
            return False
    
    return True

def get_most_specific_process_name(nodes: list) -> str:
    """Select the most specific processing name"""
    if not nodes:
        return ""
    
    # Priority: More specific and meaningful process names
    priority_keywords = [
        'columnar to row', 'row to columnar', 'filter', 'project',
        'hash join', 'broadcast join', 'sort merge join',
        'hash aggregate', 'sort aggregate', 'grouping aggregate'
    ]
    
    for keyword in priority_keywords:
        for node in nodes:
            node_name = node.get('name', '').lower()
            if keyword in node_name:
                return node.get('name', '')
    
    # Fallback: First specific node name
    for node in nodes:
        node_name = node.get('name', '')
        if is_specific_process_name(node_name):
            return node_name
    
    return ""

def get_most_specific_process_name_from_list(node_names: list) -> str:
    """Select the most specific processing name from a list of node names"""
    if not node_names:
        return ""
    
    # Priority: More specific and meaningful process names
    priority_keywords = [
        'columnar to row', 'row to columnar', 'filter', 'project',
        'hash join', 'broadcast join', 'sort merge join',
        'hash aggregate', 'sort aggregate', 'grouping aggregate'
    ]
    
    for keyword in priority_keywords:
        for name in node_names:
            if keyword in name.lower():
                return name
    
    # Fallback: First specific node name
    for name in node_names:
        if is_specific_process_name(name):
            return name
    
    return ""

def extract_shuffle_attributes(node: Dict[str, Any]) -> list:
    """
    Extract SHUFFLE_ATTRIBUTES from Shuffle node
    
    Args:
        node: Node information
        
    Returns:
        list: Detected Shuffle attributes
    """
    shuffle_attributes = []
    
    # Search for SHUFFLE_ATTRIBUTES from metadata
    metadata = node.get('metadata', [])
    if isinstance(metadata, list):
        for item in metadata:
            if isinstance(item, dict):
                item_key = item.get('key', '')
                item_label = item.get('label', '')
                item_values = item.get('values', [])
                
                # Check both key and label
                if (item_key == 'SHUFFLE_ATTRIBUTES' or 
                    item_label == 'Shuffle attributes'):
                    if isinstance(item_values, list):
                        shuffle_attributes.extend(item_values)
    
    # Search from raw_metrics as well (also check label)
    raw_metrics = node.get('metrics', [])
    if isinstance(raw_metrics, list):
        for metric in raw_metrics:
            if isinstance(metric, dict):
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_values = metric.get('values', [])
                
                if (metric_key == 'SHUFFLE_ATTRIBUTES' or 
                    metric_label == 'Shuffle attributes'):
                    if isinstance(metric_values, list):
                        shuffle_attributes.extend(metric_values)
    
    # Search from detailed_metrics as well
    detailed_metrics = node.get('detailed_metrics', {})
    if isinstance(detailed_metrics, dict):
        for key, info in detailed_metrics.items():
            if (key == 'SHUFFLE_ATTRIBUTES' or 
                (isinstance(info, dict) and info.get('label') == 'Shuffle attributes')):
                values = info.get('values', []) if isinstance(info, dict) else []
                if isinstance(values, list):
                    shuffle_attributes.extend(values)
    
    # Remove duplicates
    return list(set(shuffle_attributes))

def extract_cluster_attributes(node: Dict[str, Any]) -> list:
    """
    Extract clustering keys (SCAN_CLUSTERS) from scan node
    
    Args:
        node: Node information
        
    Returns:
        list: Detected clustering keys
    """
    cluster_attributes = []
    
    # Search for SCAN_CLUSTERS from metadata
    metadata = node.get('metadata', [])
    if isinstance(metadata, list):
        for item in metadata:
            if isinstance(item, dict):
                item_key = item.get('key', '')
                item_label = item.get('label', '')
                item_values = item.get('values', [])
                
                # Check both key and label
                if (item_key == 'SCAN_CLUSTERS' or 
                    item_label == 'Cluster attributes'):
                    if isinstance(item_values, list):
                        cluster_attributes.extend(item_values)
    
    # Search from raw_metrics as well (also check label)
    raw_metrics = node.get('metrics', [])
    if isinstance(raw_metrics, list):
        for metric in raw_metrics:
            if isinstance(metric, dict):
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_values = metric.get('values', [])
                
                if (metric_key == 'SCAN_CLUSTERS' or 
                    metric_label == 'Cluster attributes'):
                    if isinstance(metric_values, list):
                        cluster_attributes.extend(metric_values)
    
    # Search from detailed_metrics as well
    detailed_metrics = node.get('detailed_metrics', {})
    if isinstance(detailed_metrics, dict):
        for key, info in detailed_metrics.items():
            if (key == 'SCAN_CLUSTERS' or 
                (isinstance(info, dict) and info.get('label') == 'Cluster attributes')):
                values = info.get('values', []) if isinstance(info, dict) else []
                if isinstance(values, list):
                    cluster_attributes.extend(values)
    
    # Remove duplicates
    return list(set(cluster_attributes))

def extract_parallelism_metrics(node: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extract multiple Tasks total metrics and AQEShuffleRead metrics from node
    
    ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œãªã©ã§ã¯ä»¥ä¸‹ã®è¤‡æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå­˜åœ¨ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼š
    - Tasks total
    - Sink - Tasks total
    - Source - Tasks total
    - AQEShuffleRead - Number of partitions
    - AQEShuffleRead - Partition data size
    
    Args:
        node: Node information
        
    Returns:
        dict: æ¤œå‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹
            {
                "tasks_total": å€¤,
                "sink_tasks_total": å€¤,
                "source_tasks_total": å€¤,
                "all_tasks_metrics": [{"name": "Tasks total", "value": å€¤}, ...],
                "aqe_shuffle_partitions": å€¤,
                "aqe_shuffle_data_size": å€¤,
                "aqe_shuffle_avg_partition_size": å€¤,
                "aqe_shuffle_skew_warning": bool,
                "aqe_shuffle_metrics": [{"name": "AQE...", "value": å€¤}, ...]
            }
    """
    parallelism_metrics = {
        "tasks_total": 0,
        "sink_tasks_total": 0,
        "source_tasks_total": 0,
        "all_tasks_metrics": [],
        "aqe_shuffle_partitions": 0,
        "aqe_shuffle_data_size": 0,
        "aqe_shuffle_avg_partition_size": 0,
        "aqe_shuffle_skew_warning": False,
        "aqe_detected_and_handled": False,
        "aqe_shuffle_metrics": []
    }
    
    # Target Tasks total metric name patterns
    tasks_total_patterns = [
        "Tasks total",
        "Sink - Tasks total",
        "Source - Tasks total"
    ]
    
    # Target AQEShuffleRead metric name patterns
    aqe_shuffle_patterns = [
        "AQEShuffleRead - Number of partitions",
        "AQEShuffleRead - Partition data size"
    ]
    
    # 1. Search from detailed_metrics
    detailed_metrics = node.get('detailed_metrics', {})
    for metric_key, metric_info in detailed_metrics.items():
        metric_value = metric_info.get('value', 0)
        metric_label = metric_info.get('label', '')
        
        # å„Tasks totalãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        for pattern in tasks_total_patterns:
            if metric_key == pattern or metric_label == pattern:
                # ç‰¹å®šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                if pattern == "Tasks total":
                    parallelism_metrics["tasks_total"] = metric_value
                elif pattern == "Sink - Tasks total":
                    parallelism_metrics["sink_tasks_total"] = metric_value
                elif pattern == "Source - Tasks total":
                    parallelism_metrics["source_tasks_total"] = metric_value
                
                # Add to all metrics list
                parallelism_metrics["all_tasks_metrics"].append({
                    "name": pattern,
                    "value": metric_value
                })
        
        # AQEShuffleReadãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
        for pattern in aqe_shuffle_patterns:
            if metric_key == pattern or metric_label == pattern:
                # ç‰¹å®šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                if pattern == "AQEShuffleRead - Number of partitions":
                    parallelism_metrics["aqe_shuffle_partitions"] = metric_value
                elif pattern == "AQEShuffleRead - Partition data size":
                    parallelism_metrics["aqe_shuffle_data_size"] = metric_value
                
                # Add to all metrics list
                parallelism_metrics["aqe_shuffle_metrics"].append({
                    "name": pattern,
                    "value": metric_value
                })
    
    # 2. raw_metricsã‹ã‚‰æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    raw_metrics = node.get('metrics', [])
    if isinstance(raw_metrics, list):
        for metric in raw_metrics:
            if isinstance(metric, dict):
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_value = metric.get('value', 0)
                
                # å„Tasks totalãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
                for pattern in tasks_total_patterns:
                    if metric_key == pattern or metric_label == pattern:
                        # Skip if already found in detailed_metrics
                        if not any(m["name"] == pattern for m in parallelism_metrics["all_tasks_metrics"]):
                            # ç‰¹å®šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                            if pattern == "Tasks total":
                                parallelism_metrics["tasks_total"] = metric_value
                            elif pattern == "Sink - Tasks total":
                                parallelism_metrics["sink_tasks_total"] = metric_value
                            elif pattern == "Source - Tasks total":
                                parallelism_metrics["source_tasks_total"] = metric_value
                            
                            # Add to all metrics list
                            parallelism_metrics["all_tasks_metrics"].append({
                                "name": pattern,
                                "value": metric_value
                            })
                
                # AQEShuffleReadãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
                for pattern in aqe_shuffle_patterns:
                    if metric_key == pattern or metric_label == pattern:
                        # Skip if already found in detailed_metrics
                        if not any(m["name"] == pattern for m in parallelism_metrics["aqe_shuffle_metrics"]):
                            # ç‰¹å®šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                            if pattern == "AQEShuffleRead - Number of partitions":
                                parallelism_metrics["aqe_shuffle_partitions"] = metric_value
                            elif pattern == "AQEShuffleRead - Partition data size":
                                parallelism_metrics["aqe_shuffle_data_size"] = metric_value
                            
                            # Add to all metrics list
                            parallelism_metrics["aqe_shuffle_metrics"].append({
                                "name": pattern,
                                "value": metric_value
                            })
    
    # 3. key_metricsã‹ã‚‰æ¤œç´¢ï¼ˆæœ€å¾Œã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    key_metrics = node.get('key_metrics', {})
    if isinstance(key_metrics, dict):
        for metric_key, metric_value in key_metrics.items():
            for pattern in tasks_total_patterns:
                if metric_key == pattern:
                    # Skip if already found
                    if not any(m["name"] == pattern for m in parallelism_metrics["all_tasks_metrics"]):
                        # ç‰¹å®šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                        if pattern == "Tasks total":
                            parallelism_metrics["tasks_total"] = metric_value
                        elif pattern == "Sink - Tasks total":
                            parallelism_metrics["sink_tasks_total"] = metric_value
                        elif pattern == "Source - Tasks total":
                            parallelism_metrics["source_tasks_total"] = metric_value
                        
                        # Add to all metrics list
                        parallelism_metrics["all_tasks_metrics"].append({
                            "name": pattern,
                            "value": metric_value
                        })
            
            # AQEShuffleReadãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
            for pattern in aqe_shuffle_patterns:
                if metric_key == pattern:
                    # Skip if already found
                    if not any(m["name"] == pattern for m in parallelism_metrics["aqe_shuffle_metrics"]):
                        # ç‰¹å®šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                        if pattern == "AQEShuffleRead - Number of partitions":
                            parallelism_metrics["aqe_shuffle_partitions"] = metric_value
                        elif pattern == "AQEShuffleRead - Partition data size":
                            parallelism_metrics["aqe_shuffle_data_size"] = metric_value
                        
                        # Add to all metrics list
                        parallelism_metrics["aqe_shuffle_metrics"].append({
                            "name": pattern,
                            "value": metric_value
                        })
    
    # Calculate average partition size and set warnings
    if parallelism_metrics["aqe_shuffle_partitions"] > 0 and parallelism_metrics["aqe_shuffle_data_size"] > 0:
        avg_partition_size = parallelism_metrics["aqe_shuffle_data_size"] / parallelism_metrics["aqe_shuffle_partitions"]
        parallelism_metrics["aqe_shuffle_avg_partition_size"] = avg_partition_size
        
        # 512MB = 512 * 1024 * 1024 bytes
        threshold_512mb = 512 * 1024 * 1024
        if avg_partition_size >= threshold_512mb:
            parallelism_metrics["aqe_shuffle_skew_warning"] = True
        else:
            # When AQEShuffleRead metrics exist and average partition size is less than 512MB,
            # Determine that AQE has detected and handled skew
            parallelism_metrics["aqe_detected_and_handled"] = True
    
    return parallelism_metrics

def calculate_filter_rate(node: Dict[str, Any]) -> Dict[str, Any]:
    """
    ãƒãƒ¼ãƒ‰ã‹ã‚‰Size of files prunedã¨Size of files readãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ç‡ã‚’è¨ˆç®—
    
    Args:
        node: ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿
        
    Returns:
        Dict: ãƒ•ã‚£ãƒ«ã‚¿ç‡è¨ˆç®—çµæœ
    """
    import os
    debug_mode = os.environ.get('DEBUG_FILTER_ANALYSIS', 'false').lower() == 'true'
    
    filter_rate = None
    files_pruned_bytes = 0
    files_read_bytes = 0
    debug_info = []
    
    # Target metric names for search (prioritizing patterns confirmed in actual JSON files)
    pruned_metrics = [
        "Size of files pruned",  # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªæ¸ˆã¿
        "Size of files pruned before dynamic pruning",  # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªæ¸ˆã¿
        "Pruned files size", 
        "Files pruned size",
        "Num pruned files size"
    ]
    
    read_metrics = [
        "Size of files read",  # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªæ¸ˆã¿
        "Files read size",
        "Read files size",
        "Num files read size"
    ]
    
    # detailed_metricsã‹ã‚‰æ¤œç´¢
    detailed_metrics = node.get('detailed_metrics', {})
    if debug_mode:
        debug_info.append(f"detailed_metrics keys: {list(detailed_metrics.keys())[:5]}")
    
    for metric_key, metric_info in detailed_metrics.items():
        metric_label = metric_info.get('label', '')
        metric_value = metric_info.get('value', 0)
        
        # Prunedé–¢é€£ï¼ˆlabelã‚’å„ªå…ˆçš„ã«ãƒã‚§ãƒƒã‚¯ï¼‰
        for target in pruned_metrics:
            if target in metric_label and metric_value > 0:
                files_pruned_bytes += metric_value  # è¤‡æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒã‚ã‚‹å ´åˆã¯åˆè¨ˆ
                if debug_mode:
                    debug_info.append(f"Found pruned metric: {metric_label} = {metric_value}")
                break
        
        # Readé–¢é€£ï¼ˆlabelã‚’å„ªå…ˆçš„ã«ãƒã‚§ãƒƒã‚¯ï¼‰
        for target in read_metrics:
            if target in metric_label and metric_value > 0:
                files_read_bytes += metric_value  # è¤‡æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒã‚ã‚‹å ´åˆã¯åˆè¨ˆ
                if debug_mode:
                    debug_info.append(f"Found read metric: {metric_label} = {metric_value}")
                break
    
    # raw_metricsã‹ã‚‰æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    if files_pruned_bytes == 0 or files_read_bytes == 0:
        raw_metrics = node.get('metrics', [])
        if debug_mode:
            debug_info.append(f"Searching in {len(raw_metrics)} raw metrics")
        
        for metric in raw_metrics:
            metric_label = metric.get('label', '')
            metric_value = metric.get('value', 0)
            
            # Prunedé–¢é€£ï¼ˆlabelã‚’å„ªå…ˆçš„ã«ãƒã‚§ãƒƒã‚¯ï¼‰
            for target in pruned_metrics:
                if target in metric_label and metric_value > 0:
                    files_pruned_bytes += metric_value  # è¤‡æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒã‚ã‚‹å ´åˆã¯åˆè¨ˆ
                    if debug_mode:
                        debug_info.append(f"Found pruned metric in raw: {metric_label} = {metric_value}")
                    break
            
            # Readé–¢é€£ï¼ˆlabelã‚’å„ªå…ˆçš„ã«ãƒã‚§ãƒƒã‚¯ï¼‰
            for target in read_metrics:
                if target in metric_label and metric_value > 0:
                    files_read_bytes += metric_value  # è¤‡æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒã‚ã‚‹å ´åˆã¯åˆè¨ˆ
                    if debug_mode:
                        debug_info.append(f"Found read metric in raw: {metric_label} = {metric_value}")
                    break
    
    # ãƒ•ã‚£ãƒ«ã‚¿ç‡è¨ˆç®—ï¼ˆæ­£ã—ã„å¼: ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ï¼‰
    total_available_bytes = files_read_bytes + files_pruned_bytes
    if total_available_bytes > 0:
        filter_rate = files_pruned_bytes / total_available_bytes
    else:
        filter_rate = 0.0
    
    result = {
        "filter_rate": filter_rate,
        "files_pruned_bytes": files_pruned_bytes,
        "files_read_bytes": files_read_bytes,
        "has_filter_metrics": (files_read_bytes > 0 or files_pruned_bytes > 0)
    }
    
    if debug_mode:
        result["debug_info"] = debug_info
    
    return result

def format_filter_rate_display(filter_result: Dict[str, Any]) -> str:
    """
    ãƒ•ã‚£ãƒ«ã‚¿ç‡è¨ˆç®—çµæœã‚’è¡¨ç¤ºç”¨æ–‡å­—åˆ—ã«å¤‰æ›
    
    Args:
        filter_result: calculate_filter_rate()ã®çµæœ
        
    Returns:
        str: è¡¨ç¤ºç”¨æ–‡å­—åˆ—
    """
    if not filter_result["has_filter_metrics"] or filter_result["filter_rate"] is None:
        return None
    
    filter_rate = filter_result["filter_rate"]
    files_read_gb = filter_result["files_read_bytes"] / (1024 * 1024 * 1024)
    files_pruned_gb = filter_result["files_pruned_bytes"] / (1024 * 1024 * 1024)
    
    return f"ğŸ“‚ Filter rate: {filter_rate:.1%} (read: {files_read_gb:.2f}GB, pruned: {files_pruned_gb:.2f}GB)"

def extract_detailed_bottleneck_analysis(extracted_metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    Perform Cell 33-style detailed bottleneck analysis and return structured data
    
    ğŸš¨ Important: Prevention of percentage calculation degradation
    - Using the sum of parallel execution node times as total time is strictly prohibited
    - Prioritize using overall_metrics.total_time_ms (wall-clock time)
    - Use maximum node time during fallback (not sum)
    
    Args:
        extracted_metrics: Extracted metrics
        
    Returns:
        dict: Detailed bottleneck analysis results
    """
    detailed_analysis = {
        "top_bottleneck_nodes": [],
        "shuffle_optimization_hints": [],
        "spill_analysis": {
            "total_spill_gb": 0,
            "spill_nodes": [],
            "critical_spill_nodes": []
        },
        "skew_analysis": {
            "skewed_nodes": [],
            "total_skewed_partitions": 0
        },
        "performance_recommendations": []
    }
    
    # ãƒãƒ¼ãƒ‰ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆï¼ˆTOP10ï¼‰
    sorted_nodes = sorted(extracted_metrics.get('node_metrics', []), 
                         key=lambda x: x.get('key_metrics', {}).get('durationMs', 0), 
                         reverse=True)
    
    # æœ€å¤§10å€‹ã®ãƒãƒ¼ãƒ‰ã‚’å‡¦ç†
    final_sorted_nodes = sorted_nodes[:10]
    
    # ğŸš¨ é‡è¦: æ­£ã—ã„å…¨ä½“æ™‚é–“ã®è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
    # 1. overall_metricsã‹ã‚‰å…¨ä½“å®Ÿè¡Œæ™‚é–“ã‚’å–å¾—ï¼ˆwall-clock timeï¼‰
    overall_metrics = extracted_metrics.get('overall_metrics', {})
    total_duration = overall_metrics.get('total_time_ms', 0)
    
    # ğŸš¨ ä¸¦åˆ—å®Ÿè¡Œå•é¡Œã®ä¿®æ­£: task_total_time_msã‚’å„ªå…ˆä½¿ç”¨
    task_total_time_ms = overall_metrics.get('task_total_time_ms', 0)
    
    if task_total_time_ms > 0:
        total_duration = task_total_time_ms
    elif total_duration <= 0:
        # execution_time_msã‚’æ¬¡ã®å„ªå…ˆåº¦ã§ä½¿ç”¨
        execution_time_ms = overall_metrics.get('execution_time_ms', 0)
        if execution_time_ms > 0:
            total_duration = execution_time_ms
        else:
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            max_node_time = max([node.get('key_metrics', {}).get('durationMs', 0) for node in sorted_nodes], default=1)
            total_duration = int(max_node_time * 1.2)
    
    for i, node in enumerate(final_sorted_nodes):
        duration_ms = node.get('key_metrics', {}).get('durationMs', 0)
        memory_mb = node.get('key_metrics', {}).get('peakMemoryBytes', 0) / 1024 / 1024
        rows_num = node.get('key_metrics', {}).get('rowsNum', 0)
        
        # ä¸¦åˆ—åº¦æƒ…å ±ã®å–å¾—ï¼ˆä¿®æ­£ç‰ˆ: è¤‡æ•°ã®Tasks totalãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ï¼‰
        parallelism_data = extract_parallelism_metrics(node)
        
        # å¾“æ¥ã®å˜ä¸€å€¤ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
        num_tasks = parallelism_data.get('tasks_total', 0)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Sink - Tasks totalã¾ãŸã¯Source - Tasks totalãŒã‚ã‚‹å ´åˆ
        if num_tasks == 0:
            if parallelism_data.get('sink_tasks_total', 0) > 0:
                num_tasks = parallelism_data.get('sink_tasks_total', 0)
            elif parallelism_data.get('source_tasks_total', 0) > 0:
                num_tasks = parallelism_data.get('source_tasks_total', 0)
        
        # ã‚¹ãƒ”ãƒ«æ¤œå‡ºï¼ˆã‚»ãƒ«33ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        spill_detected = False
        spill_bytes = 0
        exact_spill_metrics = [
            "Num bytes spilled to disk due to memory pressure",
            "Sink - Num bytes spilled to disk due to memory pressure",
            "Sink/Num bytes spilled to disk due to memory pressure"
        ]
        
        # detailed_metricsã‹ã‚‰æ¤œç´¢
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            metric_value = metric_info.get('value', 0)
            metric_label = metric_info.get('label', '')
            
            if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                spill_detected = True
                spill_bytes = max(spill_bytes, metric_value)
                break
        
        # raw_metricsã‹ã‚‰æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not spill_detected:
            raw_metrics = node.get('metrics', [])
            for metric in raw_metrics:
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_value = metric.get('value', 0)
                
                if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                    spill_detected = True
                    spill_bytes = max(spill_bytes, metric_value)
                    break
        
        # ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºï¼ˆAQEãƒ™ãƒ¼ã‚¹ï¼‰
        skew_detected = False
        skewed_partitions = 0
        target_skew_metric = "AQEShuffleRead - Number of skewed partitions"
        
        for metric_key, metric_info in detailed_metrics.items():
            if metric_key == target_skew_metric:
                try:
                    skewed_partitions = int(metric_info.get('value', 0))
                    if skewed_partitions > 0:
                        skew_detected = True
                    break
                except (ValueError, TypeError):
                    continue
        
        node_name = get_meaningful_node_name(node, extracted_metrics)
        # ğŸš¨ é‡è¦: æ­£ã—ã„ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
        # wall-clock timeã«å¯¾ã™ã‚‹å„ãƒãƒ¼ãƒ‰ã®å®Ÿè¡Œæ™‚é–“ã®å‰²åˆ
        time_percentage = min((duration_ms / max(total_duration, 1)) * 100, 100.0)
        
        # ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®šï¼ˆAQEã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã¨AQEShuffleReadå¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã®ä¸¡æ–¹ã‚’è€ƒæ…®ï¼‰
        aqe_shuffle_skew_warning = parallelism_data.get('aqe_shuffle_skew_warning', False)
        combined_skew_detected = skew_detected or aqe_shuffle_skew_warning
        
        # ãƒãƒ¼ãƒ‰åˆ†æçµæœã‚’æ§‹é€ åŒ–
        node_analysis = {
            "rank": i + 1,
            "node_id": node.get('node_id', node.get('id', 'N/A')),
            "node_name": node_name,
            "duration_ms": duration_ms,
            "time_percentage": time_percentage,
            "memory_mb": memory_mb,
            "rows_processed": rows_num,
            "num_tasks": num_tasks,
            "parallelism_data": parallelism_data,  # è¤‡æ•°ã®Tasks totalãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±ã‚’è¿½åŠ 
            "spill_detected": spill_detected,
            "spill_bytes": spill_bytes,
            "spill_gb": spill_bytes / 1024 / 1024 / 1024 if spill_bytes > 0 else 0,
            "skew_detected": combined_skew_detected,  # AQEã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã¨AQEShuffleReadè­¦å‘Šã®ä¸¡æ–¹ã‚’è€ƒæ…®
            "aqe_skew_detected": skew_detected,  # å¾“æ¥ã®AQEã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã®ã¿
            "aqe_shuffle_skew_warning": aqe_shuffle_skew_warning,  # AQEShuffleReadå¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºè­¦å‘Š
            "skewed_partitions": skewed_partitions,
            "is_shuffle_node": "shuffle" in node_name.lower(),
            "severity": "CRITICAL" if duration_ms >= 10000 else "HIGH" if duration_ms >= 5000 else "MEDIUM" if duration_ms >= 1000 else "LOW"
        }
        
        # Shuffleãƒãƒ¼ãƒ‰ã®å ´åˆã€ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿REPARTITIONãƒ’ãƒ³ãƒˆã‚’è¿½åŠ 
        if node_analysis["is_shuffle_node"] and spill_detected and spill_bytes > 0:
            shuffle_attributes = extract_shuffle_attributes(node)
            if shuffle_attributes:
                suggested_partitions = max(num_tasks * 2, 200)
                
                # Shuffleå±æ€§ã§æ¤œå‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ ã‚’å…¨ã¦ä½¿ç”¨ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
                repartition_columns = ", ".join(shuffle_attributes)
                
                repartition_hint = {
                    "node_id": node_analysis["node_id"],
                    "attributes": shuffle_attributes,
                    "suggested_sql": f"REPARTITION({suggested_partitions}, {repartition_columns})",
                    "reason": f"Spill({node_analysis['spill_gb']:.2f}GB) improvement",
                    "priority": "HIGH",
                    "estimated_improvement": "Significant performance improvement expected",

                }
                detailed_analysis["shuffle_optimization_hints"].append(repartition_hint)
                node_analysis["repartition_hint"] = repartition_hint
        

        # ãƒ•ã‚£ãƒ«ã‚¿ç‡è¨ˆç®—ã¨æƒ…å ±æ›´æ–°
        filter_result = calculate_filter_rate(node)
        node_analysis.update({
            "filter_rate": filter_result["filter_rate"],
            "files_pruned_bytes": filter_result["files_pruned_bytes"],
            "files_read_bytes": filter_result["files_read_bytes"],
            "has_filter_metrics": filter_result["has_filter_metrics"]
        })
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã®è¿½åŠ 
        cluster_attributes = extract_cluster_attributes(node)
        node_analysis.update({
            "cluster_attributes": cluster_attributes,
            "has_clustering": len(cluster_attributes) > 0
        })
        
        detailed_analysis["top_bottleneck_nodes"].append(node_analysis)
        
        # ã‚¹ãƒ”ãƒ«åˆ†æã¸ã®è¿½åŠ 
        if spill_detected:
            detailed_analysis["spill_analysis"]["total_spill_gb"] += node_analysis["spill_gb"]
            detailed_analysis["spill_analysis"]["spill_nodes"].append({
                "node_id": node_analysis["node_id"],
                "node_name": node_name,
                "spill_gb": node_analysis["spill_gb"],
                "rank": i + 1
            })
            
            if node_analysis["spill_gb"] > 1.0:  # 1GBä»¥ä¸Šã¯é‡è¦
                detailed_analysis["spill_analysis"]["critical_spill_nodes"].append(node_analysis["node_id"])
        
        # ã‚¹ã‚­ãƒ¥ãƒ¼åˆ†æã¸ã®è¿½åŠ 
        if skew_detected:
            detailed_analysis["skew_analysis"]["total_skewed_partitions"] += skewed_partitions
            detailed_analysis["skew_analysis"]["skewed_nodes"].append({
                "node_id": node_analysis["node_id"],
                "node_name": node_name,
                "skewed_partitions": skewed_partitions,
                "rank": i + 1
            })
    
    # å…¨ä½“çš„ãªæ¨å¥¨äº‹é …ã®ç”Ÿæˆ
    if detailed_analysis["spill_analysis"]["total_spill_gb"] > 5.0:
        detailed_analysis["performance_recommendations"].append({
            "type": "memory_optimization",
            "priority": "CRITICAL",
            "description": f"Large spill({detailed_analysis['spill_analysis']['total_spill_gb']:.1f}GB) detected: Memory configuration and partitioning strategy review required"
        })
    
    if len(detailed_analysis["shuffle_optimization_hints"]) > 0:
        detailed_analysis["performance_recommendations"].append({
            "type": "shuffle_optimization", 
            "priority": "HIGH",
            "description": f"Memory optimization required for {len(detailed_analysis['shuffle_optimization_hints'])} shuffle nodes with spill occurrence"
        })
    
    if detailed_analysis["skew_analysis"]["total_skewed_partitions"] > 10:
        detailed_analysis["performance_recommendations"].append({
            "type": "skew_optimization",
            "priority": "HIGH", 
            "description": f"Data skew ({detailed_analysis['skew_analysis']['total_skewed_partitions']} partitions) detected: Data distribution review required"
        })
    
    return detailed_analysis

print("âœ… Function definition completed: get_meaningful_node_name, extract_shuffle_attributes, extract_detailed_bottleneck_analysis")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¯ Bottleneck Indicator Calculation Function
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - Execution time and compilation time ratio analysis
# MAGIC - Cache efficiency and data processing efficiency calculation
# MAGIC - Photon utilization analysis
# MAGIC - Spill detection and shuffle/parallelism issue identification

# COMMAND ----------

def calculate_bottleneck_indicators(metrics: Dict[str, Any]) -> Dict[str, Any]:
    """Calculate bottleneck metrics"""
    indicators = {}
    
    overall = metrics.get('overall_metrics', {})
    total_time = overall.get('total_time_ms', 0)
    execution_time = overall.get('execution_time_ms', 0)
    compilation_time = overall.get('compilation_time_ms', 0)
    
    if total_time > 0:
        indicators['compilation_ratio'] = compilation_time / total_time
        indicators['execution_ratio'] = execution_time / total_time
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡
    read_bytes = overall.get('read_bytes', 0)
    cache_bytes = overall.get('read_cache_bytes', 0)
    if read_bytes > 0:
        indicators['cache_hit_ratio'] = cache_bytes / read_bytes
    
    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†åŠ¹ç‡ï¼ˆå®¹é‡ãƒ™ãƒ¼ã‚¹ï¼‰
    read_bytes = overall.get('read_bytes', 0)
    
    # å®¹é‡ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿ç‡ã‚’è¨ˆç®—ï¼ˆæ­£ã—ã„å®Ÿè£…ï¼‰
    data_selectivity = calculate_filter_rate_percentage(overall, metrics)
    
    indicators['data_selectivity'] = data_selectivity
    
    # Photonä½¿ç”¨ç‡ï¼ˆã‚¿ã‚¹ã‚¯å®Ÿè¡Œæ™‚é–“ã«å¯¾ã™ã‚‹å‰²åˆï¼‰
    task_time = overall.get('task_total_time_ms', 0)
    photon_time = overall.get('photon_total_time_ms', 0)
    if task_time > 0:
        indicators['photon_ratio'] = min(photon_time / task_time, 1.0)  # æœ€å¤§100%ã«åˆ¶é™
    else:
        indicators['photon_ratio'] = 0.0
    
    # ã‚¹ãƒ”ãƒ«æ¤œå‡ºï¼ˆè©³ç´°ç‰ˆï¼šSink - Num bytes spilled to disk due to memory pressure ãƒ™ãƒ¼ã‚¹ï¼‰
    spill_detected = False
    total_spill_bytes = 0
    spill_details = []
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹åï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
    target_spill_metrics = [
        "Sink - Num bytes spilled to disk due to memory pressure",
        "Num bytes spilled to disk due to memory pressure"
    ]
    
    # å„ãƒãƒ¼ãƒ‰ã§ã‚¹ãƒ”ãƒ«æ¤œå‡ºã‚’å®Ÿè¡Œ
    for node in metrics.get('node_metrics', []):
        node_spill_found = False
        
        # 1. Search from detailed_metrics
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            metric_value = metric_info.get('value', 0)
            metric_label = metric_info.get('label', '')
            
            # è¤‡æ•°ã®ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’ãƒã‚§ãƒƒã‚¯
            if ((metric_key in target_spill_metrics or 
                 metric_label in target_spill_metrics) and metric_value > 0):
                spill_detected = True
                node_spill_found = True
                total_spill_bytes += metric_value
                spill_details.append({
                    'node_id': node.get('node_id', ''),
                    'node_name': node.get('name', ''),
                    'spill_bytes': metric_value,
                    'spill_metric': metric_key if metric_key in target_spill_metrics else metric_label,
                    'source': 'detailed_metrics'
                })
                break
        
        # 2. raw_metricsã‹ã‚‰æ¤œç´¢ï¼ˆã“ã®ãƒãƒ¼ãƒ‰ã§ã¾ã è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆï¼‰
        if not node_spill_found:
            raw_metrics = node.get('metrics', [])
            for metric in raw_metrics:
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_value = metric.get('value', 0)
                
                # è¤‡æ•°ã®ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’ãƒã‚§ãƒƒã‚¯
                if ((metric_key in target_spill_metrics or 
                     metric_label in target_spill_metrics) and metric_value > 0):
                    spill_detected = True
                    node_spill_found = True
                    total_spill_bytes += metric_value
                    spill_details.append({
                        'node_id': node.get('node_id', ''),
                        'node_name': node.get('name', ''),
                        'spill_bytes': metric_value,
                        'spill_metric': metric_key if metric_key in target_spill_metrics else metric_label,
                        'source': 'raw_metrics'
                    })
                    break
        
        # 3. key_metricsã‹ã‚‰æ¤œç´¢ï¼ˆæœ€å¾Œã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not node_spill_found:
            key_metrics = node.get('key_metrics', {})
            for key_metric_name, key_metric_value in key_metrics.items():
                # è¤‡æ•°ã®ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’ãƒã‚§ãƒƒã‚¯
                if key_metric_name in target_spill_metrics and key_metric_value > 0:
                    spill_detected = True
                    node_spill_found = True
                    total_spill_bytes += key_metric_value
                    spill_details.append({
                        'node_id': node.get('node_id', ''),
                        'node_name': node.get('name', ''),
                        'spill_bytes': key_metric_value,
                        'spill_metric': key_metric_name,
                        'source': 'key_metrics'
                    })
                    break
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: overall_metricsã‹ã‚‰ã®ç°¡æ˜“æ¤œå‡º
    if not spill_detected:
        fallback_spill_bytes = overall.get('spill_to_disk_bytes', 0)
        if fallback_spill_bytes > 0:
            spill_detected = True
            total_spill_bytes = fallback_spill_bytes
            spill_details.append({
                'node_id': 'overall',
                'node_name': 'Overall Metrics',
                'spill_bytes': fallback_spill_bytes,
                'source': 'overall_metrics'
            })
    
    indicators['has_spill'] = spill_detected
    indicators['spill_bytes'] = total_spill_bytes
    indicators['spill_details'] = spill_details
    indicators['spill_nodes_count'] = len(spill_details)
    
    # æœ€ã‚‚æ™‚é–“ã®ã‹ã‹ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸
    stage_durations = [(s['stage_id'], s['duration_ms']) for s in metrics.get('stage_metrics', []) if s['duration_ms'] > 0]
    if stage_durations:
        slowest_stage = max(stage_durations, key=lambda x: x[1])
        indicators['slowest_stage_id'] = slowest_stage[0]
        indicators['slowest_stage_duration'] = slowest_stage[1]
    
    # æœ€ã‚‚ãƒ¡ãƒ¢ãƒªã‚’ä½¿ç”¨ã™ã‚‹ãƒãƒ¼ãƒ‰
    memory_usage = []
    for node in metrics.get('node_metrics', []):
        peak_memory = node.get('key_metrics', {}).get('peakMemoryBytes', 0)
        if peak_memory > 0:
            memory_usage.append((node['node_id'], node['name'], peak_memory))
    
    if memory_usage:
        highest_memory_node = max(memory_usage, key=lambda x: x[2])
        indicators['highest_memory_node_id'] = highest_memory_node[0]
        indicators['highest_memory_node_name'] = highest_memory_node[1]
        indicators['highest_memory_bytes'] = highest_memory_node[2]
    
    # ä¸¦åˆ—åº¦ã¨ã‚·ãƒ£ãƒƒãƒ•ãƒ«å•é¡Œã®æ¤œå‡º
    shuffle_nodes = []
    low_parallelism_stages = []
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰ã®ç‰¹å®š
    for node in metrics.get('node_metrics', []):
        node_name = node.get('name', '').upper()
        if any(keyword in node_name for keyword in ['SHUFFLE', 'EXCHANGE']):
            shuffle_nodes.append({
                'node_id': node['node_id'],
                'name': node['name'],
                'duration_ms': node.get('key_metrics', {}).get('durationMs', 0),
                'rows': node.get('key_metrics', {}).get('rowsNum', 0)
            })
    
    # ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸ã®æ¤œå‡º
    for stage in metrics.get('stage_metrics', []):
        num_tasks = stage.get('num_tasks', 0)
        duration_ms = stage.get('duration_ms', 0)
        
        # ä¸¦åˆ—åº¦ãŒä½ã„ï¼ˆã‚¿ã‚¹ã‚¯æ•°ãŒå°‘ãªã„ï¼‰ã‹ã¤å®Ÿè¡Œæ™‚é–“ãŒé•·ã„ã‚¹ãƒ†ãƒ¼ã‚¸
        if num_tasks > 0 and num_tasks < 10 and duration_ms > 5000:  # 10ã‚¿ã‚¹ã‚¯æœªæº€ã€5ç§’ä»¥ä¸Š
            low_parallelism_stages.append({
                'stage_id': stage['stage_id'],
                'num_tasks': num_tasks,
                'duration_ms': duration_ms,
                'avg_task_duration': duration_ms / max(num_tasks, 1)
            })
    
    indicators['shuffle_operations_count'] = len(shuffle_nodes)
    indicators['low_parallelism_stages_count'] = len(low_parallelism_stages)
    indicators['has_shuffle_bottleneck'] = len(shuffle_nodes) > 0 and any(s['duration_ms'] > 10000 for s in shuffle_nodes)
    indicators['has_low_parallelism'] = len(low_parallelism_stages) > 0
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã®è©³ç´°æƒ…å ±
    if shuffle_nodes:
        total_shuffle_time = sum(s['duration_ms'] for s in shuffle_nodes)
        indicators['total_shuffle_time_ms'] = total_shuffle_time
        indicators['shuffle_time_ratio'] = total_shuffle_time / max(total_time, 1)
        
        # æœ€ã‚‚æ™‚é–“ã®ã‹ã‹ã‚‹ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ
        slowest_shuffle = max(shuffle_nodes, key=lambda x: x['duration_ms'])
        indicators['slowest_shuffle_duration_ms'] = slowest_shuffle['duration_ms']
        indicators['slowest_shuffle_node'] = slowest_shuffle['name']
    
    # ä½ä¸¦åˆ—åº¦ã®è©³ç´°æƒ…å ±
    if low_parallelism_stages:
        indicators['low_parallelism_details'] = low_parallelism_stages
        avg_parallelism = sum(s['num_tasks'] for s in low_parallelism_stages) / len(low_parallelism_stages)
        indicators['average_low_parallelism'] = avg_parallelism
    
    # AQEShuffleReadè­¦å‘Šã®æ¤œå‡º
    aqe_shuffle_skew_warning_detected = False
    aqe_detected_and_handled = False
    
    for node in metrics.get('node_metrics', []):
        parallelism_data = extract_parallelism_metrics(node)
        if parallelism_data.get('aqe_shuffle_skew_warning', False):
            aqe_shuffle_skew_warning_detected = True
        if parallelism_data.get('aqe_detected_and_handled', False):
            aqe_detected_and_handled = True
    
    # å„ªå…ˆé †ä½: 512MBä»¥ä¸Šã®è­¦å‘ŠãŒã‚ã‚Œã°ã€ãã‚Œã‚’å„ªå…ˆ
    # è­¦å‘ŠãŒãªã„å ´åˆã®ã¿ã€AQEå¯¾å¿œæ¸ˆã¿ã¨åˆ¤å®š
    indicators['has_aqe_shuffle_skew_warning'] = aqe_shuffle_skew_warning_detected
    indicators['has_skew'] = aqe_detected_and_handled and not aqe_shuffle_skew_warning_detected
    
    return indicators

print("âœ… Function definition completed: calculate_bottleneck_indicators")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ§¬ Liquid Clustering Analysis Function
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - Column information extraction from profiler data
# MAGIC - Filter, JOIN, and GROUP BY condition analysis
# MAGIC - Data skew and performance impact evaluation
# MAGIC - Clustering recommended column identification

# COMMAND ----------

def calculate_performance_insights_from_metrics(overall_metrics: Dict[str, Any], metrics: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±ã®ã¿ã‹ã‚‰è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ´å¯Ÿã‚’è¨ˆç®—
    """
    insights = {}
    
    # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿
    total_time_ms = overall_metrics.get('total_time_ms', 0)
    read_bytes = overall_metrics.get('read_bytes', 0)
    read_cache_bytes = overall_metrics.get('read_cache_bytes', 0)
    read_remote_bytes = overall_metrics.get('read_remote_bytes', 0)
    rows_read = overall_metrics.get('rows_read_count', 0)
    rows_produced = overall_metrics.get('rows_produced_count', 0)
    read_files = overall_metrics.get('read_files_count', 0)
    read_partitions = overall_metrics.get('read_partitions_count', 0)
    photon_time = overall_metrics.get('photon_total_time_ms', 0)
    task_time = overall_metrics.get('task_total_time_ms', 0)
    spill_bytes = overall_metrics.get('spill_to_disk_bytes', 0)
    
    # 1. ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡åˆ†æï¼ˆå®¹é‡ãƒ™ãƒ¼ã‚¹ï¼‰
    # metricsãŒNoneã®å ´åˆã¯ç©ºã®è¾æ›¸ã§åˆæœŸåŒ–
    if metrics is None:
        metrics = {'node_metrics': []}
    
    # å®¹é‡ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿ç‡ã‚’è¨ˆç®—ï¼ˆæ­£ã—ã„å®Ÿè£…ï¼‰
    filter_rate_capacity = calculate_filter_rate_percentage(overall_metrics, metrics)
    
    insights['data_efficiency'] = {
        'data_selectivity': filter_rate_capacity,
        'avg_bytes_per_file': read_bytes / max(read_files, 1),
        'avg_bytes_per_partition': read_bytes / max(read_partitions, 1),
        'avg_rows_per_file': rows_read / max(read_files, 1),
        'avg_rows_per_partition': rows_read / max(read_partitions, 1)
    }
    
    # 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡åˆ†æ
    cache_hit_ratio = read_cache_bytes / max(read_bytes, 1)
    insights['cache_efficiency'] = {
        'cache_hit_ratio': cache_hit_ratio,
        'cache_hit_percentage': cache_hit_ratio * 100,
        'remote_read_ratio': read_remote_bytes / max(read_bytes, 1),
        'cache_effectiveness': 'high' if cache_hit_ratio > 0.8 else 'medium' if cache_hit_ratio > 0.5 else 'low'
    }
    
    # 3. ä¸¦åˆ—åŒ–åŠ¹ç‡åˆ†æ
    insights['parallelization'] = {
        'files_per_second': read_files / max(total_time_ms / 1000, 1),
        'partitions_per_second': read_partitions / max(total_time_ms / 1000, 1),
        'throughput_mb_per_second': (read_bytes / 1024 / 1024) / max(total_time_ms / 1000, 1),
        'rows_per_second': rows_read / max(total_time_ms / 1000, 1)
    }
    
    # 4. PhotonåŠ¹ç‡åˆ†æ
    photon_efficiency = photon_time / max(task_time, 1)
    insights['photon_analysis'] = {
        'photon_enabled': photon_time > 0,
        'photon_efficiency': photon_efficiency,
        'photon_utilization_percentage': photon_efficiency * 100,
        'photon_effectiveness': 'high' if photon_efficiency > 0.8 else 'medium' if photon_efficiency > 0.5 else 'low'
    }
    
    # 5. ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³
    insights['resource_usage'] = {
        'memory_pressure': spill_bytes > 0,
        'spill_gb': spill_bytes / 1024 / 1024 / 1024,
        'data_processed_gb': read_bytes / 1024 / 1024 / 1024,
        'data_reduction_ratio': 1 - (rows_produced / max(rows_read, 1))
    }
    
    # 6. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™
    bottlenecks = []
    if cache_hit_ratio < 0.3:
        bottlenecks.append('Low cache efficiency')
    if read_remote_bytes / max(read_bytes, 1) > 0.8:
        bottlenecks.append('High remote read ratio')
    if photon_efficiency < 0.5 and photon_time > 0:
        bottlenecks.append('Low Photon efficiency')
    if spill_bytes > 0:
        bottlenecks.append('Memory spill occurring')
    if insights['data_efficiency']['data_selectivity'] < 0.2:
        bottlenecks.append('Low filter efficiency')
    
    insights['potential_bottlenecks'] = bottlenecks
    
    return insights

def calculate_filter_rate_percentage(overall_metrics: Dict[str, Any], metrics: Dict[str, Any]) -> float:
    """
    å®¹é‡ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿ç‡ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆoverall_metrics.read_bytesä½¿ç”¨ç‰ˆï¼‰
    
    âŒ ãƒ‡ã‚°ãƒ¬é˜²æ­¢æ³¨æ„: ã“ã®é–¢æ•°ã¯å¿…ãšoverall_metrics.read_bytesã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼
    âŒ files_read_bytesï¼ˆã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰é›†è¨ˆï¼‰ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ï¼
    
    Args:
        overall_metrics: å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆread_bytesã‚’ä½¿ç”¨ï¼‰
        metrics: å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆnode_metricsã‚’å«ã‚€ã€pruned_byteså–å¾—ç”¨ï¼‰
        
    Returns:
        float: ãƒ•ã‚£ãƒ«ã‚¿ç‡ï¼ˆ0.0-1.0ã€é«˜ã„å€¤ã»ã©åŠ¹ç‡çš„ï¼‰
               ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ = files_pruned_bytes / (overall_read_bytes + files_pruned_bytes)
    """
    import os
    debug_mode = os.environ.get('DEBUG_FILTER_ANALYSIS', 'false').lower() == 'true'
    
    # âŒ ãƒ‡ã‚°ãƒ¬é˜²æ­¢: å¿…ãšoverall_metrics.read_bytesã‚’ä½¿ç”¨ï¼
    overall_read_bytes = overall_metrics.get('read_bytes', 0)
    
    if debug_mode:
        print(f"ğŸ” Filter rate calculation debug (using overall_metrics.read_bytes version):")
        print(f"   overall_read_bytes: {overall_read_bytes:,} ({overall_read_bytes / (1024**4):.2f} TB)")
    
    try:
        # pruned_bytesã®ã¿node_metricsã‹ã‚‰å–å¾—ï¼ˆread_bytesã¯ä½¿ç”¨ã—ãªã„ï¼‰
        node_metrics = metrics.get('node_metrics', [])
        total_files_pruned_bytes = 0
        filter_metrics_found = False
        
        # å…¨ã¦ã®ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‹ã‚‰prunedæƒ…å ±ã®ã¿ã‚’é›†è¨ˆ
        for node in node_metrics:
            if node.get('tag') in ['FileScan', 'BatchScan', 'TableScan', 'UNKNOWN_DATA_SOURCE_SCAN_EXEC']:
                filter_result = calculate_filter_rate(node)
                if filter_result.get('has_filter_metrics', False):
                    files_pruned_bytes = filter_result.get('files_pruned_bytes', 0)
                    
                    if files_pruned_bytes > 0:
                        total_files_pruned_bytes += files_pruned_bytes
                        filter_metrics_found = True
                        
                        if debug_mode:
                            print(f"   Node {node.get('node_id', 'unknown')}: files_pruned_bytes = {files_pruned_bytes:,}")
        
        # âŒ ãƒ‡ã‚°ãƒ¬é˜²æ­¢: overall_read_bytes + pruned_bytes ã§è¨ˆç®—
        if filter_metrics_found and overall_read_bytes > 0:
            # æ­£ã—ã„è¨ˆç®—: ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ = files_pruned / (overall_read + files_pruned)
            total_available_bytes = overall_read_bytes + total_files_pruned_bytes
            if total_available_bytes > 0:
                overall_filter_rate = total_files_pruned_bytes / total_available_bytes
            else:
                overall_filter_rate = 0.0
                
            if debug_mode:
                print(f"   âŒ Regression prevention version: using overall_read_bytes")
                print(f"     overall_read_bytes: {overall_read_bytes:,} ({overall_read_bytes / (1024**4):.2f} TB)")
                print(f"     total_files_pruned_bytes: {total_files_pruned_bytes:,} ({total_files_pruned_bytes / (1024**4):.2f} TB)")
                print(f"     total_available_bytes: {total_available_bytes:,} ({total_available_bytes / (1024**4):.2f} TB)")
                print(f"     Pruning efficiency: {overall_filter_rate*100:.2f}%")
            return overall_filter_rate
        
        if debug_mode:
            print(f"   Filter metrics: {'Detected' if filter_metrics_found else 'Not detected'}")
            print(f"   overall_read_bytes: {overall_read_bytes:,}")
            if not filter_metrics_found:
                print(f"   âš ï¸ Pruning information is not available")
            if overall_read_bytes == 0:
                print(f"   âš ï¸ No read data available")
        
        # ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°æƒ…å ±ãŒãªã„å ´åˆã¯0ã‚’è¿”ã™
        return 0.0
        
    except Exception as e:
        if debug_mode:
            print(f"   Filter rate calculation error: {e}")
        return 0.0

def extract_liquid_clustering_data(profiler_data: Dict[str, Any], metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extract data required for Liquid Clustering analysis from SQL profiler data (for LLM analysis)
    """
    extracted_data = {
        "filter_columns": [],
        "join_columns": [],
        "groupby_columns": [],
        "aggregate_columns": [],
        "table_info": {},
        "scan_nodes": [],
        "join_nodes": [],
        "filter_nodes": [],
        "metadata_summary": {}
    }
    
    print(f"ğŸ” Starting data extraction for Liquid Clustering analysis")
    
    # ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’ç¢ºèª
    data_format = metrics.get('data_format', '')
    if data_format == 'sql_query_summary':
        print("ğŸ“Š SQL query summary format: Limited Liquid Clustering analysis")
        # test2.jsonå½¢å¼ã®å ´åˆã¯åˆ¶é™ä»˜ãã®åˆ†æã‚’è¡Œã†
        query_info = metrics.get('query_info', {})
        query_text = query_info.get('query_text', '')
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±ã®ã¿ã‹ã‚‰åŸºæœ¬çš„ãªãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’ç”Ÿæˆ
        # test2.jsonå½¢å¼ã§ã¯ planMetadatas ãŒç©ºã®ãŸã‚ã€graphs metadata ã¯åˆ©ç”¨ä¸å¯
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹é‡è¦–ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’è¡Œã†
        
        # å…¨ä½“çš„ãªãƒ•ã‚£ãƒ«ã‚¿ç‡æƒ…å ±ã‚’è¨ˆç®—
        overall_filter_rate = calculate_filter_rate_percentage(overall_metrics, metrics)
        read_bytes = overall_metrics.get('read_bytes', 0)
        read_gb = read_bytes / (1024**3) if read_bytes > 0 else 0
        
        # ãƒ—ãƒ«ãƒ¼ãƒ³é‡ã‚’æ¨å®šï¼ˆãƒ•ã‚£ãƒ«ã‚¿ç‡ã‹ã‚‰é€†ç®—ï¼‰
        if overall_filter_rate > 0 and read_bytes > 0:
            pruned_bytes = (read_bytes * overall_filter_rate) / (1 - overall_filter_rate)
            pruned_gb = pruned_bytes / (1024**3)
        else:
            pruned_bytes = 0
            pruned_gb = 0
        
        extracted_data["table_info"]["metrics_summary"] = {
            "node_name": "Metrics-Based Analysis",
            "node_tag": "QUERY_SUMMARY", 
            "node_id": "summary",
            "files_count": overall_metrics.get('read_files_count', 0),
            "partitions_count": overall_metrics.get('read_partitions_count', 0),
            "data_size_gb": read_gb,
            "rows_read": overall_metrics.get('rows_read_count', 0),
            "rows_produced": overall_metrics.get('rows_produced_count', 0),
            "data_selectivity": overall_filter_rate,
            "avg_file_size_mb": (overall_metrics.get('read_bytes', 0) / 1024 / 1024) / max(overall_metrics.get('read_files_count', 1), 1),
            "avg_partition_size_mb": (overall_metrics.get('read_bytes', 0) / 1024 / 1024) / max(overall_metrics.get('read_partitions_count', 1), 1),
            "note": "Detailed table information is not available in SQL query summary format. Executing metrics-based analysis.",
            "current_clustering_keys": [],  # ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼
            "filter_info": {  # ãƒ•ã‚£ãƒ«ã‚¿ç‡æƒ…å ±ã‚’è¿½åŠ 
                "filter_rate": overall_filter_rate,
                "files_read_bytes": read_bytes,
                "files_pruned_bytes": pruned_bytes,
                "has_filter_metrics": read_bytes > 0
            }
        }
        
        # ã‚µãƒãƒªãƒ¼ãƒãƒ¼ãƒ‰ã®æƒ…å ±ã‚’ä½¿ç”¨
        for node in metrics.get('node_metrics', []):
            node_name = node.get('name', '')
            extracted_data["scan_nodes"].append({
                "name": node_name,
                "type": node.get('tag', ''),
                "rows": node.get('key_metrics', {}).get('rowsNum', 0),
                "duration_ms": node.get('key_metrics', {}).get('durationMs', 0),
                "node_id": node.get('node_id', '')
            })
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ï¼ˆåˆ¶é™ä»˜ãï¼‰
        view_count = sum(1 for table in extracted_data["table_info"].values() if table.get('is_view', False))
        actual_table_count = sum(len(table.get('underlying_tables', [])) for table in extracted_data["table_info"].values())
        
        extracted_data["metadata_summary"] = {
            "total_nodes": len(metrics.get('node_metrics', [])),
            "total_graphs": 0,
            "filter_expressions_count": 0,
            "join_expressions_count": 0,
            "groupby_expressions_count": 0,
            "aggregate_expressions_count": 0,
            "tables_identified": len(extracted_data["table_info"]),
            "views_identified": view_count,
            "underlying_tables_estimated": actual_table_count,
            "scan_nodes_count": len(extracted_data["scan_nodes"]),
            "join_nodes_count": 0,
            "filter_nodes_count": 0,
            "analysis_limitation": "Detailed analysis is limited due to SQL query summary format"
        }
        
        print(f"âœ… Limited data extraction completed: {extracted_data['metadata_summary']}")
        
        # ãƒ“ãƒ¥ãƒ¼æƒ…å ±ã®è©³ç´°è¡¨ç¤º
        if view_count > 0:
            print(f"ğŸ” View information details:")
            for table_name, table_info in extracted_data["table_info"].items():
                if table_info.get('is_view', False):
                    print(f"  ğŸ“Š View: {table_name}")
                    print(f"     Alias: {table_info.get('alias', 'None')}")
                    print(f"     Table type: {table_info.get('table_type', 'unknown')}")
                    
                    underlying_tables = table_info.get('underlying_tables', [])
                    if underlying_tables:
                        print(f"     Estimated underlying table count: {len(underlying_tables)}")
                        for i, underlying_table in enumerate(underlying_tables[:3]):  # Display max 3
                            print(f"       - {underlying_table}")
                        if len(underlying_tables) > 3:
                            print(f"       ... and {len(underlying_tables) - 3} additional tables")
                    print()
        
        return extracted_data
    
    # é€šå¸¸ã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼å½¢å¼ã®å‡¦ç†
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿè¡Œã‚°ãƒ©ãƒ•æƒ…å ±ã‚’å–å¾—ï¼ˆè¤‡æ•°ã‚°ãƒ©ãƒ•å¯¾å¿œï¼‰
    graphs = profiler_data.get('graphs', [])
    if not graphs:
        print("âš ï¸ Graph data not found")
        return extracted_data

    # ã™ã¹ã¦ã®ã‚°ãƒ©ãƒ•ã‹ã‚‰ãƒãƒ¼ãƒ‰ã‚’åé›†
    all_nodes = []
    for graph_index, graph in enumerate(graphs):
        nodes = graph.get('nodes', [])
        for node in nodes:
            node['graph_index'] = graph_index
            all_nodes.append(node)
    
    print(f"ğŸ” Processing {len(all_nodes)} nodes from {len(graphs)} graphs")

    # ãƒãƒ¼ãƒ‰ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã‚’æŠ½å‡º
    for node in all_nodes:
        node_name = node.get('name', '')
        node_tag = node.get('tag', '')
        node_metadata = node.get('metadata', [])
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é‡è¦ãªæƒ…å ±ã‚’æŠ½å‡º
        for metadata_item in node_metadata:
            key = metadata_item.get('key', '')
            values = metadata_item.get('values', [])
            value = metadata_item.get('value', '')
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã®æŠ½å‡º
            if key == 'FILTERS' and values:
                for filter_expr in values:
                    extracted_data["filter_columns"].append({
                        "expression": filter_expr,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # GROUP BYå¼ã®æŠ½å‡º
            elif key == 'GROUPING_EXPRESSIONS' and values:
                for group_expr in values:
                    extracted_data["groupby_columns"].append({
                        "expression": group_expr,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # JOINæ¡ä»¶ã®æŠ½å‡º
            elif key in ['LEFT_KEYS', 'RIGHT_KEYS'] and values:
                for join_key in values:
                    extracted_data["join_columns"].append({
                        "expression": join_key,
                        "key_type": key,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # é›†ç´„é–¢æ•°ã®æŠ½å‡º
            elif key == 'AGGREGATE_EXPRESSIONS' and values:
                for agg_expr in values:
                    extracted_data["aggregate_columns"].append({
                        "expression": agg_expr,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®æŠ½å‡º
            elif key == 'SCAN_IDENTIFIER':
                table_name = value
                extracted_data["table_info"][table_name] = {
                    "node_name": node_name,
                    "node_tag": node_tag,
                    "node_id": node.get('id', ''),
                    "current_clustering_keys": []  # ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã‚’è¿½åŠ 
                }

    # ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—åˆ¥ã®åˆ†é¡ã¨ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã®é–¢é€£ä»˜ã‘
    node_metrics = metrics.get('node_metrics', [])
    for node in node_metrics:
        node_name = node.get('name', '')
        node_type = node.get('tag', '')
        key_metrics = node.get('key_metrics', {})
        
        if any(keyword in node_name.upper() for keyword in ['SCAN', 'FILESCAN', 'PARQUET', 'DELTA']):
            # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‹ã‚‰ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã‚’æŠ½å‡º
            cluster_attributes = extract_cluster_attributes(node)
            
            # ãƒãƒ¼ãƒ‰ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡ºã—ã¦ãƒãƒƒãƒ”ãƒ³ã‚°
            node_metadata = node.get('metadata', [])
            table_name_from_node = None
            
            for meta in node_metadata:
                meta_key = meta.get('key', '')
                meta_value = meta.get('value', '')
                if meta_key == 'SCAN_IDENTIFIER' and meta_value:
                    table_name_from_node = meta_value
                    break
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«åãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒãƒ¼ãƒ‰åã‹ã‚‰æ¨æ¸¬
            if not table_name_from_node:
                import re
                table_patterns = [
                    r'[Ss]can\s+([a-zA-Z_][a-zA-Z0-9_.]*[a-zA-Z0-9_])',
                    r'([a-zA-Z_][a-zA-Z0-9_]*\.)+([a-zA-Z_][a-zA-Z0-9_]*)',
                ]
                
                for pattern in table_patterns:
                    match = re.search(pattern, node_name)
                    if match:
                        if '.' in match.group(0):
                            table_name_from_node = match.group(0)
                        else:
                            table_name_from_node = match.group(1) if match.lastindex and match.lastindex >= 1 else match.group(0)
                        break
            
            # ãƒ•ã‚£ãƒ«ã‚¿ç‡æƒ…å ±ã‚’è¨ˆç®—
            filter_result = calculate_filter_rate(node)
            filter_rate_info = {
                "filter_rate": filter_result.get("filter_rate", 0),
                "files_read_bytes": filter_result.get("files_read_bytes", 0),
                "files_pruned_bytes": filter_result.get("files_pruned_bytes", 0),
                "has_filter_metrics": filter_result.get("has_filter_metrics", False)
            }
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã«ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã¨ãƒ•ã‚£ãƒ«ã‚¿ç‡ã‚’è¿½åŠ 
            if table_name_from_node:
                # æ—¢å­˜ã®ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’æ›´æ–°
                if table_name_from_node in extracted_data["table_info"]:
                    extracted_data["table_info"][table_name_from_node]["current_clustering_keys"] = cluster_attributes
                    extracted_data["table_info"][table_name_from_node]["filter_info"] = filter_rate_info
                else:
                    # æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’ä½œæˆ
                    extracted_data["table_info"][table_name_from_node] = {
                        "node_name": node_name,
                        "node_tag": node_type,
                        "node_id": node.get('node_id', ''),
                        "current_clustering_keys": cluster_attributes,
                        "filter_info": filter_rate_info
                    }
            
            extracted_data["scan_nodes"].append({
                "name": node_name,
                "type": node_type,
                "rows": key_metrics.get('rowsNum', 0),
                "duration_ms": key_metrics.get('durationMs', 0),
                "node_id": node.get('node_id', ''),
                "table_name": table_name_from_node,
                "current_clustering_keys": cluster_attributes
            })
        elif any(keyword in node_name.upper() for keyword in ['JOIN', 'HASH']):
            extracted_data["join_nodes"].append({
                "name": node_name,
                "type": node_type,
                "duration_ms": key_metrics.get('durationMs', 0),
                "node_id": node.get('node_id', '')
            })
        elif any(keyword in node_name.upper() for keyword in ['FILTER']):
            extracted_data["filter_nodes"].append({
                "name": node_name,
                "type": node_type,
                "duration_ms": key_metrics.get('durationMs', 0),
                "node_id": node.get('node_id', '')
            })

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼
    extracted_data["metadata_summary"] = {
        "total_nodes": len(all_nodes),
        "total_graphs": len(graphs),
        "filter_expressions_count": len(extracted_data["filter_columns"]),
        "join_expressions_count": len(extracted_data["join_columns"]),
        "groupby_expressions_count": len(extracted_data["groupby_columns"]),
        "aggregate_expressions_count": len(extracted_data["aggregate_columns"]),
        "tables_identified": len(extracted_data["table_info"]),
        "scan_nodes_count": len(extracted_data["scan_nodes"]),
        "join_nodes_count": len(extracted_data["join_nodes"]),
        "filter_nodes_count": len(extracted_data["filter_nodes"])
    }
    
    print(f"âœ… Data extraction completed: {extracted_data['metadata_summary']}")
    
    # Display detailed current clustering key information
    clustering_info_found = False
    for table_name, table_info in extracted_data["table_info"].items():
        current_keys = table_info.get('current_clustering_keys', [])
        if current_keys:
            if not clustering_info_found:
                print(f"ğŸ” Current clustering key information:")
                clustering_info_found = True
            print(f"  ğŸ“Š Table: {table_name}")
            print(f"     Current keys: {', '.join(current_keys)}")
            print(f"     Node: {table_info.get('node_name', 'Unknown')}")
            print()
    
    if not clustering_info_found:
        print(f"â„¹ï¸ No current clustering keys detected")
    
    return extracted_data

def analyze_liquid_clustering_opportunities(profiler_data: Dict[str, Any], metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    Generate Liquid Clustering analysis and recommendations using LLM
    """
    print(f"ğŸ¤– Starting LLM-based Liquid Clustering analysis")
    
    # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
    extracted_data = extract_liquid_clustering_data(profiler_data, metrics)
    
    # LLMåˆ†æç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
    overall_metrics = metrics.get('overall_metrics', {})
    bottleneck_indicators = metrics.get('bottleneck_indicators', {})
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦
    total_time_sec = overall_metrics.get('total_time_ms', 0) / 1000
    read_gb = overall_metrics.get('read_bytes', 0) / 1024 / 1024 / 1024
    rows_produced = overall_metrics.get('rows_produced_count', 0)
    rows_read = overall_metrics.get('rows_read_count', 0)
    
    # æŠ½å‡ºã—ãŸã‚«ãƒ©ãƒ æƒ…å ±ã®ã‚µãƒãƒªãƒ¼ä½œæˆï¼ˆä¸Šä½5å€‹ã¾ã§ï¼‰
    filter_summary = []
    for i, item in enumerate(extracted_data["filter_columns"][:5]):
        filter_summary.append(f"  {i+1}. {item['expression']} (node: {item['node_name']})")
    
    join_summary = []
    for i, item in enumerate(extracted_data["join_columns"][:5]):
        join_summary.append(f"  {i+1}. {item['expression']} (type: {item['key_type']}, node: {item['node_name']})")
    
    groupby_summary = []
    for i, item in enumerate(extracted_data["groupby_columns"][:5]):
        groupby_summary.append(f"  {i+1}. {item['expression']} (node: {item['node_name']})")
    
    aggregate_summary = []
    for i, item in enumerate(extracted_data["aggregate_columns"][:5]):
        aggregate_summary.append(f"  {i+1}. {item['expression']} (node: {item['node_name']})")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®ã‚µãƒãƒªãƒ¼ï¼ˆç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã¨ãƒ•ã‚£ãƒ«ã‚¿ç‡ã‚’å«ã‚€ï¼‰
    table_summary = []
    for table_name, table_info in extracted_data["table_info"].items():
        current_keys = table_info.get('current_clustering_keys', [])
        current_keys_str = ', '.join(current_keys) if current_keys else 'Not configured'
        
        # ãƒ•ã‚£ãƒ«ã‚¿ç‡æƒ…å ±ã‚’è¿½åŠ 
        filter_info = table_info.get('filter_info', {})
        filter_rate = filter_info.get('filter_rate', 0)
        files_read_bytes = filter_info.get('files_read_bytes', 0)
        files_pruned_bytes = filter_info.get('files_pruned_bytes', 0)
        
        # ãƒã‚¤ãƒˆæ•°ã‚’GBå˜ä½ã«å¤‰æ›
        read_gb = files_read_bytes / (1024**3) if files_read_bytes > 0 else 0
        pruned_gb = files_pruned_bytes / (1024**3) if files_pruned_bytes > 0 else 0
        
        if filter_info.get('has_filter_metrics', False):
            filter_str = f", filter rate: {filter_rate*100:.1f}% (read: {read_gb:.2f}GB, pruned: {pruned_gb:.2f}GB)"
        else:
            filter_str = ", filter rate: no information"
        
        table_summary.append(f"  - {table_name} (node: {table_info['node_name']}, current clustering key: {current_keys_str}{filter_str})")
    
    # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
    scan_performance = []
    for scan in extracted_data["scan_nodes"]:
        efficiency = scan['rows'] / max(scan['duration_ms'], 1)
        scan_performance.append(f"  - {scan['name']}: {scan['rows']:,} rows, {scan['duration_ms']:,}ms, efficiency={efficiency:.1f} rows/ms")

    clustering_prompt = f"""
You are a Databricks Liquid Clustering expert. Please analyze the following SQL profiler data and provide optimal Liquid Clustering recommendations.

ã€Query Performance Overviewã€‘
- Execution time: {total_time_sec:.1f} seconds
- Data read: {read_gb:.2f}GB
- Output rows: {rows_produced:,} rows
- Read rows: {rows_read:,} rows
- ãƒ•ã‚£ãƒ«ã‚¿ç‡: {calculate_filter_rate_percentage(overall_metrics, metrics):.4f}

ã€æŠ½å‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‘

ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ ({len(extracted_data["filter_columns"])}å€‹):
{chr(10).join(filter_summary)}

ğŸ”— JOINæ¡ä»¶ ({len(extracted_data["join_columns"])}å€‹):
{chr(10).join(join_summary)}

ğŸ“Š GROUP BY ({len(extracted_data["groupby_columns"])}å€‹):
{chr(10).join(groupby_summary)}

ğŸ“ˆ é›†ç´„é–¢æ•° ({len(extracted_data["aggregate_columns"])}å€‹) - âš ï¸å‚è€ƒæƒ…å ±ã®ã¿ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã«ã¯ä½¿ç”¨ç¦æ­¢ï¼‰:
{chr(10).join(aggregate_summary)}
âš ï¸ æ³¨æ„: ä¸Šè¨˜ã®é›†ç´„é–¢æ•°ã§ä½¿ç”¨ã•ã‚Œã‚‹ã‚«ãƒ©ãƒ ã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã®å€™è£œã‹ã‚‰é™¤å¤–ã—ã¦ãã ã•ã„ã€‚

ã€ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã€‘
ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(extracted_data["table_info"])}å€‹
{chr(10).join(table_summary)}

ã€ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€‘
{chr(10).join(scan_performance)}

ã€ç¾åœ¨ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã€‘
- ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ: {'ã‚ã‚Š' if bottleneck_indicators.get('has_spill', False) else 'ãªã—'}
- ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ: {bottleneck_indicators.get('shuffle_operations_count', 0)}å›
- ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸: {bottleneck_indicators.get('low_parallelism_stages_count', 0)}å€‹

ã€åˆ†æè¦æ±‚ã€‘
1. å„ãƒ†ãƒ¼ãƒ–ãƒ«ã«å¯¾ã™ã‚‹æœ€é©ãªLiquid Clusteringã‚«ãƒ©ãƒ ã®æ¨å¥¨ï¼ˆæœ€å¤§4ã‚«ãƒ©ãƒ ï¼‰
2. ã‚«ãƒ©ãƒ é¸å®šã®æ ¹æ‹ ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€JOINã€GROUP BYã§ã®ä½¿ç”¨é »åº¦ã¨é‡è¦åº¦ï¼‰
   ğŸš¨ é‡è¦: é›†ç´„é–¢æ•°ï¼ˆSUM, AVG, COUNTç­‰ï¼‰ã®å¯¾è±¡ã‚«ãƒ©ãƒ ã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã«å«ã‚ãªã„
   âœ… æœ‰åŠ¹: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã€JOINæ¡ä»¶ã€GROUP BYæ¡ä»¶ã§ä½¿ç”¨ã•ã‚Œã‚‹ã‚«ãƒ©ãƒ ã®ã¿
3. ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã¨æ¨å¥¨ã‚­ãƒ¼ã®æ¯”è¼ƒåˆ†æ
4. å®Ÿè£…å„ªå…ˆé †ä½ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸ŠåŠ¹æœé †ï¼‰
5. å…·ä½“çš„ãªSQLå®Ÿè£…ä¾‹ï¼ˆæ­£ã—ã„Databricks SQLæ§‹æ–‡ã€ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã«æ˜è¨˜ï¼‰
6. æœŸå¾…ã•ã‚Œã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„åŠ¹æœï¼ˆæ•°å€¤ã§ï¼‰

ã€ğŸš¨ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼é¸å®šã®é‡è¦ãªåˆ¶é™äº‹é …ã€‘
âŒ ç¦æ­¢: é›†ç´„é–¢æ•°ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚«ãƒ©ãƒ ï¼ˆä¾‹ï¼šSUM(sales_amount)ã®sales_amountï¼‰
âŒ ç¦æ­¢: è¨ˆç®—ã®ã¿ã«ä½¿ç”¨ã•ã‚Œã‚‹ã‚«ãƒ©ãƒ ï¼ˆä¾‹ï¼šAVG(quantity)ã®quantityï¼‰
âœ… æ¨å¥¨: WHEREå¥ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚«ãƒ©ãƒ 
âœ… æ¨å¥¨: JOIN ONå¥ã®ã‚­ãƒ¼ã‚«ãƒ©ãƒ   
âœ… æ¨å¥¨: GROUP BYå¥ã®ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã‚«ãƒ©ãƒ 
âœ… æ¨å¥¨: ORDER BYå¥ã®ã‚½ãƒ¼ãƒˆã‚­ãƒ¼ï¼ˆç¯„å›²æ¤œç´¢ãŒã‚ã‚‹å ´åˆï¼‰

ç†ç”±: é›†ç´„å¯¾è±¡ã‚«ãƒ©ãƒ ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã«å«ã‚ã¦ã‚‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹æœã‚„JOINåŠ¹ç‡ã®æ”¹å–„ãŒæœŸå¾…ã§ããšã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®åŠ¹æœã‚’è–„ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

ã€åˆ¶ç´„äº‹é …ã€‘
- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã‚„ZORDERã¯ææ¡ˆã—ãªã„ï¼ˆLiquid Clusteringã®ã¿ï¼‰
- æ­£ã—ã„Databricks SQLæ§‹æ–‡ã‚’ä½¿ç”¨ï¼š
  * æ–°è¦ãƒ†ãƒ¼ãƒ–ãƒ«: CREATE TABLE ... CLUSTER BY (col1, col2, ...)
  * æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«: ALTER TABLE table_name CLUSTER BY (col1, col2, ...)
- æœ€å¤§4ã‚«ãƒ©ãƒ ã¾ã§ã®æ¨å¥¨
- ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ã‚„ä¸¦åˆ—åº¦ã®å•é¡Œã‚‚è€ƒæ…®

ã€ğŸš¨ Important Understanding of Liquid Clustering Specificationsã€‘
- **Column Order**: In Liquid Clustering, changing the order of clustering keys does not affect "node-level data locality"
- **Actual Improvement Effects**: Improvements are in "scan efficiency", "file pruning effects", and "query performance"
- **Technical Characteristics**: Column order within CLUSTER BY is arbitrary, and (col1, col2, col3) and (col3, col1, col2) have equivalent performance

ã€ğŸš¨ Absolutely Prohibited Incorrect Expressionsã€‘
âŒ "Improve data locality by changing order"
âŒ "Improve data locality with clustering key order"  
âŒ "Node-level data placement optimization through order changes"
âœ… "No specific improvement effect from order changes (Liquid Clustering specification)"
âœ… "Improvement in scan efficiency and file pruning effects"
âœ… "Performance improvement for WHERE clauses and JOIN conditions"

ç°¡æ½”ã§å®Ÿè·µçš„ãªåˆ†æçµæœã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªå‡ºåŠ›å½¢å¼æŒ‡ç¤ºã€‘
å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®åˆ†æã§ã¯ã€å¿…ãšä»¥ä¸‹ã®å½¢å¼ã§ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã¨ãƒ•ã‚£ãƒ«ã‚¿ç‡ã‚’å«ã‚ã¦ãã ã•ã„ï¼š

## ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°

### 1. [ãƒ†ãƒ¼ãƒ–ãƒ«å] ãƒ†ãƒ¼ãƒ–ãƒ« (æœ€å„ªå…ˆ/é«˜å„ªå…ˆåº¦/ä¸­å„ªå…ˆåº¦)
**ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼**: [ç¾åœ¨è¨­å®šã•ã‚Œã¦ã„ã‚‹ã‚­ãƒ¼ ã¾ãŸã¯ "è¨­å®šãªã—"]
**æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚«ãƒ©ãƒ **: [æ¨å¥¨ã‚«ãƒ©ãƒ 1], [æ¨å¥¨ã‚«ãƒ©ãƒ 2], [æ¨å¥¨ã‚«ãƒ©ãƒ 3], [æ¨å¥¨ã‚«ãƒ©ãƒ 4]

```sql
ALTER TABLE [ãƒ†ãƒ¼ãƒ–ãƒ«å] 
CLUSTER BY ([æ¨å¥¨ã‚«ãƒ©ãƒ 1], [æ¨å¥¨ã‚«ãƒ©ãƒ 2], [æ¨å¥¨ã‚«ãƒ©ãƒ 3], [æ¨å¥¨ã‚«ãƒ©ãƒ 4]);
OPTIMIZE [ãƒ†ãƒ¼ãƒ–ãƒ«å] FULL;
```

**é¸å®šæ ¹æ‹ **:
- [ã‚«ãƒ©ãƒ 1]: [ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨é‡è¦åº¦]
- [ã‚«ãƒ©ãƒ 2]: [ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨é‡è¦åº¦]
- [ä»¥ä¸‹åŒæ§˜...]
- ğŸš¨é‡è¦: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼é †åºå¤‰æ›´ã¯ãƒãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å±€æ‰€æ€§ã«å½±éŸ¿ã—ãªã„ï¼ˆLiquid Clusteringä»•æ§˜ï¼‰
- âœ…æ”¹å–„åŠ¹æœ: ã‚¹ã‚­ãƒ£ãƒ³åŠ¹ç‡ã¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹æœã®å‘ä¸Šï¼ˆé †åºç„¡é–¢ä¿‚ï¼‰

**æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„åŠ¹æœ**:
- [å…·ä½“çš„ãªæ•°å€¤ã§ã®æ”¹å–„è¦‹è¾¼ã¿]

**ãƒ•ã‚£ãƒ«ã‚¿ç‡**: [X.X]% (èª­ã¿è¾¼ã¿: [XX.XX]GB, ãƒ—ãƒ«ãƒ¼ãƒ³: [XX.XX]GB)

ã“ã®å½¢å¼ã«ã‚ˆã‚Šã€ç¾åœ¨ã®è¨­å®šã€æ¨å¥¨è¨­å®šã€ãŠã‚ˆã³å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç¾åœ¨ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åŠ¹ç‡ã‚’æ˜ç¢ºã«è¡¨ç¤ºã—ã¦ãã ã•ã„ã€‚ãƒ•ã‚£ãƒ«ã‚¿ç‡æƒ…å ±ã¯ä¸Šè¨˜ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‹ã‚‰æ­£ç¢ºãªæ•°å€¤ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
"""

    try:
        # LLMåˆ†æã®å®Ÿè¡Œ
        provider = LLM_CONFIG["provider"]
        print(f"ğŸ¤– Analyzing Liquid Clustering using {provider}...")
        
        if provider == "databricks":
            llm_analysis = _call_databricks_llm(clustering_prompt)
        elif provider == "openai":
            llm_analysis = _call_openai_llm(clustering_prompt)
        elif provider == "azure_openai":
            llm_analysis = _call_azure_openai_llm(clustering_prompt)
        elif provider == "anthropic":
            llm_analysis = _call_anthropic_llm(clustering_prompt)
        else:
            llm_analysis = f"âŒ Unsupported LLM provider: {provider}"
        
        # åˆ†æçµæœã®æ§‹é€ åŒ–
        clustering_analysis = {
            "llm_analysis": llm_analysis,
            "extracted_data": extracted_data,
            "performance_context": {
                "total_time_sec": total_time_sec,
                "read_gb": read_gb,
                "rows_produced": rows_produced,
                "rows_read": rows_read,
                "data_selectivity": calculate_filter_rate_percentage(overall_metrics, metrics)
            },
            "summary": {
                "analysis_method": "LLM-based",
                "tables_identified": len(extracted_data["table_info"]),
                "total_filter_columns": len(extracted_data["filter_columns"]),
                "total_join_columns": len(extracted_data["join_columns"]),
                "total_groupby_columns": len(extracted_data["groupby_columns"]),
                "total_aggregate_columns": len(extracted_data["aggregate_columns"]),
                "scan_nodes_count": len(extracted_data["scan_nodes"]),
                "llm_provider": provider
            }
        }
        
        print("âœ… LLM Liquid Clustering analysis completed")
        return clustering_analysis
        
    except Exception as e:
        error_msg = f"LLM analysis error: {str(e)}"
        print(f"âŒ {error_msg}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªæŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’è¿”ã™
        return {
            "llm_analysis": f"âŒ LLMåˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ: {error_msg}",
            "extracted_data": extracted_data,
            "summary": {
                "analysis_method": "extraction-only",
                "tables_identified": len(extracted_data["table_info"]),
                "total_filter_columns": len(extracted_data["filter_columns"]),
                "error": error_msg
            }
        }

def save_liquid_clustering_analysis(clustering_analysis: Dict[str, Any], output_dir: str = "/tmp") -> Dict[str, str]:
    """
    Liquid Clusteringåˆ†æçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
    """
    import os
    import json
    from datetime import datetime
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    json_path = f"{output_dir}/liquid_clustering_analysis_{timestamp}.json"
    markdown_path = f"{output_dir}/liquid_clustering_analysis_{timestamp}.md"
    sql_path = f"{output_dir}/liquid_clustering_implementation_{timestamp}.sql"
    
    file_paths = {}
    
    try:
        # 1. JSONå½¢å¼ã§ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        # setå‹ã‚’listå‹ã«å¤‰æ›ã—ã¦JSON serializable ã«ã™ã‚‹
        json_data = convert_sets_to_lists(clustering_analysis)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        file_paths['json'] = json_path
        print(f"âœ… Saved detailed data in JSON format: {json_path}")
        
        # 2. Save analysis report in Markdown format
        markdown_content = generate_liquid_clustering_markdown_report(clustering_analysis)
        
        with open(markdown_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        file_paths['markdown'] = markdown_path
        print(f"âœ… Saved analysis report in Markdown format: {markdown_path}")
        
        # 3. Generate SQL implementation examples file
        sql_content = generate_liquid_clustering_sql_implementations(clustering_analysis)
        
        with open(sql_path, 'w', encoding='utf-8') as f:
            f.write(sql_content)
        
        file_paths['sql'] = sql_path
        print(f"âœ… Saved SQL implementation examples: {sql_path}")
        
        return file_paths
        
    except Exception as e:
        error_msg = f"ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}"
        print(f"âŒ {error_msg}")
        return {"error": error_msg}

def generate_liquid_clustering_markdown_report(clustering_analysis: Dict[str, Any]) -> str:
    """
    Liquid Clusteringåˆ†æçµæœã®Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    """
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # åŸºæœ¬æƒ…å ±ã®å–å¾—
    summary = clustering_analysis.get('summary', {})
    performance_context = clustering_analysis.get('performance_context', {})
    extracted_data = clustering_analysis.get('extracted_data', {})
    llm_analysis = clustering_analysis.get('llm_analysis', '')
    
    markdown_content = f"""# Liquid Clustering Analysis Report

**Generated Date**: {timestamp}  
**Analysis Method**: {summary.get('analysis_method', 'Unknown')}  
**LLM Provider**: {summary.get('llm_provider', 'Unknown')}

## ğŸ“Š Performance Overview

| Item | Value |
|------|-----|
| Execution Time | {performance_context.get('total_time_sec', 0):.1f} seconds |
| Data Read | {performance_context.get('read_gb', 0):.2f}GB |
| Output Rows | {performance_context.get('rows_produced', 0):,} rows |
| Read Rows | {performance_context.get('rows_read', 0):,} rows |
| Filter Rate | {performance_context.get('data_selectivity', 0):.4f} |

## ğŸ” Extracted Metadata

### Filter Conditions ({summary.get('total_filter_columns', 0)} items)
"""
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã®è©³ç´°
    filter_columns = extracted_data.get('filter_columns', [])
    for i, filter_item in enumerate(filter_columns[:10], 1):  # æœ€å¤§10å€‹ã¾ã§è¡¨ç¤º
        markdown_content += f"{i}. `{filter_item.get('expression', '')}` (ãƒãƒ¼ãƒ‰: {filter_item.get('node_name', '')})\n"
    
    if len(filter_columns) > 10:
        markdown_content += f"... ä»– {len(filter_columns) - 10}å€‹\n"
    
    markdown_content += f"""
### JOINæ¡ä»¶ ({summary.get('total_join_columns', 0)}å€‹)
"""
    
    # JOINæ¡ä»¶ã®è©³ç´°
    join_columns = extracted_data.get('join_columns', [])
    for i, join_item in enumerate(join_columns[:10], 1):
        markdown_content += f"{i}. `{join_item.get('expression', '')}` ({join_item.get('key_type', '')})\n"
    
    if len(join_columns) > 10:
        markdown_content += f"... ä»– {len(join_columns) - 10}å€‹\n"
    
    markdown_content += f"""
### GROUP BYæ¡ä»¶ ({summary.get('total_groupby_columns', 0)}å€‹)
"""
    
    # GROUP BYæ¡ä»¶ã®è©³ç´°
    groupby_columns = extracted_data.get('groupby_columns', [])
    for i, groupby_item in enumerate(groupby_columns[:10], 1):
        markdown_content += f"{i}. `{groupby_item.get('expression', '')}` (ãƒãƒ¼ãƒ‰: {groupby_item.get('node_name', '')})\n"
    
    if len(groupby_columns) > 10:
        markdown_content += f"... ä»– {len(groupby_columns) - 10}å€‹\n"
    
    markdown_content += f"""
### é›†ç´„é–¢æ•° ({summary.get('total_aggregate_columns', 0)}å€‹)
"""
    
    # é›†ç´„é–¢æ•°ã®è©³ç´°
    aggregate_columns = extracted_data.get('aggregate_columns', [])
    for i, agg_item in enumerate(aggregate_columns[:10], 1):
        markdown_content += f"{i}. `{agg_item.get('expression', '')}` (ãƒãƒ¼ãƒ‰: {agg_item.get('node_name', '')})\n"
    
    if len(aggregate_columns) > 10:
        markdown_content += f"... ä»– {len(aggregate_columns) - 10}å€‹\n"
    
    markdown_content += f"""
## ğŸ·ï¸ è­˜åˆ¥ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ« ({summary.get('tables_identified', 0)}å€‹)

"""
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®è©³ç´°ï¼ˆç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã‚’å«ã‚€ï¼‰
    table_info = extracted_data.get('table_info', {})
    for table_name, table_details in table_info.items():
        current_keys = table_details.get('current_clustering_keys', [])
        current_keys_str = ', '.join(current_keys) if current_keys else 'è¨­å®šãªã—'
        markdown_content += f"- **{table_name}** (ãƒãƒ¼ãƒ‰: {table_details.get('node_name', '')})\n"
        markdown_content += f"  - ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: `{current_keys_str}`\n"
    
    markdown_content += f"""
## ğŸ” ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰åˆ†æ ({summary.get('scan_nodes_count', 0)}å€‹)

"""
    
    # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®è©³ç´°
    scan_nodes = extracted_data.get('scan_nodes', [])
    for scan in scan_nodes:
        efficiency = scan.get('rows', 0) / max(scan.get('duration_ms', 1), 1)
        markdown_content += f"- **{scan.get('name', '')}**: {scan.get('rows', 0):,}è¡Œ, {scan.get('duration_ms', 0):,}ms, åŠ¹ç‡={efficiency:.1f}è¡Œ/ms\n"
    
    markdown_content += f"""
## ğŸ¤– LLMåˆ†æçµæœ

{llm_analysis}

## ğŸ“‹ åˆ†æã‚µãƒãƒªãƒ¼

- **åˆ†æå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°**: {summary.get('tables_identified', 0)}
- **ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶æ•°**: {summary.get('total_filter_columns', 0)}
- **JOINæ¡ä»¶æ•°**: {summary.get('total_join_columns', 0)}
- **GROUP BYæ¡ä»¶æ•°**: {summary.get('total_groupby_columns', 0)}
- **é›†ç´„é–¢æ•°æ•°**: {summary.get('total_aggregate_columns', 0)}
- **ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰æ•°**: {summary.get('scan_nodes_count', 0)}

---
*Report generation time: {timestamp}*
"""
    
    return markdown_content

def generate_liquid_clustering_sql_implementations(clustering_analysis: Dict[str, Any]) -> str:
    """
    Generate SQL examples for Liquid Clustering implementation
    """
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # åŸºæœ¬æƒ…å ±ã®å–å¾—
    extracted_data = clustering_analysis.get('extracted_data', {})
    table_info = extracted_data.get('table_info', {})
    
    sql_content = f"""-- =====================================================
-- Liquid Clustering å®Ÿè£…SQLä¾‹
-- ç”Ÿæˆæ—¥æ™‚: {timestamp}
-- =====================================================

-- ã€é‡è¦ã€‘
-- ä»¥ä¸‹ã®SQLä¾‹ã¯åˆ†æçµæœã«åŸºã¥ãæ¨å¥¨äº‹é …ã§ã™ã€‚
-- å®Ÿéš›ã®å®Ÿè£…å‰ã«ã€ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã‚„ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

"""
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã®SQLå®Ÿè£…ä¾‹ã‚’ç”Ÿæˆ
    for table_name, table_details in table_info.items():
        # ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã‚’å–å¾—
        current_keys = table_details.get('current_clustering_keys', [])
        current_keys_str = ', '.join(current_keys) if current_keys else 'è¨­å®šãªã—'
        
        sql_content += f"""
-- =====================================================
-- ãƒ†ãƒ¼ãƒ–ãƒ«: {table_name}
-- ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: {current_keys_str}
-- =====================================================

-- æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã«Liquid Clusteringã‚’é©ç”¨ã™ã‚‹å ´åˆ:
-- ALTER TABLE {table_name} CLUSTER BY (column1, column2, column3, column4);

-- æ–°è¦ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆæ™‚ã«Liquid Clusteringã‚’è¨­å®šã™ã‚‹å ´åˆ:
-- CREATE TABLE {table_name}_clustered
-- CLUSTER BY (column1, column2, column3, column4)
-- AS SELECT * FROM {table_name};

-- Delta Live Tablesã§ã®è¨­å®šä¾‹:
-- @dlt.table(
--   cluster_by=["column1", "column2", "column3", "column4"]
-- )
-- def {table_name.split('.')[-1]}_clustered():
--   return spark.table("{table_name}")

-- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çŠ¶æ³ã®ç¢ºèª:
-- DESCRIBE DETAIL {table_name};

-- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµ±è¨ˆã®ç¢ºèª:
-- ANALYZE TABLE {table_name} COMPUTE STATISTICS FOR ALL COLUMNS;

"""
    
    sql_content += f"""
-- =====================================================
-- ä¸€èˆ¬çš„ãªLiquid Clusteringå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³
-- =====================================================

-- ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é »åº¦ã®é«˜ã„ã‚«ãƒ©ãƒ ã‚’å„ªå…ˆ
-- æ¨å¥¨é †åº: 1) ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚«ãƒ©ãƒ  2) JOINæ¡ä»¶ã‚«ãƒ©ãƒ  3) GROUP BYã‚«ãƒ©ãƒ 

-- ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã‚’è€ƒæ…®ã—ãŸé †åº
-- ä½ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ â†’ é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã®é †ã§é…ç½®

-- ãƒ‘ã‚¿ãƒ¼ãƒ³3: ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãé…ç½®
-- ã‚ˆãä¸€ç·’ã«ä½¿ç”¨ã•ã‚Œã‚‹ã‚«ãƒ©ãƒ ã‚’è¿‘ã„ä½ç½®ã«é…ç½®

-- =====================================================
-- å®Ÿè£…å¾Œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼SQL
-- =====================================================

-- 1. ã‚¯ã‚¨ãƒªå®Ÿè¡Œè¨ˆç”»ã®ç¢ºèª
-- EXPLAIN SELECT ... FROM table_name WHERE ...;

-- 2. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒƒãƒ—çµ±è¨ˆã®ç¢ºèª
-- SELECT * FROM table_name WHERE filter_column = 'value';
-- -- SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒƒãƒ—æ•°ã‚’ç¢ºèª

-- 3. ãƒ‡ãƒ¼ã‚¿é…ç½®ã®ç¢ºèª
-- SELECT 
--   file_path,
--   count(*) as row_count,
--   min(cluster_column1) as min_val,
--   max(cluster_column1) as max_val
-- FROM table_name
-- GROUP BY file_path
-- ORDER BY file_path;

-- =====================================================
-- æ³¨æ„äº‹é …
-- =====================================================

-- 1. Liquid Clusteringã¯æœ€å¤§4ã‚«ãƒ©ãƒ ã¾ã§æŒ‡å®šå¯èƒ½
-- 2. ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã¨ã¯ä½µç”¨ä¸å¯
-- 3. æ—¢å­˜ã®ZORDER BYã¯è‡ªå‹•çš„ã«ç„¡åŠ¹åŒ–ã•ã‚Œã‚‹
-- 4. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®åŠ¹æœã¯æ™‚é–“ã¨ã¨ã‚‚ã«å‘ä¸Šã™ã‚‹ï¼ˆOPTIMIZEå®Ÿè¡Œã§æœ€é©åŒ–ï¼‰
-- 5. å®šæœŸçš„ãªOPTIMIZEå®Ÿè¡Œã‚’æ¨å¥¨
-- 6. **é‡è¦**: ã‚«ãƒ©ãƒ ã®æŒ‡å®šé †åºã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å½±éŸ¿ã—ã¾ã›ã‚“
--    * CLUSTER BY (col1, col2, col3) ã¨ CLUSTER BY (col3, col1, col2) ã¯åŒç­‰
--    * å¾“æ¥ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã‚„Z-ORDERã¨ã¯ç•°ãªã‚‹é‡è¦ãªç‰¹æ€§

-- OPTIMIZEå®Ÿè¡Œä¾‹:
-- OPTIMIZE table_name;

-- =====================================================
-- ç”Ÿæˆæƒ…å ±
-- =====================================================
-- ç”Ÿæˆæ—¥æ™‚: {timestamp}
-- åˆ†æå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(table_info)}
-- åŸºã¥ã„ãŸåˆ†æ: LLMã«ã‚ˆã‚‹Liquid Clusteringåˆ†æ
"""
    
    return sql_content

print("âœ… Function definition completed: analyze_liquid_clustering_opportunities, save_liquid_clustering_analysis")

# COMMAND ----------

def translate_explain_summary_to_english(explain_content: str) -> str:
    """
    EXPLAINè¦ç´„ãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¥æœ¬èªéƒ¨åˆ†ã‚’è‹±èªã«ç¿»è¨³
    
    Args:
        explain_content: EXPLAINè¦ç´„ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹
    
    Returns:
        str: è‹±èªç‰ˆEXPLAINè¦ç´„
    """
    # æ—¥æœ¬èªã‹ã‚‰è‹±èªã¸ã®ç¿»è¨³ãƒãƒƒãƒ”ãƒ³ã‚°
    translation_map = {
        # ãƒ˜ãƒƒãƒ€ãƒ¼éƒ¨åˆ†
        "# EXPLAIN + EXPLAIN COSTè¦ç´„çµæœ (optimized)": "# EXPLAIN + EXPLAIN COST Summary Results (optimized)",
        "## ğŸ“Š åŸºæœ¬æƒ…å ±": "## ğŸ“Š Basic Information", 
        "ç”Ÿæˆæ—¥æ™‚": "Generated",
        "ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—": "Query Type",
        "å…ƒã‚µã‚¤ã‚º": "Original Size",
        "è¦ç´„å¾Œã‚µã‚¤ã‚º": "Summary Size",
        "åœ§ç¸®ç‡": "Compression Ratio",
        "æ–‡å­—": "characters",
        
        # LLMè¦ç´„çµæœ
        "## ğŸ§  LLMè¦ç´„çµæœ": "## ğŸ§  LLM Summary Results",
        "# Databricks SQLã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ": "# Databricks SQL Query Performance Analysis",
        "## ğŸ“Š Physical Planè¦ç´„": "## ğŸ“Š Physical Plan Summary",
        "### ä¸»è¦ãªå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—": "### Key Processing Steps",
        "è¤‡æ•°ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—": "Data retrieval from multiple tables",
        "ã‚µãƒ–ã‚¯ã‚¨ãƒªå®Ÿè¡Œ": "Subquery execution",
        "å¹³å‡å£²ä¸Šã‚’è¨ˆç®—ã™ã‚‹ã‚µãƒ–ã‚¯ã‚¨ãƒª": "Subquery calculating average sales",
        "ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°": "Filtering",
        "å¹³å‡å£²ä¸Šã‚’è¶…ãˆã‚‹å•†å“ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°": "Filtering products exceeding average sales",
        "é›†è¨ˆå‡¦ç†": "Aggregation processing",
        "ãƒ–ãƒ©ãƒ³ãƒ‰ã€ã‚¯ãƒ©ã‚¹ã€ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®å£²ä¸Šé›†è¨ˆ": "Sales aggregation by brand, class, category",
        "JOINå‡¦ç†": "JOIN processing",
        "è¤‡æ•°ã®JOINæ“ä½œ": "Multiple JOIN operations",
        "ãŒå¤šç”¨": "is frequently used",
        "ã‚½ãƒ¼ãƒˆ": "Sorting",
        "ã§ã®ã‚½ãƒ¼ãƒˆ": "sorting by",
        "æœ€çµ‚çµæœã‚’": "Final results to",
        "è¡Œã«åˆ¶é™": "rows limit",
        
        # JOINæ–¹å¼ã¨ãƒ‡ãƒ¼ã‚¿ç§»å‹•
        "### JOINæ–¹å¼ã¨ãƒ‡ãƒ¼ã‚¿ç§»å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³": "### JOIN Methods and Data Movement Patterns",
        "ä¸»è¦JOINæ–¹å¼": "Primary JOIN Method",
        "ãƒ‡ãƒ¼ã‚¿ç§»å‹•": "Data Movement",
        "ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿ç§»å‹•": "for efficient data movement",
        "ã«ã‚ˆã‚‹é›†ç´„å‡¦ç†": "for aggregation processing",
        "ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿åˆ†æ•£": "for data distribution",
        "ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³": "partitions",
        
        # Photonåˆ©ç”¨çŠ¶æ³
        "### Photonåˆ©ç”¨çŠ¶æ³": "### Photon Usage Status",
        "é«˜åº¦ãªPhotonæ´»ç”¨": "Advanced Photon utilization",
        "ãªã©å¤šæ•°ã®Photonæœ€é©åŒ–æ¼”ç®—å­ã‚’ä½¿ç”¨": "and many other Photon optimization operators in use",
        "å®Ÿè¡Œæ™‚ã®æœ€é©åŒ–ãŒæœ‰åŠ¹": "Runtime optimization enabled",
        
        # çµ±è¨ˆæƒ…å ±ã‚µãƒãƒªãƒ¼
        "## ğŸ’° çµ±è¨ˆæƒ…å ±ã‚µãƒãƒªãƒ¼": "## ğŸ’° Statistics Summary",
        "### ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºã¨è¡Œæ•°": "### Table Size and Row Count",
        "ç´„": "approximately",
        "å„„è¡Œ": "billion rows",
        "æœ€çµ‚çµæœã‚»ãƒƒãƒˆ": "Final result set",
        "é©ç”¨å¾Œ": "after application",
        "ä¸­é–“çµæœ": "Intermediate results",
        "ä¸‡è¡Œ": "thousand rows",
        "ã‚½ãƒ¼ãƒˆå‰": "before sorting",
        
        # JOINé¸æŠç‡ã¨ãƒ•ã‚£ãƒ«ã‚¿åŠ¹ç‡
        "### JOINé¸æŠç‡ã¨ãƒ•ã‚£ãƒ«ã‚¿åŠ¹ç‡": "### JOIN Selectivity and Filter Efficiency",
        "ãƒ•ã‚£ãƒ«ã‚¿": "filter",
        "å¹´åº¦æ¡ä»¶": "Year condition",
        "ã«ã‚ˆã‚Šã€": "resulted in",
        "è¡Œã«çµã‚Šè¾¼ã¿": "rows filtered",
        "é«˜åŠ¹ç‡": "high efficiency",
        "ã‚µãƒ–ã‚¯ã‚¨ãƒªçµæœ": "Subquery result",
        "å¹³å‡å£²ä¸Šè¨ˆç®—ã®ã‚µãƒ–ã‚¯ã‚¨ãƒªã¯å˜ä¸€è¡Œã‚’è¿”å´": "Average sales calculation subquery returns single row",
        "ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªãƒ•ã‚£ãƒ«ã‚¿": "Main query filter",
        "å¹³å‡å£²ä¸Šã‚’è¶…ãˆã‚‹å•†å“ã«çµã‚Šè¾¼ã¿": "Filtered to products exceeding average sales",
        "è¡Œã«å‰Šæ¸›": "rows reduced to",
        
        # ã‚«ãƒ©ãƒ çµ±è¨ˆ
        "### ã‚«ãƒ©ãƒ çµ±è¨ˆ": "### Column Statistics",
        "ç¨®é¡ã®ç•°ãªã‚‹å€¤": "distinct values",
        "ã®ç¯„å›²": "range",
        "æ•°é‡": "quantity",
        
        # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†æ•£çŠ¶æ³
        "### ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†æ•£çŠ¶æ³": "### Partition Distribution Status",
        "ãƒãƒƒã‚·ãƒ¥ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°": "Hash partitioning",
        "ã«åŸºã¥ã": "based on",
        "ã‚·ãƒ³ã‚°ãƒ«ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³": "Single partition",
        "é›†ç´„å‡¦ç†ã‚„æœ€çµ‚çµæœã®åé›†ã«ä½¿ç”¨": "Used for aggregation processing and final result collection",
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        "## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ": "## âš¡ Performance Analysis",
        "### å®Ÿè¡Œã‚³ã‚¹ãƒˆã®å†…è¨³": "### Execution Cost Breakdown",
        "æœ€ã‚‚ã‚³ã‚¹ãƒˆãŒé«˜ã„æ“ä½œ": "Most expensive operation",
        "ã‹ã‚‰ã®ã‚¹ã‚­ãƒ£ãƒ³": "table scan",
        "ã‚µãƒ–ã‚¯ã‚¨ãƒªã‚³ã‚¹ãƒˆ": "Subquery cost",
        "ã‹ã‚‰ã®UNION ALLå‡¦ç†": "UNION ALL processing from",
        "ã«ã‚ˆã‚‹é›†è¨ˆã‚³ã‚¹ãƒˆ": "aggregation cost by",
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ
        "### ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚Šãã†ãªæ“ä½œ": "### Operations Likely to Become Bottlenecks",
        "å¤§è¦æ¨¡ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³": "Large table scan",
        "ã®ã‚¹ã‚­ãƒ£ãƒ³ãŒæœ€å¤§ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯": "scan is the biggest bottleneck",
        "è¤‡æ•°ãƒ†ãƒ¼ãƒ–ãƒ«UNION": "Multiple table UNION",
        "ã§ã®3ã¤ã®è²©å£²ãƒ†ãƒ¼ãƒ–ãƒ«": "3 sales tables in",
        "ã®çµ±åˆ": "integration",
        "ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ": "Shuffle operations",
        "ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿å†åˆ†æ•£": "data redistribution by",
        
        # æœ€é©åŒ–ã®ä½™åœ°
        "### æœ€é©åŒ–ã®ä½™åœ°ãŒã‚ã‚‹ç®‡æ‰€": "### Areas with Optimization Potential",
        "ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°": "Partition pruning",
        "ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯åŠ¹æœçš„ã ãŒã€ã•ã‚‰ã«": "filtering is effective, but further",
        "ã®è²©å£²ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æœ€é©åŒ–ãŒå¯èƒ½": "sales table partition optimization is possible",
        "JOINé †åº": "JOIN order",
        "ã®é †åºæœ€é©åŒ–": "order optimization",
        "ãƒ•ã‚£ãƒ«ã‚¿ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³": "Filter pushdown",
        "ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãŒã€ã•ã‚‰ã«æœ€é©åŒ–ã®ä½™åœ°ã‚ã‚Š": "is used, but further optimization potential exists",
        "ã‚«ãƒ©ãƒ é¸æŠ": "Column selection",
        "å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿ã‚’æ—©æœŸã«é¸æŠã™ã‚‹ã“ã¨ã§ãƒ‡ãƒ¼ã‚¿ç§»å‹•é‡ã‚’å‰Šæ¸›å¯èƒ½": "Data movement can be reduced by early selection of only necessary columns",
        "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡": "Memory usage",
        "ã®ãƒ“ãƒ«ãƒ‰å´ã®ã‚µã‚¤ã‚ºæœ€é©åŒ–": "build-side size optimization for",
        
        # ç‰¹è¨˜äº‹é …
        "### ç‰¹è¨˜äº‹é …": "### Notable Points",
        "æ´»ç”¨": "utilization",
        "ã‚¯ã‚¨ãƒªå…¨ä½“ã§": "Throughout the query",
        "æœ€é©åŒ–ãŒåŠ¹æœçš„ã«é©ç”¨ã•ã‚Œã¦ã„ã‚‹": "optimization is effectively applied",
        "çµ±è¨ˆæƒ…å ±": "Statistical information",
        "ãŒé©åˆ‡ã«åé›†ã•ã‚Œã¦ãŠã‚Šã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®åˆ¤æ–­ã«è²¢çŒ®": "is properly collected and contributes to optimizer decisions",
        "å‹•çš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°": "Dynamic filtering",
        "ãŒé©ç”¨ã•ã‚Œã€ä¸è¦ãªãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚’å›é¿": "is applied to avoid unnecessary data reading",
        "ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–å®Ÿè¡Œ": "Adaptive execution",
        "ãŒæœ‰åŠ¹ã§ã€å®Ÿè¡Œæ™‚ã®æœ€é©åŒ–ãŒæœŸå¾…ã§ãã‚‹": "is enabled, runtime optimization can be expected",
        
        # çµè«–
        "ã“ã®ã‚¯ã‚¨ãƒªã¯è¤‡é›‘ãªJOINã¨é›†è¨ˆã‚’å«ã‚€ãŒ": "This query includes complex JOINs and aggregations, but",
        "ã®åŠ¹æœçš„ãªä½¿ç”¨ã«ã‚ˆã‚Šã€æ¯”è¼ƒçš„åŠ¹ç‡çš„ã«å®Ÿè¡Œã•ã‚Œã‚‹ã¨äºˆæ¸¬ã•ã‚Œã¾ã™": "effective use is expected to execute relatively efficiently",
        "æœ€å¤§ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯å¤§è¦æ¨¡ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¹ã‚­ãƒ£ãƒ³ã¨ãƒ‡ãƒ¼ã‚¿ç§»å‹•ã«ã‚ã‚Šã¾ã™": "The biggest bottlenecks are large table scans and data movement",
        
        # çµ±è¨ˆæƒ…å ±æŠ½å‡º
        "## ğŸ’° çµ±è¨ˆæƒ…å ±æŠ½å‡º": "## ğŸ’° Statistics Extraction",
        "## ğŸ“Š çµ±è¨ˆæƒ…å ±ã‚µãƒãƒªãƒ¼ï¼ˆç°¡æ½”ç‰ˆï¼‰": "## ğŸ“Š Statistics Summary (Concise Version)",
        "ç·çµ±è¨ˆé …ç›®æ•°": "Total statistics items",
        "å€‹": "items",
        "ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆ": "Table statistics", 
        "ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±": "Partition information",
        "### ğŸ¯ ä¸»è¦çµ±è¨ˆ": "### ğŸ¯ Key Statistics",
        "ğŸ“Š ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚º": "ğŸ“Š Table Size",
        "ğŸ’¡ è©³ç´°ãªçµ±è¨ˆæƒ…å ±ã¯": "ğŸ’¡ Detailed statistics available with",
        "ã§ç¢ºèªã§ãã¾ã™": "setting"
    }
    
    # ç¿»è¨³ã‚’é©ç”¨
    translated_content = explain_content
    for jp_text, en_text in translation_map.items():
        translated_content = translated_content.replace(jp_text, en_text)
    
    return translated_content

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¤– LLM-powered Bottleneck Analysis Function
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - LLM analysis formatting of extracted metrics
# MAGIC - Multiple LLM provider support (Databricks/OpenAI/Azure/Anthropic)
# MAGIC - Detailed analysis report generation in English
# MAGIC - Error handling and fallback analysis

# COMMAND ----------

def analyze_bottlenecks_with_llm(metrics: Dict[str, Any]) -> str:
    """
    Generate comprehensive performance analysis report
    Integrates information from Cell 33 (TOP10 processes), Cell 35 (Liquid Clustering), Cell 43 (integrated optimization execution)
    Also leverages EXPLAIN + EXPLAIN COST results for more precise analysis
    
    ğŸš¨ Important: Prevention of percentage calculation degradation
    - Using the sum of parallel execution node times as total time is strictly prohibited
    - Prioritize using overall_metrics.total_time_ms (wall-clock time)
    - Use maximum node time during fallback (not sum)
    """
    from datetime import datetime
    
    print("ğŸ“Š Generating comprehensive performance analysis report (EXPLAIN+EXPLAIN COST integration)...")
    
    # === EXPLAIN + EXPLAIN COSTçµæœã®èª­ã¿è¾¼ã¿ ===
    explain_content = ""
    explain_cost_content = ""
    physical_plan = ""
    photon_explanation = ""
    cost_statistics = ""
    
    explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
    if explain_enabled.upper() == 'Y':
        import glob
        import os
        
        print("ğŸ” For bottleneck analysis: Searching EXPLAIN + EXPLAIN COST result files...")
        
        # æœ€æ–°ã®EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        explain_original_files = glob.glob("output_explain_original_*.txt")
        explain_optimized_files = glob.glob("output_explain_optimized_*.txt")
        explain_files = explain_original_files if explain_original_files else explain_optimized_files
        
        if explain_files:
            latest_explain_file = max(explain_files, key=os.path.getctime)
            try:
                with open(latest_explain_file, 'r', encoding='utf-8') as f:
                    explain_content = f.read()
                    print(f"âœ… Loaded EXPLAIN results for bottleneck analysis: {latest_explain_file}")
                
                # Physical Planã®æŠ½å‡º
                if "== Physical Plan ==" in explain_content:
                    physical_plan_start = explain_content.find("== Physical Plan ==")
                    physical_plan_end = explain_content.find("== Photon", physical_plan_start)
                    if physical_plan_end == -1:
                        physical_plan_end = len(explain_content)
                    physical_plan = explain_content[physical_plan_start:physical_plan_end].strip()
                
                # Photon Explanationã®æŠ½å‡º
                if "== Photon Explanation ==" in explain_content:
                    photon_start = explain_content.find("== Photon Explanation ==")
                    photon_explanation = explain_content[photon_start:].strip()
                    
            except Exception as e:
                print(f"âš ï¸ Failed to load EXPLAIN results for bottleneck analysis: {str(e)}")
        
        # æœ€æ–°ã®EXPLAIN COSTçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        cost_original_files = glob.glob("output_explain_cost_original_*.txt")
        cost_optimized_files = glob.glob("output_explain_cost_optimized_*.txt")
        cost_files = cost_original_files if cost_original_files else cost_optimized_files
        
        if cost_files:
            latest_cost_file = max(cost_files, key=os.path.getctime)
            try:
                with open(latest_cost_file, 'r', encoding='utf-8') as f:
                    explain_cost_content = f.read()
                    print(f"ğŸ’° Loaded EXPLAIN COST results for bottleneck analysis: {latest_cost_file}")
                
                # çµ±è¨ˆæƒ…å ±ã®æŠ½å‡º
                cost_statistics = extract_cost_statistics_from_explain_cost(explain_cost_content)
                print(f"ğŸ“Š Extracted statistics for bottleneck analysis: {len(cost_statistics)} characters")
                    
            except Exception as e:
                print(f"âš ï¸ Failed to load EXPLAIN COST results for bottleneck analysis: {str(e)}")
        
        if not explain_files and not cost_files:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ãƒã‚§ãƒƒã‚¯
            old_explain_files = glob.glob("output_explain_plan_*.txt")
            if old_explain_files:
                latest_explain_file = max(old_explain_files, key=os.path.getctime)
                try:
                    with open(latest_explain_file, 'r', encoding='utf-8') as f:
                        explain_content = f.read()
                        print(f"âœ… Loaded legacy format EXPLAIN results: {latest_explain_file}")
                        
                    if "== Physical Plan ==" in explain_content:
                        physical_plan_start = explain_content.find("== Physical Plan ==")
                        physical_plan_end = explain_content.find("== Photon", physical_plan_start)
                        if physical_plan_end == -1:
                            physical_plan_end = len(explain_content)
                        physical_plan = explain_content[physical_plan_start:physical_plan_end].strip()
                        
                    if "== Photon Explanation ==" in explain_content:
                        photon_start = explain_content.find("== Photon Explanation ==")
                        photon_explanation = explain_content[photon_start:].strip()
                except Exception as e:
                    print(f"âš ï¸ Failed to load legacy format EXPLAIN results: {str(e)}")
            else:
                print("âš ï¸ Bottleneck analysis: EXPLAINãƒ»EXPLAIN COST result files not found")
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ™‚åˆ»
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # === 1. åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å–å¾— ===
    overall_metrics = metrics.get('overall_metrics', {})
    bottleneck_indicators = metrics.get('bottleneck_indicators', {})
    
    total_time_sec = overall_metrics.get('total_time_ms', 0) / 1000
    read_gb = overall_metrics.get('read_bytes', 0) / 1024 / 1024 / 1024
    cache_hit_ratio = bottleneck_indicators.get('cache_hit_ratio', 0) * 100
    data_selectivity = bottleneck_indicators.get('data_selectivity', 0) * 100
    
    # Photonæƒ…å ±
    photon_enabled = overall_metrics.get('photon_enabled', False)
    photon_utilization = min(overall_metrics.get('photon_utilization_ratio', 0) * 100, 100.0)
    
    # ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«æƒ…å ±
    shuffle_count = bottleneck_indicators.get('shuffle_operations_count', 0)
    has_shuffle_bottleneck = bottleneck_indicators.get('has_shuffle_bottleneck', False)
    has_low_parallelism = bottleneck_indicators.get('has_low_parallelism', False)
    low_parallelism_count = bottleneck_indicators.get('low_parallelism_stages_count', 0)
    
    # ã‚¹ãƒ”ãƒ«æƒ…å ±
    has_spill = bottleneck_indicators.get('has_spill', False)
    spill_bytes = bottleneck_indicators.get('spill_bytes', 0)
    spill_gb = spill_bytes / 1024 / 1024 / 1024 if spill_bytes > 0 else 0
    
    # ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºæƒ…å ±
    has_skew = bottleneck_indicators.get('has_skew', False)
    has_aqe_shuffle_skew_warning = bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False)
    
    # === 2. ã‚»ãƒ«33: TOP10ãƒ—ãƒ­ã‚»ã‚¹åˆ†ææƒ…å ±ã®å–å¾— ===
    # å…¨ãƒãƒ¼ãƒ‰ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ç”¨ï¼‰
    all_sorted_nodes = sorted(metrics['node_metrics'], 
                             key=lambda x: x['key_metrics'].get('durationMs', 0), 
                             reverse=True)
    
    # TOP5ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŠ½å‡ºç”¨
    sorted_nodes = all_sorted_nodes[:5]
    
    # ğŸš¨ é‡è¦: æ­£ã—ã„å…¨ä½“æ™‚é–“ã®è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
    # 1. overall_metrics.total_time_msã‚’å„ªå…ˆä½¿ç”¨ï¼ˆwall-clock timeï¼‰
    total_time_ms = overall_metrics.get('total_time_ms', 0)
    
    # ğŸš¨ ä¸¦åˆ—å®Ÿè¡Œå•é¡Œã®ä¿®æ­£: task_total_time_msã‚’å„ªå…ˆä½¿ç”¨
    # å€‹åˆ¥ãƒãƒ¼ãƒ‰æ™‚é–“ã¯ä¸¦åˆ—ã‚¿ã‚¹ã‚¯ã®ç´¯ç©æ™‚é–“ã®ãŸã‚ã€åŒã˜ãç´¯ç©æ™‚é–“ã§ã‚ã‚‹task_total_time_msã¨æ¯”è¼ƒ
    task_total_time_ms = overall_metrics.get('task_total_time_ms', 0)
    
    if task_total_time_ms > 0:
        total_time_ms = task_total_time_ms
        print(f"âœ… Debug: Parallel execution support - using task_total_time_ms: {total_time_ms:,} ms ({total_time_ms/3600000:.1f} hours)")
    elif total_time_ms <= 0:
        # execution_time_msã‚’æ¬¡ã®å„ªå…ˆåº¦ã§ä½¿ç”¨
        execution_time_ms = overall_metrics.get('execution_time_ms', 0)
        if execution_time_ms > 0:
            total_time_ms = execution_time_ms
            print(f"âš ï¸ Debug: task_total_time_ms unavailable, using execution_time_ms: {total_time_ms} ms")
        else:
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨ãƒãƒ¼ãƒ‰ã®åˆè¨ˆæ™‚é–“
            max_node_time = max([node['key_metrics'].get('durationMs', 0) for node in all_sorted_nodes], default=1)
            total_time_ms = int(max_node_time * 1.2)
            print(f"âš ï¸ Debug: Final fallback - using estimated time: {total_time_ms} ms")
    
    print(f"ğŸ“Š Debug: Total time used for percentage calculation: {total_time_ms:,} ms ({total_time_ms/1000:.1f} sec)")
    
    critical_processes = []
    for i, node in enumerate(sorted_nodes):
        duration_ms = node['key_metrics'].get('durationMs', 0)
        duration_sec = duration_ms / 1000
        
        # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ï¼ˆ100%ã‚’ä¸Šé™ã¨ã™ã‚‹ï¼‰
        percentage = min((duration_ms / max(total_time_ms, 1)) * 100, 100.0)
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®é‡è¦åº¦åˆ¤å®š
        severity = "CRITICAL" if duration_ms >= 10000 else "HIGH" if duration_ms >= 5000 else "MEDIUM"
        
        # æ„å‘³ã®ã‚ã‚‹ãƒãƒ¼ãƒ‰åã‚’å–å¾—
        node_name = get_meaningful_node_name(node, metrics)
        short_name = node_name[:80] + "..." if len(node_name) > 80 else node_name
        
        critical_processes.append({
            'rank': i + 1,
            'name': short_name,
            'duration_sec': duration_sec,
            'percentage': percentage,
            'severity': severity
        })
    
    # === 3. ã‚»ãƒ«35: Liquid Clusteringåˆ†ææƒ…å ±ã®å–å¾— ===
    liquid_analysis = metrics.get('liquid_clustering_analysis', {})
    extracted_data = liquid_analysis.get('extracted_data', {})
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±
    table_info = extracted_data.get('table_info', {})
    identified_tables = list(table_info.keys())[:5]  # TOP5ãƒ†ãƒ¼ãƒ–ãƒ«
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ»JOINãƒ»GROUP BYæƒ…å ±
    filter_columns = extracted_data.get('filter_columns', [])[:10]
    join_columns = extracted_data.get('join_columns', [])[:10]
    groupby_columns = extracted_data.get('groupby_columns', [])[:10]
    
    # === 4. ã‚»ãƒ«43: çµ±åˆæœ€é©åŒ–å‡¦ç†ã§ã®è©³ç´°ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã®å–å¾— ===
    try:
        detailed_bottleneck = extract_detailed_bottleneck_analysis(metrics)
    except Exception as e:
        print(f"âš ï¸ Error in detailed bottleneck analysis: {e}")
        detailed_bottleneck = {
            'top_bottleneck_nodes': [],
            'performance_recommendations': []
        }
    
    # === 5. åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ ===
    
    report_lines = []
    
    # ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚µãƒãƒªãƒ¼
    report_lines.append("# ğŸ“Š Databricks SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŒ…æ‹¬åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    report_lines.append(f"**ç”Ÿæˆæ—¥æ™‚**: {timestamp}")
    report_lines.append("")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦
    report_lines.append("## 1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦")
    report_lines.append("")
    report_lines.append("### ä¸»è¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™")
    report_lines.append("")
    report_lines.append("| æŒ‡æ¨™ | å€¤ | è©•ä¾¡ |")
    report_lines.append("|------|-----|------|")
    report_lines.append(f"| Execution Time | {total_time_sec:.1f}s | {'âœ… Good' if total_time_sec < 60 else 'âš ï¸ Needs Improvement'} |")
    report_lines.append(f"| Data Read | {read_gb:.2f}GB | {'âœ… Good' if read_gb < 10 else 'âš ï¸ Large Volume'} |")
    report_lines.append(f"| Photon Enabled | {'Yes' if photon_enabled else 'No'} | {'âœ… Good' if photon_enabled else 'âŒ Not Enabled'} |")
    report_lines.append(f"| Cache Efficiency | {cache_hit_ratio:.1f}% | {'âœ… Good' if cache_hit_ratio > 80 else 'âš ï¸ Needs Improvement'} |")
    report_lines.append(f"| Filter Rate | {data_selectivity:.1f}% | {'âœ… Good' if data_selectivity > 50 else 'âš ï¸ Check Filter Conditions'} |")
    report_lines.append(f"| Shuffle Operations | {shuffle_count} times | {'âœ… Good' if shuffle_count < 5 else 'âš ï¸ Many'} |")
    report_lines.append(f"| Spill Occurred | {'Yes' if has_spill else 'No'} | {'âŒ Problem' if has_spill else 'âœ… Good'} |")
    
    # ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã®åˆ¤å®š
    if has_skew:
        skew_status = "Detected & handled by AQE"
        skew_evaluation = "ğŸ”§ AQE handled"
    elif has_aqe_shuffle_skew_warning:
        skew_status = "Potential skew possibility"
        skew_evaluation = "âš ï¸ Improvement needed"
    else:
        skew_status = "Not detected"
        skew_evaluation = "âœ… Good"
    
    report_lines.append(f"| Skew Detection | {skew_status} | {skew_evaluation} |")
    report_lines.append("")
    
    # ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ
    report_lines.append("## 2. ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ")
    report_lines.append("")
    
    # Photonåˆ†æ
    photon_status = "æœ‰åŠ¹" if photon_enabled else "ç„¡åŠ¹"
    photon_recommendation = ""
    if not photon_enabled:
        photon_recommendation = " â†’ **Photonæœ‰åŠ¹åŒ–ã‚’å¼·ãæ¨å¥¨**"
    elif photon_utilization < 50:
        photon_recommendation = " â†’ **Photonåˆ©ç”¨ç‡å‘ä¸ŠãŒå¿…è¦**"
    elif photon_utilization < 80:
        photon_recommendation = " â†’ **Photonè¨­å®šã®æœ€é©åŒ–ã‚’æ¨å¥¨**"
    else:
        photon_recommendation = " â†’ **æœ€é©åŒ–æ¸ˆã¿**"
    
    report_lines.append("### Photonã‚¨ãƒ³ã‚¸ãƒ³")
    report_lines.append(f"- **çŠ¶æ…‹**: {photon_status} (åˆ©ç”¨ç‡: {photon_utilization:.1f}%){photon_recommendation}")
    report_lines.append("")
    
    # ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«åˆ†æ
    report_lines.append("### ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«")
    shuffle_status = "âŒ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚ã‚Š" if has_shuffle_bottleneck else "âœ… è‰¯å¥½"
    parallelism_status = "âŒ ä½ä¸¦åˆ—åº¦ã‚ã‚Š" if has_low_parallelism else "âœ… é©åˆ‡"
    
    report_lines.append(f"- **ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ**: {shuffle_count}å› ({shuffle_status})")
    report_lines.append(f"- **ä¸¦åˆ—åº¦**: {parallelism_status}")
    if has_low_parallelism:
        report_lines.append(f"  - ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸: {low_parallelism_count}å€‹")
    report_lines.append("")
    
    # ã‚¹ãƒ”ãƒ«åˆ†æ
    report_lines.append("### ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³")
    if has_spill:
        report_lines.append(f"- **ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«**: âŒ ç™ºç”Ÿä¸­ ({spill_gb:.2f}GB)")
        report_lines.append("  - **å¯¾å¿œå¿…è¦**: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šã®è¦‹ç›´ã—ã€ã‚¯ã‚¨ãƒªæœ€é©åŒ–")
    else:
        report_lines.append("- **ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«**: âœ… ãªã—")
    report_lines.append("")
    
    # TOP5 Processing Time Bottlenecks
    report_lines.append("## 3. TOP5 Processing Time Bottlenecks")
    report_lines.append("")
    
    for process in critical_processes:
        severity_icon = "ğŸ”´" if process['severity'] == "CRITICAL" else "ğŸŸ " if process['severity'] == "HIGH" else "ğŸŸ¡"
        report_lines.append(f"### {process['rank']}. {severity_icon} {process['name']}")
        report_lines.append(f"   - **Execution Time**: {process['duration_sec']:.1f}s ({process['percentage']:.1f}% of total)")
        report_lines.append(f"   - **Severity**: {process['severity']}")
        report_lines.append("")
    
    # Liquid Clustering Recommendations
    report_lines.append("## 4. Liquid Clustering Recommendations")
    report_lines.append("")
    
    if identified_tables:
        report_lines.append("### å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«")
        for i, table_name in enumerate(identified_tables, 1):
            # ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã‚’å–å¾—
            table_details = table_info.get(table_name, {})
            current_keys = table_details.get('current_clustering_keys', [])
            current_keys_str = ', '.join(current_keys) if current_keys else 'è¨­å®šãªã—'
            
            report_lines.append(f"{i}. `{table_name}`")
            report_lines.append(f"   - ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: `{current_keys_str}`")
        report_lines.append("")
    
    if filter_columns or join_columns or groupby_columns:
        report_lines.append("### æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼")
        
        if filter_columns:
            report_lines.append("**ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚«ãƒ©ãƒ  (é«˜å„ªå…ˆåº¦)**:")
            for i, col in enumerate(filter_columns[:5], 1):
                expression = col.get('expression', 'Unknown')
                report_lines.append(f"  {i}. `{expression}`")
            report_lines.append("")
        
        if join_columns:
            report_lines.append("**JOINæ¡ä»¶ã‚«ãƒ©ãƒ  (ä¸­å„ªå…ˆåº¦)**:")
            for i, col in enumerate(join_columns[:5], 1):
                expression = col.get('expression', 'Unknown')
                key_type = col.get('key_type', '')
                report_lines.append(f"  {i}. `{expression}` ({key_type})")
            report_lines.append("")
        
        if groupby_columns:
            report_lines.append("**GROUP BYæ¡ä»¶ã‚«ãƒ©ãƒ  (ä¸­å„ªå…ˆåº¦)**:")
            for i, col in enumerate(groupby_columns[:5], 1):
                expression = col.get('expression', 'Unknown')
                report_lines.append(f"  {i}. `{expression}`")
            report_lines.append("")
    
    # å®Ÿè£…SQLä¾‹
    if identified_tables:
        report_lines.append("### å®Ÿè£…SQLä¾‹")
        for table_name in identified_tables[:2]:  # TOP2ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿
            # ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã‚’å–å¾—
            table_details = table_info.get(table_name, {})
            current_keys = table_details.get('current_clustering_keys', [])
            current_keys_str = ', '.join(current_keys) if current_keys else 'è¨­å®šãªã—'
            
            report_lines.append(f"```sql")
            report_lines.append(f"-- {table_name}ãƒ†ãƒ¼ãƒ–ãƒ«ã«Liquid Clusteringã‚’é©ç”¨")
            report_lines.append(f"-- ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: {current_keys_str}")
            report_lines.append(f"ALTER TABLE {table_name}")
            report_lines.append(f"CLUSTER BY (column1, column2, column3, column4);")
            report_lines.append(f"```")
            report_lines.append("")
    
    # Optimization recommendation actions
    report_lines.append("## 5. Recommended Optimization Actions")
    report_lines.append("")
    
    # Priority-based recommendations
    high_priority_actions = []
    medium_priority_actions = []
    low_priority_actions = []
    
    # CRITICAL/HIGH priority actions
    if not photon_enabled:
        high_priority_actions.append("**Enable Photon Engine** - Expected up to 50% performance improvement")
    
    if has_spill:
        high_priority_actions.append(f"**Resolve Memory Spill** - Eliminate {spill_gb:.2f}GB spill")
    
    if has_shuffle_bottleneck:
        high_priority_actions.append("**Shuffle Optimization** - JOIN order and REPARTITION application")
    
    # MEDIUM actions
    if photon_enabled and photon_utilization < 80:
        medium_priority_actions.append("**Improve Photon Utilization** - Configuration optimization")
    
    if has_low_parallelism:
        medium_priority_actions.append("**Improve Parallelism** - Cluster configuration review")
    
    if cache_hit_ratio < 50:
        medium_priority_actions.append("**Improve Cache Efficiency** - Data access pattern optimization")
    
    # Liquid Clustering
    if identified_tables:
        medium_priority_actions.append("**Implement Liquid Clustering** - Clustering of key tables")
    
    # LOW actions
    if data_selectivity < 50:
        low_priority_actions.append("**WHERE Clause Optimization** - Improve filter efficiency")
    
    # Action output
    if high_priority_actions:
        report_lines.append("### ğŸš¨ Urgent Response (HIGH Priority)")
        for i, action in enumerate(high_priority_actions, 1):
            report_lines.append(f"{i}. {action}")
        report_lines.append("")
    
    if medium_priority_actions:
        report_lines.append("### âš ï¸ Important Improvements (MEDIUM Priority)")
        for i, action in enumerate(medium_priority_actions, 1):
            report_lines.append(f"{i}. {action}")
        report_lines.append("")
    
    if low_priority_actions:
        report_lines.append("### ğŸ“ Long-term Optimization (LOW Priority)")
        for i, action in enumerate(low_priority_actions, 1):
            report_lines.append(f"{i}. {action}")
        report_lines.append("")
    
    # Expected effects
    report_lines.append("## 6. Expected Performance Improvements")
    report_lines.append("")
    
    total_improvement_estimate = 0
    improvement_details = []
    
    if not photon_enabled:
        total_improvement_estimate += 40
        improvement_details.append("- **Photon Activation**: 30-50% execution time reduction")
    
    if has_spill:
        total_improvement_estimate += 25
        improvement_details.append(f"- **Spill Resolution**: 20-30% execution time reduction ({spill_gb:.2f}GB spill reduction)")
    
    if has_shuffle_bottleneck:
        total_improvement_estimate += 20
        improvement_details.append("- **Shuffle Optimization**: 15-25% execution time reduction")
    
    if identified_tables:
        total_improvement_estimate += 15
        improvement_details.append("- **Liquid Clustering**: 10-20% execution time reduction")
    
    # Set upper limit for improvement effects
    total_improvement_estimate = min(total_improvement_estimate, 80)
    
    if improvement_details:
        for detail in improvement_details:
            report_lines.append(detail)
        report_lines.append("")
        report_lines.append(f"**Overall Improvement Estimate**: Up to {total_improvement_estimate}% execution time reduction")
    else:
        report_lines.append("Current performance is relatively good. Fine-tuning optimizations can expect 5-10% improvement.")
    
    # === Detailed analysis based on EXPLAIN + EXPLAIN COST results ===
    if explain_enabled.upper() == 'Y' and (physical_plan or cost_statistics):
        report_lines.append("")
        report_lines.append("## 6. EXPLAIN + EXPLAIN COST Detailed Analysis")
        report_lines.append("")
        
        if physical_plan:
            report_lines.append("### ğŸ” Physical Plan Analysis")
            report_lines.append("")
            
            # Extract important information from Physical Plan
            plan_analysis = []
            if "Exchange" in physical_plan:
                plan_analysis.append("- **Shuffle Operation Detected**: Potential data transfer bottleneck")
            if "BroadcastExchange" in physical_plan:
                plan_analysis.append("- **BROADCAST JOIN Applied**: Efficient distribution of small tables")
            if "HashAggregate" in physical_plan:
                plan_analysis.append("- **Hash Aggregation Processing**: Memory efficiency optimization is important")
            if "FileScan" in physical_plan:
                plan_analysis.append("- **File Scan Operation**: Check I/O efficiency and filter pushdown")
            if "SortMergeJoin" in physical_plan:
                plan_analysis.append("- **Sort Merge JOIN**: Large table joins, consider BROADCAST application")
            
            if plan_analysis:
                for analysis in plan_analysis:
                    report_lines.append(analysis)
            else:
                report_lines.append("- Physical Plan detailed information is available")
            report_lines.append("")
        
        if photon_explanation:
            report_lines.append("### ğŸš€ Photon Explanation Analysis")
            report_lines.append("")
            
            photon_analysis = []
            if "photon" in photon_explanation.lower():
                photon_analysis.append("- **Photon Processing Information**: Vectorized processing optimization details")
            if "unsupported" in photon_explanation.lower():
                photon_analysis.append("- **Unsupported Function Detected**: Opportunity to improve Photon utilization")
            if "compiled" in photon_explanation.lower():
                photon_analysis.append("- **Compilation Processing**: Runtime optimization application status")
            
            if photon_analysis:
                for analysis in photon_analysis:
                    report_lines.append(analysis)
            else:
                report_lines.append("- Photon execution detailed information is available")
            report_lines.append("")
        
        if cost_statistics:
            report_lines.append("### ğŸ’° EXPLAIN COST Statistical Analysis")
            report_lines.append("")
            
            # Extract important information from EXPLAIN COST statistics
            cost_analysis = []
            if "ã‚µã‚¤ã‚ºæƒ…å ±" in cost_statistics:
                cost_analysis.append("- **Table Size Statistics**: Improved BROADCAST judgment accuracy with accurate size information")
            if "è¡Œæ•°æƒ…å ±" in cost_statistics:
                cost_analysis.append("- **Row Count Statistics**: Partition number optimization and memory usage prediction")
            if "é¸æŠç‡æƒ…å ±" in cost_statistics:
                cost_analysis.append("- **Selectivity Statistics**: Filter efficiency optimization and WHERE condition order adjustment")
            if "ã‚³ã‚¹ãƒˆæƒ…å ±" in cost_statistics:
                cost_analysis.append("- **Cost Estimation**: JOIN strategy and access path selection optimization")
            if "ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±" in cost_statistics:
                cost_analysis.append("- **Partition Statistics**: Data distribution optimization and skew countermeasures")
            
            if cost_analysis:
                for analysis in cost_analysis:
                    report_lines.append(analysis)
                report_lines.append("")
                report_lines.append("**Benefits of Statistics-Based Optimization**:")
                report_lines.append("- Optimization based on actual statistics rather than guesswork")
                report_lines.append("- Proactive bottleneck prediction and spill avoidance")
                report_lines.append("- Optimal strategy selection through accurate cost estimation")
            else:
                report_lines.append("- EXPLAIN COST statistical information is available")
            report_lines.append("")
    elif explain_enabled.upper() == 'Y':
        report_lines.append("")
        report_lines.append("## 6. EXPLAIN Analysis")
        report_lines.append("")
        report_lines.append("âš ï¸ EXPLAINãƒ»EXPLAIN COST result files not found")
        report_lines.append("Statistics-based detailed analysis requires prior EXPLAIN execution")
        report_lines.append("")
    
    report_lines.append("")
    report_lines.append("---")
    report_lines.append(f"*Report generated: {timestamp} | Analysis engine: Databricks SQL Profiler + EXPLAIN integration*")
    
    print("âœ… Comprehensive performance analysis report (EXPLAIN+EXPLAIN COST integration) completed")
    
    return "\n".join(report_lines)


def _call_databricks_llm(prompt: str) -> str:
    """Call Databricks Model Serving API"""
    try:
        # Databricksãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—
        try:
            token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
        except Exception:
            token = os.environ.get('DATABRICKS_TOKEN')
            if not token:
                return "âŒ Failed to obtain Databricks token. Please set the environment variable DATABRICKS_TOKEN."
        
        # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹URLã®å–å¾—
        try:
            workspace_url = spark.conf.get("spark.databricks.workspaceUrl")
        except Exception:
            workspace_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get("browserHostName").get()
        
        config = LLM_CONFIG["databricks"]
        endpoint_url = f"https://{workspace_url}/serving-endpoints/{config['endpoint_name']}/invocations"
        
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        # æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ãªå ´åˆã¯è¿½åŠ 
        if config.get("thinking_enabled", False):
            payload["thinking"] = {
                "type": "enabled",
                "budget_tokens": config.get("thinking_budget_tokens", 65536)
            }
        
        # ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ï¼ˆSQLæœ€é©åŒ–ç”¨ã«å¢—å¼·ï¼‰
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    print(f"ğŸ”„ Retrying... (attempt {attempt + 1}/{max_retries})")
                
                response = requests.post(endpoint_url, headers=headers, json=payload, timeout=300)
                
                if response.status_code == 200:
                    result = response.json()
                    analysis_text = result.get('choices', [{}])[0].get('message', {}).get('content', '')
                    print("âœ… Bottleneck analysis completed")
                    return analysis_text
                else:
                    error_msg = f"API Error: Status code {response.status_code}"
                    if response.status_code == 400:
                        # 400ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯è©³ç´°ãªè§£æ±ºç­–ã‚’æä¾›
                        error_detail = response.text
                        if "maximum tokens" in error_detail.lower():
                            if attempt == max_retries - 1:
                                detailed_error = f"""âŒ {error_msg}

ğŸ”§ Token limit error solutions:
1. Reduce LLM_CONFIG["databricks"]["max_tokens"] to 65536 (64K)
2. Retry with simpler query
3. Perform manual SQL optimization
4. Split query and optimize incrementally

ğŸ’¡ Recommended settings:
LLM_CONFIG["databricks"]["max_tokens"] = 65536
LLM_CONFIG["databricks"]["thinking_budget_tokens"] = 32768

Detailed error: {error_detail}"""
                                print(detailed_error)
                                return detailed_error
                            else:
                                print(f"âš ï¸ {error_msg} (Token limit) - Retrying...")
                                continue
                    
                    if attempt == max_retries - 1:
                        print(f"âŒ {error_msg}\nResponse: {response.text}")
                        return f"{error_msg}\nResponse: {response.text}"
                    else:
                        print(f"âš ï¸ {error_msg} - Retrying...")
                        continue
                        
            except requests.exceptions.Timeout:
                if attempt == max_retries - 1:
                    timeout_msg = f"""â° Timeout Error: Databricks endpoint response did not complete within 300 seconds.

ğŸ”§ Solutions:
1. Check LLM endpoint operational status
2. Reduce prompt size
3. Use a higher performance model
4. Execute SQL optimization manually

ğŸ’¡ Recommended Actions:
- Check query complexity
- Scale up Databricks Model Serving endpoint
- Test execution with simpler queries"""
                    print(f"âŒ {timeout_msg}")
                    return timeout_msg
                else:
                    print(f"â° Timeout occurred (300 seconds) - Retrying... (attempt {attempt + 1}/{max_retries})")
                    continue
                    
    except Exception as e:
        return f"Databricks API call error: {str(e)}"

def _call_openai_llm(prompt: str) -> str:
    """Call OpenAI API"""
    try:
        config = LLM_CONFIG["openai"]
        api_key = config["api_key"] or os.environ.get('OPENAI_API_KEY')
        
        if not api_key:
            return "âŒ OpenAI API key is not configured. Please set LLM_CONFIG['openai']['api_key'] or environment variable OPENAI_API_KEY."
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": config["model"],
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        response = requests.post("https://api.openai.com/v1/chat/completions", 
                               headers=headers, json=payload, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result['choices'][0]['message']['content']
            print("âœ… OpenAI analysis completed")
            return analysis_text
        else:
            return f"OpenAI API Error: Status code {response.status_code}\n{response.text}"
            
    except Exception as e:
        return f"OpenAI API call error: {str(e)}"

def _call_azure_openai_llm(prompt: str) -> str:
    """Call Azure OpenAI API"""
    try:
        config = LLM_CONFIG["azure_openai"]
        api_key = config["api_key"] or os.environ.get('AZURE_OPENAI_API_KEY')
        
        if not api_key or not config["endpoint"] or not config["deployment_name"]:
            return "âŒ Azure OpenAI configuration is incomplete. Please set api_key, endpoint, and deployment_name."
        
        endpoint_url = f"{config['endpoint']}/openai/deployments/{config['deployment_name']}/chat/completions?api-version={config['api_version']}"
        
        headers = {
            "api-key": api_key,
            "Content-Type": "application/json"
        }
        
        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        response = requests.post(endpoint_url, headers=headers, json=payload, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result['choices'][0]['message']['content']
            print("âœ… Azure OpenAI analysis completed")
            return analysis_text
        else:
            return f"Azure OpenAI API Error: Status code {response.status_code}\n{response.text}"
            
    except Exception as e:
        return f"Azure OpenAI API call error: {str(e)}"

def _call_anthropic_llm(prompt: str) -> str:
    """Call Anthropic API"""
    try:
        config = LLM_CONFIG["anthropic"]
        api_key = config["api_key"] or os.environ.get('ANTHROPIC_API_KEY')
        
        if not api_key:
            return "âŒ Anthropic APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚LLM_CONFIG['anthropic']['api_key']ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ANTHROPIC_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        
        headers = {
            "x-api-key": api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        payload = {
            "model": config["model"],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"],
            "messages": [{"role": "user", "content": prompt}]
        }
        
        response = requests.post("https://api.anthropic.com/v1/messages", 
                               headers=headers, json=payload, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result['content'][0]['text']
            print("âœ… Anthropic analysis completed")
            return analysis_text
        else:
            return f"Anthropic API Error: Status code {response.status_code}\n{response.text}"
            
    except Exception as e:
        return f"Anthropic API call error: {str(e)}"

print("âœ… Function definition completed: analyze_bottlenecks_with_llm")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“‹ LLM Bottleneck Analysis Execution Preparation
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Verification and display of configured LLM provider
# MAGIC - Analysis start preparation and message display
# MAGIC - Stability improvement through prompt optimization

# COMMAND ----------

# LLMãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Ÿè¡Œã®æº–å‚™
provider = LLM_CONFIG["provider"]

print(f"\nğŸ¤– ã€Starting SQL bottleneck analysis with {provider.upper()} LLMã€‘")
print("=" * 80)

if provider == "databricks":
    endpoint = LLM_CONFIG["databricks"]["endpoint_name"]
    print(f"ğŸ”— Databricks Model Serving endpoint: {endpoint}")
    print("âš ï¸  Model Serving endpoint must be operational")
elif provider == "openai":
    model = LLM_CONFIG["openai"]["model"]
    print(f"ğŸ”— OpenAI model: {model}")
    print("âš ï¸  OpenAI API key is required")
elif provider == "azure_openai":
    deployment = LLM_CONFIG["azure_openai"]["deployment_name"]
    print(f"ğŸ¤– Starting Azure OpenAI ({deployment}) bottleneck analysis...")
    print("âš ï¸  Azure OpenAI API key and endpoint are required")
elif provider == "anthropic":
    model = LLM_CONFIG["anthropic"]["model"]
    print(f"ğŸ¤– Starting Anthropic ({model}) bottleneck analysis...")
    print("âš ï¸  Anthropic API key is required")

print("ğŸ“ Simplifying analysis prompts to reduce timeout risk...")
print()

# Check if extracted_metrics variable is defined
try:
    extracted_metrics
    print("âœ… extracted_metrics variable confirmed")
    analysis_result = analyze_bottlenecks_with_llm(extracted_metrics)
except NameError:
    print("âŒ extracted_metrics variable is not defined")
    print("âš ï¸ Please run Cell 12 (Performance metrics extraction) first")
    print("ğŸ“‹ Correct execution order: Cell 11 â†’ Cell 12 â†’ Cell 15")
    print("ğŸ”„ Setting default analysis results")
    analysis_result = """
ğŸ¤– LLMãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ

âŒ åˆ†æã«å¿…è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚

ğŸ“‹ è§£æ±ºæ–¹æ³•:
1. ã‚»ãƒ«11ã§JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
2. ã‚»ãƒ«12ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã™ã‚‹
3. ã“ã®ã‚»ãƒ«ï¼ˆã‚»ãƒ«15ï¼‰ã‚’å†å®Ÿè¡Œã™ã‚‹

âš ï¸ å…ˆã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºã‚’å®Œäº†ã—ã¦ã‹ã‚‰åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
"""
except Exception as e:
    print(f"âŒ Error occurred during LLM analysis: {str(e)}")
    analysis_result = f"LLM analysis error: {str(e)}"

# COMMAND ----------

# MAGIC %md
# MAGIC # ğŸš€ Query Profile Analysis Section
# MAGIC
# MAGIC **Main analysis processing starts from here**
# MAGIC
# MAGIC ğŸ“‹ **Execution Steps:**
# MAGIC 1. Execute all cells in the ğŸ”§ Configuration & Setup section above
# MAGIC 2. Run the following cells in order to perform analysis
# MAGIC 3. If errors occur, re-execute from the configuration section
# MAGIC
# MAGIC âš ï¸ **Important Notes:**
# MAGIC - Execute in order: ğŸ”§ Configuration & Setup â†’ ğŸš€ Main Processing â†’ ğŸ”§ SQL Optimization sections
# MAGIC - File path configuration must be done in the first cell
# MAGIC - Verify LLM endpoint configuration

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸš€ SQL Profiler JSON File Loading Execution
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - JSON file loading from configured file path
# MAGIC - File size and basic information display
# MAGIC - Error handling and processing stop control

# COMMAND ----------

print("=" * 80)
print("ğŸš€ Databricks SQL Profiler Analysis Tool")
print("=" * 80)
print(f"ğŸ“ Target analysis file: {JSON_FILE_PATH}")
print()

# File existence check
import os
if not os.path.exists(JSON_FILE_PATH):
    print("âŒ File not found:")
    print(f"   Specified path: {JSON_FILE_PATH}")
    print()
    print("ğŸ’¡ File path configuration hints:")
    print("   1. Set the correct path for JSON_FILE_PATH variable in Cell 2")
    print("   2. Available option examples:")
    print("      - /Volumes/main/base/mitsuhiro_vol/pre_tuning_plan_file.json")
    print("      - /Volumes/main/base/mitsuhiro_vol/nophoton.json")
    print("      - /Volumes/main/base/mitsuhiro_vol/POC1.json")
    print("   3. If file is in DBFS FileStore:")
    print("      - /FileStore/shared_uploads/your_username/filename.json")
    print("âš ï¸ Stopping processing.")
    raise RuntimeError(f"Specified file not found: {JSON_FILE_PATH}")

# Load SQL profiler JSON file
profiler_data = load_profiler_json(JSON_FILE_PATH)
if not profiler_data:
    print("âŒ Failed to load JSON file. Please check the file format.")
    print("âš ï¸ Stopping processing.")
    # dbutils.notebook.exit("File loading failed")  # Commented out for safety
    raise RuntimeError("Failed to load JSON file.")

print(f"âœ… Data loading completed")
print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“Š Performance Metrics Extraction and Overview Display
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Metrics extraction from profiler data
# MAGIC - Query basic information display
# MAGIC - Overall performance indicator calculation and display
# MAGIC - Liquid Clustering analysis result display

# COMMAND ----------

# ğŸ“Š Performance metrics extraction
extracted_metrics = extract_performance_metrics(profiler_data)
print("âœ… Performance metrics extracted")

# Display extracted metrics overview
print("\n" + "=" * 50)
print("ğŸ“ˆ Extracted Metrics Overview")
print("=" * 50)

query_info = extracted_metrics['query_info']
overall_metrics = extracted_metrics['overall_metrics']
bottleneck_indicators = extracted_metrics['bottleneck_indicators']

print(f"ğŸ†” Query ID: {query_info['query_id']}")
print(f"ğŸ“Š Status: {query_info['status']}")
print(f"ğŸ‘¤ Execution User: {query_info['user']}")
print(f"â±ï¸ Execution Time: {overall_metrics['total_time_ms']:,} ms ({overall_metrics['total_time_ms']/1000:.2f} sec)")
print(f"ğŸ’¾ Data Read: {overall_metrics['read_bytes']/1024/1024/1024:.2f} GB")
print(f"ğŸ“ˆ Output Rows: {overall_metrics['rows_produced_count']:,} rows")
print(f"ğŸ“‰ Read Rows: {overall_metrics['rows_read_count']:,} rows")
print(f"ğŸ¯ Filter Rate: {bottleneck_indicators.get('data_selectivity', 0):.4f} ({bottleneck_indicators.get('data_selectivity', 0)*100:.2f}%)")
print(f"ğŸ”§ Stage Count: {len(extracted_metrics['stage_metrics'])}")
print(f"ğŸ—ï¸ Node Count: {len(extracted_metrics['node_metrics'])}")

# Display Liquid Clustering analysis results
liquid_analysis = extracted_metrics['liquid_clustering_analysis']
liquid_summary = liquid_analysis.get('summary', {})
print(f"ğŸ—‚ï¸ Liquid Clustering Target Tables: {liquid_summary.get('tables_identified', 0)}")
print(f"ğŸ“Š High Impact Tables: {liquid_summary.get('high_impact_tables', 0)}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ” Bottleneck Indicator Details
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Photon engine usage and performance analysis
# MAGIC - Shuffle operations and parallelism issue detection
# MAGIC - Detailed display of various performance indicators

# COMMAND ----------

# ğŸ“‹ Detailed bottleneck indicator display
print("\n" + "=" * 50)
print("ğŸ” Bottleneck Indicator Details")
print("=" * 50)

# Photon-related indicators
photon_enabled = overall_metrics.get('photon_enabled', False)
photon_utilization_ratio = overall_metrics.get('photon_utilization_ratio', 0)
photon_utilization = min(photon_utilization_ratio * 100, 100.0)  # Limit to max 100%
photon_emoji = "âœ…" if photon_enabled and photon_utilization > 80 else "âš ï¸" if photon_enabled else "âŒ"

# Detailed information about utilization rate
if photon_enabled:
    photon_total_ms = overall_metrics.get('photon_total_time_ms', 0)
    task_total_ms = overall_metrics.get('task_total_time_ms', 0)
    print(f"{photon_emoji} Photon Engine: Enabled (Utilization: {photon_utilization:.1f}%)")
    print(f"   ğŸ“Š Photon Execution Time: {photon_total_ms:,} ms | Total Task Time: {task_total_ms:,} ms")
else:
    print(f"{photon_emoji} Photon Engine: Disabled")

# Parallelism and shuffle-related indicators
shuffle_count = bottleneck_indicators.get('shuffle_operations_count', 0)
has_shuffle_bottleneck = bottleneck_indicators.get('has_shuffle_bottleneck', False)
has_low_parallelism = bottleneck_indicators.get('has_low_parallelism', False)
low_parallelism_count = bottleneck_indicators.get('low_parallelism_stages_count', 0)

shuffle_emoji = "ğŸš¨" if has_shuffle_bottleneck else "âš ï¸" if shuffle_count > 5 else "âœ…"
print(f"{shuffle_emoji} Shuffle Operations: {shuffle_count} times ({'Bottleneck detected' if has_shuffle_bottleneck else 'Normal'})")

parallelism_emoji = "ğŸš¨" if has_low_parallelism else "âœ…"
print(f"{parallelism_emoji} Parallelism: {'Issues detected' if has_low_parallelism else 'Appropriate'} (Low parallelism stages: {low_parallelism_count})")

print()
print("ğŸ“Š Other Indicators:")

for key, value in bottleneck_indicators.items():
    # Skip newly added indicators as they are already displayed above
    if key in ['shuffle_operations_count', 'has_shuffle_bottleneck', 'has_low_parallelism', 
               'low_parallelism_stages_count', 'total_shuffle_time_ms', 'shuffle_time_ratio',
               'slowest_shuffle_duration_ms', 'slowest_shuffle_node', 'low_parallelism_details',
               'average_low_parallelism']:
        continue
        
    if 'ratio' in key:
        emoji = "ğŸ“Š" if value < 0.1 else "âš ï¸" if value < 0.3 else "ğŸš¨"
        print(f"{emoji} {key}: {value:.3f} ({value*100:.1f}%)")
    elif 'bytes' in key and key != 'has_spill':
        if value > 0:
            emoji = "ğŸ’¾" if value < 1024*1024*1024 else "âš ï¸"  # Normal if under 1GB, caution if over
            print(f"{emoji} {key}: {value:,} bytes ({value/1024/1024:.2f} MB)")
    elif key == 'has_spill':
        emoji = "âŒ" if not value else "âš ï¸"
        print(f"{emoji} {key}: {'Yes' if value else 'No'}")
    elif 'duration' in key:
        emoji = "â±ï¸"
        print(f"{emoji} {key}: {value:,} ms ({value/1000:.2f} sec)")
    else:
        emoji = "â„¹ï¸"
        print(f"{emoji} {key}: {value}")

print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ’¾ Metrics Storage and Time Consumption Analysis
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Save extracted metrics in JSON format
# MAGIC - Convert set types to list types
# MAGIC - Detailed analysis of top 10 most time-consuming processes
# MAGIC - Specific metrics-based spill detection and AQE-based skew analysis
# MAGIC
# MAGIC ğŸ’¿ **Spill Detection Logic**:
# MAGIC - Target metric: `"Sink - Num bytes spilled to disk due to memory pressure"`
# MAGIC - Judgment condition: Spill detected when above metric value > 0
# MAGIC - Search targets: detailed_metrics â†’ raw_metrics â†’ key_metrics in order
# MAGIC
# MAGIC ğŸ¯ **Skew Detection Logic**:
# MAGIC - `AQEShuffleRead - Number of skewed partitions`: AQE-based skew detection
# MAGIC - Judgment condition: Skew detected when metric value > 0
# MAGIC - Importance: Judgment based on detected value
# MAGIC - Statistics-based judgment is deprecated (AQE-based judgment recommended)
# MAGIC
# MAGIC ğŸ’¡ **Debug Mode**: To display detailed spill/skew judgment basis
# MAGIC ```python
# MAGIC import os
# MAGIC os.environ['DEBUG_SPILL_ANALYSIS'] = 'true'   # Detailed display of specific metrics spill judgment
# MAGIC os.environ['DEBUG_SKEW_ANALYSIS'] = 'true'    # Detailed display of AQE-based skew judgment
# MAGIC ```

# COMMAND ----------

# ğŸ› Debug mode configuration (optional)
# 
# **Execute only when you want to display detailed spill/skew judgment basis**
# 
# ğŸ“‹ Configuration details:
# - DEBUG_SPILL_ANALYSIS=true: Display detailed basis for specific metrics spill judgment
# - DEBUG_SKEW_ANALYSIS=true: Display detailed basis for AQE-based skew judgment
# 
# ğŸ’¿ Spill debug display content:
# - Target metric: "Sink - Num bytes spilled to disk due to memory pressure"
# - Search results in each data source (detailed_metrics, raw_metrics, key_metrics)
# - Values and judgment results when metrics are found
# - List of other spill-related metrics (reference information)
# 
# ğŸ¯ Skew debug display content:
# - AQEShuffleRead - Number of skewed partitions metric value
# - Judgment basis for AQE-based skew detection
# - Number of detected skews and importance level
# - Statistics-based judgment is deprecated (AQE-based judgment recommended)

import os

# Uncomment to enable debug display for specific metrics spill analysis
# os.environ['DEBUG_SPILL_ANALYSIS'] = 'true'

# Uncomment to enable debug display for AQE-based skew analysis  
# os.environ['DEBUG_SKEW_ANALYSIS'] = 'true'

print("ğŸ› Debug mode configuration:")
print(f"   Specific metrics spill analysis debug: {os.environ.get('DEBUG_SPILL_ANALYSIS', 'false')}")
print(f"   AQE-based skew analysis debug: {os.environ.get('DEBUG_SKEW_ANALYSIS', 'false')}")
print("   â€» Setting to 'true' displays detailed judgment basis information")
print()
print("ğŸ’¿ Specific metrics spill detection criteria:")
print('   ğŸ¯ Target: "Sink - Num bytes spilled to disk due to memory pressure"')
print("   âœ… Judgment condition: Value > 0")
print()
print("ğŸ¯ AQE-based skew detection criteria:")
print("   ğŸ“Š AQEShuffleRead - Number of skewed partitions > 0")
print("   ğŸ“Š Judgment condition: Metric value > 0")
print("   ğŸ“Š Importance: Based on detected value")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸŒ Top 10 Most Time-Consuming Processes
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Saving extracted metrics in JSON format
# MAGIC - Converting set types to list types
# MAGIC - Detailed analysis of the top 10 most time-consuming processes
# MAGIC - Spill detection and data skew analysis
# MAGIC - Spark stage execution analysis

# COMMAND ----------

# ğŸ’¾ æŠ½å‡ºã—ãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã¯é™¤å¤–ï¼ˆä¸è¦ï¼‰
def format_thinking_response(response) -> str:
    """
    thinking_enabled: Trueã®å ´åˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’äººé–“ã«èª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›
    æ€è€ƒéç¨‹ï¼ˆthinkingï¼‰ã¨ã‚·ã‚°ãƒãƒãƒ£ï¼ˆsignatureï¼‰ç­‰ã®ä¸è¦ãªæƒ…å ±ã¯é™¤å¤–ã—ã€æœ€çµ‚çš„ãªçµè«–ã®ã¿ã‚’è¡¨ç¤º
    JSONæ§‹é€ ã‚„ä¸é©åˆ‡ãªæ–‡å­—åˆ—ã®éœ²å‡ºã‚’é˜²æ­¢
    """
    import re  # reãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
    
    if not isinstance(response, list):
        # ãƒªã‚¹ãƒˆã§ãªã„å ´åˆã¯æ–‡å­—åˆ—ã¨ã—ã¦å‡¦ç†ã—ã€ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        cleaned_text = clean_response_text(str(response))
        return cleaned_text
    
    # é™¤å¤–ã™ã¹ãã‚­ãƒ¼ã®ãƒªã‚¹ãƒˆï¼ˆæ‹¡å¼µï¼‰
    excluded_keys = {
        'thinking', 'signature', 'metadata', 'id', 'request_id', 
        'timestamp', 'uuid', 'reasoning', 'type', 'model'
    }
    
    formatted_parts = []
    
    for item in response:
        if isinstance(item, dict):
            # æœ€ã‚‚é©åˆ‡ãªãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            content = extract_best_content_from_dict(item, excluded_keys)
            if content:
                cleaned_content = clean_response_text(content)
                if is_valid_content(cleaned_content):
                    formatted_parts.append(cleaned_content)
        else:
            # è¾æ›¸ã§ãªã„å ´åˆã‚‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            cleaned_content = clean_response_text(str(item))
            if is_valid_content(cleaned_content):
                formatted_parts.append(cleaned_content)
    
    final_result = '\n'.join(formatted_parts)
    
    # æœ€çµ‚çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    final_result = final_quality_check(final_result)
    
    return final_result

def extract_best_content_from_dict(item_dict, excluded_keys):
    """Extract optimal content from dictionary"""
    # å„ªå…ˆé †ä½: text > summary_text > content > message > ãã®ä»–
    priority_keys = ['text', 'summary_text', 'content', 'message', 'response']
    
    for key in priority_keys:
        if key in item_dict and item_dict[key]:
            content = str(item_dict[key])
            # JSONæ§‹é€ ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
            if not looks_like_json_structure(content):
                return content
    
    # å„ªå…ˆã‚­ãƒ¼ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ä»–ã®ã‚­ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆé™¤å¤–ã‚­ãƒ¼ä»¥å¤–ï¼‰
    for key, value in item_dict.items():
        if key not in excluded_keys and value and isinstance(value, str):
            if not looks_like_json_structure(value):
                return value
    
    return None

def looks_like_json_structure(text):
    """Check if text contains JSON structure"""
    json_indicators = [
        "{'type':", '[{\'type\':', '{"type":', '[{"type":',
        "'text':", '"text":', "'summary_text':", '"summary_text":',
        'reasoning', 'metadata', 'signature'
    ]
    text_lower = text.lower()
    return any(indicator.lower() in text_lower for indicator in json_indicators)

def clean_response_text(text):
    """Clean up response text"""
    import re
    
    if not text or not isinstance(text, str):
        return ""
    
    # æ”¹è¡Œã‚³ãƒ¼ãƒ‰ã®æ­£è¦åŒ–
    text = text.replace('\\n', '\n').replace('\\t', '\t')
    
    # JSONæ§‹é€ ã®é™¤å»
    
    # å…¸å‹çš„ãªJSONæ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å»
    json_patterns = [
        r"'type':\s*'[^']*'",
        r'"type":\s*"[^"]*"',
        r"\[?\{'type':[^}]*\}[,\]]?",
        r'\[?\{"type":[^}]*\}[,\]]?',
        r"'reasoning':\s*\[[^\]]*\]",
        r'"reasoning":\s*\[[^\]]*\]',
        r"'signature':\s*'[A-Za-z0-9+/=]{50,}'",
        r'"signature":\s*"[A-Za-z0-9+/=]{50,}"'
    ]
    
    for pattern in json_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)
    
    # ä¸å®Œå…¨ãªJSONãƒ–ãƒ©ã‚±ãƒƒãƒˆã®é™¤å»
    text = re.sub(r'^\s*[\[\{]', '', text)  # å…ˆé ­ã® [ ã‚„ {
    text = re.sub(r'[\]\}]\s*$', '', text)  # æœ«å°¾ã® ] ã‚„ }
    text = re.sub(r'^\s*[,;]\s*', '', text)  # å…ˆé ­ã®ã‚«ãƒ³ãƒã‚„ã‚»ãƒŸã‚³ãƒ­ãƒ³
    
    # é€£ç¶šã™ã‚‹ç©ºç™½ãƒ»æ”¹è¡Œã®æ­£è¦åŒ–
    text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)  # 3ã¤ä»¥ä¸Šã®é€£ç¶šæ”¹è¡Œã‚’2ã¤ã«
    text = re.sub(r'[ \t]+', ' ', text)  # é€£ç¶šã™ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ãƒ»ã‚¿ãƒ–ã‚’1ã¤ã«
    
    # å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
    text = text.strip()
    
    return text

def is_valid_content(text):
    """Check if content is valid"""
    import re
    
    if not text or len(text.strip()) < 10:
        return False
    
    # ç„¡åŠ¹ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
    invalid_patterns = [
        r'^[{\[\'"]*$',  # JSONæ§‹é€ ã®ã¿
        r'^[,;:\s]*$',   # åŒºåˆ‡ã‚Šæ–‡å­—ã®ã¿
        r'^\s*reasoning\s*$',  # reasoningã®ã¿
        r'^\s*metadata\s*$',   # metadataã®ã¿
        r'^[A-Za-z0-9+/=]{50,}$',  # Base64ã£ã½ã„é•·ã„æ–‡å­—åˆ—
    ]
    
    for pattern in invalid_patterns:
        if re.match(pattern, text.strip(), re.IGNORECASE):
            return False
    
    return True

def final_quality_check(text):
    """Final quality check and cleanup"""
    import re  # reãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
    
    if not text:
        return "åˆ†æçµæœã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
    
    # è¨€èªã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå®‰å…¨ãªå¤‰æ•°ã‚¢ã‚¯ã‚»ã‚¹ï¼‰
    try:
        language = globals().get('OUTPUT_LANGUAGE', 'ja')  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ—¥æœ¬èª
    except:
        language = 'ja'
    
    if language == 'ja':
        text = ensure_japanese_consistency(text)
    elif language == 'en':
        text = ensure_english_consistency(text)
    
    # æœ€å°é™ã®é•·ã•ãƒã‚§ãƒƒã‚¯
    if len(text.strip()) < 20:
        if language == 'ja':
            return "åˆ†æçµæœãŒä¸å®Œå…¨ã§ã™ã€‚è©³ç´°ãªåˆ†æã‚’å®Ÿè¡Œä¸­ã§ã™ã€‚"
        else:
            return "Analysis result is incomplete. Detailed analysis in progress."
    
    return text

def ensure_japanese_consistency(text):
    """Ensure Japanese text consistency"""
    import re
    
    # æ˜ã‚‰ã‹ã«ç ´æã—ã¦ã„ã‚‹éƒ¨åˆ†ã‚’é™¤å»
    # ä¾‹: "æ­£caientify="predicate_liquid_referencet1" ã®ã‚ˆã†ãªç ´ææ–‡å­—åˆ—
    text = re.sub(r'[a-zA-Z0-9_="\']{20,}', '', text)
    
    # ä¸å®Œå…¨ãªãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®ä¿®æ­£
    text = re.sub(r'#\s*[^#\n]*["\'>]+[^#\n]*', '', text)  # ç ´æã—ãŸãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼
    
    # æ„å‘³ä¸æ˜ãªæ–‡å­—åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é™¤å»ï¼ˆæ‹¡å¼µï¼‰
    nonsense_patterns = [
        r'addressing_sales_column\d*',
        r'predicate_liquid_reference[a-zA-Z0-9]*',
        r'bottlenars\s+effect',
        r'å®Ÿè£…éä¿å­˜åœ¨',
        r'è£ç¥¨ã®end_by',
        r'riconsistall',
        r'caientify[a-zA-Z0-9="\']*',
        r'iving\s+[a-zA-Z0-9]*',
        r'o\s+Matteré…è³›',
        r'ubsãŒä½ã„åƒ®æ€§',
        r'åˆ°ç”°ãƒ‡ãƒ¼ã‚¿ã®æ–¹åŠ¹æ€§',
        r'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹.*topic.*é …è¡Œã«è€ƒ',
        r'ï¼»[^ï¼½]*ï¼½">[^<]*',  # ç ´æã—ãŸHTML/XMLè¦ç´ 
        r'\]\s*">\s*$'  # æ–‡æœ«ã®ç ´æã—ãŸã‚¿ã‚°
    ]
    
    for pattern in nonsense_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # é€£ç¶šã™ã‚‹è¨˜å·ã®é™¤å»
    text = re.sub(r'["\'>]{2,}', '', text)
    text = re.sub(r'[=\'"]{3,}', '', text)
    
    # ç ´æã—ãŸæ—¥æœ¬èªã®ä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³
    broken_japanese_patterns = [
        (r'ã®æ–¹æ³•å‹•çš„ãŒã‚‰', 'å‹•çš„ãªæ–¹æ³•ã§'),
        (r'æ€è€ƒã«æ²¿ã£ã¦é€²ã‚ã¦ã„ãã¾ã™ã€‚$', 'æ€è€ƒã«æ²¿ã£ã¦åˆ†æã‚’é€²ã‚ã¾ã™ã€‚'),
        (r'ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«æ²¿ã£ãŸæ”¹å–„ã‚’.*ã¾ã§ã—ã¦ã„ã‚‹ã®', 'ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«æ²¿ã£ãŸæ”¹å–„ææ¡ˆ'),
    ]
    
    for broken, fixed in broken_japanese_patterns:
        text = re.sub(broken, fixed, text, flags=re.IGNORECASE)
    
    # ç©ºè¡Œã®æ­£è¦åŒ–
    text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
    
    return text.strip()

def ensure_english_consistency(text):
    """Ensure English text consistency"""
    import re
    
    # åŒæ§˜ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’è‹±èªç”¨ã«å®Ÿè£…
    text = re.sub(r'[^\x00-\x7F\s]{10,}', '', text)  # éASCIIæ–‡å­—ã®é•·ã„é€£ç¶šã‚’é™¤å»
    
    return text.strip()

def extract_main_content_from_thinking_response(response) -> str:
    """
    thinkingå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆtextã¾ãŸã¯summary_textï¼‰ã®ã¿ã‚’æŠ½å‡º
    thinkingã€signatureç­‰ã®ä¸è¦ãªæƒ…å ±ã¯é™¤å¤–
    JSONæ§‹é€ ã‚„ç ´æã—ãŸãƒ†ã‚­ã‚¹ãƒˆã®æ··å…¥ã‚’é˜²æ­¢
    """
    if not isinstance(response, list):
        cleaned_text = clean_response_text(str(response))
        return final_quality_check(cleaned_text)
    
    # é™¤å¤–ã™ã¹ãã‚­ãƒ¼
    excluded_keys = {
        'thinking', 'signature', 'metadata', 'id', 'request_id', 
        'timestamp', 'uuid', 'reasoning', 'type', 'model'
    }
    
    for item in response:
        if isinstance(item, dict):
            # æœ€é©ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            content = extract_best_content_from_dict(item, excluded_keys)
            if content:
                cleaned_content = clean_response_text(content)
                if is_valid_content(cleaned_content):
                    return final_quality_check(cleaned_content)
    
    # ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…¨ä½“ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    return format_thinking_response(response)

def convert_sets_to_lists(obj):
    """Convert set types to list types for JSON serialization"""
    if isinstance(obj, set):
        return list(obj)
    elif isinstance(obj, dict):
        return {key: convert_sets_to_lists(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_sets_to_lists(item) for item in obj]
    else:
        return obj

# output_extracted_metrics ã®ç”Ÿæˆã¯é™¤å¤–ï¼ˆä¸è¦ï¼‰

# ğŸŒ Top 10 Most Time-Consuming Processes
print(f"\nğŸŒ Top 10 Most Time-Consuming Processes")
print("=" * 80)
print("ğŸ“Š Icon explanations: â±ï¸Time ğŸ’¾Memory ğŸ”¥ğŸŒParallelism ğŸ’¿Spill âš–ï¸Skew")
print('ğŸ’¿ Spill judgment: "Sink - Num bytes spilled to disk due to memory pressure" > 0')
print("ğŸ¯ Skew judgment: 'AQEShuffleRead - Number of skewed partitions' > 0")

# Sort nodes by execution time
sorted_nodes = sorted(extracted_metrics['node_metrics'], 
                     key=lambda x: x['key_metrics'].get('durationMs', 0), 
                     reverse=True)

# Process maximum 10 nodes
final_sorted_nodes = sorted_nodes[:10]

if final_sorted_nodes:
    # ğŸš¨ Important: Correct total time calculation (regression prevention)
    # 1. Get total execution time from overall_metrics (wall-clock time)
    overall_metrics = extracted_metrics.get('overall_metrics', {})
    total_duration = overall_metrics.get('total_time_ms', 0)
    
    # ğŸš¨ Fix parallel execution issue: Prioritize task_total_time_ms
    task_total_time_ms = overall_metrics.get('task_total_time_ms', 0)
    
    if task_total_time_ms > 0:
        total_duration = task_total_time_ms
        print(f"âœ… Console display: Parallel execution support - using task_total_time_ms: {total_duration:,} ms ({total_duration/3600000:.1f} hours)")
    elif total_duration <= 0:
        # Use execution_time_ms as next priority
        execution_time_ms = overall_metrics.get('execution_time_ms', 0)
        if execution_time_ms > 0:
            total_duration = execution_time_ms
            print(f"âš ï¸ Console display: task_total_time_ms unavailable, using execution_time_ms: {total_duration} ms")
        else:
            # Final fallback
            max_node_time = max([node['key_metrics'].get('durationMs', 0) for node in sorted_nodes], default=1)
            total_duration = int(max_node_time * 1.2)
            print(f"âš ï¸ Console display: Final fallback - using estimated time: {total_duration} ms")
    
    print(f"ğŸ“Š Cumulative task execution time (parallel): {total_duration:,} ms ({total_duration/3600000:.1f} hours)")
    print(f"ğŸ“ˆ TOP10 total time (parallel execution): {sum(node['key_metrics'].get('durationMs', 0) for node in final_sorted_nodes):,} ms")

    print()
    
    for i, node in enumerate(final_sorted_nodes):
        rows_num = node['key_metrics'].get('rowsNum', 0)
        duration_ms = node['key_metrics'].get('durationMs', 0)
        memory_mb = node['key_metrics'].get('peakMemoryBytes', 0) / 1024 / 1024
        
        # ğŸš¨ é‡è¦: æ­£ã—ã„ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
        # wall-clock timeã«å¯¾ã™ã‚‹å„ãƒãƒ¼ãƒ‰ã®å®Ÿè¡Œæ™‚é–“ã®å‰²åˆ
        time_percentage = min((duration_ms / max(total_duration, 1)) * 100, 100.0)
        
        # æ™‚é–“ã®é‡è¦åº¦ã«åŸºã¥ã„ã¦ã‚¢ã‚¤ã‚³ãƒ³ã‚’é¸æŠ
        if duration_ms >= 10000:  # 10ç§’ä»¥ä¸Š
            time_icon = "ï¿½"
            severity = "CRITICAL"
        elif duration_ms >= 5000:  # 5ç§’ä»¥ä¸Š
            time_icon = "ğŸŸ "
            severity = "HIGH"
        elif duration_ms >= 1000:  # 1ç§’ä»¥ä¸Š
            time_icon = "ğŸŸ¡"
            severity = "MEDIUM"
        else:
            time_icon = "ï¿½"
            severity = "LOW"
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ã‚¢ã‚¤ã‚³ãƒ³
        memory_icon = "ï¿½" if memory_mb < 100 else "âš ï¸" if memory_mb < 1000 else "ğŸš¨"
        
        # ã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹ãƒãƒ¼ãƒ‰åã‚’å–å¾—
        raw_node_name = node['name']
        node_name = get_meaningful_node_name(node, extracted_metrics)
        short_name = node_name[:100] + "..." if len(node_name) > 100 else node_name
        
        # ä¸¦åˆ—åº¦æƒ…å ±ã®å–å¾—ï¼ˆä¿®æ­£ç‰ˆ: è¤‡æ•°ã®Tasks totalãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ï¼‰
        parallelism_data = extract_parallelism_metrics(node)
        
        # å¾“æ¥ã®å˜ä¸€å€¤ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
        num_tasks = parallelism_data.get('tasks_total', 0)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Sink - Tasks totalã¾ãŸã¯Source - Tasks totalãŒã‚ã‚‹å ´åˆ
        if num_tasks == 0:
            if parallelism_data.get('sink_tasks_total', 0) > 0:
                num_tasks = parallelism_data.get('sink_tasks_total', 0)
            elif parallelism_data.get('source_tasks_total', 0) > 0:
                num_tasks = parallelism_data.get('source_tasks_total', 0)
        
        # ãƒ‡ã‚£ã‚¹ã‚¯ã‚¹ãƒ”ãƒ«ã‚¢ã‚¦ãƒˆã®æ¤œå‡ºï¼ˆãƒ¡ãƒ¢ãƒªãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ã«ã‚ˆã‚‹ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹å¯¾å¿œæ”¹å–„ç‰ˆï¼‰
        spill_detected = False
        spill_bytes = 0
        spill_details = []
        
        # ã‚¹ãƒ”ãƒ«æ¤œå‡ºã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹åãƒªã‚¹ãƒˆï¼ˆæ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®ã¿ï¼‰
        exact_spill_metrics = [
            "Num bytes spilled to disk due to memory pressure",
            "Sink - Num bytes spilled to disk due to memory pressure",
            "Sink/Num bytes spilled to disk due to memory pressure"
        ]
        
        # 1. detailed_metricsã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            metric_value = metric_info.get('value', 0)
            metric_label = metric_info.get('label', '')
            
            # æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§ã®ã¿ãƒãƒƒãƒãƒ³ã‚°
            if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                spill_detected = True
                spill_bytes = max(spill_bytes, metric_value)  # æœ€å¤§å€¤ã‚’ä½¿ç”¨
                spill_details.append({
                    'metric_name': metric_key,
                    'value': metric_value,
                    'label': metric_label,
                    'source': 'detailed_metrics',
                    'matched_field': 'key' if metric_key in exact_spill_metrics else 'label',
                    'matched_pattern': metric_key if metric_key in exact_spill_metrics else metric_label
                })
                break  # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½¿ç”¨
        
        # 2. detailed_metricsã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ç”Ÿãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢
        if not spill_detected:
            raw_metrics = node.get('metrics', [])
            for metric in raw_metrics:
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_value = metric.get('value', 0)
                
                # æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§ã®ã¿ãƒãƒƒãƒãƒ³ã‚°
                if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                    spill_detected = True
                    spill_bytes = max(spill_bytes, metric_value)  # æœ€å¤§å€¤ã‚’ä½¿ç”¨
                    spill_details.append({
                        'metric_name': metric_key,
                        'value': metric_value,
                        'label': metric_label,
                        'source': 'raw_metrics',
                        'matched_field': 'key' if metric_key in exact_spill_metrics else 'label',
                        'matched_pattern': metric_key if metric_key in exact_spill_metrics else metric_label
                    })
                    break  # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½¿ç”¨
        
        # 3. key_metricsã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢
        if not spill_detected:
            key_metrics = node.get('key_metrics', {})
            for exact_metric in exact_spill_metrics:
                if exact_metric in key_metrics and key_metrics[exact_metric] > 0:
                    spill_detected = True
                    spill_bytes = max(spill_bytes, key_metrics[exact_metric])  # æœ€å¤§å€¤ã‚’ä½¿ç”¨
                    spill_details.append({
                        'metric_name': f"key_metrics.{exact_metric}",
                        'value': key_metrics[exact_metric],
                        'label': f"Key metric: {exact_metric}",
                        'source': 'key_metrics',
                        'matched_field': 'key',
                        'matched_pattern': exact_metric
                    })
                    break
        
        # Data skew detection (AQE-based precise judgment)
        skew_detected = False
        skew_details = []
        skewed_partitions = 0  # Number of skewed partitions
        
        # AQE-based skew detection: "AQEShuffleRead - Number of skewed partitions" > 0
        target_aqe_metrics = [
            "AQEShuffleRead - Number of skewed partitions",
            "AQEShuffleRead - Number of skewed partition splits"
        ]
        
        aqe_skew_value = 0
        aqe_split_value = 0
        aqe_metric_name = ""
        aqe_split_metric_name = ""
        
        # 1. detailed_metricsã§æ¤œç´¢
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            if metric_key == "AQEShuffleRead - Number of skewed partitions":
                aqe_skew_value = metric_info.get('value', 0)
                aqe_metric_name = metric_key
            elif metric_key == "AQEShuffleRead - Number of skewed partition splits":
                aqe_split_value = metric_info.get('value', 0)
                aqe_split_metric_name = metric_key
            elif metric_info.get('label', '') == "AQEShuffleRead - Number of skewed partitions":
                aqe_skew_value = metric_info.get('value', 0)
                aqe_metric_name = metric_info.get('label', '')
            elif metric_info.get('label', '') == "AQEShuffleRead - Number of skewed partition splits":
                aqe_split_value = metric_info.get('value', 0)
                aqe_split_metric_name = metric_info.get('label', '')
        
        # 2. raw_metricsã§æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if aqe_skew_value == 0 or aqe_split_value == 0:
            raw_metrics = node.get('metrics', [])
            if isinstance(raw_metrics, list):
                for raw_metric in raw_metrics:
                    if isinstance(raw_metric, dict):
                        # 'label'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æœ€åˆã«ãƒã‚§ãƒƒã‚¯
                        raw_metric_label = raw_metric.get('label', '')
                        if raw_metric_label == "AQEShuffleRead - Number of skewed partitions" and aqe_skew_value == 0:
                            aqe_skew_value = raw_metric.get('value', 0)
                            aqe_metric_name = raw_metric_label
                        elif raw_metric_label == "AQEShuffleRead - Number of skewed partition splits" and aqe_split_value == 0:
                            aqe_split_value = raw_metric.get('value', 0)
                            aqe_split_metric_name = raw_metric_label
                        
                        # 'key'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚ãƒã‚§ãƒƒã‚¯
                        raw_metric_key = raw_metric.get('key', '')
                        if raw_metric_key == "AQEShuffleRead - Number of skewed partitions" and aqe_skew_value == 0:
                            aqe_skew_value = raw_metric.get('value', 0)
                            aqe_metric_name = raw_metric_key
                        elif raw_metric_key == "AQEShuffleRead - Number of skewed partition splits" and aqe_split_value == 0:
                            aqe_split_value = raw_metric.get('value', 0)
                            aqe_split_metric_name = raw_metric_key
                        
                        # 'metricName'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚ãƒã‚§ãƒƒã‚¯ï¼ˆå¾“æ¥ã®äº’æ›æ€§ï¼‰
                        raw_metric_name = raw_metric.get('metricName', '')
                        if raw_metric_name == "AQEShuffleRead - Number of skewed partitions" and aqe_skew_value == 0:
                            aqe_skew_value = raw_metric.get('value', 0)
                            aqe_metric_name = raw_metric_name
                        elif raw_metric_name == "AQEShuffleRead - Number of skewed partition splits" and aqe_split_value == 0:
                            aqe_split_value = raw_metric.get('value', 0)
                            aqe_split_metric_name = raw_metric_name
        
        # 3. key_metricsã§æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if aqe_skew_value == 0 or aqe_split_value == 0:
            key_metrics = node.get('key_metrics', {})
            for key_metric_name, key_metric_value in key_metrics.items():
                if "AQEShuffleRead - Number of skewed partitions" in key_metric_name and aqe_skew_value == 0:
                    aqe_skew_value = key_metric_value
                    aqe_metric_name = key_metric_name
                elif "AQEShuffleRead - Number of skewed partition splits" in key_metric_name and aqe_split_value == 0:
                    aqe_split_value = key_metric_value
                    aqe_split_metric_name = key_metric_name
        
        # AQE skew judgment
        if aqe_skew_value > 0:
            skew_detected = True
            skewed_partitions = aqe_skew_value  # Set number of skewed partitions
            severity_level = "High" if aqe_skew_value >= 5 else "Medium"
            
            # Basic AQE skew detection information
            description = f'AQE skew detected: {aqe_metric_name} = {aqe_skew_value} > threshold 0 [Importance:{severity_level}]'
            
            # Add detailed information if split value is also available
            if aqe_split_value > 0:
                description += f' | AQE detection details: Spark automatically detected {aqe_skew_value} skewed partitions'
                description += f' | AQE automatic handling: Spark automatically split into {aqe_split_value} partitions'
            
            skew_details.append({
                'type': 'aqe_skew',
                'value': aqe_skew_value,
                'split_value': aqe_split_value,
                'threshold': 0,
                'metric_name': aqe_metric_name,
                'split_metric_name': aqe_split_metric_name,
                'severity': severity_level,
                'description': description
            })
        
        # Use only AQE-based skew detection (spill-based judgment removed)
        # Reason: AQEShuffleRead - Number of skewed partitions is the accurate skew judgment standard
        
        # ä¸¦åˆ—åº¦ã‚¢ã‚¤ã‚³ãƒ³
        parallelism_icon = "ğŸ”¥" if num_tasks >= 10 else "âš ï¸" if num_tasks >= 5 else "ğŸŒ"
        # ã‚¹ãƒ”ãƒ«ã‚¢ã‚¤ã‚³ãƒ³
        spill_icon = "ğŸ’¿" if spill_detected else "âœ…"
        # ã‚¹ã‚­ãƒ¥ãƒ¼ã‚¢ã‚¤ã‚³ãƒ³
        skew_icon = "âš–ï¸" if skew_detected else "âœ…"
        
        print(f"{i+1:2d}. {time_icon}{memory_icon}{parallelism_icon}{spill_icon}{skew_icon} [{severity:8}] {short_name}")
        print(f"    â±ï¸  Execution time: {duration_ms:>8,} ms ({duration_ms/1000:>6.1f} sec) - {time_percentage:>5.1f}% of cumulative time")
        print(f"    ğŸ“Š Rows processed: {rows_num:>8,} rows")
        print(f"    ğŸ’¾ Peak memory: {memory_mb:>6.1f} MB")
        # Display multiple Tasks total metrics
        parallelism_display = []
        for task_metric in parallelism_data.get('all_tasks_metrics', []):
            parallelism_display.append(f"{task_metric['name']}: {task_metric['value']}")
        
        if parallelism_display:
            print(f"    ğŸ”§ Parallelism: {' | '.join(parallelism_display)}")
        else:
            print(f"    ğŸ”§ Parallelism: {num_tasks:>3d} tasks")
        
        # Skew judgment (considering both AQE skew detection and AQEShuffleRead average partition size)
        aqe_shuffle_skew_warning = parallelism_data.get('aqe_shuffle_skew_warning', False)
        
        if skew_detected:
            skew_status = "Detected & handled by AQE"
        elif aqe_shuffle_skew_warning:
            skew_status = "Potential skew possibility"
        else:
            skew_status = "None"
        
        print(f"    ğŸ’¿ Spill: {'Yes' if spill_detected else 'No'} | âš–ï¸ Skew: {skew_status}")
        
        # Display AQEShuffleRead metrics
        aqe_shuffle_metrics = parallelism_data.get('aqe_shuffle_metrics', [])
        if aqe_shuffle_metrics:
            aqe_display = []
            for aqe_metric in aqe_shuffle_metrics:
                if aqe_metric['name'] == "AQEShuffleRead - Number of partitions":
                    aqe_display.append(f"Partitions: {aqe_metric['value']}")
                elif aqe_metric['name'] == "AQEShuffleRead - Partition data size":
                    aqe_display.append(f"Data size: {aqe_metric['value']:,} bytes")
            
            if aqe_display:
                print(f"    ğŸ”„ AQEShuffleRead: {' | '.join(aqe_display)}")
                
                # Average partition size and warning display
                avg_partition_size = parallelism_data.get('aqe_shuffle_avg_partition_size', 0)
                if avg_partition_size > 0:
                    avg_size_mb = avg_partition_size / (1024 * 1024)
                    print(f"    ğŸ“Š Average partition size: {avg_size_mb:.2f} MB")
                    
                    # Warning when 512MB or more
                    if parallelism_data.get('aqe_shuffle_skew_warning', False):
                        print(f"    âš ï¸  ã€WARNINGã€‘ Average partition size exceeds 512MB - Potential skew possibility")
        
        # Calculate efficiency indicator (rows/sec)
        if duration_ms > 0:
            rows_per_sec = (rows_num * 1000) / duration_ms
            print(f"    ğŸš€ Processing efficiency: {rows_per_sec:>8,.0f} rows/sec")
        
# ãƒ•ã‚£ãƒ«ã‚¿ç‡è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ä»˜ãï¼‰
        filter_result = calculate_filter_rate(node)
        filter_display = format_filter_rate_display(filter_result)
        if filter_display:
            print(f"    {filter_display}")
        else:
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼šãªãœãƒ•ã‚£ãƒ«ã‚¿ç‡ãŒè¡¨ç¤ºã•ã‚Œãªã„ã‹ã‚’ç¢ºèª
            if filter_result["has_filter_metrics"]:
                print(f"    ğŸ“‚ Filter rate: {filter_result['filter_rate']:.1%} (read: {filter_result['files_read_bytes']/(1024*1024*1024):.2f}GB, pruned: {filter_result['files_pruned_bytes']/(1024*1024*1024):.2f}GB)")
            else:
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œç´¢ã®ãƒ‡ãƒãƒƒã‚°
                debug_info = []
                detailed_metrics = node.get('detailed_metrics', {})
                for metric_key, metric_info in detailed_metrics.items():
                    metric_label = metric_info.get('label', '')
                    if 'file' in metric_label.lower() and ('read' in metric_label.lower() or 'prun' in metric_label.lower()):
                        debug_info.append(f"{metric_label}: {metric_info.get('value', 0)}")
                
                if debug_info:
                    print(f"    ğŸ“‚ Filter-related metrics detected: {', '.join(debug_info[:2])}")
                # else:
                #     print(f"    ğŸ“‚ Filter rate: metrics not detected")
        
        # ã‚¹ãƒ”ãƒ«è©³ç´°æƒ…å ±ï¼ˆã‚·ãƒ³ãƒ—ãƒ«è¡¨ç¤ºï¼‰
        spill_display = ""
        if spill_detected and spill_bytes > 0:
            spill_mb = spill_bytes / 1024 / 1024
            if spill_mb >= 1024:  # GBå˜ä½
                spill_display = f"{spill_mb/1024:.2f} GB"
            else:  # MBå˜ä½
                spill_display = f"{spill_mb:.1f} MB"
            print(f"    ğŸ’¿ Spill: {spill_display}")
        
        # Shuffleãƒãƒ¼ãƒ‰ã®å ´åˆã¯å¸¸ã«Shuffle attributesã‚’è¡¨ç¤º
        if "shuffle" in short_name.lower():
            shuffle_attributes = extract_shuffle_attributes(node)
            if shuffle_attributes:
                print(f"    ğŸ”„ Shuffle attributes: {', '.join(shuffle_attributes)}")
                
                # REPARTITIONãƒ’ãƒ³ãƒˆã®ææ¡ˆï¼ˆã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã®ã¿ï¼‰
                if spill_detected and spill_bytes > 0 and spill_display:
                    suggested_partitions = max(num_tasks * 2, 200)  # æœ€å°200ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
                    
                    # Shuffleå±æ€§ã§æ¤œå‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ ã‚’å…¨ã¦ä½¿ç”¨ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
                    repartition_columns = ", ".join(shuffle_attributes)
                    
                    print(f"    ğŸ’¡ Optimization suggestion: REPARTITION({suggested_partitions}, {repartition_columns})")
                    print(f"       Reason: To improve spill ({spill_display})")
                    print(f"       Target: Complete use of all {len(shuffle_attributes)} shuffle attribute columns")
            else:
                print(f"    ğŸ”„ Shuffle attributes: Not configured")
        
        # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®å ´åˆã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã‚’è¡¨ç¤º
        if "scan" in short_name.lower():
            cluster_attributes = extract_cluster_attributes(node)
            if cluster_attributes:
                print(f"    ğŸ“Š Clustering keys: {', '.join(cluster_attributes)}")
            else:
                print(f"    ğŸ“Š Clustering keys: Not configured")

        
        # Skew details (simplified display)
        if skew_detected and skewed_partitions > 0:
            print(f"    âš–ï¸ Skew details: {skewed_partitions} skewed partitions")
        
        # Also display Node ID
        print(f"    ğŸ†” Node ID: {node.get('node_id', node.get('id', 'N/A'))}")
        print()
        
else:
    print("âš ï¸ Node metrics not found")

print()

# ğŸ”¥ Sparkã‚¹ãƒ†ãƒ¼ã‚¸å®Ÿè¡Œåˆ†æ
if extracted_metrics['stage_metrics']:
    print("\nğŸ”¥ Spark Stage Execution Analysis")
    print("=" * 60)
    
    stage_metrics = extracted_metrics['stage_metrics']
    total_stages = len(stage_metrics)
    completed_stages = len([s for s in stage_metrics if s.get('status') == 'COMPLETE'])
    failed_stages = len([s for s in stage_metrics if s.get('num_failed_tasks', 0) > 0])
    
    print(f"ğŸ“Š Stage overview: Total {total_stages} stages (completed: {completed_stages}, with failed tasks: {failed_stages})")
    print()
    
    # ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
    sorted_stages = sorted(stage_metrics, key=lambda x: x.get('duration_ms', 0), reverse=True)
    
    print("â±ï¸ Stage execution time ranking:")
    print("-" * 60)
    
    for i, stage in enumerate(sorted_stages[:5]):  # TOP5ã‚¹ãƒ†ãƒ¼ã‚¸ã®ã¿è¡¨ç¤º
        stage_id = stage.get('stage_id', 'N/A')
        status = stage.get('status', 'UNKNOWN')
        duration_ms = stage.get('duration_ms', 0)
        num_tasks = stage.get('num_tasks', 0)
        failed_tasks = stage.get('num_failed_tasks', 0)
        complete_tasks = stage.get('num_complete_tasks', 0)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã«å¿œã˜ãŸã‚¢ã‚¤ã‚³ãƒ³
        if status == 'COMPLETE' and failed_tasks == 0:
            status_icon = "âœ…"
        elif failed_tasks > 0:
            status_icon = "âš ï¸"
        else:
            status_icon = "â“"
        
        # ä¸¦åˆ—åº¦ã‚¢ã‚¤ã‚³ãƒ³
        parallelism_icon = "ğŸ”¥" if num_tasks >= 10 else "âš ï¸" if num_tasks >= 5 else "ğŸŒ"
        
        # å®Ÿè¡Œæ™‚é–“ã®é‡è¦åº¦
        if duration_ms >= 10000:
            time_icon = "ğŸ”´"
            severity = "CRITICAL"
        elif duration_ms >= 5000:
            time_icon = "ğŸŸ "
            severity = "HIGH"
        elif duration_ms >= 1000:
            time_icon = "ğŸŸ¡"
            severity = "MEDIUM"
        else:
            time_icon = "ğŸŸ¢"
            severity = "LOW"
        
        print(f"{i+1}. {status_icon}{parallelism_icon}{time_icon} Stage {stage_id} [{severity:8}]")
        print(f"   â±ï¸ Execution time: {duration_ms:,} ms ({duration_ms/1000:.1f} sec)")
        print(f"   ğŸ”§ Tasks: {complete_tasks}/{num_tasks} completed (failed: {failed_tasks})")
        
        # ã‚¿ã‚¹ã‚¯ã‚ãŸã‚Šã®å¹³å‡æ™‚é–“
        if num_tasks > 0:
            avg_task_time = duration_ms / num_tasks
            print(f"   ğŸ“Š Average task time: {avg_task_time:.1f} ms")
        
        # åŠ¹ç‡æ€§è©•ä¾¡
        if num_tasks > 0:
            task_efficiency = "é«˜åŠ¹ç‡" if num_tasks >= 10 and failed_tasks == 0 else "è¦æ”¹å–„" if failed_tasks > 0 else "æ¨™æº–"
            print(f"   ğŸ¯ Efficiency: {task_efficiency}")
        
        print()
    
    if len(sorted_stages) > 5:
        print(f"... {len(sorted_stages) - 5} other stages")
    
    # å•é¡Œã®ã‚ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ
    problematic_stages = [s for s in stage_metrics if s.get('num_failed_tasks', 0) > 0 or s.get('duration_ms', 0) > 30000]
    if problematic_stages:
        print("\nğŸš¨ Stages requiring attention:")
        print("-" * 40)
        for stage in problematic_stages[:3]:
            stage_id = stage.get('stage_id', 'N/A')
            duration_sec = stage.get('duration_ms', 0) / 1000
            failed_tasks = stage.get('num_failed_tasks', 0)
            
            issues = []
            if failed_tasks > 0:
                issues.append(f"å¤±æ•—ã‚¿ã‚¹ã‚¯{failed_tasks}å€‹")
            if duration_sec > 30:
                issues.append(f"é•·æ™‚é–“å®Ÿè¡Œ({duration_sec:.1f}sec)")
            
            print(f"   âš ï¸ Stage {stage_id}: {', '.join(issues)}")
    
    
    print()
else:
    print("\nğŸ”¥ Spark Stage Execution Analysis")
    print("=" * 60)
    print("âš ï¸ Stage metrics not found")
    print()

print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ—‚ï¸ Detailed Display of Liquid Clustering Analysis Results
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Detailed display of recommended clustering columns by table
# MAGIC - Analysis of expected performance improvements
# MAGIC - Detailed analysis of column usage patterns
# MAGIC - Display of pushdown filter information
# MAGIC - Presentation of SQL implementation examples

# COMMAND ----------

# ğŸ—‚ï¸ LLMã«ã‚ˆã‚‹Liquid Clusteringåˆ†æçµæœã®è©³ç´°è¡¨ç¤º
print("\n" + "=" * 50)
print("ğŸ¤– LLM Liquid Clustering Recommendation Analysis")
print("=" * 50)

# LLMãƒ™ãƒ¼ã‚¹ã®Liquid Clusteringåˆ†æã‚’å®Ÿè¡Œ
liquid_analysis = extracted_metrics['liquid_clustering_analysis']

# LLMåˆ†æçµæœã‚’è¡¨ç¤º
print("\nğŸ¤– LLM Analysis Results:")
print("=" * 50)
llm_analysis = liquid_analysis.get('llm_analysis', '')
if llm_analysis:
    print(llm_analysis)
else:
    print("âŒ LLM analysis results not found")

# æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦ã‚’è¡¨ç¤º
extracted_data = liquid_analysis.get('extracted_data', {})
metadata_summary = extracted_data.get('metadata_summary', {})

print(f"\nğŸ“Š Extracted data overview:")
print(f"   ğŸ” Filter conditions: {metadata_summary.get('filter_expressions_count', 0)} items")
print(f"   ğŸ”— JOIN conditions: {metadata_summary.get('join_expressions_count', 0)} items")
print(f"   ğŸ“Š GROUP BY conditions: {metadata_summary.get('groupby_expressions_count', 0)} items")
print(f"   ğŸ“ˆ Aggregate functions: {metadata_summary.get('aggregate_expressions_count', 0)} items")
print(f"   ğŸ·ï¸ Identified tables: {metadata_summary.get('tables_identified', 0)} items")
print(f"   ğŸ“‚ Scan nodes: {metadata_summary.get('scan_nodes_count', 0)} items")

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨ç¤º
performance_context = liquid_analysis.get('performance_context', {})
print(f"\nâš¡ Performance information:")
print(f"   â±ï¸ Execution time: {performance_context.get('total_time_sec', 0):.1f} seconds")
print(f"   ğŸ’¾ Data read: {performance_context.get('read_gb', 0):.2f}GB")
print(f"   ğŸ“Š Output rows: {performance_context.get('rows_produced', 0):,} rows")
print(f"   ğŸ¯ Filter rate: {performance_context.get('data_selectivity', 0):.4f}")

# Output analysis results to file
print(f"\nğŸ’¾ Outputting analysis results to file...")
try:
    saved_files = save_liquid_clustering_analysis(liquid_analysis, "/tmp")
    
    if "error" in saved_files:
        print(f"âŒ File output error: {saved_files['error']}")
    else:
        print(f"âœ… File output completed:")
        for file_type, file_path in saved_files.items():
            if file_type == "json":
                print(f"   ğŸ“„ JSON detailed data: {file_path}")
            elif file_type == "markdown":
                print(f"   ğŸ“ Markdown report: {file_path}")
            elif file_type == "sql":
                print(f"   ğŸ”§ SQL implementation example: {file_path}")
                
except Exception as e:
    print(f"âŒ Error occurred during file output: {str(e)}")

# ã‚µãƒãƒªãƒ¼æƒ…å ±
summary = liquid_analysis.get('summary', {})
print(f"\nğŸ“‹ Analysis summary:")
print(f"   ğŸ”¬ Analysis method: {summary.get('analysis_method', 'Unknown')}")
print(f"   ğŸ¤– LLM provider: {summary.get('llm_provider', 'Unknown')}")
print(f"   ğŸ“Š Target table count: {summary.get('tables_identified', 0)}")
print(f"   ğŸ“ˆ Extracted column count: Filter({summary.get('total_filter_columns', 0)}) + JOIN({summary.get('total_join_columns', 0)}) + GROUP BY({summary.get('total_groupby_columns', 0)})")

print()

# COMMAND ----------

# ğŸ¤– è¨­å®šã•ã‚ŒãŸLLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ
provider = LLM_CONFIG["provider"]
if provider == "databricks":
    endpoint_name = LLM_CONFIG["databricks"]["endpoint_name"]
    print(f"ğŸ¤– Starting bottleneck analysis with Databricks Model Serving ({endpoint_name})...")
    print(f"âš ï¸  Model Serving endpoint '{endpoint_name}' is required")
elif provider == "openai":
    model = LLM_CONFIG["openai"]["model"]
    print(f"ğŸ¤– Starting bottleneck analysis with OpenAI ({model})...")
    print("âš ï¸  OpenAI API key is required")
elif provider == "azure_openai":
    deployment = LLM_CONFIG["azure_openai"]["deployment_name"]
    print(f"ğŸ¤– Starting bottleneck analysis with Azure OpenAI ({deployment})...")
    print("âš ï¸  Azure OpenAI API key and endpoint are required")
elif provider == "anthropic":
    model = LLM_CONFIG["anthropic"]["model"]
    print(f"ğŸ¤– Starting bottleneck analysis with Anthropic ({model})...")
    print("âš ï¸  Anthropic API key is required")

print("ğŸ“ Simplifying analysis prompt to reduce timeout risk...")
print()

analysis_result = analyze_bottlenecks_with_llm(extracted_metrics)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¯ Display of LLM Bottleneck Analysis Results
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Display of detailed analysis results by the configured LLM provider
# MAGIC - Visualization of bottleneck identification and improvement recommendations
# MAGIC - Formatting and readable display of analysis results

# COMMAND ----------

# ğŸ“Š åˆ†æçµæœã®è¡¨ç¤º
print("\n" + "=" * 80)
print(f"ğŸ¯ ã€SQL Bottleneck Analysis Results by {provider.upper()} LLMã€‘")
print("=" * 80)
print()
print(analysis_result)
print()
print("=" * 80)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ’¾ Saving Analysis Results and Completion Summary
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Saving LLM analysis results to text files
# MAGIC - Recording basic information of analysis targets
# MAGIC - Displaying overall processing completion summary
# MAGIC - Listing output files

# COMMAND ----------

# ğŸ’¾ åˆ†æçµæœã®ä¿å­˜ã¨å®Œäº†ã‚µãƒãƒªãƒ¼
from datetime import datetime
# output_bottleneck_analysis_result_XXX.txtãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›ã¯å»ƒæ­¢ï¼ˆoptimization_reportã«çµ±åˆï¼‰

# æœ€çµ‚çš„ãªã‚µãƒãƒªãƒ¼
print("\n" + "ğŸ‰" * 20)
print("ğŸ ã€Processing Completion Summaryã€‘")
print("ğŸ‰" * 20)
print("âœ… SQL profiler JSON file loading completed")
print(f"âœ… Performance metrics extraction completed")

# LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±ã®å‹•çš„è¡¨ç¤º
try:
    current_provider = LLM_CONFIG.get('provider', 'unknown')
    provider_display_names = {
        'databricks': f"Databricks ({LLM_CONFIG.get('databricks', {}).get('endpoint_name', 'Model Serving')})",
        'openai': f"OpenAI ({LLM_CONFIG.get('openai', {}).get('model', 'GPT-4')})",
        'azure_openai': f"Azure OpenAI ({LLM_CONFIG.get('azure_openai', {}).get('deployment_name', 'GPT-4')})",
        'anthropic': f"Anthropic ({LLM_CONFIG.get('anthropic', {}).get('model', 'Claude')})"
    }
    provider_display = provider_display_names.get(current_provider, f"{current_provider}ï¼ˆæœªçŸ¥ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼‰")
    print(f"âœ… Bottleneck analysis completed by {provider_display}")
except Exception as e:
    print("âœ… LLM bottleneck analysis completed")

print("âœ… Analysis results will be integrated into optimization_report later")
print()
print("ğŸš€ Analysis complete! Please check the results and use them for query optimization.")
print("ğŸ‰" * 20)

# COMMAND ----------

# MAGIC %md
# MAGIC # ğŸ”§ SQL Optimization Function Section
# MAGIC
# MAGIC **This section performs SQL query optimization**
# MAGIC
# MAGIC ğŸ“‹ **Optimization Process:**
# MAGIC - Extract original query from profiler data
# MAGIC - Execute query optimization using LLM
# MAGIC - Generate optimization result files
# MAGIC - Prepare for test execution
# MAGIC
# MAGIC âš ï¸ **Prerequisites:** Please complete the main processing section before execution

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ”§ SQL Optimization Related Function Definitions
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - `extract_original_query_from_profiler_data`: Extract original query from profiler data
# MAGIC - `generate_optimized_query_with_llm`: Query optimization based on LLM analysis results
# MAGIC - `save_optimized_sql_files`: Save various optimization result files

# COMMAND ----------

def extract_original_query_from_profiler_data(profiler_data: Dict[str, Any]) -> str:
    """
    ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã‚’æŠ½å‡º
    """
    
    # è¤‡æ•°ã®å ´æ‰€ã‹ã‚‰SQLã‚¯ã‚¨ãƒªã‚’æ¢ã™
    query_candidates = []
    
    # 1. query.queryText ã‹ã‚‰æŠ½å‡º
    if 'query' in profiler_data and 'queryText' in profiler_data['query']:
        query_text = profiler_data['query']['queryText']
        if query_text and query_text.strip():
            query_candidates.append(query_text.strip())
    
    # 2. metadata ã‹ã‚‰æŠ½å‡º
    if 'metadata' in profiler_data:
        metadata = profiler_data['metadata']
        for key, value in metadata.items():
            if 'sql' in key.lower() or 'query' in key.lower():
                if isinstance(value, str) and value.strip():
                    query_candidates.append(value.strip())
    
    # 3. graphs ã® metadata ã‹ã‚‰æŠ½å‡º
    if 'graphs' in profiler_data:
        for graph in profiler_data['graphs']:
            nodes = graph.get('nodes', [])
            for node in nodes:
                node_metadata = node.get('metadata', [])
                for meta in node_metadata:
                    if meta.get('key', '').upper() in ['SQL', 'QUERY', 'SQL_TEXT']:
                        value = meta.get('value', '')
                        if value and value.strip():
                            query_candidates.append(value.strip())
    
    # æœ€ã‚‚é•·ã„ã‚¯ã‚¨ãƒªã‚’é¸æŠï¼ˆé€šå¸¸ã€æœ€ã‚‚å®Œå…¨ãªã‚¯ã‚¨ãƒªï¼‰
    if query_candidates:
        original_query = max(query_candidates, key=len)
        return original_query
    
    return ""

def extract_table_size_estimates_from_plan(profiler_data: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã®æ¨å®šã‚µã‚¤ã‚ºæƒ…å ±ã‚’æŠ½å‡º
    
    æ³¨æ„: Databricksã‚¯ã‚¨ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ estimatedSizeInBytes ãŒå«ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€
    ã“ã®æ©Ÿèƒ½ã¯ç¾åœ¨ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã®æ¨å®šã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
    
    Args:
        profiler_data: ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿
        
    Returns:
        Dict: ç©ºã®è¾æ›¸ï¼ˆæ©Ÿèƒ½ç„¡åŠ¹åŒ–ï¼‰
    """
    # Databricksã‚¯ã‚¨ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«estimatedSizeInBytesãŒå«ã¾ã‚Œã¦ã„ãªã„ãŸã‚ç„¡åŠ¹åŒ–
    return {}

def extract_table_name_from_scan_node(node: Dict[str, Any]) -> str:
    """
    ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
    
    Args:
        node: å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã®ãƒãƒ¼ãƒ‰
        
    Returns:
        str: ãƒ†ãƒ¼ãƒ–ãƒ«å
    """
    try:
        # è¤‡æ•°ã®æ–¹æ³•ã§ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡ºã‚’è©¦è¡Œ
        
        # 1. node outputã‹ã‚‰ã®æŠ½å‡º
        output = node.get("output", "")
        if output:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: [col1#123, col2#456] table_name
            import re
            table_match = re.search(r'\]\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)', output)
            if table_match:
                return table_match.group(1)
        
        # 2. nodeè©³ç´°ã‹ã‚‰ã®æŠ½å‡º
        details = node.get("details", "")
        if details:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: Location: /path/to/table/name
            location_match = re.search(r'Location:.*?([a-zA-Z_][a-zA-Z0-9_]*)', details)
            if location_match:
                return location_match.group(1)
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: Table: database.table_name
            table_match = re.search(r'Table:\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)', details)
            if table_match:
                return table_match.group(1)
        
        # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®æŠ½å‡º
        metadata = node.get("metadata", [])
        for meta in metadata:
            if meta.get("key") == "table" or meta.get("key") == "relation":
                values = meta.get("values", [])
                if values:
                    return str(values[0])
        
        # 4. nodeåã‹ã‚‰ã®æ¨æ¸¬ï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
        node_name = node.get("nodeName", "")
        if "delta" in node_name.lower():
            # Delta Scan ã®å ´åˆã€è©³ç´°æƒ…å ±ã‹ã‚‰æŠ½å‡º
            pass
    
    except Exception as e:
        print(f"âš ï¸ Error in table name extraction: {str(e)}")
    
    return None

def extract_broadcast_table_names(profiler_data: Dict[str, Any], broadcast_nodes: list) -> Dict[str, Any]:
    """
    BROADCASTãƒãƒ¼ãƒ‰ã‹ã‚‰é–¢é€£ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
    """
    broadcast_table_info = {
        "broadcast_tables": [],
        "broadcast_table_mapping": {},
        "broadcast_nodes_with_tables": []
    }
    
    # å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã®ã‚°ãƒ©ãƒ•æƒ…å ±ã‚’å–å¾—
    graphs = profiler_data.get('graphs', [])
    if not graphs:
        return broadcast_table_info
    
    # å…¨ãƒãƒ¼ãƒ‰ã‚’åé›†
    all_nodes = []
    for graph in graphs:
        nodes = graph.get('nodes', [])
        all_nodes.extend(nodes)
    
    # ã‚¨ãƒƒã‚¸æƒ…å ±ã‚’åé›†ï¼ˆãƒãƒ¼ãƒ‰é–“ã®é–¢ä¿‚ï¼‰
    all_edges = []
    for graph in graphs:
        edges = graph.get('edges', [])
        all_edges.extend(edges)
    
    # å„BROADCASTãƒãƒ¼ãƒ‰ã«ã¤ã„ã¦é–¢é€£ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç‰¹å®š
    for broadcast_node in broadcast_nodes:
        broadcast_node_id = broadcast_node.get('node_id', '')
        broadcast_node_name = broadcast_node.get('node_name', '')
        
        # BROADCASTãƒãƒ¼ãƒ‰ã‹ã‚‰ç›´æ¥ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
        table_names = set()
        
        # 1. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
        metadata = broadcast_node.get('metadata', [])
        for meta in metadata:
            key = meta.get('key', '')
            value = meta.get('value', '')
            values = meta.get('values', [])
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ç¤ºã™ã‚­ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
            if key in ['SCAN_IDENTIFIER', 'TABLE_NAME', 'RELATION']:
                if value:
                    table_names.add(value)
                table_names.update(values)
        
        # 2. ãƒãƒ¼ãƒ‰åã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æ¨å®š
        if 'SCAN' in broadcast_node_name:
            # "Broadcast Scan delta orders" â†’ "orders"
            import re
            table_match = re.search(r'SCAN\s+(?:DELTA|PARQUET|JSON|CSV)?\s*([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)', broadcast_node_name, re.IGNORECASE)
            if table_match:
                table_names.add(table_match.group(1))
        
        # 3. ã‚¨ãƒƒã‚¸æƒ…å ±ã‹ã‚‰é–¢é€£ã™ã‚‹ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‚’ç‰¹å®š
        for edge in all_edges:
            source_id = edge.get('source', '')
            target_id = edge.get('target', '')
            
            # BROADCASTãƒãƒ¼ãƒ‰ã«å…¥åŠ›ã•ã‚Œã‚‹ãƒãƒ¼ãƒ‰ã‚’æ¤œç´¢
            if target_id == broadcast_node_id:
                # å…¥åŠ›ãƒãƒ¼ãƒ‰ãŒã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‹ãƒã‚§ãƒƒã‚¯
                for node in all_nodes:
                    if node.get('id', '') == source_id:
                        node_name = node.get('name', '').upper()
                        if any(keyword in node_name for keyword in ['SCAN', 'FILESCAN']):
                            # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
                            scan_table_name = extract_table_name_from_scan_node(node)
                            if scan_table_name:
                                table_names.add(scan_table_name)
        
        # 4. åŒã˜ã‚°ãƒ©ãƒ•å†…ã®ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã¨ã®é–¢é€£ä»˜ã‘
        for node in all_nodes:
            node_name = node.get('name', '').upper()
            if any(keyword in node_name for keyword in ['SCAN', 'FILESCAN']):
                # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®åå‰ãŒBROADCASTãƒãƒ¼ãƒ‰åã«å«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                scan_table_name = extract_table_name_from_scan_node(node)
                if scan_table_name:
                    # ãƒ†ãƒ¼ãƒ–ãƒ«åã®éƒ¨åˆ†ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
                    if any(part in broadcast_node_name for part in scan_table_name.split('.') if len(part) > 2):
                        table_names.add(scan_table_name)
        
        # çµæœã‚’è¨˜éŒ²
        table_names_list = list(table_names)
        if table_names_list:
            broadcast_table_info["broadcast_tables"].extend(table_names_list)
            broadcast_table_info["broadcast_table_mapping"][broadcast_node_id] = table_names_list
            
            # BROADCASTãƒãƒ¼ãƒ‰æƒ…å ±ã‚’æ‹¡å¼µ
            enhanced_broadcast_node = broadcast_node.copy()
            enhanced_broadcast_node["associated_tables"] = table_names_list
            enhanced_broadcast_node["table_count"] = len(table_names_list)
            broadcast_table_info["broadcast_nodes_with_tables"].append(enhanced_broadcast_node)
    
    # é‡è¤‡ã‚’é™¤å»
    broadcast_table_info["broadcast_tables"] = list(set(broadcast_table_info["broadcast_tables"]))
    
    return broadcast_table_info

def extract_execution_plan_info(profiler_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    JSONãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’æŠ½å‡º
    """
    plan_info = {
        "broadcast_nodes": [],
        "join_nodes": [],
        "scan_nodes": [],
        "shuffle_nodes": [],
        "aggregate_nodes": [],
        "plan_summary": {},
        "broadcast_already_applied": False,
        "join_strategies": [],
        "table_scan_details": {},
        "broadcast_table_info": {}
    }
    
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿè¡Œã‚°ãƒ©ãƒ•æƒ…å ±ã‚’å–å¾—
    graphs = profiler_data.get('graphs', [])
    if not graphs:
        return plan_info
    
    # ã™ã¹ã¦ã®ã‚°ãƒ©ãƒ•ã‹ã‚‰ãƒãƒ¼ãƒ‰ã‚’åé›†
    all_nodes = []
    for graph_index, graph in enumerate(graphs):
        nodes = graph.get('nodes', [])
        for node in nodes:
            node['graph_index'] = graph_index
            all_nodes.append(node)
    
    # ãƒãƒ¼ãƒ‰åˆ†æ
    for node in all_nodes:
        node_name = node.get('name', '').upper()
        node_tag = node.get('tag', '').upper()
        node_metadata = node.get('metadata', [])
        
        # BROADCASTãƒãƒ¼ãƒ‰ã®æ¤œå‡º
        if 'BROADCAST' in node_name or 'BROADCAST' in node_tag:
            plan_info["broadcast_already_applied"] = True
            broadcast_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "metadata": []
            }
            
            # BROADCASTã«é–¢é€£ã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                value = meta.get('value', '')
                values = meta.get('values', [])
                
                if any(keyword in key.upper() for keyword in ['BROADCAST', 'BUILD', 'PROBE']):
                    broadcast_info["metadata"].append({
                        "key": key,
                        "value": value,
                        "values": values
                    })
            
            plan_info["broadcast_nodes"].append(broadcast_info)
        
        # JOINãƒãƒ¼ãƒ‰ã®æ¤œå‡ºã¨æˆ¦ç•¥åˆ†æ
        elif any(keyword in node_name for keyword in ['JOIN', 'HASH']):
            join_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "join_strategy": "unknown",
                "join_keys": [],
                "join_type": "unknown"
            }
            
            # JOINæˆ¦ç•¥ã®ç‰¹å®š
            if 'BROADCAST' in node_name:
                join_info["join_strategy"] = "broadcast_hash_join"
            elif 'SORT' in node_name and 'MERGE' in node_name:
                join_info["join_strategy"] = "sort_merge_join"
            elif 'HASH' in node_name:
                join_info["join_strategy"] = "shuffle_hash_join"
            elif 'NESTED' in node_name:
                join_info["join_strategy"] = "broadcast_nested_loop_join"
            
            # JOINã‚¿ã‚¤ãƒ—ã®ç‰¹å®š
            if 'INNER' in node_name:
                join_info["join_type"] = "inner"
            elif 'LEFT' in node_name:
                join_info["join_type"] = "left"
            elif 'RIGHT' in node_name:
                join_info["join_type"] = "right"
            elif 'OUTER' in node_name:
                join_info["join_type"] = "outer"
            
            # JOINæ¡ä»¶ã®æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                values = meta.get('values', [])
                
                if key in ['LEFT_KEYS', 'RIGHT_KEYS']:
                    join_info["join_keys"].extend(values)
            
            plan_info["join_nodes"].append(join_info)
            plan_info["join_strategies"].append(join_info["join_strategy"])
        
        # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®è©³ç´°åˆ†æ
        elif any(keyword in node_name for keyword in ['SCAN', 'FILESCAN']):
            scan_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "table_name": "unknown",
                "file_format": "unknown",
                "pushed_filters": [],
                "output_columns": []
            }
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«åã¨ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                value = meta.get('value', '')
                values = meta.get('values', [])
                
                if key == 'SCAN_IDENTIFIER':
                    scan_info["table_name"] = value
                elif key == 'OUTPUT':
                    scan_info["output_columns"] = values
                elif key == 'PUSHED_FILTERS' or key == 'FILTERS':
                    scan_info["pushed_filters"] = values
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®æ¨å®š
            if 'DELTA' in node_name:
                scan_info["file_format"] = "delta"
            elif 'PARQUET' in node_name:
                scan_info["file_format"] = "parquet"
            elif 'JSON' in node_name:
                scan_info["file_format"] = "json"
            elif 'CSV' in node_name:
                scan_info["file_format"] = "csv"
            
            plan_info["scan_nodes"].append(scan_info)
            plan_info["table_scan_details"][scan_info["table_name"]] = scan_info
        
        # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰ã®æ¤œå‡º
        elif any(keyword in node_name for keyword in ['SHUFFLE', 'EXCHANGE']):
            shuffle_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "partition_keys": []
            }
            
            # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±ã®æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                values = meta.get('values', [])
                
                if key in ['PARTITION_EXPRESSIONS', 'PARTITION_KEYS']:
                    shuffle_info["partition_keys"] = values
            
            plan_info["shuffle_nodes"].append(shuffle_info)
        
        # é›†ç´„ãƒãƒ¼ãƒ‰ã®æ¤œå‡º
        elif any(keyword in node_name for keyword in ['AGGREGATE', 'GROUP']):
            agg_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "group_keys": [],
                "aggregate_expressions": []
            }
            
            # é›†ç´„æƒ…å ±ã®æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                values = meta.get('values', [])
                
                if key == 'GROUPING_EXPRESSIONS':
                    agg_info["group_keys"] = values
                elif key == 'AGGREGATE_EXPRESSIONS':
                    agg_info["aggregate_expressions"] = values
            
            plan_info["aggregate_nodes"].append(agg_info)
    
    # ãƒ—ãƒ©ãƒ³ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ
    plan_info["plan_summary"] = {
        "total_nodes": len(all_nodes),
        "broadcast_nodes_count": len(plan_info["broadcast_nodes"]),
        "join_nodes_count": len(plan_info["join_nodes"]),
        "scan_nodes_count": len(plan_info["scan_nodes"]),
        "shuffle_nodes_count": len(plan_info["shuffle_nodes"]),
        "aggregate_nodes_count": len(plan_info["aggregate_nodes"]),
        "unique_join_strategies": list(set(plan_info["join_strategies"])),
        "has_broadcast_joins": plan_info["broadcast_already_applied"],
        "tables_scanned": len(plan_info["table_scan_details"])
    }
    
    # BROADCASTãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’æŠ½å‡º
    if plan_info["broadcast_nodes"]:
        broadcast_table_info = extract_broadcast_table_names(profiler_data, plan_info["broadcast_nodes"])
        plan_info["broadcast_table_info"] = broadcast_table_info
        
        # ãƒ—ãƒ©ãƒ³ã‚µãƒãƒªãƒ¼ã«BROADCASTãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’è¿½åŠ 
        plan_info["plan_summary"]["broadcast_tables"] = broadcast_table_info["broadcast_tables"]
        plan_info["plan_summary"]["broadcast_table_count"] = len(broadcast_table_info["broadcast_tables"])
    
    # å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‹ã‚‰ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæ¨å®šæƒ…å ±ã‚’è¿½åŠ ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
    plan_info["table_size_estimates"] = {}  # extract_table_size_estimates_from_plan(profiler_data)
    
    return plan_info

def get_spark_broadcast_threshold() -> float:
    """
    Sparkã®å®Ÿéš›ã®broadcasté–¾å€¤è¨­å®šã‚’å–å¾—
    """
    try:
        # Sparkã®è¨­å®šå€¤ã‚’å–å¾—
        threshold_bytes = spark.conf.get("spark.databricks.optimizer.autoBroadcastJoinThreshold", "31457280")  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30MB
        threshold_mb = float(threshold_bytes) / 1024 / 1024
        return threshold_mb
    except:
        # å–å¾—ã§ããªã„å ´åˆã¯æ¨™æº–çš„ãª30MBã‚’è¿”ã™
        return 30.0

def estimate_uncompressed_size(compressed_size_mb: float, file_format: str = "parquet") -> float:
    """
    åœ§ç¸®ã‚µã‚¤ã‚ºã‹ã‚‰éåœ§ç¸®ã‚µã‚¤ã‚ºã‚’æ¨å®šï¼ˆ3.0å€å›ºå®šï¼‰
    
    æ³¨æ„: å®Ÿéš›ã®estimatedSizeInBytesãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€
    ä¿å®ˆçš„ãª3.0å€åœ§ç¸®ç‡ã§çµ±ä¸€ã—ã¦æ¨å®šã—ã¾ã™ã€‚
    """
    # ä¿å®ˆçš„ãª3.0å€åœ§ç¸®ç‡ã§çµ±ä¸€ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ï¼‰
    compression_ratio = 3.0
    
    return compressed_size_mb * compression_ratio

def analyze_broadcast_feasibility(metrics: Dict[str, Any], original_query: str, plan_info: Dict[str, Any] = None) -> Dict[str, Any]:
    """
    BROADCASTãƒ’ãƒ³ãƒˆã®é©ç”¨å¯èƒ½æ€§ã‚’åˆ†æï¼ˆæ­£ç¢ºãª30MBé–¾å€¤é©ç”¨ï¼‰
    """
    broadcast_analysis = {
        "is_join_query": False,
        "broadcast_candidates": [],
        "recommendations": [],
        "feasibility": "not_applicable",
        "reasoning": [],
        "spark_threshold_mb": get_spark_broadcast_threshold(),
        "compression_analysis": {},
        "detailed_size_analysis": [],
        "execution_plan_analysis": {},
        "existing_broadcast_nodes": [],
        "already_optimized": False,
        "broadcast_applied_tables": []
    }
    
    # ã‚¯ã‚¨ãƒªã«JOINãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    query_upper = original_query.upper()
    join_types = ['JOIN', 'INNER JOIN', 'LEFT JOIN', 'RIGHT JOIN', 'LEFT OUTER JOIN', 'RIGHT OUTER JOIN', 'SEMI JOIN', 'ANTI JOIN']
    has_join = any(join_type in query_upper for join_type in join_types)
    
    if not has_join:
        broadcast_analysis["reasoning"].append("JOINã‚¯ã‚¨ãƒªã§ã¯ãªã„ãŸã‚ã€BROADCASTãƒ’ãƒ³ãƒˆã¯é©ç”¨ä¸å¯")
        return broadcast_analysis
    
    broadcast_analysis["is_join_query"] = True
    broadcast_analysis["reasoning"].append(f"Spark BROADCASTé–¾å€¤: {broadcast_analysis['spark_threshold_mb']:.1f}MBï¼ˆéåœ§ç¸®ï¼‰")
    
    # å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã®åˆ†æ
    if plan_info:
        plan_summary = plan_info.get("plan_summary", {})
        broadcast_nodes = plan_info.get("broadcast_nodes", [])
        join_nodes = plan_info.get("join_nodes", [])
        table_scan_details = plan_info.get("table_scan_details", {})
        table_size_estimates = plan_info.get("table_size_estimates", {})
        
        # æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã®è¨˜éŒ²
        broadcast_analysis["existing_broadcast_nodes"] = broadcast_nodes
        broadcast_analysis["already_optimized"] = len(broadcast_nodes) > 0
        
        # ãƒ—ãƒ©ãƒ³åˆ†æçµæœã®è¨˜éŒ²
        broadcast_analysis["execution_plan_analysis"] = {
            "has_broadcast_joins": plan_summary.get("has_broadcast_joins", False),
            "unique_join_strategies": plan_summary.get("unique_join_strategies", []),
            "broadcast_nodes_count": len(broadcast_nodes),
            "join_nodes_count": len(join_nodes),
            "scan_nodes_count": plan_summary.get("scan_nodes_count", 0),
            "shuffle_nodes_count": plan_summary.get("shuffle_nodes_count", 0),
            "tables_in_plan": list(table_scan_details.keys())
        }
        
        # æ—¢ã«BROADCASTãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹å ´åˆã®è©³ç´°è¨˜éŒ²
        if broadcast_nodes:
            broadcast_analysis["reasoning"].append(f"âœ… å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã§æ—¢ã«BROADCAST JOINãŒé©ç”¨æ¸ˆã¿: {len(broadcast_nodes)}å€‹ã®ãƒãƒ¼ãƒ‰")
            
            # BROADCASTãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’å–å¾—
            broadcast_table_info = plan_info.get("broadcast_table_info", {})
            broadcast_tables = broadcast_table_info.get("broadcast_tables", [])
            
            if broadcast_tables:
                broadcast_analysis["reasoning"].append(f"ğŸ“‹ BROADCASTã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(broadcast_tables)}")
                broadcast_analysis["broadcast_applied_tables"] = broadcast_tables
                
                # å„BROADCASTãƒãƒ¼ãƒ‰ã®è©³ç´°
                broadcast_nodes_with_tables = broadcast_table_info.get("broadcast_nodes_with_tables", [])
                for i, node in enumerate(broadcast_nodes_with_tables[:3]):  # æœ€å¤§3å€‹ã¾ã§è¡¨ç¤º
                    node_name_short = node['node_name'][:50] + "..." if len(node['node_name']) > 50 else node['node_name']
                    associated_tables = node.get('associated_tables', [])
                    if associated_tables:
                        broadcast_analysis["reasoning"].append(f"  â€¢ BROADCAST Node {i+1}: {node_name_short}")
                        broadcast_analysis["reasoning"].append(f"    â””â”€ ãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(associated_tables)}")
                    else:
                        broadcast_analysis["reasoning"].append(f"  â€¢ BROADCAST Node {i+1}: {node_name_short} (ãƒ†ãƒ¼ãƒ–ãƒ«åæœªç‰¹å®š)")
            else:
                # BROADCASTãƒãƒ¼ãƒ‰ã¯å­˜åœ¨ã™ã‚‹ãŒãƒ†ãƒ¼ãƒ–ãƒ«åãŒç‰¹å®šã§ããªã„å ´åˆ
                for i, node in enumerate(broadcast_nodes[:3]):  # æœ€å¤§3å€‹ã¾ã§è¡¨ç¤º
                    broadcast_analysis["reasoning"].append(f"  â€¢ BROADCAST Node {i+1}: {node['node_name'][:50]}... (ãƒ†ãƒ¼ãƒ–ãƒ«åè§£æä¸­)")
        else:
            # BROADCASTæœªé©ç”¨ã ãŒã€JOINãŒå­˜åœ¨ã™ã‚‹å ´åˆ
            if join_nodes:
                join_strategies = set(node["join_strategy"] for node in join_nodes)
                broadcast_analysis["reasoning"].append(f"ğŸ” ç¾åœ¨ã®JOINæˆ¦ç•¥: {', '.join(join_strategies)}")
                broadcast_analysis["reasoning"].append("ğŸ’¡ BROADCASTæœ€é©åŒ–ã®æ©Ÿä¼šã‚’æ¤œè¨ä¸­...")
    else:
        broadcast_analysis["reasoning"].append("âš ï¸ å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¨å®šã«åŸºã¥ãåˆ†æã‚’å®Ÿè¡Œ")
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±ã‚’å–å¾—
    overall_metrics = metrics.get('overall_metrics', {})
    node_metrics = metrics.get('node_metrics', [])
    
    # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’æŠ½å‡º
    scan_nodes = []
    total_compressed_bytes = 0
    total_rows_all_tables = 0
    
    for node in node_metrics:
        node_name = node.get('name', '').upper()
        if any(keyword in node_name for keyword in ['SCAN', 'FILESCAN', 'PARQUET', 'DELTA']):
            key_metrics = node.get('key_metrics', {})
            rows_num = key_metrics.get('rowsNum', 0)
            duration_ms = key_metrics.get('durationMs', 0)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®æ¨å®šï¼ˆãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’å„ªå…ˆï¼‰
            file_format = "parquet"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            table_name_from_plan = "unknown"
            
            # ãƒ—ãƒ©ãƒ³æƒ…å ±ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã¨ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’å–å¾—
            if plan_info and plan_info.get("table_scan_details"):
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©³ç´°ãªãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
                node_metadata = node.get('metadata', [])
                for meta in node_metadata:
                    meta_key = meta.get('key', '')
                    meta_value = meta.get('value', '')
                    if meta_key in ['SCAN_IDENTIFIER', 'SCAN_TABLE', 'TABLE_NAME'] and meta_value:
                        # ãƒ—ãƒ©ãƒ³ã®è©³ç´°ã¨ç…§åˆ
                        for plan_table, scan_detail in plan_info["table_scan_details"].items():
                            if meta_value in plan_table or plan_table in meta_value:
                                table_name_from_plan = plan_table
                                if scan_detail["file_format"] != "unknown":
                                    file_format = scan_detail["file_format"]
                                break
                        break
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒãƒ¼ãƒ‰åã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’æ¨å®š
            if file_format == "parquet":  # ã¾ã ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å ´åˆ
                if "DELTA" in node_name:
                    file_format = "delta"
                elif "PARQUET" in node_name:
                    file_format = "parquet"
                elif "JSON" in node_name:
                    file_format = "json"
                elif "CSV" in node_name:
                    file_format = "csv"
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æ¨å®šã®ã¿ä½¿ç”¨ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ï¼‰
            estimated_compressed_mb = 0
            estimated_uncompressed_mb = 0
            size_source = "metrics_estimation"
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æ¨å®š
            total_read_bytes = overall_metrics.get('read_bytes', 0)
            total_rows = overall_metrics.get('rows_read_count', 0)
            
            if total_rows > 0 and total_read_bytes > 0 and rows_num > 0:
                # å…¨ä½“ã®èª­ã¿è¾¼ã¿é‡ã‹ã‚‰ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®å‰²åˆã‚’è¨ˆç®—
                table_ratio = rows_num / total_rows
                estimated_compressed_bytes = total_read_bytes * table_ratio
                estimated_compressed_mb = estimated_compressed_bytes / 1024 / 1024
                 
                # éåœ§ç¸®ã‚µã‚¤ã‚ºã‚’æ¨å®š
                estimated_uncompressed_mb = estimate_uncompressed_size(estimated_compressed_mb, file_format)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è¡Œæ•°ãƒ™ãƒ¼ã‚¹ã®æ¨å®šï¼ˆä¿å®ˆçš„ï¼‰
                # å¹³å‡è¡Œã‚µã‚¤ã‚ºã‚’æ¨å®šï¼ˆéåœ§ç¸®ï¼‰
                if total_rows > 0 and total_read_bytes > 0:
                    # å…¨ä½“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åœ§ç¸®å¾Œã®å¹³å‡è¡Œã‚µã‚¤ã‚ºã‚’è¨ˆç®—
                    compressed_avg_row_size = total_read_bytes / total_rows
                    # åœ§ç¸®ç‡ã‚’è€ƒæ…®ã—ã¦éåœ§ç¸®ã‚µã‚¤ã‚ºã‚’æ¨å®š
                    uncompressed_avg_row_size = compressed_avg_row_size * estimate_uncompressed_size(1.0, file_format)
                else:
                    # å®Œå…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ä¸€èˆ¬çš„ãªéåœ§ç¸®è¡Œã‚µã‚¤ã‚ºï¼ˆ1KBï¼‰
                    uncompressed_avg_row_size = 1024
                
                estimated_compressed_mb = (rows_num * compressed_avg_row_size) / 1024 / 1024 if 'compressed_avg_row_size' in locals() else 0
                estimated_uncompressed_mb = (rows_num * uncompressed_avg_row_size) / 1024 / 1024
            
            # æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯
            is_already_broadcasted = False
            if plan_info and plan_info.get("broadcast_nodes"):
                for broadcast_node in plan_info["broadcast_nodes"]:
                    # ãƒ†ãƒ¼ãƒ–ãƒ«åã®éƒ¨åˆ†ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
                    broadcast_node_name = broadcast_node["node_name"]
                    if (table_name_from_plan != "unknown" and 
                        any(part in broadcast_node_name for part in table_name_from_plan.split('.') if len(part) > 3)):
                        is_already_broadcasted = True
                        break
                    # ãƒãƒ¼ãƒ‰åã§ã®ç…§åˆ
                    elif any(part in broadcast_node_name for part in node_name.split() if len(part) > 3):
                        is_already_broadcasted = True
                        break

            scan_info = {
                "node_name": node_name,
                "table_name_from_plan": table_name_from_plan,
                "rows": rows_num,
                "duration_ms": duration_ms,
                "estimated_compressed_mb": estimated_compressed_mb,
                "estimated_uncompressed_mb": estimated_uncompressed_mb,
                "file_format": file_format,
                "compression_ratio": 3.0,  # å›ºå®š3.0å€åœ§ç¸®ç‡
                "node_id": node.get('node_id', ''),
                "is_already_broadcasted": is_already_broadcasted,
                "size_estimation_source": size_source,
                "size_confidence": "medium"  # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æ¨å®šã®ãŸã‚ä¸­ç¨‹åº¦ä¿¡é ¼åº¦
            }
            scan_nodes.append(scan_info)
            
            total_compressed_bytes += estimated_compressed_bytes if 'estimated_compressed_bytes' in locals() else 0
            total_rows_all_tables += rows_num
    
    # BROADCASTå€™è£œã®åˆ¤å®šï¼ˆ30MBé–¾å€¤ä½¿ç”¨ï¼‰
    broadcast_threshold_mb = broadcast_analysis["spark_threshold_mb"]  # å®Ÿéš›ã®Sparkè¨­å®šå€¤
    broadcast_safe_mb = broadcast_threshold_mb * 0.8  # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ï¼ˆ80%ï¼‰
    broadcast_max_mb = broadcast_threshold_mb * 10    # æ˜ã‚‰ã‹ã«å¤§ãã™ãã‚‹é–¾å€¤
    
    small_tables = []
    large_tables = []
    marginal_tables = []
    
    # åœ§ç¸®åˆ†æã®è¨˜éŒ²
    broadcast_analysis["compression_analysis"] = {
        "total_compressed_gb": total_compressed_bytes / 1024 / 1024 / 1024 if total_compressed_bytes > 0 else 0,
        "total_rows": total_rows_all_tables,
        "avg_compression_ratio": 0
    }
    
    for scan in scan_nodes:
        uncompressed_size_mb = scan["estimated_uncompressed_mb"]
        compressed_size_mb = scan["estimated_compressed_mb"]
        
        # è©³ç´°ã‚µã‚¤ã‚ºåˆ†æã®è¨˜éŒ²
        table_display_name = scan.get("table_name_from_plan", scan["node_name"])
        is_already_broadcasted = scan.get("is_already_broadcasted", False)
        
        size_analysis = {
            "table": table_display_name,
            "node_name": scan["node_name"],
            "rows": scan["rows"],
            "compressed_mb": compressed_size_mb,
            "uncompressed_mb": uncompressed_size_mb,
            "file_format": scan["file_format"],
            "compression_ratio": scan["compression_ratio"],
            "broadcast_decision": "",
            "decision_reasoning": "",
            "is_already_broadcasted": is_already_broadcasted
        }
        
        # 30MBé–¾å€¤ã§ã®åˆ¤å®šï¼ˆéåœ§ç¸®ã‚µã‚¤ã‚ºï¼‰- æ—¢å­˜é©ç”¨çŠ¶æ³ã‚’è€ƒæ…®
        if is_already_broadcasted:
            # æ—¢ã«BROADCASTãŒé©ç”¨æ¸ˆã¿
            small_tables.append(scan)  # çµ±è¨ˆç›®çš„ã§è¨˜éŒ²
            size_analysis["broadcast_decision"] = "already_applied"
            size_analysis["decision_reasoning"] = f"æ—¢ã«BROADCASTé©ç”¨æ¸ˆã¿ï¼ˆæ¨å®šã‚µã‚¤ã‚º: éåœ§ç¸®{uncompressed_size_mb:.1f}MBï¼‰"
            broadcast_analysis["broadcast_candidates"].append({
                "table": table_display_name,
                "estimated_uncompressed_mb": uncompressed_size_mb,
                "estimated_compressed_mb": compressed_size_mb,
                "rows": scan["rows"],
                "file_format": scan["file_format"],
                "compression_ratio": scan["compression_ratio"],
                "broadcast_feasible": True,
                "confidence": "confirmed",
                "status": "already_applied",
                "reasoning": f"å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã§æ—¢ã«BROADCASTé©ç”¨ç¢ºèªæ¸ˆã¿ï¼ˆæ¨å®šã‚µã‚¤ã‚º: éåœ§ç¸®{uncompressed_size_mb:.1f}MBã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æ¨å®šï¼‰"
            })
        elif uncompressed_size_mb <= broadcast_safe_mb and scan["rows"] > 0:
            # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³å†…ï¼ˆ24MBä»¥ä¸‹ï¼‰- å¼·ãæ¨å¥¨
            small_tables.append(scan)
            size_analysis["broadcast_decision"] = "strongly_recommended"
            size_analysis["decision_reasoning"] = f"éåœ§ç¸®{uncompressed_size_mb:.1f}MB â‰¤ å®‰å…¨é–¾å€¤{broadcast_safe_mb:.1f}MB"
            broadcast_analysis["broadcast_candidates"].append({
                "table": table_display_name,
                "estimated_uncompressed_mb": uncompressed_size_mb,
                "estimated_compressed_mb": compressed_size_mb,
                "rows": scan["rows"],
                "file_format": scan["file_format"],
                "compression_ratio": scan["compression_ratio"],
                "broadcast_feasible": True,
                "confidence": "high",
                "status": "new_recommendation",
                "reasoning": f"éåœ§ç¸®æ¨å®šã‚µã‚¤ã‚º {uncompressed_size_mb:.1f}MBï¼ˆå®‰å…¨é–¾å€¤ {broadcast_safe_mb:.1f}MB ä»¥ä¸‹ï¼‰ã§BROADCASTå¼·ãæ¨å¥¨ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æ¨å®šã€3.0å€åœ§ç¸®ç‡ï¼‰"
            })
        elif uncompressed_size_mb <= broadcast_threshold_mb and scan["rows"] > 0:
            # é–¾å€¤å†…ã ãŒå®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ã¯è¶…éï¼ˆ24-30MBï¼‰- æ¡ä»¶ä»˜ãæ¨å¥¨
            marginal_tables.append(scan)
            size_analysis["broadcast_decision"] = "conditionally_recommended"
            size_analysis["decision_reasoning"] = f"éåœ§ç¸®{uncompressed_size_mb:.1f}MB â‰¤ é–¾å€¤{broadcast_threshold_mb:.1f}MBï¼ˆå®‰å…¨ãƒãƒ¼ã‚¸ãƒ³è¶…éï¼‰"
            broadcast_analysis["broadcast_candidates"].append({
                "table": table_display_name,
                "estimated_uncompressed_mb": uncompressed_size_mb,
                "estimated_compressed_mb": compressed_size_mb,
                "rows": scan["rows"],
                "file_format": scan["file_format"],
                "compression_ratio": scan["compression_ratio"],
                "broadcast_feasible": True,
                "confidence": "medium",
                "status": "new_recommendation",
                "reasoning": f"éåœ§ç¸®æ¨å®šã‚µã‚¤ã‚º {uncompressed_size_mb:.1f}MBï¼ˆé–¾å€¤ {broadcast_threshold_mb:.1f}MB ä»¥ä¸‹ã ãŒå®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ {broadcast_safe_mb:.1f}MB è¶…éï¼‰ã§æ¡ä»¶ä»˜ãBROADCASTæ¨å¥¨ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æ¨å®šã€3.0å€åœ§ç¸®ç‡ï¼‰"
            })
        elif uncompressed_size_mb > broadcast_max_mb:
            # æ˜ã‚‰ã‹ã«å¤§ãã™ãã‚‹ï¼ˆ300MBè¶…ï¼‰
            large_tables.append(scan)
            size_analysis["broadcast_decision"] = "not_recommended"
            size_analysis["decision_reasoning"] = f"éåœ§ç¸®{uncompressed_size_mb:.1f}MB > æœ€å¤§é–¾å€¤{broadcast_max_mb:.1f}MB"
            broadcast_analysis["reasoning"].append(f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_display_name}: éåœ§ç¸®{uncompressed_size_mb:.1f}MB - BROADCASTä¸å¯ï¼ˆ>{broadcast_max_mb:.1f}MBï¼‰")
        else:
            # ä¸­é–“ã‚µã‚¤ã‚ºã®ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ30-300MBï¼‰
            large_tables.append(scan)
            size_analysis["broadcast_decision"] = "not_recommended"
            size_analysis["decision_reasoning"] = f"éåœ§ç¸®{uncompressed_size_mb:.1f}MB > é–¾å€¤{broadcast_threshold_mb:.1f}MB"
            broadcast_analysis["reasoning"].append(f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_display_name}: éåœ§ç¸®{uncompressed_size_mb:.1f}MB - BROADCASTéæ¨å¥¨ï¼ˆ>{broadcast_threshold_mb:.1f}MBé–¾å€¤ï¼‰")
        
        broadcast_analysis["detailed_size_analysis"].append(size_analysis)
    
    # åœ§ç¸®åˆ†æã‚µãƒãƒªãƒ¼ã®æ›´æ–°
    if scan_nodes:
        total_uncompressed_mb = sum(scan["estimated_uncompressed_mb"] for scan in scan_nodes)
        total_compressed_mb = sum(scan["estimated_compressed_mb"] for scan in scan_nodes)
        if total_compressed_mb > 0:
            broadcast_analysis["compression_analysis"]["avg_compression_ratio"] = total_uncompressed_mb / total_compressed_mb
        broadcast_analysis["compression_analysis"]["total_uncompressed_mb"] = total_uncompressed_mb
        broadcast_analysis["compression_analysis"]["total_compressed_mb"] = total_compressed_mb
    
    # ç·ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é‡ã¨ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆåœ§ç¸®ãƒ™ãƒ¼ã‚¹ï¼‰
    total_read_gb = overall_metrics.get('read_bytes', 0) / 1024 / 1024 / 1024
    estimated_total_compressed_mb = sum(scan["estimated_compressed_mb"] for scan in scan_nodes)
    
    if estimated_total_compressed_mb > 0:
        size_ratio = (total_read_gb * 1024) / estimated_total_compressed_mb
        if size_ratio > 3 or size_ratio < 0.3:
            broadcast_analysis["reasoning"].append(f"æ¨å®šåœ§ç¸®ã‚µã‚¤ã‚º({estimated_total_compressed_mb:.1f}MB)ã¨å®Ÿèª­ã¿è¾¼ã¿é‡({total_read_gb:.1f}GB)ã«ä¹–é›¢ã‚ã‚Š - ã‚µã‚¤ã‚ºæ¨å®šã«æ³¨æ„")
        else:
            broadcast_analysis["reasoning"].append(f"ã‚µã‚¤ã‚ºæ¨å®šæ•´åˆæ€§: æ¨å®šåœ§ç¸®{estimated_total_compressed_mb:.1f}MB vs å®Ÿéš›{total_read_gb:.1f}GBï¼ˆæ¯”ç‡:{size_ratio:.2f}ï¼‰")
    
    # BROADCASTæ¨å¥¨äº‹é …ã®ç”Ÿæˆï¼ˆ30MBé–¾å€¤å¯¾å¿œã€æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã‚’è€ƒæ…®ï¼‰
    total_broadcast_candidates = len(small_tables) + len(marginal_tables)
    total_tables = len(scan_nodes)
    
    if small_tables or marginal_tables:
        if large_tables:
            # æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã‚’è€ƒæ…®ã—ãŸåˆ¤å®š
            if broadcast_analysis["already_optimized"]:
                broadcast_analysis["feasibility"] = "already_optimized_with_improvements"
                broadcast_analysis["recommendations"] = [
                    f"âœ… æ—¢ã«BROADCAST JOINé©ç”¨æ¸ˆã¿ - è¿½åŠ æ”¹å–„ã®æ¤œè¨",
                    f"ğŸ¯ è¿½åŠ æœ€é©åŒ–ãƒ†ãƒ¼ãƒ–ãƒ«: {total_broadcast_candidates}å€‹ï¼ˆå…¨{total_tables}å€‹ä¸­ï¼‰",
                    f"  âœ… å¼·ãæ¨å¥¨: {len(small_tables)}å€‹ï¼ˆå®‰å…¨é–¾å€¤{broadcast_safe_mb:.1f}MBä»¥ä¸‹ï¼‰",
                    f"  âš ï¸ æ¡ä»¶ä»˜ãæ¨å¥¨: {len(marginal_tables)}å€‹ï¼ˆé–¾å€¤{broadcast_threshold_mb:.1f}MBä»¥ä¸‹ã€è¦æ³¨æ„ï¼‰",
                    f"  âŒ éæ¨å¥¨: {len(large_tables)}å€‹ï¼ˆé–¾å€¤è¶…éï¼‰"
                ]
            else:
                broadcast_analysis["feasibility"] = "recommended"
                broadcast_analysis["recommendations"] = [
                    f"ğŸ¯ BROADCASTæ¨å¥¨ãƒ†ãƒ¼ãƒ–ãƒ«: {total_broadcast_candidates}å€‹ï¼ˆå…¨{total_tables}å€‹ä¸­ï¼‰",
                    f"  âœ… å¼·ãæ¨å¥¨: {len(small_tables)}å€‹ï¼ˆå®‰å…¨é–¾å€¤{broadcast_safe_mb:.1f}MBä»¥ä¸‹ï¼‰",
                    f"  âš ï¸ æ¡ä»¶ä»˜ãæ¨å¥¨: {len(marginal_tables)}å€‹ï¼ˆé–¾å€¤{broadcast_threshold_mb:.1f}MBä»¥ä¸‹ã€è¦æ³¨æ„ï¼‰",
                    f"  âŒ éæ¨å¥¨: {len(large_tables)}å€‹ï¼ˆé–¾å€¤è¶…éï¼‰"
                ]
        else:
            # å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå°ã•ã„å ´åˆ
            if broadcast_analysis["already_optimized"]:
                broadcast_analysis["feasibility"] = "already_optimized_complete"
                broadcast_analysis["recommendations"] = [
                    f"âœ… æ—¢ã«BROADCAST JOINé©ç”¨æ¸ˆã¿ - æœ€é©åŒ–å®Œäº†",
                    f"ğŸ¯ å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ{total_tables}å€‹ï¼‰ãŒBROADCASTé–¾å€¤ä»¥ä¸‹ã§é©åˆ‡ã«å‡¦ç†æ¸ˆã¿",
                    f"  âœ… å¼·ãæ¨å¥¨: {len(small_tables)}å€‹",
                    f"  âš ï¸ æ¡ä»¶ä»˜ãæ¨å¥¨: {len(marginal_tables)}å€‹",
                    "ğŸ“‹ ç¾åœ¨ã®è¨­å®šãŒæœ€é©ã§ã™"
                ]
            else:
                broadcast_analysis["feasibility"] = "all_small"
                broadcast_analysis["recommendations"] = [
                    f"ğŸ¯ å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ{total_tables}å€‹ï¼‰ãŒBROADCASTé–¾å€¤ä»¥ä¸‹",
                    f"  âœ… å¼·ãæ¨å¥¨: {len(small_tables)}å€‹",
                    f"  âš ï¸ æ¡ä»¶ä»˜ãæ¨å¥¨: {len(marginal_tables)}å€‹",
                    "ğŸ“‹ æœ€å°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å„ªå…ˆçš„ã«BROADCASTã™ã‚‹ã“ã¨ã‚’æ¨å¥¨"
                ]
        
        # å…·ä½“çš„ãªBROADCASTå€™è£œã®è©³ç´°
        for small_table in small_tables:
            broadcast_analysis["recommendations"].append(
                f"ğŸ”¹ BROADCAST({small_table['node_name']}) - éåœ§ç¸®{small_table['estimated_uncompressed_mb']:.1f}MBï¼ˆåœ§ç¸®{small_table['estimated_compressed_mb']:.1f}MBã€{small_table['file_format']}ã€åœ§ç¸®ç‡{small_table['compression_ratio']:.1f}xï¼‰"
            )
        
        for marginal_table in marginal_tables:
            broadcast_analysis["recommendations"].append(
                f"ğŸ”¸ BROADCAST({marginal_table['node_name']}) - éåœ§ç¸®{marginal_table['estimated_uncompressed_mb']:.1f}MBï¼ˆæ¡ä»¶ä»˜ãã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¦æ³¨æ„ï¼‰"
            )
            
    elif large_tables:
        broadcast_analysis["feasibility"] = "not_recommended"
        broadcast_analysis["recommendations"] = [
            f"âŒ å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ{len(large_tables)}å€‹ï¼‰ãŒ30MBé–¾å€¤è¶…éã®ãŸã‚BROADCASTéæ¨å¥¨",
            f"ğŸ“Š æœ€å°ãƒ†ãƒ¼ãƒ–ãƒ«ã§ã‚‚éåœ§ç¸®{min(scan['estimated_uncompressed_mb'] for scan in large_tables):.1f}MB",
            "ğŸ”§ ä»£æ›¿æœ€é©åŒ–æ‰‹æ³•ã‚’æ¨å¥¨:",
            "  â€¢ Liquid Clusteringå®Ÿè£…",
            "  â€¢ ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°",
            "  â€¢ ã‚¯ã‚¨ãƒªæœ€é©åŒ–ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ç­‰ï¼‰",
            "  â€¢ spark.databricks.optimizer.autoBroadcastJoinThresholdè¨­å®šå€¤ã®èª¿æ•´æ¤œè¨"
        ]
    else:
        broadcast_analysis["feasibility"] = "insufficient_data"
        broadcast_analysis["recommendations"] = [
            "âš ï¸ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€æ‰‹å‹•ã§ã®ã‚µã‚¤ã‚ºç¢ºèªãŒå¿…è¦",
            "ğŸ“‹ ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºã‚’ç¢ºèª:",
            "  â€¢ DESCRIBE DETAIL table_name",
            "  â€¢ SELECT COUNT(*) FROM table_name",
            "  â€¢ SHOW TABLE EXTENDED LIKE 'table_name'"
        ]
    
    # 30MBé–¾å€¤ã«ãƒ’ãƒƒãƒˆã™ã‚‹ç‰¹åˆ¥ãªã‚±ãƒ¼ã‚¹åˆ†æï¼ˆsmall_tables + marginal_tables ã‚’è€ƒæ…®ï¼‰
    all_30mb_candidates = small_tables + marginal_tables  # 30MBä»¥ä¸‹ã®å…¨å€™è£œ
    
    if all_30mb_candidates:
        broadcast_analysis["30mb_hit_analysis"] = {
            "has_30mb_candidates": True,
            "candidate_count": len(all_30mb_candidates),
            "small_tables_count": len(small_tables),  # 24MBä»¥ä¸‹ï¼ˆå¼·ãæ¨å¥¨ï¼‰
            "marginal_tables_count": len(marginal_tables),  # 24-30MBï¼ˆæ¡ä»¶ä»˜ãæ¨å¥¨ï¼‰
            "smallest_table_mb": min(scan["estimated_uncompressed_mb"] for scan in all_30mb_candidates),
            "largest_candidate_mb": max(scan["estimated_uncompressed_mb"] for scan in all_30mb_candidates),
            "total_candidate_size_mb": sum(scan["estimated_uncompressed_mb"] for scan in all_30mb_candidates),
            "recommended_broadcast_table": all_30mb_candidates[0]["node_name"] if all_30mb_candidates else None,
            "memory_impact_estimation": f"{sum(scan['estimated_uncompressed_mb'] for scan in all_30mb_candidates):.1f}MB ãŒãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã«ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ"
        }
        
        # æœ€é©ãªBROADCASTå€™è£œã®ç‰¹å®šï¼ˆå…¨30MBå€™è£œã‹ã‚‰é¸æŠï¼‰
        if len(all_30mb_candidates) > 1:
            optimal_candidate = min(all_30mb_candidates, key=lambda x: x["estimated_uncompressed_mb"])
            broadcast_analysis["30mb_hit_analysis"]["optimal_candidate"] = {
                "table": optimal_candidate["node_name"],
                "size_mb": optimal_candidate["estimated_uncompressed_mb"],
                "rows": optimal_candidate["rows"],
                "reasoning": f"æœ€å°ã‚µã‚¤ã‚º{optimal_candidate['estimated_uncompressed_mb']:.1f}MBã§æœ€ã‚‚åŠ¹ç‡çš„"
            }
        
        # 30MBé–¾å€¤å†…ã®è©³ç´°åˆ†é¡æƒ…å ±ã‚’è¿½åŠ 
        broadcast_analysis["30mb_hit_analysis"]["size_classification"] = {
            "safe_zone_tables": len(small_tables),  # 0-24MBï¼ˆå®‰å…¨ãƒãƒ¼ã‚¸ãƒ³å†…ï¼‰
            "caution_zone_tables": len(marginal_tables),  # 24-30MBï¼ˆè¦æ³¨æ„ï¼‰
            "safe_zone_description": "24MBä»¥ä¸‹ï¼ˆå¼·ãæ¨å¥¨ã€å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³å†…ï¼‰",
            "caution_zone_description": "24-30MBï¼ˆæ¡ä»¶ä»˜ãæ¨å¥¨ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¦æ³¨æ„ï¼‰"
        }
    else:
        broadcast_analysis["30mb_hit_analysis"] = {
            "has_30mb_candidates": False,
            "reason": f"å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ãŒ30MBé–¾å€¤ã‚’è¶…éï¼ˆæœ€å°: {min(scan['estimated_uncompressed_mb'] for scan in scan_nodes):.1f}MBï¼‰" if scan_nodes else "ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ãªã—"
        }
    
    return broadcast_analysis

def extract_structured_physical_plan(physical_plan: str) -> Dict[str, Any]:
    """
    Structured extraction of important information only from Physical Plan (countermeasure for token limits)
    
    Args:
        physical_plan: Complete text of Physical Plan
    
    Returns:
        Dict: Structured important information
    """
    import re
    
    extracted = {
        "joins": [],           # JOINæƒ…å ±ï¼ˆç¨®é¡ã€æ¡ä»¶ã€çµ±è¨ˆï¼‰
        "scans": [],          # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆã‚µã‚¤ã‚ºã€è¡Œæ•°ï¼‰  
        "exchanges": [],      # ãƒ‡ãƒ¼ã‚¿ç§»å‹•ï¼ˆShuffleã€Broadcastï¼‰
        "aggregates": [],     # é›†ç´„å‡¦ç†ï¼ˆGROUP BYã€SUMç­‰ï¼‰
        "filters": [],        # ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã¨é¸æŠç‡
        "photon_usage": {},   # Photonåˆ©ç”¨çŠ¶æ³
        "bottlenecks": [],    # ç‰¹å®šã•ã‚ŒãŸãƒœãƒˆãƒ«ãƒãƒƒã‚¯
        "statistics": {},     # æ•°å€¤çµ±è¨ˆã‚µãƒãƒªãƒ¼
        "total_size": len(physical_plan),
        "extraction_summary": ""
    }
    
    try:
        lines = physical_plan.split('\n')
        join_count = scan_count = exchange_count = 0
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # JOINæƒ…å ±ã®æŠ½å‡ºï¼ˆå¾“æ¥å½¢å¼ + Photonå½¢å¼å®Œå…¨å¯¾å¿œï¼‰
            # å¾“æ¥ã®Spark JOINå½¢å¼ï¼ˆStatisticsä»˜ãï¼‰
            join_match = re.search(r'(\w*Join)\s+([^,\n]+).*?Statistics\(([^)]+)\)', line)
            # Photon JOINå½¢å¼ï¼ˆStatisticsç„¡ã—ã€è©³ç´°ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜ãï¼‰
            photon_join_match = re.search(r'(Photon\w*Join)\s+\[([^\]]+)\],\s*\[([^\]]+)\],\s*(\w+),\s*(\w+)', line)
            
            if join_match or photon_join_match:
                if join_match:
                    # å¾“æ¥ã®Spark JOINå½¢å¼
                    join_type = join_match.group(1)
                    condition = join_match.group(2).strip()
                    stats = join_match.group(3)
                    
                    # çµ±è¨ˆæƒ…å ±ã‹ã‚‰æ•°å€¤æŠ½å‡º
                    size_match = re.search(r'sizeInBytes=([0-9.]+)\s*([KMGT]i?B)?', stats)
                    rows_match = re.search(r'rowCount=(\d+)', stats)
                    
                    size_str = f"{size_match.group(1)}{size_match.group(2) or 'B'}" if size_match else "unknown"
                    rows_str = rows_match.group(1) if rows_match else "unknown"
                    
                elif photon_join_match:
                    # Photon JOINå½¢å¼ã®è©³ç´°æŠ½å‡º
                    join_type = photon_join_match.group(1)  # PhotonBroadcastHashJoinç­‰
                    left_keys = photon_join_match.group(2)   # å·¦å´ã®JOINã‚­ãƒ¼
                    right_keys = photon_join_match.group(3)  # å³å´ã®JOINã‚­ãƒ¼
                    join_method = photon_join_match.group(4) # Inner, Leftç­‰
                    build_side = photon_join_match.group(5)  # BuildRight, BuildLeftç­‰
                    
                    # JOINæ¡ä»¶ã®æ§‹æˆ
                    condition = f"{left_keys} = {right_keys} ({join_method}, {build_side})"
                    
                    # Photon JOINã¯çµ±è¨ˆæƒ…å ±ãŒåˆ¥ã®å ´æ‰€ã«ã‚ã‚‹ãŸã‚ã€ã“ã“ã§ã¯åŸºæœ¬æƒ…å ±ã®ã¿
                    size_str = "photon_optimized"
                    rows_str = "photon_optimized"
                
                extracted["joins"].append({
                    "type": join_type,
                    "condition": condition[:100],  # æ¡ä»¶ã‚’100æ–‡å­—ã«åˆ¶é™
                    "size": size_str,
                    "rows": rows_str
                })
                join_count += 1
                
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³æƒ…å ±ã®æŠ½å‡ºï¼ˆå¾“æ¥å½¢å¼ + Photonå½¢å¼å®Œå…¨å¯¾å¿œï¼‰
            elif ('FileScan' in line and 'Statistics(' in line) or ('PhotonScan' in line and 'parquet' in line):
                # å¾“æ¥å½¢å¼ï¼šStatisticsä»˜ãFileScan
                stats_match = re.search(r'Statistics\(([^)]+)\)', line)
                # Photonå½¢å¼ï¼šPhotonScan parquet table_name[columns]
                photon_scan_match = re.search(r'PhotonScan\s+parquet\s+([a-zA-Z_][a-zA-Z0-9_.]*)\[([^\]]+)\]', line)
                # å¾“æ¥å½¢å¼ï¼šFileScan
                file_scan_match = re.search(r'FileScan\s+([^,\s\[]+)', line)
                
                if (stats_match and file_scan_match) or photon_scan_match:
                    if photon_scan_match:
                        # Photonå½¢å¼ã®å ´åˆ
                        table = photon_scan_match.group(1)  # ãƒ†ãƒ¼ãƒ–ãƒ«å
                        columns = photon_scan_match.group(2)  # åˆ—ãƒªã‚¹ãƒˆ
                        stats = None  # PhotonScanã«ã¯çµ±è¨ˆæƒ…å ±ãŒåŒä¸€è¡Œã«ãªã„
                        
                        # ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆã®ä¿å­˜ï¼ˆPhotonç”¨ã®æ§‹é€ ï¼‰
                        extracted["scans"].append({
                            "table": table[:50],
                            "columns": columns[:100],
                            "type": "PhotonScan",
                            "size": "photon_scan",
                            "rows": "photon_scan"
                        })
                        scan_count += 1
                        
                    elif stats_match and file_scan_match:
                        # å¾“æ¥å½¢å¼ã®å ´åˆ
                        stats = stats_match.group(1)
                        table = file_scan_match.group(1)
                        
                        size_match = re.search(r'sizeInBytes=([0-9.]+)\s*([KMGT]i?B)?', stats)
                        rows_match = re.search(r'rowCount=(\d+)', stats)
                        
                        extracted["scans"].append({
                            "table": table[:50],  # ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’50æ–‡å­—ã«åˆ¶é™
                            "type": "FileScan",
                            "size": f"{size_match.group(1)}{size_match.group(2) or 'B'}" if size_match else "unknown",
                            "rows": rows_match.group(1) if rows_match else "unknown"
                        })
                        scan_count += 1
                    
            # ãƒ‡ãƒ¼ã‚¿ç§»å‹•ï¼ˆExchangeï¼‰ã®æŠ½å‡º
            elif 'Exchange' in line:
                if 'BroadcastExchange' in line:
                    extracted["exchanges"].append({"type": "BROADCAST", "detail": line[:100]})
                elif 'ShuffleExchange' in line or 'Exchange' in line:
                    extracted["exchanges"].append({"type": "SHUFFLE", "detail": line[:100]})
                exchange_count += 1
                
            # é›†ç´„å‡¦ç†ã®æŠ½å‡º
            elif 'Aggregate' in line or 'HashAggregate' in line:
                extracted["aggregates"].append({"type": "AGGREGATE", "detail": line[:100]})
                
            # Photonåˆ©ç”¨çŠ¶æ³ã®ç¢ºèª
            elif 'Photon' in line:
                if 'PhotonResultStage' in line:
                    extracted["photon_usage"]["result_stage"] = True
                elif 'PhotonHashJoin' in line:
                    extracted["photon_usage"]["hash_join"] = True
                elif 'PhotonProject' in line:
                    extracted["photon_usage"]["project"] = True
        
        # çµ±è¨ˆã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        extracted["statistics"] = {
            "total_joins": join_count,
            "total_scans": scan_count,  
            "total_exchanges": exchange_count,
            "photon_operations": len([k for k, v in extracted["photon_usage"].items() if v])
        }
        
        # æŠ½å‡ºã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        extracted["extraction_summary"] = f"ğŸ“Š Structured extraction completed: JOIN({join_count}) SCAN({scan_count}) EXCHANGE({exchange_count}) PHOTON({len(extracted['photon_usage'])})"
        
        # ğŸš¨ ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾ç­–: æƒ…å ±é‡ãŒå¤šã„å ´åˆã®è‡ªå‹•è¦ç´„
        total_joins_scans = join_count + scan_count
        if total_joins_scans > 30:  # é–¾å€¤ã‚’å¤§å¹…ã«å¼•ãä¸Šã’: JOIN+SCANåˆè¨ˆãŒ30å€‹ä»¥ä¸Š
            # é‡è¦åº¦é †ã«ä¸¦ã³æ›¿ãˆã¦ãƒˆãƒƒãƒ—æƒ…å ±ã®ã¿ä¿æŒ
            extracted = apply_token_limit_optimization(extracted, max_joins=20, max_scans=15)  # åˆ¶é™ã‚’å¤§å¹…ç·©å’Œ
            extracted["extraction_summary"] += f" â†’ ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾ç­–ã§JOIN/SCANæƒ…å ±ã‚’è¦ç´„æ¸ˆã¿"
        elif total_joins_scans > 15:  # ä¸­é–“é–¾å€¤: 15-30å€‹ã®å ´åˆ
            # ä¸­ç¨‹åº¦ã®è¦ç´„
            extracted = apply_token_limit_optimization(extracted, max_joins=12, max_scans=10)
            extracted["extraction_summary"] += f" â†’ ä¸­ç¨‹åº¦ã®JOIN/SCANæƒ…å ±è¦ç´„æ¸ˆã¿"
        
    except Exception as e:
        extracted["extraction_error"] = str(e)
        
    return extracted

def extract_structured_cost_statistics(explain_cost_content: str) -> Dict[str, Any]:
    """
    Structured extraction of numerical statistics only from EXPLAIN COST (countermeasure for token limits)
    
    Args:
        explain_cost_content: Complete EXPLAIN COST results
    
    Returns:
        Dict: Structured statistical information
    """
    import re
    
    extracted = {
        "table_stats": {},      # Table-specific statistics (size, row count)
        "join_costs": {},       # JOIN-specific cost estimates  
        "selectivity": {},      # Filter selectivity
        "partition_info": {},   # Partition statistics
        "memory_estimates": {}, # Memory usage predictions
        "cost_breakdown": {},   # Cost breakdown
        "critical_stats": {},   # Critical statistical values
        "total_size": len(explain_cost_content),
        "extraction_summary": ""
    }
    
    try:
        lines = explain_cost_content.split('\n')
        tables_found = costs_found = memory_found = 0
        
        # é‡è¦çµ±è¨ˆå€¤ã‚’è¿½è·¡
        largest_table = {"name": "", "size": 0, "size_str": ""}
        total_rows = 0
        broadcast_candidates = []
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«åã¨ã‚µã‚¤ã‚ºã®å¯¾å¿œã‚’è¿½è·¡
        table_name_size_map = {}  # {table_name: {"size_bytes": int, "size_str": str, "rows": int}}
        current_table_context = None  # ç¾åœ¨å‡¦ç†ä¸­ã®ãƒ†ãƒ¼ãƒ–ãƒ«å
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ğŸ” ãƒ†ãƒ¼ãƒ–ãƒ«åã®æŠ½å‡ºï¼ˆRelationã‹ã‚‰ï¼‰
            table_name_match = re.search(r'Relation\s+([a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z_][a-zA-Z0-9_]*)', line)
            if table_name_match:
                current_table_context = table_name_match.group(1)
                
            # ğŸ” ãƒ†ãƒ¼ãƒ–ãƒ«åã®æŠ½å‡ºï¼ˆJoinæ¡ä»¶ã‹ã‚‰ï¼‰
            elif 'Join' in line and '=' in line:
                # JOINæ¡ä»¶ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æ¨å®š (ä¾‹: ty_brand#456 = ly_brand#789)
                join_match = re.search(r'([a-zA-Z_][a-zA-Z0-9_]*)[#.]', line)
                if join_match and not current_table_context:
                    # JOINæ¡ä»¶ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«æ¨å®š
                    prefix = join_match.group(1)
                    if len(prefix) > 2:  # æ„å‘³ã®ã‚ã‚‹ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
                        current_table_context = f"{prefix}_table"
                
            # ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆã®æŠ½å‡º
            if 'Statistics(' in line:
                # ã‚µã‚¤ã‚ºæƒ…å ±ã®æŠ½å‡º
                size_match = re.search(r'sizeInBytes=([0-9.]+)\s*([KMGT]i?B)?', line)
                rows_match = re.search(r'rowCount=(\d+)', line)
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«åã®æ±ºå®š
                if current_table_context:
                    table_name = current_table_context
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è¡Œç•ªå·ã‹ã‚‰æ¨å®š
                    line_table_match = re.search(r'([a-zA-Z_][a-zA-Z0-9_]*)', line)
                    table_name = line_table_match.group(1) if line_table_match else f"table_{tables_found}"
                
                if size_match:
                    size_val = float(size_match.group(1))
                    size_unit = size_match.group(2) or 'B'
                    size_str = f"{size_val}{size_unit}"
                    
                    # ã‚µã‚¤ã‚ºå¤‰æ›ï¼ˆãƒã‚¤ãƒˆå˜ä½ï¼‰
                    size_bytes = size_val
                    if 'KiB' in size_unit:
                        size_bytes *= 1024
                    elif 'MiB' in size_unit:
                        size_bytes *= 1024 * 1024
                    elif 'GiB' in size_unit:
                        size_bytes *= 1024 * 1024 * 1024
                    elif 'TiB' in size_unit:
                        size_bytes *= 1024 * 1024 * 1024 * 1024
                    
                    # è¡Œæ•°ã®å–å¾—
                    rows = int(rows_match.group(1)) if rows_match else 0
                    
                    # ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆã®ä¿å­˜
                    extracted["table_stats"][table_name] = {
                        "size_bytes": size_bytes,
                        "size_str": size_str,
                        "rows": rows,
                        "is_broadcast_candidate": size_bytes < 30 * 1024 * 1024  # 30MB
                    }
                    
                    # æœ€å¤§ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¿½è·¡
                    if size_bytes > largest_table["size"]:
                        largest_table = {"name": table_name, "size": size_bytes, "size_str": size_str}
                    
                    # ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆå€™è£œï¼ˆ30MBæœªæº€ï¼‰
                    if size_bytes < 30 * 1024 * 1024:  # 30MB
                        broadcast_candidates.append({"table": table_name, "size": size_str})
                    
                    tables_found += 1
                    total_rows += rows
                    
                # ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆæ¬¡ã®ãƒ†ãƒ¼ãƒ–ãƒ«ç”¨ï¼‰
                current_table_context = None
                    
            # ã‚³ã‚¹ãƒˆæƒ…å ±ã®æŠ½å‡º  
            elif 'Cost(' in line:
                cost_match = re.search(r'Cost\(([0-9.]+)\)', line)
                if cost_match:
                    extracted["cost_breakdown"][f"operation_{costs_found}"] = float(cost_match.group(1))
                    costs_found += 1
                    
            # ãƒ¡ãƒ¢ãƒªé–¢é€£æƒ…å ±ã®æŠ½å‡º
            elif any(keyword in line.lower() for keyword in ['memory', 'spill', 'threshold']):
                if 'memory' in line.lower():
                    memory_match = re.search(r'(\d+(?:\.\d+)?)\s*([KMGT]i?B)', line)
                    if memory_match:
                        extracted["memory_estimates"][f"estimate_{memory_found}"] = f"{memory_match.group(1)}{memory_match.group(2)}"
                        memory_found += 1
        
        # é‡è¦çµ±è¨ˆå€¤ã®ã¾ã¨ã‚
        extracted["critical_stats"] = {
            "largest_table": largest_table,
            "total_rows": total_rows,
            "broadcast_candidates": broadcast_candidates[:5],  # ä¸Šä½5å€‹ã¾ã§
            "tables_analyzed": tables_found,
            "cost_operations": costs_found,
            "memory_estimates": memory_found,
            "table_breakdown": {
                "total_tables": len(extracted["table_stats"]),
                "largest_table_name": largest_table.get("name", "unknown"),
                "broadcast_table_names": [bc.get("table", "unknown") for bc in broadcast_candidates[:3]]
            }
        }
        
        # æŠ½å‡ºã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        extracted["extraction_summary"] = f"ğŸ’° Statistics extraction completed: Tables({tables_found}) Cost({costs_found}) Memory({memory_found}) BROADCAST candidates({len(broadcast_candidates)})"
        
    except Exception as e:
        extracted["extraction_error"] = str(e)
        
    return extracted

def apply_token_limit_optimization(extracted: Dict[str, Any], max_joins: int = 5, max_scans: int = 8) -> Dict[str, Any]:
    """
    ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾ç­–: JOIN/SCANæƒ…å ±ã®é‡è¦åº¦åˆ¥è¦ç´„
    
    Args:
        extracted: æŠ½å‡ºã•ã‚ŒãŸæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
        max_joins: ä¿æŒã™ã‚‹JOINæ•°ã®ä¸Šé™
        max_scans: ä¿æŒã™ã‚‹SCANæ•°ã®ä¸Šé™
    
    Returns:
        æœ€é©åŒ–ã•ã‚ŒãŸæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
    """
    
    # JOINæƒ…å ±ã®é‡è¦åº¦åˆ¥ã‚½ãƒ¼ãƒˆ
    joins = extracted.get("joins", [])
    if len(joins) > max_joins:
        # é‡è¦åº¦é †åº: Broadcast > Hash > Sort > Nested
        join_priority = {
            "PhotonBroadcastHashJoin": 1,
            "BroadcastHashJoin": 2,
            "PhotonHashJoin": 3,
            "HashJoin": 4,
            "SortMergeJoin": 5,
            "NestedLoopJoin": 6
        }
        
        # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
        sorted_joins = sorted(joins, key=lambda j: join_priority.get(j.get("type", ""), 10))
        
        # ä¸Šä½ã®ã¿ä¿æŒã€æ®‹ã‚Šã¯è¦ç´„
        top_joins = sorted_joins[:max_joins]
        remaining_count = len(joins) - max_joins
        
        if remaining_count > 0:
            summary_join = {
                "type": "SUMMARY",
                "condition": f"Other {remaining_count} JOIN operations (details omitted)",
                "size": "multiple",
                "rows": "multiple"
            }
            top_joins.append(summary_join)
        
        extracted["joins"] = top_joins
    
    # SCANæƒ…å ±ã®é‡è¦åº¦åˆ¥ã‚½ãƒ¼ãƒˆ
    scans = extracted.get("scans", [])
    if len(scans) > max_scans:
        # é‡è¦åº¦é †åº: PhotonScan > FileScanã€ãƒ†ãƒ¼ãƒ–ãƒ«åã®é•·ã•ï¼ˆè©³ç´°åº¦ï¼‰
        def scan_priority(scan):
            priority = 1 if scan.get("type") == "PhotonScan" else 2
            table_length = len(scan.get("table", ""))
            return (priority, -table_length)  # ãƒ†ãƒ¼ãƒ–ãƒ«åãŒé•·ã„ï¼ˆè©³ç´°ï¼‰ã»ã©é‡è¦
        
        # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
        sorted_scans = sorted(scans, key=scan_priority)
        
        # ä¸Šä½ã®ã¿ä¿æŒã€æ®‹ã‚Šã¯è¦ç´„
        top_scans = sorted_scans[:max_scans]
        remaining_count = len(scans) - max_scans
        
        if remaining_count > 0:
            # æ®‹ã‚Šã®ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’é›†ç´„
            remaining_tables = [s.get("table", "unknown")[:20] for s in sorted_scans[max_scans:]]
            table_summary = ", ".join(remaining_tables[:3])
            if len(remaining_tables) > 3:
                table_summary += f" ä»–{len(remaining_tables)-3}å€‹"
                
            summary_scan = {
                "table": f"SUMMARY({table_summary})",
                "type": "SUMMARY",
                "size": "multiple",
                "rows": "multiple"
            }
            top_scans.append(summary_scan)
        
        extracted["scans"] = top_scans
    
    # çµ±è¨ˆæƒ…å ±ã®æ›´æ–°
    extracted["statistics"]["optimization_applied"] = True
    extracted["statistics"]["original_joins"] = len(joins)
    extracted["statistics"]["original_scans"] = len(scans)
    extracted["statistics"]["optimized_joins"] = len(extracted["joins"])
    extracted["statistics"]["optimized_scans"] = len(extracted["scans"])
    
    return extracted

def extract_cost_statistics_from_explain_cost(explain_cost_content: str) -> str:
    """
    EXPLAIN COSTçµæœã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚’æŠ½å‡ºã—ã¦æ§‹é€ åŒ–ï¼ˆæ”¹å–„ç‰ˆ + ã‚µã‚¤ã‚ºåˆ¶é™ï¼‰
    
    Args:
        explain_cost_content: EXPLAIN COSTã®çµæœæ–‡å­—åˆ—
    
    Returns:
        æ§‹é€ åŒ–ã•ã‚ŒãŸçµ±è¨ˆæƒ…å ±æ–‡å­—åˆ—ï¼ˆãƒ¬ãƒãƒ¼ãƒˆç”¨ã«ç°¡æ½”åŒ–ï¼‰
    """
    if not explain_cost_content:
        return ""
    
    # ğŸš¨ ãƒ¬ãƒãƒ¼ãƒˆè‚¥å¤§åŒ–é˜²æ­¢ï¼šã‚µãƒãƒªãƒ¼æƒ…å ±ã®ã¿æŠ½å‡º
    statistics_counts = {
        "ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆ": 0,
        "è¡Œæ•°æƒ…å ±": 0, 
        "ã‚µã‚¤ã‚ºæƒ…å ±": 0,
        "ã‚³ã‚¹ãƒˆæƒ…å ±": 0,
        "é¸æŠç‡æƒ…å ±": 0,
        "ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±": 0,
        "ãƒ¡ãƒ¢ãƒªæƒ…å ±": 0,
        "JOINæƒ…å ±": 0
    }
    
    # é‡è¦ãªçµ±è¨ˆå€¤ã®ã¿æŠ½å‡ºï¼ˆè©³ç´°ã¯é™¤å¤–ï¼‰
    key_statistics = []
    MAX_KEY_STATS = 5  # é‡è¦çµ±è¨ˆæƒ…å ±ã®æœ€å¤§æ•°
    
    try:
        lines = explain_cost_content.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆæƒ…å ±ã®æŠ½å‡ºï¼ˆã‚«ã‚¦ãƒ³ãƒˆã®ã¿ï¼‰
            if 'statistics=' in line.lower() or 'stats=' in line.lower() or 'Statistics(' in line:
                statistics_counts["ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆ"] += 1
                if len(key_statistics) < MAX_KEY_STATS and 'sizeInBytes' in line:
                    # é‡è¦ãªã‚µã‚¤ã‚ºæƒ…å ±ã®ã¿æŠ½å‡º
                    if 'GiB' in line or 'TiB' in line:
                        key_statistics.append(f"ğŸ“Š ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚º: {line[:100]}...")
            
            # è¡Œæ•°æƒ…å ±ã®æŠ½å‡ºï¼ˆã‚«ã‚¦ãƒ³ãƒˆã®ã¿ï¼‰
            elif 'rows=' in line.lower() or 'rowcount=' in line.lower() or 'rows:' in line.lower():
                statistics_counts["è¡Œæ•°æƒ…å ±"] += 1
            
            # ã‚µã‚¤ã‚ºæƒ…å ±ã®æŠ½å‡ºï¼ˆã‚«ã‚¦ãƒ³ãƒˆã®ã¿ï¼‰
            elif ('size=' in line.lower() or 'sizeinbytes=' in line.lower() or 'sizeInBytes=' in line 
                  or 'GB' in line or 'MB' in line or 'size:' in line.lower()):
                statistics_counts["ã‚µã‚¤ã‚ºæƒ…å ±"] += 1
            
            # ãã®ä»–ã®çµ±è¨ˆæƒ…å ±ã®ã‚«ã‚¦ãƒ³ãƒˆ
            elif ('cost=' in line.lower() or 'Cost(' in line or 'cost:' in line.lower()):
                statistics_counts["ã‚³ã‚¹ãƒˆæƒ…å ±"] += 1
            elif ('selectivity=' in line.lower() or 'filter=' in line.lower()):
                statistics_counts["é¸æŠç‡æƒ…å ±"] += 1
            elif ('partition' in line.lower() and ('count' in line.lower() or 'size' in line.lower())):
                statistics_counts["ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±"] += 1
            elif ('memory' in line.lower() or 'spill' in line.lower()):
                statistics_counts["ãƒ¡ãƒ¢ãƒªæƒ…å ±"] += 1
            elif ('join' in line.lower() and ('cost' in line.lower() or 'selectivity' in line.lower())):
                statistics_counts["JOINæƒ…å ±"] += 1
    
    except Exception as e:
        return f"âš ï¸ çµ±è¨ˆæƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    # ç°¡æ½”ãªã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ
    summary_lines = ["## ğŸ“Š çµ±è¨ˆæƒ…å ±ã‚µãƒãƒªãƒ¼ï¼ˆç°¡æ½”ç‰ˆï¼‰"]
    
    total_stats = sum(statistics_counts.values())
    if total_stats > 0:
        summary_lines.append(f"- **ç·çµ±è¨ˆé …ç›®æ•°**: {total_stats}å€‹")
        
        for stat_type, count in statistics_counts.items():
            if count > 0:
                summary_lines.append(f"- **{stat_type}**: {count}å€‹")
        
        if key_statistics:
            summary_lines.append("\n### ğŸ¯ ä¸»è¦çµ±è¨ˆ")
            summary_lines.extend(key_statistics)
        
        summary_lines.append(f"\nğŸ’¡ è©³ç´°ãªçµ±è¨ˆæƒ…å ±ã¯ DEBUG_ENABLED='Y' ã§ç¢ºèªã§ãã¾ã™")
    else:
        summary_lines.append("- çµ±è¨ˆæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    return '\n'.join(summary_lines)


def generate_optimized_query_with_llm(original_query: str, analysis_result: str, metrics: Dict[str, Any]) -> str:
    """
    Optimize SQL query based on detailed bottleneck analysis results from Cell 33 (processing speed priority)
    Also leverages statistical information when EXPLAIN + EXPLAIN COST execution flag is Y
    """
    
    # EXPLAIN + EXPLAIN COSTçµæœãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆEXPLAIN_ENABLEDãŒYã®å ´åˆï¼‰
    explain_content = ""
    explain_cost_content = ""
    physical_plan = ""
    photon_explanation = ""
    cost_statistics = ""
    
    explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
    if explain_enabled.upper() == 'Y':
        import glob
        import os
        
        print("ğŸ” Searching for EXPLAIN + EXPLAIN COST result files...")
        
        # 1. Search for latest EXPLAIN result files (supporting new filename patterns)
        explain_original_files = glob.glob("output_explain_original_*.txt")
        explain_optimized_files = glob.glob("output_explain_optimized_*.txt")
        
        # Prioritize original query EXPLAIN results, use optimized if not available
        explain_files = explain_original_files if explain_original_files else explain_optimized_files
        
        if explain_files:
            latest_explain_file = max(explain_files, key=os.path.getctime)
            try:
                with open(latest_explain_file, 'r', encoding='utf-8') as f:
                    explain_content = f.read()
                    print(f"âœ… Loaded EXPLAIN result file: {latest_explain_file}")
                
                # Extract and process Physical Plan (structured extraction support)
                if "== Physical Plan ==" in explain_content:
                    physical_plan_start = explain_content.find("== Physical Plan ==")
                    physical_plan_end = explain_content.find("== Photon", physical_plan_start)
                    if physical_plan_end == -1:
                        physical_plan_end = len(explain_content)
                    physical_plan_raw = explain_content[physical_plan_start:physical_plan_end].strip()
                    print(f"ğŸ“Š Extracted Physical Plan information: {len(physical_plan_raw)} characters")
                    
                                    # ğŸ§  æ§‹é€ åŒ–æŠ½å‡º vs å¾“æ¥ã®åˆ‡ã‚Šè©°ã‚ã®é¸æŠ
                structured_enabled = globals().get('STRUCTURED_EXTRACTION_ENABLED', 'Y')
                debug_enabled = globals().get('DEBUG_ENABLED', 'N')
                
                # ğŸ§  æ§‹é€ åŒ–æŠ½å‡º vs å¾“æ¥ã®åˆ‡ã‚Šè©°ã‚ã®é¸æŠ
                structured_enabled = globals().get('STRUCTURED_EXTRACTION_ENABLED', 'Y')
                debug_enabled = globals().get('DEBUG_ENABLED', 'N')
                
                if structured_enabled.upper() == 'Y':
                    # ğŸš€ æ§‹é€ åŒ–æŠ½å‡ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
                    try:
                        structured_plan = extract_structured_physical_plan(physical_plan_raw)
                        
                        # Convert structured results to JSON format string
                        import json
                        physical_plan = json.dumps(structured_plan, ensure_ascii=False, indent=2)
                        
                        print(f"ğŸ§  Structured extraction completed: {len(physical_plan_raw):,} â†’ {len(physical_plan):,} characters")
                        print(f"   {structured_plan.get('extraction_summary', 'ğŸ“Š Structured extraction completed')}")
                        
                        # When DEBUG_ENABLED='Y', save structured results and original data
                        if debug_enabled.upper() == 'Y':
                            try:
                                from datetime import datetime
                                timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                                
                                # Save structured results
                                structured_plan_filename = f"output_physical_plan_structured_{timestamp}.json"
                                with open(structured_plan_filename, 'w', encoding='utf-8') as f:
                                    f.write(physical_plan)
                                
                                print(f"ğŸ“„ Saved structured Physical Plan: {structured_plan_filename}")
                                
                            except Exception as save_error:
                                print(f"âš ï¸ Failed to save Physical Plan: {str(save_error)}")
                                
                    except Exception as extraction_error:
                        print(f"âš ï¸ Structured extraction failed, falling back to traditional method: {str(extraction_error)}")
                        # Fallback: Traditional truncation method
                        MAX_PLAN_SIZE = 30000
                        if len(physical_plan_raw) > MAX_PLAN_SIZE:
                            physical_plan = physical_plan_raw[:MAX_PLAN_SIZE] + "\n\nStructured extraction failed, truncated to limit"
                            print(f"âš ï¸ Fallback: Physical Plan truncated to {MAX_PLAN_SIZE} characters")
                        else:
                            physical_plan = physical_plan_raw
                            print(f"âš ï¸ Physical Plan truncated to {MAX_PLAN_SIZE} characters due to token limit")
                
                # Extract Photon Explanation
                if "== Photon Explanation ==" in explain_content:
                    photon_start = explain_content.find("== Photon Explanation ==")
                    photon_explanation = explain_content[photon_start:].strip()
                    print(f"ğŸš€ Extracted Photon Explanation information: {len(photon_explanation)} characters")
                    
            except Exception as e:
                print(f"âš ï¸ Failed to load EXPLAIN result file: {str(e)}")
                explain_content = ""
        
        # 2. Search for latest EXPLAIN COST result files
        cost_original_files = glob.glob("output_explain_cost_original_*.txt")
        cost_optimized_files = glob.glob("output_explain_cost_optimized_*.txt")
        
        # Prioritize original query EXPLAIN COST results, use optimized if not available
        cost_files = cost_original_files if cost_original_files else cost_optimized_files
        
        if cost_files:
            latest_cost_file = max(cost_files, key=os.path.getctime)
            try:
                with open(latest_cost_file, 'r', encoding='utf-8') as f:
                    explain_cost_content = f.read()
                    print(f"ğŸ’° Loaded EXPLAIN COST result file: {latest_cost_file}")
                
                # Extract statistical information (structured extraction support)
                structured_enabled = globals().get('STRUCTURED_EXTRACTION_ENABLED', 'Y')
                
                if structured_enabled.upper() == 'Y':
                    # ğŸš€ æ§‹é€ åŒ–æŠ½å‡ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
                    try:
                        structured_cost = extract_structured_cost_statistics(explain_cost_content)
                        
                        # Convert structured results to JSON format string
                        import json
                        cost_statistics = json.dumps(structured_cost, ensure_ascii=False, indent=2)
                        
                        print(f"ğŸ’° EXPLAIN COST structured extraction completed: {len(explain_cost_content):,} â†’ {len(cost_statistics):,} characters (compression ratio: {len(explain_cost_content)//len(cost_statistics) if len(cost_statistics) > 0 else 0}x)")
                        print(f"   {structured_cost.get('extraction_summary', 'ğŸ’° Statistical extraction completed')}")
                        
                    except Exception as extraction_error:
                        print(f"âš ï¸ EXPLAIN COST structured extraction failed, falling back to traditional method: {str(extraction_error)}")
                        # Fallback: Traditional extraction method
                        cost_statistics = extract_cost_statistics_from_explain_cost(explain_cost_content)
                        print(f"ğŸ“Š Extracted EXPLAIN COST statistics (traditional method): {len(cost_statistics)} characters")
                else:
                    # ğŸ”„ Traditional extraction approach
                    cost_statistics = extract_cost_statistics_from_explain_cost(explain_cost_content)
                    print(f"ğŸ“Š Extracted EXPLAIN COST statistics: {len(cost_statistics)} characters")
                
                # ğŸš¨ When DEBUG_ENABLED='Y', always save extracted statistical information
                debug_enabled = globals().get('DEBUG_ENABLED', 'N')
                if debug_enabled.upper() == 'Y':
                    try:
                        from datetime import datetime
                        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                        extracted_stats_filename = f"output_explain_cost_statistics_extracted_{timestamp}.json"
                        
                        with open(extracted_stats_filename, 'w', encoding='utf-8') as f:
                            f.write(f"# Extracted EXPLAIN COST statistical information (Generated date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\n")
                            f.write(f"# Extraction size: {len(cost_statistics):,} characters\n")
                            f.write(f"# Source file: {latest_cost_file}\n\n")
                            f.write(cost_statistics)
                        
                        print(f"ğŸ“„ Saved extracted statistical information: {extracted_stats_filename}")
                        
                    except Exception as save_error:
                        print(f"âš ï¸ Failed to save extracted statistical information: {str(save_error)}")
                
                # Size limit for statistical information (countermeasure for LLM token limits)
                MAX_STATISTICS_SIZE = 50000  # ç´„50KBåˆ¶é™
                if len(cost_statistics) > MAX_STATISTICS_SIZE:
                    # ğŸš¨ DEBUG_ENABLED='Y'ã®å ´åˆã€å®Œå…¨ãªEXPLAIN COSTçµ±è¨ˆæƒ…å ±ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                    debug_enabled = globals().get('DEBUG_ENABLED', 'N')
                    if debug_enabled.upper() == 'Y':
                        try:
                            from datetime import datetime
                            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                            full_stats_filename = f"output_explain_cost_statistics_full_{timestamp}.txt"
                            
                            with open(full_stats_filename, 'w', encoding='utf-8') as f:
                                f.write(f"# Complete EXPLAIN COST statistical information (Generated date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\n")
                                f.write(f"# Original size: {len(cost_statistics):,} characters\n")
                                f.write(f"# LLM usage size: {MAX_STATISTICS_SIZE:,} characters\n\n")
                                f.write(cost_statistics)
                            
                            print(f"ğŸ“„ Saved complete EXPLAIN COST statistical information: {full_stats_filename}")
                            
                        except Exception as save_error:
                            print(f"âš ï¸ Failed to save EXPLAIN COST statistical information: {str(save_error)}")
                    
                    truncated_statistics = cost_statistics[:MAX_STATISTICS_SIZE]
                    truncated_statistics += f"\n\nâš ï¸ Statistical information was too large, truncated to {MAX_STATISTICS_SIZE} characters"
                    cost_statistics = truncated_statistics
                    print(f"âš ï¸ Statistical information truncated to {MAX_STATISTICS_SIZE} characters due to token limit")
                    
            except Exception as e:
                print(f"âš ï¸ Failed to load EXPLAIN COST result file: {str(e)}")
                explain_cost_content = ""
        
        if not explain_files and not cost_files:
            print("âš ï¸ EXPLAINãƒ»EXPLAIN COST result files not found")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ãƒã‚§ãƒƒã‚¯
            old_explain_files = glob.glob("output_explain_plan_*.txt")
            if old_explain_files:
                latest_explain_file = max(old_explain_files, key=os.path.getctime)
                try:
                    with open(latest_explain_file, 'r', encoding='utf-8') as f:
                        explain_content = f.read()
                        print(f"âœ… Loaded legacy format EXPLAIN result file: {latest_explain_file}")
                        
                    # Physical PlanæŠ½å‡ºï¼ˆæ—§å½¢å¼å¯¾å¿œï¼‰
                    if "== Physical Plan ==" in explain_content:
                        physical_plan_start = explain_content.find("== Physical Plan ==")
                        physical_plan_end = explain_content.find("== Photon", physical_plan_start)
                        if physical_plan_end == -1:
                            physical_plan_end = len(explain_content)
                        physical_plan = explain_content[physical_plan_start:physical_plan_end].strip()
                        
                    if "== Photon Explanation ==" in explain_content:
                        photon_start = explain_content.find("== Photon Explanation ==")
                        photon_explanation = explain_content[photon_start:].strip()
                except Exception as e:
                    print(f"âš ï¸ Failed to load legacy format EXPLAIN result file: {str(e)}")
            else:
                print("âš ï¸ EXPLAIN result files not found")
    
    # å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã®æŠ½å‡ºï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ï¼‰
    profiler_data = metrics.get('raw_profiler_data', {})
    plan_info = None
    if profiler_data:
        plan_info = extract_execution_plan_info(profiler_data)
    
    # BROADCASTé©ç”¨å¯èƒ½æ€§ã®åˆ†æï¼ˆãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’å«ã‚€ï¼‰
    # ğŸ¯ BROADCASTæœ€é©åŒ–ã¯ç„¡åŠ¹åŒ–ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã«ã‚ˆã‚Šé™¤å¤–ï¼‰
    # ğŸš¨ é‡è¦: ã™ã¹ã¦ã®å¿…è¦ãªã‚­ãƒ¼ã‚’å«ã‚ã‚‹ï¼ˆKeyErroré˜²æ­¢ï¼‰
    broadcast_analysis = {
        "feasibility": "disabled", 
        "broadcast_candidates": [], 
        "recommendations": [],
        "reasoning": ["BROADCASTãƒ’ãƒ³ãƒˆã¯æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®åŸå› ã¨ãªã‚‹ãŸã‚ç„¡åŠ¹åŒ–"], 
        "is_join_query": True,
        "already_optimized": False,  # ğŸš¨ ç·Šæ€¥ä¿®æ­£: å¿…é ˆã‚­ãƒ¼è¿½åŠ 
        "spark_threshold_mb": 30.0,
        "compression_analysis": {},
        "detailed_size_analysis": [],
        "execution_plan_analysis": {},
        "existing_broadcast_nodes": [],
        "broadcast_applied_tables": [],
        # ğŸš¨ ç·Šæ€¥ä¿®æ­£: 30mb_hit_analysis ã‚­ãƒ¼è¿½åŠ ï¼ˆKeyErroré˜²æ­¢ï¼‰
        "30mb_hit_analysis": {
            "has_30mb_candidates": False,
            "reason": "BROADCASTãƒ’ãƒ³ãƒˆã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹ãŸã‚åˆ†æå¯¾è±¡å¤–"
        }
    }
    
    # ãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã§ä½¿ç”¨ï¼‰
    if plan_info:
        metrics['execution_plan_info'] = plan_info
    
    # ğŸš€ ã‚»ãƒ«33ã‚¹ã‚¿ã‚¤ãƒ«ã®è©³ç´°ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’å®Ÿè¡Œ
    detailed_bottleneck = extract_detailed_bottleneck_analysis(metrics)
    
    # æœ€é©åŒ–ã®ãŸã‚ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’æº–å‚™ï¼ˆè©³ç´°ç‰ˆï¼‰
    optimization_context = []
    performance_critical_issues = []
    
    # åŸºæœ¬çš„ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯æƒ…å ±ã®æŠ½å‡º
    bottlenecks = metrics.get('bottleneck_indicators', {})
    
    if bottlenecks.get('has_spill', False):
        spill_gb = bottlenecks.get('spill_bytes', 0) / 1024 / 1024 / 1024
        optimization_context.append(f"ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ: {spill_gb:.1f}GB - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®æ”¹å–„ãŒå¿…è¦")
    
    if bottlenecks.get('has_shuffle_bottleneck', False):
        optimization_context.append("ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ - JOINã¨GROUP BYã®æœ€é©åŒ–ãŒå¿…è¦")
    
    if bottlenecks.get('cache_hit_ratio', 0) < 0.5:
        optimization_context.append("ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ä½ä¸‹ - ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœ€é©åŒ–ãŒå¿…è¦")
    
    # ğŸ¯ è©³ç´°ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœã‹ã‚‰ã®è¿½åŠ æƒ…å ±
    if detailed_bottleneck["spill_analysis"]["total_spill_gb"] > 0:
        total_spill = detailed_bottleneck["spill_analysis"]["total_spill_gb"]
        spill_nodes_count = len(detailed_bottleneck["spill_analysis"]["spill_nodes"])
        performance_critical_issues.append(f"ğŸš¨ CRITICAL: åˆè¨ˆ{total_spill:.1f}GBã®ã‚¹ãƒ”ãƒ«ãŒ{spill_nodes_count}å€‹ã®ãƒãƒ¼ãƒ‰ã§ç™ºç”Ÿ")
        
        # æœ€ã‚‚é‡è¦ãªã‚¹ãƒ”ãƒ«ãƒãƒ¼ãƒ‰ã‚’ç‰¹å®š
        if detailed_bottleneck["spill_analysis"]["spill_nodes"]:
            top_spill_node = max(detailed_bottleneck["spill_analysis"]["spill_nodes"], key=lambda x: x["spill_gb"])
            performance_critical_issues.append(f"   æœ€å¤§ã‚¹ãƒ”ãƒ«ãƒãƒ¼ãƒ‰: {top_spill_node['node_name']} ({top_spill_node['spill_gb']:.2f}GB)")
    
    if detailed_bottleneck["skew_analysis"]["total_skewed_partitions"] > 0:
        total_skew = detailed_bottleneck["skew_analysis"]["total_skewed_partitions"]
        skewed_nodes_count = len(detailed_bottleneck["skew_analysis"]["skewed_nodes"])
        performance_critical_issues.append(f"âš–ï¸ ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼: {total_skew}å€‹ã®ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãŒ{skewed_nodes_count}å€‹ã®ãƒãƒ¼ãƒ‰ã§æ¤œå‡º")
    
    # TOP3ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãƒãƒ¼ãƒ‰ã®è©³ç´°åˆ†æ
    top3_bottlenecks = detailed_bottleneck["top_bottleneck_nodes"][:3]
    performance_critical_issues.append("ğŸ“Š TOP3å‡¦ç†æ™‚é–“ãƒœãƒˆãƒ«ãƒãƒƒã‚¯:")
    for node in top3_bottlenecks:
        severity_icon = "ğŸ”´" if node["severity"] == "CRITICAL" else "ğŸŸ " if node["severity"] == "HIGH" else "ğŸŸ¡"
        performance_critical_issues.append(f"   {severity_icon} #{node['rank']}: {node['node_name'][:60]}...")
        performance_critical_issues.append(f"      å®Ÿè¡Œæ™‚é–“: {node['duration_ms']:,}ms ({node['time_percentage']:.1f}%) | ãƒ¡ãƒ¢ãƒª: {node['memory_mb']:.1f}MB")
        if node["spill_detected"]:
            performance_critical_issues.append(f"      ğŸ’¿ ã‚¹ãƒ”ãƒ«: {node['spill_gb']:.2f}GB - ç·Šæ€¥å¯¾å¿œå¿…è¦")
        if node["skew_detected"]:
            performance_critical_issues.append(f"      âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼: {node['skewed_partitions']}ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ - ãƒ‡ãƒ¼ã‚¿åˆ†æ•£æ”¹å–„å¿…è¦")
    
    # ğŸ”„ REPARTITIONãƒ’ãƒ³ãƒˆã®è©³ç´°ç”Ÿæˆï¼ˆã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿ï¼‰
    repartition_hints = []
    if detailed_bottleneck["shuffle_optimization_hints"]:
        repartition_hints.append("ğŸ”„ REPARTITIONãƒ’ãƒ³ãƒˆï¼ˆã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿ï¼‰:")
        for hint in detailed_bottleneck["shuffle_optimization_hints"]:
            priority_icon = "ğŸš¨" if hint["priority"] == "HIGH" else "ğŸ“ˆ"
            repartition_hints.append(f"   {priority_icon} ãƒãƒ¼ãƒ‰ID {hint['node_id']}: {hint['suggested_sql']}")
            repartition_hints.append(f"      å±æ€§: {', '.join(hint['attributes'])}")
            repartition_hints.append(f"      ç†ç”±: {hint['reason']}")
            repartition_hints.append(f"      åŠ¹æœ: {hint['estimated_improvement']}")
            
            # ã‚¯ã‚¨ãƒªã¸ã®é©ç”¨æ–¹æ³•ã®å…·ä½“çš„ãªææ¡ˆ
            main_attr = hint['attributes'][0]
            if 'GROUP BY' in original_query.upper():
                repartition_hints.append(f"      é©ç”¨ææ¡ˆ: GROUP BYå‰ã«REPARTITION({hint['suggested_sql'].split('(')[1]}")
            elif 'JOIN' in original_query.upper():
                repartition_hints.append(f"      é©ç”¨ææ¡ˆ: JOINå‰ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’{hint['suggested_sql']}ã§ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³")
    
    # ğŸ“Š å‡¦ç†é€Ÿåº¦é‡è¦–ã®æœ€é©åŒ–æ¨å¥¨äº‹é …
    speed_optimization_recommendations = []
    for rec in detailed_bottleneck["performance_recommendations"]:
        priority_icon = "ğŸš¨" if rec["priority"] == "CRITICAL" else "âš ï¸" if rec["priority"] == "HIGH" else "ğŸ“"
        speed_optimization_recommendations.append(f"{priority_icon} {rec['type'].upper()}: {rec['description']}")
    
    # Liquid Clusteringæ¨å¥¨æƒ…å ±ï¼ˆLLMãƒ™ãƒ¼ã‚¹å¯¾å¿œï¼‰
    liquid_analysis = metrics.get('liquid_clustering_analysis', {})
    extracted_data = liquid_analysis.get('extracted_data', {})
    table_info = extracted_data.get('table_info', {})
    
    clustering_recommendations = []
    if table_info:
        for table_name in list(table_info.keys())[:3]:  # ä¸Šä½3ãƒ†ãƒ¼ãƒ–ãƒ«
            clustering_recommendations.append(f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_name}: LLMåˆ†æã«ã‚ˆã‚‹æ¨å¥¨ã‚«ãƒ©ãƒ ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¨å¥¨")
    
    # æœ€é©åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆï¼ˆç°¡æ½”ç‰ˆã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿ï¼‰
    
    # åˆ†æçµæœã‚’ç°¡æ½”åŒ–ï¼ˆ128Kåˆ¶é™å†…ã§æœ€å¤§åŠ¹ç‡åŒ–ï¼‰
    analysis_summary = ""
    if isinstance(analysis_result, str) and len(analysis_result) > 2000:
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®¹é‡ã®ç¢ºä¿ã®ãŸã‚ã€åˆ†æçµæœã¯è¦ç‚¹ã®ã¿ã«åœ§ç¸®
        analysis_summary = analysis_result[:2000] + "...[è¦ç´„ï¼šä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ã¿ä¿æŒ]"
    else:
        analysis_summary = str(analysis_result)
    
    # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æƒ…å ±ã®ç°¡æ½”åŒ–
    bottleneck_summary = "ã€".join(optimization_context[:3]) if optimization_context else "ç‰¹ã«ãªã—"
    
    # Liquid Clusteringæ¨å¥¨ã®ç°¡æ½”åŒ–
    clustering_summary = "ã€".join(clustering_recommendations[:2]) if clustering_recommendations else "ç‰¹ã«ãªã—"
    
    # ğŸš¨ JOINæˆ¦ç•¥åˆ†æã®ç°¡ç•¥åŒ–ï¼ˆBROADCASTãƒ’ãƒ³ãƒˆç„¡åŠ¹åŒ–ï¼‰
    broadcast_summary = ["ğŸ¯ æœ€é©åŒ–æ–¹é‡: JOINé †åºæœ€é©åŒ–ï¼ˆSparkã®è‡ªå‹•æˆ¦ç•¥ã‚’æ´»ç”¨ã€ãƒ’ãƒ³ãƒˆä¸ä½¿ç”¨ï¼‰"]
    
    optimization_prompt = f"""
ã‚ãªãŸã¯Databricksã®SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®**è©³ç´°ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ**ã‚’åŸºã«ã€**å‡¦ç†é€Ÿåº¦é‡è¦–**ã§SQLã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªå‡¦ç†æ–¹é‡ã€‘
- ä¸€å›ã®å‡ºåŠ›ã§å®Œå…¨ãªSQLã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„
- æ®µéšçš„ãªå‡ºåŠ›ã‚„è¤‡æ•°å›ã«åˆ†ã‘ã¦ã®å‡ºåŠ›ã¯ç¦æ­¢ã§ã™
- thinkingæ©Ÿèƒ½ã§æ§‹é€ ç†è§£â†’ä¸€å›ã§å®Œå…¨ãªSQLå‡ºåŠ›
- **âŒ BROADCASTãƒ’ãƒ³ãƒˆï¼ˆ/*+ BROADCAST */ã€/*+ BROADCAST(table) */ï¼‰ã¯ä¸€åˆ‡ä½¿ç”¨ç¦æ­¢**
- **âœ… JOINæˆ¦ç•¥ã¯Sparkã®è‡ªå‹•æœ€é©åŒ–ã«å§”ã­ã¦ãƒ’ãƒ³ãƒˆä¸ä½¿ç”¨ã§æœ€é©åŒ–**

ã€å…ƒã®SQLã‚¯ã‚¨ãƒªã€‘
```sql
{original_query}
```

ã€ğŸ“Š ã‚»ãƒ«33è©³ç´°ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœã€‘
{chr(10).join(performance_critical_issues) if performance_critical_issues else "ç‰¹åˆ¥ãªé‡è¦èª²é¡Œã¯è¨­å®šãªã—"}

ã€ğŸ”„ REPARTITIONãƒ’ãƒ³ãƒˆï¼ˆã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿ï¼‰ã€‘
{chr(10).join(repartition_hints) if repartition_hints else "ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„ãŸã‚ã€REPARTITIONãƒ’ãƒ³ãƒˆã¯é©ç”¨å¯¾è±¡å¤–ã§ã™"}

ã€ğŸš€ å‡¦ç†é€Ÿåº¦é‡è¦–ã®æœ€é©åŒ–æ¨å¥¨äº‹é …ã€‘
{chr(10).join(speed_optimization_recommendations) if speed_optimization_recommendations else "ç‰¹åˆ¥ãªæ¨å¥¨äº‹é …ã¯ã‚ã‚Šã¾ã›ã‚“"}

ã€åŸºæœ¬çš„ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯æƒ…å ±ã€‘
{chr(10).join(optimization_context) if optimization_context else "ä¸»è¦ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯è¨­å®šãªã—"}

ã€JOINæˆ¦ç•¥åˆ†æçµæœã€‘
Sparkã®è‡ªå‹•JOINæˆ¦ç•¥ã‚’ä½¿ç”¨ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ãƒ’ãƒ³ãƒˆã¯ä½¿ç”¨ã›ãšï¼‰

ã€Liquid Clusteringæ¨å¥¨ã€‘
{chr(10).join(clustering_recommendations) if clustering_recommendations else "ç‰¹åˆ¥ãªæ¨å¥¨äº‹é …ã¯ã‚ã‚Šã¾ã›ã‚“"}

ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æçµæœï¼ˆã‚µãƒãƒªãƒ¼ï¼‰ã€‘
{analysis_summary}

ã€ğŸ” EXPLAINçµæœåˆ†æï¼ˆEXPLAIN_ENABLED=Yã®å ´åˆã®ã¿ï¼‰ã€‘
{f'''
**Physical Planåˆ†æ:**
```
{physical_plan}
```

**Photon Explanationåˆ†æ:**
```
{photon_explanation}
```

**Physical Planæœ€é©åŒ–ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆ:**
- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ã®åŠ¹ç‡æ€§
- ã‚¸ãƒ§ã‚¤ãƒ³æˆ¦ç•¥ã®å¦¥å½“æ€§
- ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œã®æœ€å°åŒ–
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆåˆ—é¸æŠï¼‰ã®æœ€é©åŒ–
- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ã®æ´»ç”¨

**Photonæœ€é©åŒ–ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆ:**
- Photonæœªå¯¾å¿œé–¢æ•°ã®æ¤œå‡ºã¨ä»£æ›¿é–¢æ•°ã¸ã®å¤‰æ›´
- ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†ã«é©ã—ãŸé–¢æ•°ã®é¸æŠ
- Photonåˆ©ç”¨ç‡å‘ä¸Šã®ãŸã‚ã®æ›¸å¼å¤‰æ›´
- ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚æœ€é©åŒ–ã®æ´»ç”¨
''' if explain_enabled.upper() == 'Y' and (physical_plan or photon_explanation) else '(EXPLAINå®Ÿè¡ŒãŒç„¡åŠ¹ã€ã¾ãŸã¯EXPLAINçµæœãŒåˆ©ç”¨ã§ãã¾ã›ã‚“)'}

ã€ğŸ’° EXPLAIN COSTçµ±è¨ˆæƒ…å ±åˆ†æï¼ˆçµ±è¨ˆãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ï¼‰ã€‘
{f'''
**æ§‹é€ åŒ–EXPLAIN COSTçµ±è¨ˆæƒ…å ±:**
```json
{cost_statistics}
```

**ğŸ§  æ§‹é€ åŒ–çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨æŒ‡é‡:**
ä¸Šè¨˜ã¯æ§‹é€ åŒ–æŠ½å‡ºã•ã‚ŒãŸçµ±è¨ˆæƒ…å ±ã§ã™ã€‚ä»¥ä¸‹ã®é …ç›®ã‚’é‡ç‚¹çš„ã«åˆ†æã—ã¦ãã ã•ã„ï¼š

- **table_stats**: ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥è©³ç´°çµ±è¨ˆï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«åã€ã‚µã‚¤ã‚ºã€è¡Œæ•°ï¼‰
- **critical_stats**: é‡è¦çµ±è¨ˆå€¤ï¼ˆæœ€å¤§ãƒ†ãƒ¼ãƒ–ãƒ«ã€ç·è¡Œæ•°ã€å°ãƒ†ãƒ¼ãƒ–ãƒ«å€™è£œï¼‰
- **largest_table**: æœ€å¤§ãƒ†ãƒ¼ãƒ–ãƒ«ã®åå‰ã¨ã‚µã‚¤ã‚ºï¼ˆJOINé †åºã®åŸºæº–ï¼‰
- **small_table_candidates**: å°ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«åã¨ã‚µã‚¤ã‚ºï¼‰
- **table_breakdown**: ãƒ†ãƒ¼ãƒ–ãƒ«åã®è©³ç´°ï¼ˆæœ€å¤§ãƒ†ãƒ¼ãƒ–ãƒ«åã€å°ãƒ†ãƒ¼ãƒ–ãƒ«åï¼‰

**ğŸ¯ ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ä½¿ã£ãŸç²¾å¯†æœ€é©åŒ–:**
1. **JOINé †åºã®æœ€é©åŒ–:**
   - ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºã«åŸºã¥ãåŠ¹ç‡çš„ãªJOINé †åºã®æ±ºå®š
   - å°ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å¤§ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®æ®µéšçš„çµåˆ

2. **JOINé †åºã®å…·ä½“çš„ææ¡ˆ:**
   - largest_table.nameã‚’æœ€å¾Œã«é…ç½®
   - table_statsã®ã‚µã‚¤ã‚ºé †ã§JOINé †åºã‚’æœ€é©åŒ–
   - å…·ä½“çš„ãªãƒ†ãƒ¼ãƒ–ãƒ«åã§JOINæ–‡ã‚’æ”¹å–„

3. **æ›–æ˜§æ€§è§£æ±ºã®å…·ä½“çš„ææ¡ˆ:**
   - ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ†ãƒ¼ãƒ–ãƒ«åã¨table_statsã‚’ç…§åˆ
   - å…·ä½“çš„ãªã‚¨ã‚¤ãƒªã‚¢ã‚¹ææ¡ˆï¼ˆä¾‹: `store_sales.ss_item_sk`ï¼‰

**ğŸš€ æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿è§£æã®å®Ÿè¡Œä¾‹:**
1. table_statså†…ã§å°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç‰¹å®šã—ã€åŠ¹ç‡çš„ãªJOINé †åºã‚’æ±ºå®š
2. largest_table_nameãŒ1GBä»¥ä¸Š â†’ å¤§ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦æœ€çµ‚JOINã«é…ç½®
3. JOINé †åºã®å…·ä½“çš„ãªæ”¹å–„ææ¡ˆã‚’ç”Ÿæˆ
4. ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æ˜ç¤ºã—ãŸJOINé †åºææ¡ˆã‚’ç”Ÿæˆ

**ğŸš¨ ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾ç­–ã«ã¤ã„ã¦:**
- JOIN/SCANæƒ…å ±ãŒå¤šæ•°ã®å ´åˆã€é‡è¦åº¦é †ã«è¦ç´„æ¸ˆã¿
- SUMMARYé …ç›®ã¯è¤‡æ•°æ“ä½œã®é›†ç´„ã‚’ç¤ºã—ã¾ã™
- è©³ç´°ã¯ optimization_applied ãƒ•ãƒ©ã‚°ã§ç¢ºèªå¯èƒ½
- Physical PlanãŒ100KBè¶…ã®å ´åˆã¯è‡ªå‹•èª¿æ•´æ¸ˆã¿
''' if explain_enabled.upper() == 'Y' and cost_statistics else '(EXPLAIN COSTå®Ÿè¡ŒãŒç„¡åŠ¹ã€ã¾ãŸã¯çµ±è¨ˆæƒ…å ±ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“)'}

ã€ğŸ¯ å‡¦ç†é€Ÿåº¦é‡è¦–ã®æœ€é©åŒ–è¦æ±‚ã€‘
**æœ€é‡è¦**: ä»¥ä¸‹ã®é †åºã§å‡¦ç†é€Ÿåº¦ã®æ”¹å–„ã‚’å„ªå…ˆã—ã¦ãã ã•ã„

1. **ğŸš¨ CRITICALå„ªå…ˆåº¦**: ã‚¹ãƒ”ãƒ«å¯¾ç­–ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ”¹å–„ï¼‰
   - å¤§é‡ã‚¹ãƒ”ãƒ«ï¼ˆ5GBä»¥ä¸Šï¼‰ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã¯æœ€å„ªå…ˆã§å¯¾å‡¦
   - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªJOINé †åºã®æ¤œè¨
   - ä¸­é–“çµæœã®ã‚µã‚¤ã‚ºå‰Šæ¸›

2. **ğŸ”„ REPARTITIONãƒ’ãƒ³ãƒˆé©ç”¨**ï¼ˆğŸš¨ **ã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®å ´åˆã®ã¿** - é‡è¦ãªæ¡ä»¶ï¼‰
   - âŒ **ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„å ´åˆ**: REPARTITIONãƒ’ãƒ³ãƒˆã¯ä¸€åˆ‡é©ç”¨ã—ãªã„
   - âœ… **ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã®ã¿**: REPARTITIONãƒ’ãƒ³ãƒˆã‚’é©ç”¨
   - âš ï¸ **è¨˜è¼‰ãƒ«ãƒ¼ãƒ«**: ã‚¹ãƒ”ãƒ«æœªæ¤œå‡ºã®å ´åˆã¯ã€ŒREPARTITIONã®é©ç”¨ã€ã‚’ä¸€åˆ‡è¨˜è¼‰ã—ãªã„
   - æ¤œå‡ºã•ã‚ŒãŸShuffle attributesã‚’åŸºã«å…·ä½“çš„ãªREPARTITIONãƒ’ãƒ³ãƒˆã‚’é©ç”¨ï¼ˆã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿ï¼‰

3. **âš–ï¸ ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼å¯¾ç­–**
   - ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ï¼ˆ10å€‹ä»¥ä¸Šï¼‰æ¤œå‡ºæ™‚ã¯åˆ†æ•£æ”¹å–„ã‚’å„ªå…ˆ
   - é©åˆ‡ãªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚­ãƒ¼ã®é¸æŠ
   - ãƒ‡ãƒ¼ã‚¿åˆ†æ•£ã®å‡ç­‰åŒ–

4. **ğŸ“ˆ ã‚·ãƒ£ãƒƒãƒ•ãƒ«æœ€é©åŒ–**
   - ã‚·ãƒ£ãƒƒãƒ•ãƒ«é‡ã®æœ€å°åŒ–
   - é©åˆ‡ãªJOINæˆ¦ç•¥ã®é¸æŠ
   - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è»¢é€é‡ã®å‰Šæ¸›

5. **ğŸ¯ JOINæˆ¦ç•¥æœ€é©åŒ–**
   - å°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å…ˆã«å‡¦ç†ã™ã‚‹åŠ¹ç‡çš„ãªJOINé †åº
   - Sparkã®è‡ªå‹•æœ€é©åŒ–ã‚’æ´»ç”¨ã—ãŸJOINæˆ¦ç•¥ï¼ˆãƒ’ãƒ³ãƒˆä¸ä½¿ç”¨ï¼‰
   - ä¸­é–“çµæœã®ã‚µã‚¤ã‚ºæœ€å°åŒ–

6. **ğŸ’¾ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–**
   - ä¸è¦ãªã‚«ãƒ©ãƒ ã®é™¤å»
   - é©åˆ‡ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é †åº
   - ä¸­é–“çµæœã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨

7. **ğŸ”§ å®Ÿè¡Œãƒ—ãƒ©ãƒ³æœ€é©åŒ–**
   - PHOTONã‚¨ãƒ³ã‚¸ãƒ³æœ€é©åŒ–ï¼ˆç›®æ¨™ã¯Photonåˆ©ç”¨ç‡90%ä»¥ä¸Š)
   - Liquid Clusteringæ´»ç”¨ (Whereæ¡ä»¶ã®æ›¸ãæ›ãˆå«ã‚€æ¤œè¨ã‚’å®Ÿæ–½ï¼‰
   - CTEæ´»ç”¨ã«ã‚ˆã‚‹å…±é€šåŒ–

8. **ğŸ“Š EXPLAINçµæœã«åŸºã¥ãæœ€é©åŒ–**ï¼ˆEXPLAIN_ENABLED=Yã®å ´åˆï¼‰
   - **Physical Planåˆ†æã«åŸºã¥ãæœ€é©åŒ–**: 
     - éåŠ¹ç‡ãªã‚¹ã‚­ãƒ£ãƒ³æ“ä½œã®æ”¹å–„
     - ã‚¸ãƒ§ã‚¤ãƒ³é †åºã®æœ€é©åŒ–ï¼ˆSparkã®è‡ªå‹•åˆ¤å®šã«ä¾å­˜ï¼‰
     - ä¸è¦ãªã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œã®å‰Šé™¤
     - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ã®é©ç”¨
   - **Photonæœªå¯¾å¿œé–¢æ•°ã®æœ€é©åŒ–**:
     - Photon Explanationã§æ¤œå‡ºã•ã‚ŒãŸæœªå¯¾å¿œé–¢æ•°ã®ä»£æ›¿é–¢æ•°ã¸ã®å¤‰æ›´
     - ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†ã«é©ã—ãŸé–¢æ•°ã¸ã®æ›¸ãæ›ãˆ
     - Photonåˆ©ç”¨ç‡å‘ä¸Šã®ãŸã‚ã®é–¢æ•°é¸æŠ
     - ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚æœ€é©åŒ–ã®æ´»ç”¨

9. **ğŸ¯ JOINé †åºã¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã®æœ€é©åŒ–**ï¼ˆé‡è¦ãªæ§‹é€ çš„æœ€é©åŒ–ï¼‰
   - **åŠ¹ç‡çš„ãªJOINé †åº**: å°ã•ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å¤§ãã„ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®æ®µéšçš„çµåˆ
   - **Sparkã®è‡ªå‹•JOINæˆ¦ç•¥**: ã‚¨ãƒ³ã‚¸ãƒ³ã®è‡ªå‹•åˆ¤å®šã«å§”ã­ã‚‹ã“ã¨ã§ã‚¨ãƒ©ãƒ¼å›é¿
   - **çµåˆå¾Œã®REPARTITION**: çµåˆå¾Œã«GROUP BYã®åŠ¹ç‡åŒ–ã®ãŸã‚REPARTITIONãƒ’ãƒ³ãƒˆã‚’é©ç”¨
   - **CTEæ§‹é€ ã®æ´»ç”¨**: å¿…è¦ã«å¿œã˜ã¦CTEã‚’ä½¿ã£ã¦æ®µéšçš„ã«å‡¦ç†ã™ã‚‹æ§‹é€ ã§å‡ºåŠ›
   - **ã‚¹ãƒ”ãƒ«å›é¿ã¨ä¸¦åˆ—åº¦**: ã‚¹ãƒ”ãƒ«ã‚’å›é¿ã—ã¤ã¤ã€ä¸¦åˆ—åº¦ã®é«˜ã„å‡¦ç†ãŒã§ãã‚‹ã‚ˆã†æœ€é©åŒ–
   
   **ğŸ”„ æ¨å¥¨ã™ã‚‹å‡¦ç†ãƒ•ãƒ­ãƒ¼:**
   ```sql
   -- âœ… æ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³: åŠ¹ç‡çš„JOINé †åº â†’ CTE â†’ REPARTITION â†’ GROUP BY
   WITH efficient_joined AS (
     SELECT 
       large_table.columns...,
       small_table.columns...
     FROM small_table  -- å°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å…ˆã«é…ç½®
       JOIN large_table ON small_table.key = large_table.key
   ),
   repartitioned_for_groupby AS (
     SELECT /*+ REPARTITION(200, group_key) */
       columns...
     FROM efficient_joined
   )
   SELECT 
     group_key,
     COUNT(*),
     SUM(amount)
   FROM repartitioned_for_groupby
   GROUP BY group_key
   ```

ã€ğŸ”„ REPARTITIONãƒ’ãƒ³ãƒˆé©ç”¨ãƒ«ãƒ¼ãƒ« - æ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ã€‘
REPARTITIONãƒ’ãƒ³ãƒˆã‚’ä»˜ä¸ã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã®æœ€é©åŒ–ãƒ«ãƒ¼ãƒ«ã‚’å®ˆã£ã¦ãã ã•ã„ï¼š

ğŸš¨ **æœ€é‡è¦ãƒ«ãƒ¼ãƒ«**: 
- **âŒ ã‚¹ãƒ”ãƒ«æœªæ¤œå‡ºæ™‚**: REPARTITIONãƒ’ãƒ³ãƒˆã¯çµ¶å¯¾ã«é©ç”¨ãƒ»è¨˜è¼‰ã—ã¦ã¯ã„ã‘ãªã„
- **âœ… ã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿**: REPARTITIONãƒ’ãƒ³ãƒˆã‚’é©ç”¨
- **âš ï¸ è¨˜è¼‰ç¦æ­¢**: ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„å ´åˆã€æ¨å¥¨äº‹é …ã‚„ç·Šæ€¥å¯¾å¿œã«ã€ŒREPARTITIONé©ç”¨ã€ã‚’å«ã‚ãªã„

æŠ€è¡“è©³ç´°:
- **REPARTITIONãƒ’ãƒ³ãƒˆã¯ SELECT /*+ REPARTITION(ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°, ã‚«ãƒ©ãƒ å) ã®å½¢å¼ã§æŒ‡å®š**
- **REPARTITIONãƒ’ãƒ³ãƒˆã®é©ç”¨ä½ç½®ã¯ã€å¯¾è±¡ã¨ãªã‚‹JOINã‚„GROUP BYã‚’å«ã‚€SELECTã®ç›´å‰ã§ã‚ã‚‹ãŸã‚ã€å‡ºåŠ›ã•ã‚ŒãŸoutput_explain_plan_*.txtã®Physical Planã‹ã‚‰å®Ÿè¡Œè¨ˆç”»ã‚’ç†è§£ã—ã€é©åˆ‡ãªä½ç½®ã«REPARTITION ãƒ’ãƒ³ãƒˆã‚’ä»˜ä¸ã™ã‚‹ã“ã¨**

**ğŸš¨ REPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ã®é‡è¦ãªæ§‹æ–‡ãƒ«ãƒ¼ãƒ«:**
1. **JOINã‚„GROUP BYã®å‡¦ç†æ®µéšã§åŠ¹æœã‚’ç™ºæ®ã™ã‚‹ãŸã‚ã€å¿…ãšã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ã™ã‚‹**
2. **ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®SELECTæ–‡ã«é…ç½®ã™ã‚‹ã¨æœ€çµ‚å‡ºåŠ›æ®µéšã®ã¿ã«å½±éŸ¿ã—ã€JOIN/GROUP BYå‡¦ç†æ®µéšã«ã¯å½±éŸ¿ã—ãªã„**
3. **è¤‡æ•°ã®REPARTITIONãƒ’ãƒ³ãƒˆã¯å„ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«å€‹åˆ¥ã«é…ç½®ã™ã‚‹**
4. **ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã¨ã‚«ãƒ©ãƒ åã¯å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦æŒ‡å®šã™ã‚‹**

ğŸš¨ **REPARTITIONãƒ’ãƒ³ãƒˆé©ç”¨ã®å³æ ¼ãªãƒ«ãƒ¼ãƒ«**ï¼š
- **âŒ ã‚¹ãƒ”ãƒ«æœªæ¤œå‡º**: REPARTITIONãƒ’ãƒ³ãƒˆã¯çµ¶å¯¾ã«é©ç”¨ã—ãªã„ãƒ»è¨˜è¼‰ã—ãªã„
- **âœ… ã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿**: GROUP BYå‰ã«REPARTITION(æ¨å¥¨æ•°, group_by_column)
- **âœ… ã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿**: JOINå‰ã«REPARTITION(æ¨å¥¨æ•°, join_key)
- **é‡è¦**: ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ŒREPARTITIONã®é©ç”¨ã€ã‚’æ¨å¥¨äº‹é …ã«å«ã‚ãªã„
- **è¨˜è¼‰ç¦æ­¢**: ã‚¹ãƒ”ãƒ«æœªæ¤œå‡ºæ™‚ã«ã€Œç·Šæ€¥å¯¾å¿œ: REPARTITIONã®é©ç”¨ã€ç­‰ã‚’è¨˜è¼‰ã—ã¦ã¯ã„ã‘ãªã„

**ğŸš¨ CREATE TABLE AS SELECT (CTAS) ã§ã®REPARTITIONé…ç½®ã®é‡è¦ãªæ³¨æ„äº‹é …:**
- CREATE TABLE AS SELECTæ–‡ã§ã¯ã€ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®SELECTå¥ã«REPARTITIONãƒ’ãƒ³ãƒˆã‚’é…ç½®ã™ã‚‹ã¨ã€**æœ€çµ‚çš„ãªå‡ºåŠ›æ›¸ãè¾¼ã¿æ®µéšã®ã¿ã«å½±éŸ¿**ã—ã€JOIN ã‚„é›†è¨ˆãªã©ã®ä¸­é–“å‡¦ç†æ®µéšã«ã¯å½±éŸ¿ã—ãªã„
- JOINã®å‰ã«ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã‚’åˆ¶å¾¡ã™ã‚‹ã«ã¯ã€**REPARTITIONãƒ’ãƒ³ãƒˆã‚’ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ã™ã‚‹å¿…è¦ãŒã‚ã‚‹**
- ã“ã‚Œã«ã‚ˆã‚Šã€SparkãŒãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã®é©åˆ‡ãªæ™‚ç‚¹ã§ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ã—ã€æ›¸ãè¾¼ã¿æ®µéšã§ã¯ãªãå®Ÿè¡Œæ®µéšã§æœ€é©åŒ–ã•ã‚Œã‚‹

**æ­£ã—ã„CTAS REPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ä¾‹:**
```sql
-- âŒ é–“é•ã„: ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®SELECTå¥ï¼ˆæ›¸ãè¾¼ã¿æ®µéšã®ã¿ã«å½±éŸ¿ï¼‰
CREATE TABLE optimized_table AS
SELECT /*+ REPARTITION(200, join_key) */
  t1.column1, t2.column2
FROM table1 t1
  JOIN table2 t2 ON t1.join_key = t2.join_key

-- âœ… æ­£ã—ã„: ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ï¼ˆJOINå‡¦ç†æ®µéšã§æœ€é©åŒ–ï¼‰
CREATE TABLE optimized_table AS
SELECT 
  t1.column1, t2.column2
FROM (
  SELECT /*+ REPARTITION(200, join_key) */
    column1, join_key
  FROM table1
) t1
  JOIN table2 t2 ON t1.join_key = t2.join_key
```

**ğŸš¨ å…¨èˆ¬çš„ãªREPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ã®é‡è¦ãªæ³¨æ„äº‹é …:**
- **CTASä»¥å¤–ã®ã‚¯ã‚¨ãƒªã§ã‚‚åŒæ§˜**ï¼šãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®ã‚¯ã‚¨ãƒªã«REPARTITIONãƒ’ãƒ³ãƒˆã‚’é…ç½®ã™ã‚‹ã¨ã€**æœ€çµ‚çš„ãªå‡ºåŠ›æ®µéšã®ã¿ã«å½±éŸ¿**ã—ã€JOIN ã‚„é›†è¨ˆãªã©ã®ä¸­é–“å¤‰æ›æ®µéšã«ã¯å½±éŸ¿ã—ãªã„
- ã“ã®å‹•ä½œã¯ã€çµæœã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã«æ›¸ãè¾¼ã‚€ã‹ã©ã†ã‹ã«é–¢ä¿‚ãªã**ã™ã¹ã¦ã®Spark SQLã‚¯ã‚¨ãƒªã§ä¸€è²«**ã—ã¦ã„ã‚‹
- JOINã®å…¥åŠ›æ®µéšã§ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’ç¢ºå®Ÿã«å®Ÿè¡Œã™ã‚‹ã«ã¯ã€**REPARTITIONãƒ’ãƒ³ãƒˆã‚’ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ã™ã‚‹å¿…è¦ãŒã‚ã‚‹**
- ã“ã‚Œã«ã‚ˆã‚Šã€SparkãŒé©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã®æ™‚ç‚¹ã§ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ã—ã€æœ€çµ‚å‡ºåŠ›æ®µéšã§ã¯ãªãå®Ÿè¡Œæ®µéšã§æœ€é©åŒ–ã•ã‚Œã‚‹

**ä¸€èˆ¬çš„ãªã‚¯ã‚¨ãƒªã§ã®æ­£ã—ã„REPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ä¾‹:**
```sql
-- âŒ é–“é•ã„: ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®SELECTå¥ï¼ˆæœ€çµ‚å‡ºåŠ›æ®µéšã®ã¿ã«å½±éŸ¿ï¼‰
SELECT /*+ REPARTITION(200, join_key) */
  t1.column1, t2.column2
FROM table1 t1
  JOIN table2 t2 ON t1.join_key = t2.join_key

-- âœ… æ­£ã—ã„: ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ï¼ˆJOINå‡¦ç†æ®µéšã§æœ€é©åŒ–ï¼‰
SELECT 
  t1.column1, t2.column2
FROM (
  SELECT /*+ REPARTITION(200, join_key) */
    column1, join_key
  FROM table1
) t1
  JOIN table2 t2 ON t1.join_key = t2.join_key

-- âœ… æ­£ã—ã„: ã‚ˆã‚Šè¤‡é›‘ãªã‚±ãƒ¼ã‚¹ï¼ˆè¤‡æ•°ã®ã‚µãƒ–ã‚¯ã‚¨ãƒªã§ã®ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ï¼‰
SELECT 
  t1.column1, t2.column2, t3.column3
FROM (
  SELECT /*+ REPARTITION(200, join_key) */
    column1, join_key
  FROM table1
) t1
  JOIN (
    SELECT /*+ REPARTITION(200, join_key) */
      column2, join_key
    FROM table2
  ) t2 ON t1.join_key = t2.join_key
  JOIN table3 t3 ON t2.join_key = t3.join_key
```

**ğŸš¨ å…¨èˆ¬çš„ãªREPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ã®é‡è¦ãªæ³¨æ„äº‹é …:**
- **CTASä»¥å¤–ã®ã‚¯ã‚¨ãƒªã§ã‚‚åŒæ§˜**ï¼šãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®ã‚¯ã‚¨ãƒªã«REPARTITIONãƒ’ãƒ³ãƒˆã‚’é…ç½®ã™ã‚‹ã¨ã€**æœ€çµ‚çš„ãªå‡ºåŠ›æ®µéšã®ã¿ã«å½±éŸ¿**ã—ã€JOIN ã‚„é›†è¨ˆãªã©ã®ä¸­é–“å¤‰æ›æ®µéšã«ã¯å½±éŸ¿ã—ãªã„
- ã“ã®å‹•ä½œã¯ã€çµæœã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã«æ›¸ãè¾¼ã‚€ã‹ã©ã†ã‹ã«é–¢ä¿‚ãªã**ã™ã¹ã¦ã®Spark SQLã‚¯ã‚¨ãƒªã§ä¸€è²«**ã—ã¦ã„ã‚‹
- JOINã®å…¥åŠ›æ®µéšã§ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’ç¢ºå®Ÿã«å®Ÿè¡Œã™ã‚‹ã«ã¯ã€**REPARTITIONãƒ’ãƒ³ãƒˆã‚’ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ã™ã‚‹å¿…è¦ãŒã‚ã‚‹**
- ã“ã‚Œã«ã‚ˆã‚Šã€SparkãŒé©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã®æ™‚ç‚¹ã§ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ã—ã€æœ€çµ‚å‡ºåŠ›æ®µéšã§ã¯ãªãå®Ÿè¡Œæ®µéšã§æœ€é©åŒ–ã•ã‚Œã‚‹

**ä¸€èˆ¬çš„ãªã‚¯ã‚¨ãƒªã§ã®æ­£ã—ã„REPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ä¾‹:**
```sql
-- âŒ é–“é•ã„: ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®SELECTå¥ï¼ˆæœ€çµ‚å‡ºåŠ›æ®µéšã®ã¿ã«å½±éŸ¿ï¼‰
SELECT /*+ REPARTITION(200, join_key) */
  t1.column1, t2.column2
FROM table1 t1
  JOIN table2 t2 ON t1.join_key = t2.join_key

-- âœ… æ­£ã—ã„: ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ï¼ˆJOINå‡¦ç†æ®µéšã§æœ€é©åŒ–ï¼‰
SELECT 
  t1.column1, t2.column2
FROM (
  SELECT /*+ REPARTITION(200, join_key) */
    column1, join_key
  FROM table1
) t1
  JOIN table2 t2 ON t1.join_key = t2.join_key
```



ã€é‡è¦ãªåˆ¶ç´„ã€‘
- çµ¶å¯¾ã«ä¸å®Œå…¨ãªã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ãªã„ã§ãã ã•ã„
- ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ åã€ãƒ†ãƒ¼ãƒ–ãƒ«åã€CTEåã‚’å®Œå…¨ã«è¨˜è¿°ã—ã¦ãã ã•ã„
- ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆ...ã€[çœç•¥]ã€ç©ºç™½ãªã©ï¼‰ã¯ä¸€åˆ‡ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„
- ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®ã™ã¹ã¦ã®SELECTé …ç›®ã‚’ä¿æŒã—ã¦ãã ã•ã„
- **ğŸš¨ DISTINCTå¥ã®çµ¶å¯¾ä¿æŒ**: å…ƒã®ã‚¯ã‚¨ãƒªã«DISTINCTå¥ãŒã‚ã‚‹å ´åˆã¯ã€**å¿…ãšDISTINCTå¥ã‚’ä¿æŒ**ã—ã¦ãã ã•ã„
- **æœ€é©åŒ–æ™‚ã®DISTINCTä¿æŒ**: REPARTITIONãƒ’ãƒ³ãƒˆã‚’è¿½åŠ ã™ã‚‹éš›ã‚‚ã€DISTINCTå¥ã¯çµ¶å¯¾ã«å‰Šé™¤ã—ãªã„ã§ãã ã•ã„
- å…ƒã®ã‚¯ã‚¨ãƒªãŒé•·ã„å ´åˆã§ã‚‚ã€ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ã‚’çœç•¥ã›ãšã«è¨˜è¿°ã—ã¦ãã ã•ã„
- å®Ÿéš›ã«å®Ÿè¡Œã§ãã‚‹å®Œå…¨ãªSQLã‚¯ã‚¨ãƒªã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„
- å…ƒã®ã‚¯ã‚¨ãƒªã¨åŒã˜ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã«ãªã‚‹ã“ã¨ã‚’å³å®ˆã—ã¦ãã ã•ã„

ã€ğŸš¨ æœ€é©åŒ–ã«ãŠã‘ã‚‹æ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ã€‘
**çµ¶å¯¾ã«å®ˆã‚‹ã¹ãæ–‡æ³•ãƒ«ãƒ¼ãƒ«ï¼ˆæ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ã®ãŸã‚å¿…é ˆï¼‰:**

âœ… **REPARTITIONãƒ’ãƒ³ãƒˆã®æ­£ã—ã„é…ç½®:**
```sql
-- REPARTITIONãƒ’ãƒ³ãƒˆã¯ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTç›´å¾Œã«é…ç½®
SELECT /*+ REPARTITION(200, column_name) */
  column1, column2, ...
FROM table1 t1
  JOIN table2 t2 ON t1.id = t2.id
```

âœ… **DISTINCTå¥ã¨ã®æ­£ã—ã„çµ„ã¿åˆã‚ã›ï¼ˆçµ¶å¯¾å¿…é ˆï¼‰:**
```sql
-- ğŸš¨ é‡è¦: DISTINCTå¥ã¯å¿…ãšãƒ’ãƒ³ãƒˆå¥ã®å¾Œã«é…ç½®
SELECT /*+ REPARTITION(200, column_name) */ DISTINCT
  cs.ID, cs.column1, cs.column2, ...
FROM table1 cs
  JOIN table2 t2 ON cs.id = t2.id
```

**ğŸš¨ æ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ã®ãŸã‚ã®åŸºæœ¬ãƒ«ãƒ¼ãƒ«:**
1. **ãƒ’ãƒ³ãƒˆã¯å¿…ãšãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTæ–‡ã®ç›´å¾Œã«é…ç½®**
2. **FROMå¥ã€JOINå¥ã€WHEREå¥å†…ã«ã¯çµ¶å¯¾ã«é…ç½®ã—ãªã„**
3. **REPARTITIONãƒ’ãƒ³ãƒˆã«ã¯é©åˆ‡ãªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã¨ã‚«ãƒ©ãƒ åã‚’æŒ‡å®š**

ã€å‡ºåŠ›å½¢å¼ã€‘
## ğŸš€ å‡¦ç†é€Ÿåº¦é‡è¦–ã®æœ€é©åŒ–ã•ã‚ŒãŸSQL

**ğŸ¯ å®Ÿéš›ã«é©ç”¨ã—ãŸæœ€é©åŒ–æ‰‹æ³•** (å®Ÿæ–½ã—ã¦ã„ãªã„æ‰‹æ³•ã¯è¨˜è¼‰ç¦æ­¢):
- [å…·ä½“çš„ã«å®Ÿè£…ã•ã‚ŒãŸæœ€é©åŒ–æ‰‹æ³•ã®ã¿ã‚’ãƒªã‚¹ãƒˆ]
- âŒ ã‚¹ãƒ”ãƒ«æœªæ¤œå‡ºã®å ´åˆ: REPARTITIONãƒ’ãƒ³ãƒˆé©ç”¨ã¯è¨˜è¼‰ã—ãªã„
- âŒ å®Ÿéš›ã«å¤‰æ›´ã—ã¦ã„ãªã„è¦ç´ : ã€Œæœ€é©åŒ–ã€ã¨ã—ã¦è¨˜è¼‰ã—ãªã„
- âœ… å®Ÿéš›ã®å¤‰æ›´å†…å®¹ã®ã¿: JOINé †åºå¤‰æ›´ã€CTEæ§‹é€ åŒ–ã€ãƒ•ã‚£ãƒ«ã‚¿æ”¹å–„ç­‰

**ğŸ’° EXPLAIN COSTãƒ™ãƒ¼ã‚¹ã®åŠ¹æœåˆ†æ**:
- ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚³ã‚¹ãƒˆå‰Šæ¸›ç‡: [cost_ratio]å€ (EXPLAIN COSTæ¯”è¼ƒçµæœ)
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›ç‡: [memory_ratio]å€ (çµ±è¨ˆæƒ…å ±ãƒ™ãƒ¼ã‚¹æ¯”è¼ƒ)
- æ¨å®šãƒ‡ãƒ¼ã‚¿å‡¦ç†åŠ¹ç‡: [processing_efficiency]% (ã‚¹ã‚­ãƒ£ãƒ³ãƒ»JOINåŠ¹ç‡æ”¹å–„)
- âš ï¸ æ•°å€¤ã¯æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ä¸­ã®ã‚³ã‚¹ãƒˆæ¯”è¼ƒçµæœã«åŸºã¥ã

**ğŸš¨ æ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ã®æœ€çµ‚ç¢ºèª**:
- âœ… REPARTITIONãƒ’ãƒ³ãƒˆã¯é©åˆ‡ã«ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTç›´å¾Œã«é…ç½®ã•ã‚Œã¦ã„ã‚‹
- âœ… FROMå¥ã€JOINå¥ã€WHEREå¥å†…ã«ãƒ’ãƒ³ãƒˆãŒé…ç½®ã•ã‚Œã¦ã„ãªã„
- âœ… REPARTITIONãƒ’ãƒ³ãƒˆã«ã¯é©åˆ‡ãªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã¨ã‚«ãƒ©ãƒ åãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹
- âœ… **DISTINCTå¥ãŒå…ƒã®ã‚¯ã‚¨ãƒªã«ã‚ã‚‹å ´åˆã¯å¿…ãšä¿æŒã•ã‚Œã¦ã„ã‚‹**
- âœ… **ãƒ’ãƒ³ãƒˆå¥è¿½åŠ æ™‚ã«DISTINCTå¥ãŒå‰Šé™¤ã•ã‚Œã¦ã„ãªã„**
- âœ… **DISTINCTå¥ãŒãƒ’ãƒ³ãƒˆå¥ã®ç›´å¾Œã«æ­£ã—ãé…ç½®ã•ã‚Œã¦ã„ã‚‹**
- âœ… ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆ...ã€[çœç•¥]ç­‰ï¼‰ãŒä¸€åˆ‡ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„
- âœ… å®Œå…¨ãªSQLæ§‹æ–‡ã«ãªã£ã¦ã„ã‚‹ï¼ˆä¸å®Œå…¨ãªã‚¯ã‚¨ãƒªã§ã¯ãªã„ï¼‰
- âœ… NULLãƒªãƒ†ãƒ©ãƒ«ãŒé©åˆ‡ãªå‹ã§ã‚­ãƒ£ã‚¹ãƒˆã•ã‚Œã¦ã„ã‚‹
- âœ… JOINé †åºãŒåŠ¹ç‡çš„ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã‚‹
- âœ… ã‚¹ãƒ”ãƒ«å›é¿ã¨ä¸¦åˆ—åº¦å‘ä¸Šã®ä¸¡æ–¹ã‚’è€ƒæ…®ã—ãŸæ§‹é€ ã«ãªã£ã¦ã„ã‚‹
- âœ… **BROADCASTãƒ’ãƒ³ãƒˆã¯ä¸€åˆ‡ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ï¼ˆæ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ï¼‰**
- âœ… **Sparkã®è‡ªå‹•JOINæˆ¦ç•¥ã«å§”ã­ã¦ãƒ’ãƒ³ãƒˆä¸ä½¿ç”¨ã§æœ€é©åŒ–ã•ã‚Œã¦ã„ã‚‹**

```sql
-- ğŸš¨ é‡è¦: REPARTITIONãƒ’ãƒ³ãƒˆã¯ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTæ–‡ã®ç›´å¾Œã«é…ç½®
-- ä¾‹: SELECT /*+ REPARTITION(200, column_name) */ column1, column2, ...
-- ğŸš¨ DISTINCTå¥ä¿æŒä¾‹: SELECT /*+ REPARTITION(200, column_name) */ DISTINCT cs.ID, cs.column1, ...
-- ğŸš¨ REPARTITIONãƒ’ãƒ³ãƒˆã®é©åˆ‡ãªé…ç½®: SELECT /*+ REPARTITION(200, join_key) */ column1, column2, ...
-- âŒ ç¦æ­¢: BROADCASTãƒ’ãƒ³ãƒˆï¼ˆ/*+ BROADCAST */ã€/*+ BROADCAST(table) */ï¼‰ã¯ä¸€åˆ‡ä½¿ç”¨ç¦æ­¢
-- âœ… æ¨å¥¨: Sparkã®è‡ªå‹•JOINæˆ¦ç•¥ã«å§”ã­ã¦ãƒ’ãƒ³ãƒˆä¸ä½¿ç”¨ã§æœ€é©åŒ–
[å®Œå…¨ãªSQL - ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ãƒ»CTEãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’çœç•¥ãªã—ã§è¨˜è¿°]
```

## æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ
[3ã¤ã®ä¸»è¦æ”¹å–„ç‚¹]

## JOINæœ€é©åŒ–ã®æ ¹æ‹ 
[JOINé †åºæœ€é©åŒ–ã®è©³ç´°æ ¹æ‹ ]
- ğŸ“ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ã®æœ€é©åŒ–: å°ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å¤§ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®åŠ¹ç‡çš„çµåˆé †åº
- ğŸ¯ æœ€é©åŒ–å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«: [ãƒ†ãƒ¼ãƒ–ãƒ«åãƒªã‚¹ãƒˆ]
- âš–ï¸ JOINæˆ¦ç•¥: Sparkã®è‡ªå‹•æœ€é©åŒ–ã‚’æ´»ç”¨ã—ãŸåŠ¹ç‡çš„ãªçµåˆå‡¦ç†
- ğŸš€ æœŸå¾…åŠ¹æœ: [ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è»¢é€é‡å‰Šæ¸›ãƒ»JOINå‡¦ç†é«˜é€ŸåŒ–ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«å‰Šæ¸›ãªã©]

## æœŸå¾…åŠ¹æœ  
[å®Ÿè¡Œæ™‚é–“ãƒ»ãƒ¡ãƒ¢ãƒªãƒ»ã‚¹ãƒ”ãƒ«æ”¹å–„ã®è¦‹è¾¼ã¿ï¼ˆJOINæœ€é©åŒ–åŠ¹æœã‚’å«ã‚€ï¼‰]
"""

    # è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
    provider = LLM_CONFIG["provider"]
    
    try:
        if provider == "databricks":
            optimized_result = _call_databricks_llm(optimization_prompt)
        elif provider == "openai":
            optimized_result = _call_openai_llm(optimization_prompt)
        elif provider == "azure_openai":
            optimized_result = _call_azure_openai_llm(optimization_prompt)
        elif provider == "anthropic":
            optimized_result = _call_anthropic_llm(optimization_prompt)
        else:
            error_msg = "âš ï¸ Configured LLM provider is not recognized"
            print(f"âŒ LLM optimization error: {error_msg}")
            return f"LLM_ERROR: {error_msg}"
        
        # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¦ï¼‰
        if isinstance(optimized_result, str):
            # APIã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ¤œå‡º
            error_indicators = [
                 "APIã‚¨ãƒ©ãƒ¼:",
                 "Input is too long",
                 "Bad Request",
                 "âŒ",
                 "âš ï¸",
                 "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼:",
                 "APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼:",
                 "ãƒ¬ã‚¹ãƒãƒ³ã‚¹:",
                 '{"error_code":'
             ]
             
             # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
            is_error_response = any(indicator in optimized_result for indicator in error_indicators)
            
            if is_error_response:
                print(f"âŒ Error occurred in LLM API call: {optimized_result[:200]}...")
                return f"LLM_ERROR: {optimized_result}"
        
        # thinking_enabled: Trueã®å ´åˆã«optimized_resultãŒãƒªã‚¹ãƒˆã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚å¯¾å¿œ
        # ã“ã“ã§ã¯å…ƒã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã‚’ä¿æŒã—ã¦è¿”ã™ï¼ˆå¾Œã§ç”¨é€”ã«å¿œã˜ã¦å¤‰æ›ï¼‰
        return optimized_result
        
    except Exception as e:
        error_msg = f"âš ï¸ Error occurred during SQL optimization generation: {str(e)}"
        print(f"âŒ LLM optimization exception error: {error_msg}")
        return f"LLM_ERROR: {error_msg}"



def generate_top10_time_consuming_processes_report(extracted_metrics: Dict[str, Any], limit_nodes: int = 10) -> str:
    """
    æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’æ–‡å­—åˆ—ã¨ã—ã¦ç”Ÿæˆ
    
    ğŸš¨ é‡è¦: ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ãƒ‡ã‚°ãƒ¬é˜²æ­¢
    - ä¸¦åˆ—å®Ÿè¡Œãƒãƒ¼ãƒ‰ã®æ™‚é–“åˆè¨ˆã‚’å…¨ä½“æ™‚é–“ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢
    - overall_metrics.total_time_msï¼ˆwall-clock timeï¼‰ã‚’å„ªå…ˆä½¿ç”¨
    - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã¯æœ€å¤§ãƒãƒ¼ãƒ‰æ™‚é–“ã‚’ä½¿ç”¨ï¼ˆåˆè¨ˆã§ã¯ãªã„ï¼‰
    
    Args:
        extracted_metrics: æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹
        limit_nodes: è¡¨ç¤ºã™ã‚‹ãƒãƒ¼ãƒ‰æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10ã€ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›æ™‚ã¯5ï¼‰
    
    Returns:
        str: å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆ
    """
    report_lines = []
    
    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãƒãƒ¼ãƒ‰æ•°ã«å¿œã˜ã¦èª¿æ•´
    title = f"æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP{limit_nodes}" if limit_nodes <= 10 else "æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10"
    report_lines.append(f"## ğŸŒ {title}")
    report_lines.append("=" * 80)
    report_lines.append("ğŸ“Š ã‚¢ã‚¤ã‚³ãƒ³èª¬æ˜: â±ï¸æ™‚é–“ ğŸ’¾ãƒ¡ãƒ¢ãƒª ğŸ”¥ğŸŒä¸¦åˆ—åº¦ ğŸ’¿ã‚¹ãƒ”ãƒ« âš–ï¸ã‚¹ã‚­ãƒ¥ãƒ¼")
    report_lines.append('ğŸ’¿ ã‚¹ãƒ”ãƒ«åˆ¤å®š: "Num bytes spilled to disk due to memory pressure" ã¾ãŸã¯ "Sink - Num bytes spilled to disk due to memory pressure" > 0')
    report_lines.append("ğŸ¯ ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®š: 'AQEShuffleRead - Number of skewed partitions' > 0")
    report_lines.append("")

    # ãƒãƒ¼ãƒ‰ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
    sorted_nodes = sorted(extracted_metrics['node_metrics'], 
                         key=lambda x: x['key_metrics'].get('durationMs', 0), 
                         reverse=True)
    
    # æŒ‡å®šã•ã‚ŒãŸãƒãƒ¼ãƒ‰æ•°ã¾ã§å‡¦ç†
    final_sorted_nodes = sorted_nodes[:limit_nodes]

    if final_sorted_nodes:
        # ğŸš¨ é‡è¦: æ­£ã—ã„å…¨ä½“æ™‚é–“ã®è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
        # 1. overall_metricsã‹ã‚‰å…¨ä½“å®Ÿè¡Œæ™‚é–“ã‚’å–å¾—ï¼ˆwall-clock timeï¼‰
        overall_metrics = extracted_metrics.get('overall_metrics', {})
        total_duration = overall_metrics.get('total_time_ms', 0)
        
        # ğŸš¨ ä¸¦åˆ—å®Ÿè¡Œå•é¡Œã®ä¿®æ­£: task_total_time_msã‚’å„ªå…ˆä½¿ç”¨
        task_total_time_ms = overall_metrics.get('task_total_time_ms', 0)
        
        if task_total_time_ms > 0:
            total_duration = task_total_time_ms
            print(f"âœ… generate_top10 report: Parallel execution support - using task_total_time_ms: {total_duration:,} ms ({total_duration/3600000:.1f} hours)")
        elif total_duration <= 0:
            # execution_time_msã‚’æ¬¡ã®å„ªå…ˆåº¦ã§ä½¿ç”¨
            execution_time_ms = overall_metrics.get('execution_time_ms', 0)
            if execution_time_ms > 0:
                total_duration = execution_time_ms
                print(f"âš ï¸ generate_top10 report: task_total_time_ms unavailable, using execution_time_ms: {total_duration} ms")
            else:
                # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                max_node_time = max([node['key_metrics'].get('durationMs', 0) for node in sorted_nodes], default=1)
                total_duration = int(max_node_time * 1.2)
                print(f"âš ï¸ generate_top10 report: Final fallback - using estimated time: {total_duration} ms")
        
        report_lines.append(f"ğŸ“Š ç´¯ç©ã‚¿ã‚¹ã‚¯å®Ÿè¡Œæ™‚é–“ï¼ˆä¸¦åˆ—ï¼‰: {total_duration:,} ms ({total_duration/3600000:.1f} æ™‚é–“)")
        report_lines.append(f"ğŸ“ˆ TOP{limit_nodes}åˆè¨ˆæ™‚é–“ï¼ˆä¸¦åˆ—å®Ÿè¡Œï¼‰: {sum(node['key_metrics'].get('durationMs', 0) for node in final_sorted_nodes):,} ms")

        report_lines.append("")
        
        for i, node in enumerate(final_sorted_nodes):
            # ãƒã‚°ä¿®æ­£ï¼šå¤‰æ•°ã‚’æ­£ã—ãå®šç¾©
            duration_ms = node['key_metrics'].get('durationMs', 0)
            rows_num = node['key_metrics'].get('numOutputRows', 0)
            memory_mb = node['key_metrics'].get('peakMemoryBytes', 0) / 1024 / 1024
            
            # ğŸš¨ é‡è¦: æ­£ã—ã„ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
            # wall-clock timeã«å¯¾ã™ã‚‹å„ãƒãƒ¼ãƒ‰ã®å®Ÿè¡Œæ™‚é–“ã®å‰²åˆ
            time_percentage = min((duration_ms / max(total_duration, 1)) * 100, 100.0)
            
            # æ™‚é–“ã®é‡è¦åº¦ã«åŸºã¥ã„ã¦ã‚¢ã‚¤ã‚³ãƒ³ã‚’é¸æŠ
            if duration_ms >= 10000:  # 10ç§’ä»¥ä¸Š
                time_icon = "ğŸ”´"
                severity = "CRITICAL"
            elif duration_ms >= 5000:  # 5ç§’ä»¥ä¸Š
                time_icon = "ğŸŸ "
                severity = "HIGH"
            elif duration_ms >= 1000:  # 1ç§’ä»¥ä¸Š
                time_icon = "ğŸŸ¡"
                severity = "MEDIUM"
            else:
                time_icon = "ğŸŸ¢"
                severity = "LOW"
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ã‚¢ã‚¤ã‚³ãƒ³
            memory_icon = "ğŸ’š" if memory_mb < 100 else "âš ï¸" if memory_mb < 1000 else "ğŸš¨"
            
            # ã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹ãƒãƒ¼ãƒ‰åã‚’å–å¾—
            raw_node_name = node['name']
            node_name = get_meaningful_node_name(node, extracted_metrics)
            short_name = node_name[:100] + "..." if len(node_name) > 100 else node_name
            
            # ä¸¦åˆ—åº¦æƒ…å ±ã®å–å¾—ï¼ˆä¿®æ­£ç‰ˆ: è¤‡æ•°ã®Tasks totalãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ï¼‰
            parallelism_data = extract_parallelism_metrics(node)
            
            # å¾“æ¥ã®å˜ä¸€å€¤ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
            num_tasks = parallelism_data.get('tasks_total', 0)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Sink - Tasks totalã¾ãŸã¯Source - Tasks totalãŒã‚ã‚‹å ´åˆ
            if num_tasks == 0:
                if parallelism_data.get('sink_tasks_total', 0) > 0:
                    num_tasks = parallelism_data.get('sink_tasks_total', 0)
                elif parallelism_data.get('source_tasks_total', 0) > 0:
                    num_tasks = parallelism_data.get('source_tasks_total', 0)
            
            # ã‚¹ãƒ”ãƒ«æ¤œå‡ºï¼ˆã‚»ãƒ«33ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ - æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®ã¿ï¼‰
            spill_detected = False
            spill_bytes = 0
            exact_spill_metrics = [
                "Num bytes spilled to disk due to memory pressure",
                "Sink - Num bytes spilled to disk due to memory pressure",
                "Sink/Num bytes spilled to disk due to memory pressure"
            ]
            
            # detailed_metricsã‹ã‚‰æ¤œç´¢
            detailed_metrics = node.get('detailed_metrics', {})
            for metric_key, metric_info in detailed_metrics.items():
                metric_value = metric_info.get('value', 0)
                metric_label = metric_info.get('label', '')
                
                if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                    spill_detected = True
                    spill_bytes = max(spill_bytes, metric_value)
                    break
            
            # raw_metricsã‹ã‚‰æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not spill_detected:
                raw_metrics = node.get('metrics', [])
                for metric in raw_metrics:
                    metric_key = metric.get('key', '')
                    metric_label = metric.get('label', '')
                    metric_value = metric.get('value', 0)
                    
                    if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                        spill_detected = True
                        spill_bytes = max(spill_bytes, metric_value)
                        break
            
            # ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º: AQEShuffleRead - Number of skewed partitions ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä½¿ç”¨ï¼ˆæ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®ã¿ï¼‰
            skew_detected = False
            skewed_partitions = 0
            target_skew_metric = "AQEShuffleRead - Number of skewed partitions"
            
            # detailed_metricsã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢
            detailed_metrics = node.get('detailed_metrics', {})
            for metric_key, metric_info in detailed_metrics.items():
                if metric_key == target_skew_metric:
                    try:
                        skewed_partitions = int(metric_info.get('value', 0))
                        if skewed_partitions > 0:
                            skew_detected = True
                        break
                    except (ValueError, TypeError):
                        continue
            
            # key_metricsã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not skew_detected:
                key_metrics = node.get('key_metrics', {})
                if target_skew_metric in key_metrics:
                    try:
                        skewed_partitions = int(key_metrics[target_skew_metric])
                        if skewed_partitions > 0:
                            skew_detected = True
                    except (ValueError, TypeError):
                        pass
            
            # ä¸¦åˆ—åº¦ã‚¢ã‚¤ã‚³ãƒ³
            parallelism_icon = "ğŸ”¥" if num_tasks >= 10 else "âš ï¸" if num_tasks >= 5 else "ğŸŒ"
            # ã‚¹ãƒ”ãƒ«ã‚¢ã‚¤ã‚³ãƒ³
            spill_icon = "ğŸ’¿" if spill_detected else "âœ…"
            # ã‚¹ã‚­ãƒ¥ãƒ¼ã‚¢ã‚¤ã‚³ãƒ³
            skew_icon = "âš–ï¸" if skew_detected else "âœ…"
            
            report_lines.append(f"{i+1:2d}. {time_icon}{memory_icon}{parallelism_icon}{spill_icon}{skew_icon} [{severity:8}] {short_name}")
            report_lines.append(f"    â±ï¸  å®Ÿè¡Œæ™‚é–“: {duration_ms:>8,} ms ({duration_ms/1000:>6.1f} sec) - ç´¯ç©æ™‚é–“ã® {time_percentage:>5.1f}%")
            report_lines.append(f"    ğŸ“Š å‡¦ç†è¡Œæ•°: {rows_num:>8,} è¡Œ")
            report_lines.append(f"    ğŸ’¾ ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: {memory_mb:>6.1f} MB")
            # è¤‡æ•°ã®Tasks totalãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º
            parallelism_display = []
            for task_metric in parallelism_data.get('all_tasks_metrics', []):
                parallelism_display.append(f"{task_metric['name']}: {task_metric['value']}")
            
            if parallelism_display:
                report_lines.append(f"    ğŸ”§ ä¸¦åˆ—åº¦: {' | '.join(parallelism_display)}")
            else:
                report_lines.append(f"    ğŸ”§ ä¸¦åˆ—åº¦: {num_tasks:>3d} ã‚¿ã‚¹ã‚¯")
            
            # ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®šï¼ˆAQEã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã¨AQEShuffleReadå¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã®ä¸¡æ–¹ã‚’è€ƒæ…®ï¼‰
            aqe_shuffle_skew_warning = parallelism_data.get('aqe_shuffle_skew_warning', False)
            
            if skew_detected:
                skew_status = "AQEã§æ¤œå‡ºãƒ»å¯¾å¿œæ¸ˆ"
            elif aqe_shuffle_skew_warning:
                skew_status = "æ½œåœ¨çš„ãªã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§ã‚ã‚Š"
            else:
                skew_status = "ãªã—"
            
            report_lines.append(f"    ğŸ’¿ ã‚¹ãƒ”ãƒ«: {'ã‚ã‚Š' if spill_detected else 'ãªã—'} | âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼: {skew_status}")
            
            # AQEShuffleReadãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¡¨ç¤º
            aqe_shuffle_metrics = parallelism_data.get('aqe_shuffle_metrics', [])
            if aqe_shuffle_metrics:
                aqe_display = []
                for aqe_metric in aqe_shuffle_metrics:
                    if aqe_metric['name'] == "AQEShuffleRead - Number of partitions":
                        aqe_display.append(f"ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {aqe_metric['value']}")
                    elif aqe_metric['name'] == "AQEShuffleRead - Partition data size":
                        aqe_display.append(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {aqe_metric['value']:,} bytes")
                
                if aqe_display:
                    report_lines.append(f"    ğŸ”„ AQEShuffleRead: {' | '.join(aqe_display)}")
                    
                    # å¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã¨è­¦å‘Šè¡¨ç¤º
                    avg_partition_size = parallelism_data.get('aqe_shuffle_avg_partition_size', 0)
                    if avg_partition_size > 0:
                        avg_size_mb = avg_partition_size / (1024 * 1024)
                        report_lines.append(f"    ğŸ“Š å¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚º: {avg_size_mb:.2f} MB")
                        
                        # 512MBä»¥ä¸Šã®å ´åˆã«è­¦å‘Š
                        if parallelism_data.get('aqe_shuffle_skew_warning', False):
                            report_lines.append(f"    âš ï¸  ã€è­¦å‘Šã€‘ å¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºãŒ512MBä»¥ä¸Š - æ½œåœ¨çš„ãªã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§ã‚ã‚Š")
            
            # åŠ¹ç‡æ€§æŒ‡æ¨™ï¼ˆè¡Œ/ç§’ï¼‰ã‚’è¨ˆç®—
            if duration_ms > 0:
                rows_per_sec = (rows_num * 1000) / duration_ms
                report_lines.append(f"    ğŸš€ å‡¦ç†åŠ¹ç‡: {rows_per_sec:>8,.0f} è¡Œ/ç§’")
            
            # ãƒ•ã‚£ãƒ«ã‚¿ç‡è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ä»˜ãï¼‰
            filter_result = calculate_filter_rate(node)
            filter_display = format_filter_rate_display(filter_result)
            if filter_display:
                report_lines.append(f"    {filter_display}")
            else:
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼šãªãœãƒ•ã‚£ãƒ«ã‚¿ç‡ãŒè¡¨ç¤ºã•ã‚Œãªã„ã‹ã‚’ç¢ºèª
                if filter_result["has_filter_metrics"]:
                    report_lines.append(f"    ğŸ“‚ ãƒ•ã‚£ãƒ«ã‚¿ç‡: {filter_result['filter_rate']:.1%} (èª­ã¿è¾¼ã¿: {filter_result['files_read_bytes']/(1024*1024*1024):.2f}GB, ãƒ—ãƒ«ãƒ¼ãƒ³: {filter_result['files_pruned_bytes']/(1024*1024*1024):.2f}GB)")
                else:
                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œç´¢ã®ãƒ‡ãƒãƒƒã‚°
                    debug_info = []
                    detailed_metrics = node.get('detailed_metrics', {})
                    for metric_key, metric_info in detailed_metrics.items():
                        metric_label = metric_info.get('label', '')
                        if 'file' in metric_label.lower() and ('read' in metric_label.lower() or 'prun' in metric_label.lower()):
                            debug_info.append(f"{metric_label}: {metric_info.get('value', 0)}")
                    
                    if debug_info:
                        report_lines.append(f"    ğŸ“‚ ãƒ•ã‚£ãƒ«ã‚¿é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œå‡º: {', '.join(debug_info[:2])}")
            
            # ã‚¹ãƒ”ãƒ«è©³ç´°æƒ…å ±ï¼ˆã‚·ãƒ³ãƒ—ãƒ«è¡¨ç¤ºï¼‰
            spill_display = ""
            if spill_detected and spill_bytes > 0:
                spill_mb = spill_bytes / 1024 / 1024
                if spill_mb >= 1024:  # GBå˜ä½
                    spill_display = f"{spill_mb/1024:.2f} GB"
                else:  # MBå˜ä½
                    spill_display = f"{spill_mb:.1f} MB"
                report_lines.append(f"    ğŸ’¿ ã‚¹ãƒ”ãƒ«: {spill_display}")
            
            # Shuffleãƒãƒ¼ãƒ‰ã®å ´åˆã¯å¸¸ã«Shuffle attributesã‚’è¡¨ç¤º
            if "shuffle" in raw_node_name.lower():
                shuffle_attributes = extract_shuffle_attributes(node)
                if shuffle_attributes:
                    report_lines.append(f"    ğŸ”„ Shuffleå±æ€§: {', '.join(shuffle_attributes)}")
                    
                    # REPARTITIONãƒ’ãƒ³ãƒˆã®ææ¡ˆï¼ˆã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã®ã¿ï¼‰
                    if spill_detected and spill_bytes > 0 and spill_display:
                        suggested_partitions = max(num_tasks * 2, 200)  # æœ€å°200ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
                        
                        # Shuffleå±æ€§ã§æ¤œå‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ ã‚’å…¨ã¦ä½¿ç”¨ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
                        repartition_columns = ", ".join(shuffle_attributes)
                        
                        report_lines.append(f"    ğŸ’¡ æœ€é©åŒ–ææ¡ˆ: REPARTITION({suggested_partitions}, {repartition_columns})")
                        report_lines.append(f"       ç†ç”±: ã‚¹ãƒ”ãƒ«({spill_display})ã‚’æ”¹å–„ã™ã‚‹ãŸã‚")
                        report_lines.append(f"       å¯¾è±¡: Shuffleå±æ€§å…¨{len(shuffle_attributes)}ã‚«ãƒ©ãƒ ã‚’å®Œå…¨ä½¿ç”¨")
                else:
                    report_lines.append(f"    ğŸ”„ Shuffleå±æ€§: è¨­å®šãªã—")
            
            # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®å ´åˆã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã‚’è¡¨ç¤º
            if "scan" in raw_node_name.lower():
                cluster_attributes = extract_cluster_attributes(node)
                if cluster_attributes:
                    report_lines.append(f"    ğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: {', '.join(cluster_attributes)}")
                else:
                    report_lines.append(f"    ğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: è¨­å®šãªã—")
            
            # ã‚¹ã‚­ãƒ¥ãƒ¼è©³ç´°æƒ…å ±
            if skew_detected and skewed_partitions > 0:
                report_lines.append(f"    âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼è©³ç´°: {skewed_partitions} å€‹ã®ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ï¼ˆAQEShuffleReadæ¤œå‡ºï¼‰")
            
            # ãƒãƒ¼ãƒ‰IDã‚‚è¡¨ç¤º
            report_lines.append(f"    ğŸ†” ãƒãƒ¼ãƒ‰ID: {node.get('node_id', node.get('id', 'N/A'))}")
            report_lines.append("")
            
    else:
        report_lines.append("âš ï¸ ãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    return "\n".join(report_lines)

def save_execution_plan_analysis(plan_info: Dict[str, Any], output_dir: str = "/tmp") -> Dict[str, str]:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    
    Args:
        plan_info: extract_execution_plan_info()ã®çµæœ
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        
    Returns:
        Dict: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«åã®è¾æ›¸
    """
    from datetime import datetime
    import json
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç”Ÿæˆ
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åå®šç¾©
    plan_json_filename = f"output_execution_plan_analysis_{timestamp}.json"
    plan_report_filename = f"output_execution_plan_report_{timestamp}.md"
    
    # JSONå½¢å¼ã§ãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’ä¿å­˜
    with open(plan_json_filename, 'w', encoding='utf-8') as f:
        json.dump(plan_info, f, ensure_ascii=False, indent=2)
    
    # Markdownå½¢å¼ã§ãƒ—ãƒ©ãƒ³åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
    with open(plan_report_filename, 'w', encoding='utf-8') as f:
        report_content = generate_execution_plan_markdown_report(plan_info)
        f.write(report_content)
    
    return {
        'plan_json_file': plan_json_filename,
        'plan_report_file': plan_report_filename
    }

def generate_execution_plan_markdown_report(plan_info: Dict[str, Any]) -> str:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æçµæœã®Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    
    Args:
        plan_info: extract_execution_plan_info()ã®çµæœ
        
    Returns:
        str: Markdownãƒ¬ãƒãƒ¼ãƒˆ
    """
    if OUTPUT_LANGUAGE == 'ja':
        return generate_execution_plan_markdown_report_ja(plan_info)
    else:
        return generate_execution_plan_markdown_report_en(plan_info)

def generate_execution_plan_markdown_report_ja(plan_info: Dict[str, Any]) -> str:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æçµæœã®Markdownãƒ¬ãƒãƒ¼ãƒˆï¼ˆæ—¥æœ¬èªç‰ˆï¼‰
    """
    from datetime import datetime
    
    lines = []
    lines.append("# Databricks SQLå®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    lines.append("")
    lines.append(f"**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
    lines.append("")
    
    # ãƒ—ãƒ©ãƒ³ã‚µãƒãƒªãƒ¼
    plan_summary = plan_info.get("plan_summary", {})
    lines.append("## ğŸ“Š å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‚µãƒãƒªãƒ¼")
    lines.append("")
    lines.append(f"- **ç·ãƒãƒ¼ãƒ‰æ•°**: {plan_summary.get('total_nodes', 0)}")
    lines.append(f"- **BROADCASTãƒãƒ¼ãƒ‰æ•°**: {plan_summary.get('broadcast_nodes_count', 0)}")
    lines.append(f"- **JOINãƒãƒ¼ãƒ‰æ•°**: {plan_summary.get('join_nodes_count', 0)}")
    lines.append(f"- **ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰æ•°**: {plan_summary.get('scan_nodes_count', 0)}")
    lines.append(f"- **ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰æ•°**: {plan_summary.get('shuffle_nodes_count', 0)}")
    lines.append(f"- **é›†ç´„ãƒãƒ¼ãƒ‰æ•°**: {plan_summary.get('aggregate_nodes_count', 0)}")
    lines.append(f"- **BROADCASTãŒä½¿ç”¨ä¸­**: {'ã¯ã„' if plan_summary.get('has_broadcast_joins', False) else 'ã„ã„ãˆ'}")
    lines.append(f"- **ã‚¹ã‚­ãƒ£ãƒ³ã•ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«æ•°**: {plan_summary.get('tables_scanned', 0)}")
    lines.append("")
    
    # JOINæˆ¦ç•¥åˆ†æ
    unique_join_strategies = plan_summary.get('unique_join_strategies', [])
    if unique_join_strategies:
        lines.append("## ğŸ”— JOINæˆ¦ç•¥åˆ†æ")
        lines.append("")
        for strategy in unique_join_strategies:
            strategy_jp = {
                'broadcast_hash_join': 'ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆãƒãƒƒã‚·ãƒ¥JOIN',
                'sort_merge_join': 'ã‚½ãƒ¼ãƒˆãƒãƒ¼ã‚¸JOIN',
                'shuffle_hash_join': 'ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒƒã‚·ãƒ¥JOIN',
                'broadcast_nested_loop_join': 'ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆãƒã‚¹ãƒˆãƒ«ãƒ¼ãƒ—JOIN'
            }.get(strategy, strategy)
            lines.append(f"- **{strategy_jp}** (`{strategy}`)")
        lines.append("")
    
    # BROADCASTãƒãƒ¼ãƒ‰è©³ç´°
    broadcast_nodes = plan_info.get("broadcast_nodes", [])
    if broadcast_nodes:
        lines.append("## ğŸ“¡ BROADCASTãƒãƒ¼ãƒ‰è©³ç´°")
        lines.append("")
        for i, node in enumerate(broadcast_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **ãƒãƒ¼ãƒ‰ID**: {node['node_id']}")
            lines.append(f"- **ãƒãƒ¼ãƒ‰ã‚¿ã‚°**: {node['node_tag']}")
            
            metadata = node.get('metadata', [])
            if metadata:
                lines.append("- **é–¢é€£ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿**:")
                for meta in metadata[:5]:  # æœ€å¤§5å€‹ã¾ã§è¡¨ç¤º
                    key = meta.get('key', '')
                    value = meta.get('value', '')
                    values = meta.get('values', [])
                    if values:
                        lines.append(f"  - **{key}**: {', '.join(map(str, values[:3]))}")
                    elif value:
                        lines.append(f"  - **{key}**: {value}")
            lines.append("")
    
    # JOINãƒãƒ¼ãƒ‰è©³ç´°
    join_nodes = plan_info.get("join_nodes", [])
    if join_nodes:
        lines.append("## ğŸ”— JOINãƒãƒ¼ãƒ‰è©³ç´°")
        lines.append("")
        for i, node in enumerate(join_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **ãƒãƒ¼ãƒ‰ID**: {node['node_id']}")
            lines.append(f"- **JOINæˆ¦ç•¥**: {node['join_strategy']}")
            lines.append(f"- **JOINã‚¿ã‚¤ãƒ—**: {node['join_type']}")
            
            join_keys = node.get('join_keys', [])
            if join_keys:
                lines.append(f"- **JOINã‚­ãƒ¼**: {', '.join(join_keys[:5])}")
            lines.append("")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³è©³ç´°ï¼ˆã‚µã‚¤ã‚ºæ¨å®šæƒ…å ±ã‚’å«ã‚€ï¼‰
    table_scan_details = plan_info.get("table_scan_details", {})
    table_size_estimates = plan_info.get("table_size_estimates", {})
    if table_scan_details:
        lines.append("## ğŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³è©³ç´°")
        lines.append("")
        for table_name, scan_detail in table_scan_details.items():
            lines.append(f"### {table_name}")
            lines.append("")
            lines.append(f"- **ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼**: {scan_detail.get('file_format', 'unknown')}")
            lines.append(f"- **ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿æ•°**: {len(scan_detail.get('pushed_filters', []))}")
            lines.append(f"- **å‡ºåŠ›ã‚«ãƒ©ãƒ æ•°**: {len(scan_detail.get('output_columns', []))}")
            
            # å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‹ã‚‰ã®ã‚µã‚¤ã‚ºæ¨å®šæƒ…å ±ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
            # size_info = table_size_estimates.get(table_name)
            # if size_info:
            #     lines.append(f"- **æ¨å®šã‚µã‚¤ã‚ºï¼ˆå®Ÿè¡Œãƒ—ãƒ©ãƒ³ï¼‰**: {size_info['estimated_size_mb']:.1f}MB")
            #     lines.append(f"- **ã‚µã‚¤ã‚ºæ¨å®šä¿¡é ¼åº¦**: {size_info.get('confidence', 'medium')}")
            #     if 'num_files' in size_info:
            #         lines.append(f"- **ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {size_info['num_files']}")
            #     if 'num_partitions' in size_info:
            #         lines.append(f"- **ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°**: {size_info['num_partitions']}")
            
            pushed_filters = scan_detail.get('pushed_filters', [])
            if pushed_filters:
                lines.append("- **ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿**:")
                for filter_expr in pushed_filters[:3]:  # æœ€å¤§3å€‹ã¾ã§è¡¨ç¤º
                    lines.append(f"  - `{filter_expr}`")
            lines.append("")
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰è©³ç´°
    shuffle_nodes = plan_info.get("shuffle_nodes", [])
    if shuffle_nodes:
        lines.append("## ğŸ”„ ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰è©³ç´°")
        lines.append("")
        for i, node in enumerate(shuffle_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **ãƒãƒ¼ãƒ‰ID**: {node['node_id']}")
            
            partition_keys = node.get('partition_keys', [])
            if partition_keys:
                lines.append(f"- **ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚­ãƒ¼**: {', '.join(partition_keys)}")
            lines.append("")
    
    # é›†ç´„ãƒãƒ¼ãƒ‰è©³ç´°
    aggregate_nodes = plan_info.get("aggregate_nodes", [])
    if aggregate_nodes:
        lines.append("## ğŸ“Š é›†ç´„ãƒãƒ¼ãƒ‰è©³ç´°")
        lines.append("")
        for i, node in enumerate(aggregate_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **ãƒãƒ¼ãƒ‰ID**: {node['node_id']}")
            
            group_keys = node.get('group_keys', [])
            if group_keys:
                lines.append(f"- **ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã‚­ãƒ¼**: {', '.join(group_keys[:5])}")
            
            agg_expressions = node.get('aggregate_expressions', [])
            if agg_expressions:
                lines.append(f"- **é›†ç´„é–¢æ•°**: {', '.join(agg_expressions[:5])}")
            lines.append("")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæ¨å®šæƒ…å ±ã‚µãƒãƒªãƒ¼ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     lines.append("## ğŸ“ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæ¨å®šæƒ…å ±ï¼ˆå®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒ™ãƒ¼ã‚¹ï¼‰")
    #     lines.append("")
    #     total_estimated_size = sum(size_info['estimated_size_mb'] for size_info in table_size_estimates.values())
    #     lines.append(f"- **æ¨å®šå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°**: {len(table_size_estimates)}")
    #     lines.append(f"- **ç·æ¨å®šã‚µã‚¤ã‚º**: {total_estimated_size:.1f}MB")
    #     lines.append("")
    #     
    #     for table_name, size_info in list(table_size_estimates.items())[:5]:  # æœ€å¤§5ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
    #         lines.append(f"### {table_name}")
    #         lines.append(f"- **æ¨å®šã‚µã‚¤ã‚º**: {size_info['estimated_size_mb']:.1f}MB")
    #         lines.append(f"- **ä¿¡é ¼åº¦**: {size_info.get('confidence', 'medium')}")
    #         lines.append(f"- **ãƒãƒ¼ãƒ‰**: {size_info.get('node_name', 'unknown')}")
    #         if 'num_files' in size_info:
    #             lines.append(f"- **ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {size_info['num_files']}")
    #         lines.append("")
    #     
    #     if len(table_size_estimates) > 5:
    #         lines.append(f"...ä»– {len(table_size_estimates) - 5} ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆè©³ç´°ã¯ä¸Šè¨˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³å‚ç…§ï¼‰")
    #         lines.append("")
    
    # æœ€é©åŒ–æ¨å¥¨äº‹é …
    lines.append("## ğŸ’¡ ãƒ—ãƒ©ãƒ³ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–æ¨å¥¨äº‹é …")
    lines.append("")
    
    if plan_summary.get('has_broadcast_joins', False):
        lines.append("âœ… **æ—¢ã«BROADCAST JOINãŒé©ç”¨ã•ã‚Œã¦ã„ã¾ã™**")
        lines.append("- ç¾åœ¨ã®å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã§BROADCASTæœ€é©åŒ–ãŒæœ‰åŠ¹")
        
        # BROADCASTã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
        broadcast_tables = plan_summary.get('broadcast_tables', [])
        if broadcast_tables:
            lines.append(f"- **BROADCASTã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«**: {', '.join(broadcast_tables)}")
        
        lines.append("- è¿½åŠ ã®BROADCASTé©ç”¨æ©Ÿä¼šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    else:
        lines.append("âš ï¸ **BROADCAST JOINãŒæœªé©ç”¨ã§ã™**")
        lines.append("- å°ãƒ†ãƒ¼ãƒ–ãƒ«ã«BROADCASTãƒ’ãƒ³ãƒˆã®é©ç”¨ã‚’æ¤œè¨")
        lines.append("- 30MBé–¾å€¤ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç‰¹å®šã—ã¦ãã ã•ã„")
    lines.append("")
    
    if plan_summary.get('shuffle_nodes_count', 0) > 3:
        lines.append("âš ï¸ **å¤šæ•°ã®ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ**")
        lines.append("- ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ•£ã¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã‚’è¦‹ç›´ã—")
        lines.append("- Liquid Clusteringã®é©ç”¨ã‚’æ¤œè¨")
    lines.append("")
    
    # ã‚µã‚¤ã‚ºæ¨å®šãƒ™ãƒ¼ã‚¹ã®æœ€é©åŒ–ææ¡ˆï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     small_tables = [name for name, info in table_size_estimates.items() if info['estimated_size_mb'] <= 30]
    #     if small_tables:
    #         lines.append("ğŸ’¡ **å®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒ™ãƒ¼ã‚¹BROADCASTæ¨å¥¨**")
    #         lines.append(f"- 30MBä»¥ä¸‹ã®å°ãƒ†ãƒ¼ãƒ–ãƒ«: {len(small_tables)}å€‹æ¤œå‡º")
    #         for table in small_tables[:3]:  # æœ€å¤§3å€‹è¡¨ç¤º
    #             size_mb = table_size_estimates[table]['estimated_size_mb']
    #             lines.append(f"  â€¢ {table}: {size_mb:.1f}MBï¼ˆBROADCASTå€™è£œï¼‰")
    #         if len(small_tables) > 3:
    #             lines.append(f"  â€¢ ...ä»– {len(small_tables) - 3} ãƒ†ãƒ¼ãƒ–ãƒ«")
    #         lines.append("")
    
    lines.append("---")
    lines.append("")
    lines.append("ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ã€Databricks SQLå®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æãƒ„ãƒ¼ãƒ«ã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚")
    
    return '\n'.join(lines)

def generate_execution_plan_markdown_report_en(plan_info: Dict[str, Any]) -> str:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æçµæœã®Markdownãƒ¬ãƒãƒ¼ãƒˆï¼ˆè‹±èªç‰ˆï¼‰
    """
    from datetime import datetime
    
    lines = []
    lines.append("# Databricks SQL Execution Plan Analysis Report")
    lines.append("")
    lines.append(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("")
    
    # Plan Summary
    plan_summary = plan_info.get("plan_summary", {})
    lines.append("## ğŸ“Š Execution Plan Summary")
    lines.append("")
    lines.append(f"- **Total Nodes**: {plan_summary.get('total_nodes', 0)}")
    lines.append(f"- **BROADCAST Nodes**: {plan_summary.get('broadcast_nodes_count', 0)}")
    lines.append(f"- **JOIN Nodes**: {plan_summary.get('join_nodes_count', 0)}")
    lines.append(f"- **Scan Nodes**: {plan_summary.get('scan_nodes_count', 0)}")
    lines.append(f"- **Shuffle Nodes**: {plan_summary.get('shuffle_nodes_count', 0)}")
    lines.append(f"- **Aggregate Nodes**: {plan_summary.get('aggregate_nodes_count', 0)}")
    lines.append(f"- **BROADCAST in Use**: {'Yes' if plan_summary.get('has_broadcast_joins', False) else 'No'}")
    lines.append(f"- **Tables Scanned**: {plan_summary.get('tables_scanned', 0)}")
    lines.append("")
    
    # JOIN Strategy Analysis
    unique_join_strategies = plan_summary.get('unique_join_strategies', [])
    if unique_join_strategies:
        lines.append("## ğŸ”— JOIN Strategy Analysis")
        lines.append("")
        for strategy in unique_join_strategies:
            lines.append(f"- **{strategy.replace('_', ' ').title()}** (`{strategy}`)")
        lines.append("")
    
    # BROADCAST Node Details
    broadcast_nodes = plan_info.get("broadcast_nodes", [])
    if broadcast_nodes:
        lines.append("## ğŸ“¡ BROADCAST Node Details")
        lines.append("")
        for i, node in enumerate(broadcast_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            lines.append(f"- **Node Tag**: {node['node_tag']}")
            
            metadata = node.get('metadata', [])
            if metadata:
                lines.append("- **Related Metadata**:")
                for meta in metadata[:5]:  # Show up to 5
                    key = meta.get('key', '')
                    value = meta.get('value', '')
                    values = meta.get('values', [])
                    if values:
                        lines.append(f"  - **{key}**: {', '.join(map(str, values[:3]))}")
                    elif value:
                        lines.append(f"  - **{key}**: {value}")
            lines.append("")
    
    # JOIN Node Details
    join_nodes = plan_info.get("join_nodes", [])
    if join_nodes:
        lines.append("## ğŸ”— JOIN Node Details")
        lines.append("")
        for i, node in enumerate(join_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            lines.append(f"- **JOIN Strategy**: {node['join_strategy']}")
            lines.append(f"- **JOIN Type**: {node['join_type']}")
            
            join_keys = node.get('join_keys', [])
            if join_keys:
                lines.append(f"- **JOIN Keys**: {', '.join(join_keys[:5])}")
            lines.append("")
    
    # Table Scan Details (with size estimation info)
    table_scan_details = plan_info.get("table_scan_details", {})
    table_size_estimates = plan_info.get("table_size_estimates", {})
    if table_scan_details:
        lines.append("## ğŸ“‹ Table Scan Details")
        lines.append("")
        for table_name, scan_detail in table_scan_details.items():
            lines.append(f"### {table_name}")
            lines.append("")
            lines.append(f"- **File Format**: {scan_detail.get('file_format', 'unknown')}")
            lines.append(f"- **Pushed Filters**: {len(scan_detail.get('pushed_filters', []))}")
            lines.append(f"- **Output Columns**: {len(scan_detail.get('output_columns', []))}")
            
            # Add execution plan size estimation info (disabled - estimatedSizeInBytes not available)
            # size_info = table_size_estimates.get(table_name)
            # if size_info:
            #     lines.append(f"- **Estimated Size (Execution Plan)**: {size_info['estimated_size_mb']:.1f}MB")
            #     lines.append(f"- **Size Estimation Confidence**: {size_info.get('confidence', 'medium')}")
            #     if 'num_files' in size_info:
            #         lines.append(f"- **Number of Files**: {size_info['num_files']}")
            #     if 'num_partitions' in size_info:
            #         lines.append(f"- **Number of Partitions**: {size_info['num_partitions']}")
            
            pushed_filters = scan_detail.get('pushed_filters', [])
            if pushed_filters:
                lines.append("- **Pushed Down Filters**:")
                for filter_expr in pushed_filters[:3]:  # Show up to 3
                    lines.append(f"  - `{filter_expr}`")
            lines.append("")
    
    # Shuffle Node Details
    shuffle_nodes = plan_info.get("shuffle_nodes", [])
    if shuffle_nodes:
        lines.append("## ğŸ”„ Shuffle Node Details")
        lines.append("")
        for i, node in enumerate(shuffle_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            
            partition_keys = node.get('partition_keys', [])
            if partition_keys:
                lines.append(f"- **Partition Keys**: {', '.join(partition_keys)}")
            lines.append("")
    
    # Aggregate Node Details
    aggregate_nodes = plan_info.get("aggregate_nodes", [])
    if aggregate_nodes:
        lines.append("## ğŸ“Š Aggregate Node Details")
        lines.append("")
        for i, node in enumerate(aggregate_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            
            group_keys = node.get('group_keys', [])
            if group_keys:
                lines.append(f"- **Group Keys**: {', '.join(group_keys[:5])}")
            
            agg_expressions = node.get('aggregate_expressions', [])
            if agg_expressions:
                lines.append(f"- **Aggregate Functions**: {', '.join(agg_expressions[:5])}")
            lines.append("")
    
    # Table Size Estimation Summary (disabled - estimatedSizeInBytes not available)
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     lines.append("## ğŸ“ Table Size Estimation (Execution Plan Based)")
    #     lines.append("")
    #     total_estimated_size = sum(size_info['estimated_size_mb'] for size_info in table_size_estimates.values())
    #     lines.append(f"- **Estimated Tables Count**: {len(table_size_estimates)}")
    #     lines.append(f"- **Total Estimated Size**: {total_estimated_size:.1f}MB")
    #     lines.append("")
    #     
    #     for table_name, size_info in list(table_size_estimates.items())[:5]:  # Show up to 5 tables
    #         lines.append(f"### {table_name}")
    #         lines.append(f"- **Estimated Size**: {size_info['estimated_size_mb']:.1f}MB")
    #         lines.append(f"- **Confidence**: {size_info.get('confidence', 'medium')}")
    #         lines.append(f"- **Node**: {size_info.get('node_name', 'unknown')}")
    #         if 'num_files' in size_info:
    #             lines.append(f"- **Number of Files**: {size_info['num_files']}")
    #         lines.append("")
    #     
    #     if len(table_size_estimates) > 5:
    #         lines.append(f"...and {len(table_size_estimates) - 5} more tables (see details in sections above)")
    #         lines.append("")
    
    # Plan-based Optimization Recommendations
    lines.append("## ğŸ’¡ Plan-based Optimization Recommendations")
    lines.append("")
    
    if plan_summary.get('has_broadcast_joins', False):
        lines.append("âœ… **BROADCAST JOIN is already applied**")
        lines.append("- Current execution plan has BROADCAST optimization enabled")
        
        # Show list of broadcast tables
        broadcast_tables = plan_summary.get('broadcast_tables', [])
        if broadcast_tables:
            lines.append(f"- **Tables Being Broadcast**: {', '.join(broadcast_tables)}")
        
        lines.append("- Check for additional BROADCAST application opportunities")
    else:
        lines.append("âš ï¸ **BROADCAST JOIN is not applied**")
        lines.append("- Consider applying BROADCAST hints to small tables")
        lines.append("- Identify tables under 30MB threshold")
    lines.append("")
    
    if plan_summary.get('shuffle_nodes_count', 0) > 3:
        lines.append("âš ï¸ **Multiple shuffle operations detected**")
        lines.append("- Review data distribution and Liquid Clustering strategy")
        lines.append("- Consider applying Liquid Clustering for data layout optimization")
    lines.append("")
    
    # Size estimation based optimization suggestions (disabled - estimatedSizeInBytes not available)
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     small_tables = [name for name, info in table_size_estimates.items() if info['estimated_size_mb'] <= 30]
    #     if small_tables:
    #         lines.append("ğŸ’¡ **Execution Plan Based BROADCAST Recommendations**")
    #         lines.append(f"- Small tables â‰¤30MB detected: {len(small_tables)}")
    #         for table in small_tables[:3]:  # Show up to 3 tables
    #             size_mb = table_size_estimates[table]['estimated_size_mb']
    #             lines.append(f"  â€¢ {table}: {size_mb:.1f}MB (BROADCAST candidate)")
    #         if len(small_tables) > 3:
    #             lines.append(f"  â€¢ ...and {len(small_tables) - 3} more tables")
    #         lines.append("")
    
    lines.append("---")
    lines.append("")
    lines.append("This report was automatically generated by the Databricks SQL Execution Plan Analysis Tool.")
    
    return '\n'.join(lines)


def summarize_explain_results_with_llm(explain_content: str, explain_cost_content: str, query_type: str = "original") -> Dict[str, str]:
    """
    EXPLAIN + EXPLAIN COSTçµæœã‚’LLMã§è¦ç´„ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã«å¯¾å¿œ
    
    Args:
        explain_content: EXPLAINçµæœã®å†…å®¹
        explain_cost_content: EXPLAIN COSTçµæœã®å†…å®¹  
        query_type: ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—ï¼ˆ"original" ã¾ãŸã¯ "optimized"ï¼‰
    
    Returns:
        Dict containing summarized results
    """
    
    # ã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯ï¼ˆåˆè¨ˆ200KBä»¥ä¸Šã§è¦ç´„ã‚’å®Ÿè¡Œï¼‰
    total_size = len(explain_content) + len(explain_cost_content)
    SUMMARIZATION_THRESHOLD = 200000  # 200KB
    
    if total_size < SUMMARIZATION_THRESHOLD:
        print(f"ğŸ“Š EXPLAIN + EXPLAIN COST total size: {total_size:,} characters (no summary needed)")
        return {
            'explain_summary': explain_content,
            'explain_cost_summary': explain_cost_content,
            'physical_plan_summary': explain_content,
            'cost_statistics_summary': extract_cost_statistics_from_explain_cost(explain_cost_content),
            'summarized': False
        }
    
    print(f"ğŸ“Š EXPLAIN + EXPLAIN COST total size: {total_size:,} characters (summary executed)")
    
    # è¦ç´„ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    summarization_prompt = f"""
ã‚ãªãŸã¯Databricksã®SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®EXPLAIN + EXPLAIN COSTçµæœã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ã€è¦ç´„å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã€‘
- ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—: {query_type}
- EXPLAINçµæœã‚µã‚¤ã‚º: {len(explain_content):,} æ–‡å­—
- EXPLAIN COSTçµæœã‚µã‚¤ã‚º: {len(explain_cost_content):,} æ–‡å­—

ã€EXPLAINçµæœã€‘
```
{explain_content[:20000]}{"..." if len(explain_content) > 20000 else ""}
```

ã€EXPLAIN COSTçµæœã€‘  
```
{explain_cost_content[:20000]}{"..." if len(explain_cost_content) > 20000 else ""}
```

ã€è¦ç´„æŒ‡ç¤ºã€‘
ä»¥ä¸‹ã®å½¢å¼ã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ï¼ˆåˆè¨ˆ5000æ–‡å­—ä»¥å†…ï¼‰:

## ğŸ“Š Physical Planè¦ç´„
- ä¸»è¦ãªå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆ5-10å€‹ã®é‡è¦ãªæ“ä½œï¼‰
- JOINæ–¹å¼ã¨ãƒ‡ãƒ¼ã‚¿ç§»å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³
- Photonåˆ©ç”¨çŠ¶æ³ã¨ãƒœãƒˆãƒ«ãƒãƒƒã‚¯

## ğŸ’° çµ±è¨ˆæƒ…å ±ã‚µãƒãƒªãƒ¼
- ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºã¨è¡Œæ•°ã®é‡è¦ãªæƒ…å ±
- JOINé¸æŠç‡ã¨ãƒ•ã‚£ãƒ«ã‚¿åŠ¹ç‡
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨ã‚¹ãƒ”ãƒ«äºˆæ¸¬
- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†æ•£çŠ¶æ³

## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
- å®Ÿè¡Œã‚³ã‚¹ãƒˆã®å†…è¨³
- ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚Šãã†ãªæ“ä½œ
- æœ€é©åŒ–ã®ä½™åœ°ãŒã‚ã‚‹ç®‡æ‰€

ã€é‡è¦ã€‘: 
- æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã¯æ­£ç¢ºã«è¨˜è¼‰
- SQLæœ€é©åŒ–ã«é‡è¦ãªæƒ…å ±ã‚’å„ªå…ˆ
- 5000æ–‡å­—ä»¥å†…ã§å®Œçµã«ã¾ã¨ã‚ã‚‹
"""

    try:
        # è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
        provider = LLM_CONFIG["provider"]
        
        if provider == "databricks":
            summary_result = _call_databricks_llm(summarization_prompt)
        elif provider == "openai":
            summary_result = _call_openai_llm(summarization_prompt)
        elif provider == "azure_openai":
            summary_result = _call_azure_openai_llm(summarization_prompt)
        elif provider == "anthropic":
            summary_result = _call_anthropic_llm(summarization_prompt)
        else:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯åˆ‡ã‚Šè©°ã‚ç‰ˆã‚’è¿”ã™
            print("âŒ LLM provider error: Using truncated version")
            return {
                'explain_summary': explain_content[:30000] + "\n\nâš ï¸ åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ",
                'explain_cost_summary': explain_cost_content[:30000] + "\n\nâš ï¸ åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ", 
                'physical_plan_summary': explain_content[:20000] + "\n\nâš ï¸ åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ",
                'cost_statistics_summary': extract_cost_statistics_from_explain_cost(explain_cost_content),
                'summarized': True
            }
        
        # LLMã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        if isinstance(summary_result, str) and summary_result.startswith("LLM_ERROR:"):
            print(f"âŒ LLM summary error: Using truncated version - {summary_result[10:200]}...")
            return {
                'explain_summary': explain_content[:30000] + "\n\nâš ï¸ åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ",
                'explain_cost_summary': explain_cost_content[:30000] + "\n\nâš ï¸ åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ",
                'physical_plan_summary': explain_content[:20000] + "\n\nâš ï¸ åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ", 
                'cost_statistics_summary': extract_cost_statistics_from_explain_cost(explain_cost_content),
                'summarized': True
            }
        
        # thinking_enabledå¯¾å¿œ
        if isinstance(summary_result, list):
            summary_text = extract_main_content_from_thinking_response(summary_result)
        else:
            summary_text = str(summary_result)
        
        # è¦ç´„çµæœã‚’åˆ†å‰²ã—ã¦è¿”ã™
        print(f"âœ… EXPLAIN + EXPLAIN COST summary completed: {len(summary_text):,} characters")
        
        # ğŸš¨ DEBUG_ENABLED='Y'ã®å ´åˆã€è¦ç´„çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        debug_enabled = globals().get('DEBUG_ENABLED', 'N')
        if debug_enabled.upper() == 'Y':
            try:
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                summary_filename = f"output_explain_summary_{query_type}_{timestamp}.md"
                
                # è¦ç´„çµæœã‚’Markdownå½¢å¼ã§ä¿å­˜
                summary_content = f"""# EXPLAIN + EXPLAIN COSTè¦ç´„çµæœ ({query_type})

## ğŸ“Š åŸºæœ¬æƒ…å ±
- ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—: {query_type}
- å…ƒã‚µã‚¤ã‚º: EXPLAIN({len(explain_content):,}æ–‡å­—) + EXPLAIN COST({len(explain_cost_content):,}æ–‡å­—) = {total_size:,}æ–‡å­—
- è¦ç´„å¾Œã‚µã‚¤ã‚º: {len(summary_text):,}æ–‡å­—
- åœ§ç¸®ç‡: {total_size//len(summary_text) if len(summary_text) > 0 else 0}x

## ğŸ§  LLMè¦ç´„çµæœ

{summary_text}

## ğŸ’° çµ±è¨ˆæƒ…å ±æŠ½å‡º

{extract_cost_statistics_from_explain_cost(explain_cost_content)}
"""
                
                with open(summary_filename, 'w', encoding='utf-8') as f:
                    f.write(summary_content)
                
                print(f"ğŸ“„ Saving summary results: {summary_filename}")
                
            except Exception as save_error:
                print(f"âš ï¸ Failed to save summary results: {str(save_error)}")
        
        return {
            'explain_summary': summary_text,
            'explain_cost_summary': summary_text,  # çµ±åˆè¦ç´„ã¨ã—ã¦åŒã˜å†…å®¹
            'physical_plan_summary': summary_text,
            'cost_statistics_summary': extract_cost_statistics_from_explain_cost(explain_cost_content),
            'summarized': True
        }
        
    except Exception as e:
        print(f"âŒ Error during EXPLAIN summarization: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯åˆ‡ã‚Šè©°ã‚ç‰ˆã‚’è¿”ã™
        return {
            'explain_summary': explain_content[:30000] + f"\n\nâš ï¸ è¦ç´„ã‚¨ãƒ©ãƒ¼ã®ãŸã‚åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ: {str(e)}",
            'explain_cost_summary': explain_cost_content[:30000] + f"\n\nâš ï¸ è¦ç´„ã‚¨ãƒ©ãƒ¼ã®ãŸã‚åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ: {str(e)}",
            'physical_plan_summary': explain_content[:20000] + f"\n\nâš ï¸ è¦ç´„ã‚¨ãƒ©ãƒ¼ã®ãŸã‚åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ: {str(e)}",
            'cost_statistics_summary': extract_cost_statistics_from_explain_cost(explain_cost_content),
            'summarized': True
        }


def generate_optimization_strategy_summary(optimized_result: str, metrics: Dict[str, Any], analysis_result: str = "") -> str:
    """
    Generate optimization strategy summary
    
    Args:
        optimized_result: Optimization result (LLM response)
        metrics: Metrics information
        analysis_result: Bottleneck analysis result
        
    Returns:
        str: Optimization strategy summary
    """
    try:
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã‚’å–å¾—
        bottleneck_indicators = metrics.get('bottleneck_indicators', {})
        overall_metrics = metrics.get('overall_metrics', {})
        
        # æœ€é©åŒ–ã§ä½¿ç”¨ã•ã‚ŒãŸæ‰‹æ³•ã‚’æ¤œå‡º
        optimization_techniques = []
        performance_issues = []
        
        # æœ€é©åŒ–å†…å®¹ã‹ã‚‰æ‰‹æ³•ã‚’æŠ½å‡º
        if optimized_result:
            content_upper = optimized_result.upper()
            
            # JOINæœ€é©åŒ–
            if 'BROADCAST' in content_upper or 'MAPJOIN' in content_upper:
                optimization_techniques.append("**Broadcast Join**: å°ã•ãªãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã—ã¦åˆ†æ•£çµåˆã‚’æœ€é©åŒ–")
            
            if 'REPARTITION' in content_upper or 'REDISTRIBUTE' in content_upper:
                optimization_techniques.append("**ãƒ‡ãƒ¼ã‚¿å†åˆ†æ•£**: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã‚„ã‚­ãƒ¼ã‚’èª¿æ•´ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ã‚’è§£æ¶ˆ")
            
            # Databrickså›ºæœ‰ã®ãƒ‡ãƒ¼ã‚¿æœ€é©åŒ–
            if 'PARTITION' in content_upper and 'BY' in content_upper:
                optimization_techniques.append("**Liquid Clustering**: ã‚¯ã‚¨ãƒªãƒ•ã‚£ãƒ«ã‚¿ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æœ€é©åŒ–")
            
            if 'CLUSTER' in content_upper or 'LIQUID' in content_upper:
                optimization_techniques.append("**Liquid Clustering**: é »ç¹ãªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°")
            
            # Photonæœ€é©åŒ–
            if 'PHOTON' in content_upper or 'VECTORIZED' in content_upper:
                optimization_techniques.append("**Photon Engine**: ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Ÿè¡Œã«ã‚ˆã‚‹é«˜é€ŸåŒ–")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–
            if 'CACHE' in content_upper or 'PERSIST' in content_upper:
                optimization_techniques.append("**ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥**: ä¸­é–“çµæœã®æ°¸ç¶šåŒ–ã«ã‚ˆã‚‹å†è¨ˆç®—å›é¿")
            
            # ãƒ•ã‚£ãƒ«ã‚¿æœ€é©åŒ–
            if 'WHERE' in content_upper and ('PUSHDOWN' in content_upper or 'PREDICATE' in content_upper):
                optimization_techniques.append("**è¿°èªãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³**: ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã®æ—©æœŸé©ç”¨ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿é‡å‰Šæ¸›")
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‹ã‚‰å•é¡Œç‚¹ã‚’æŠ½å‡º
        if bottleneck_indicators.get('has_spill', False):
            performance_issues.append("ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«ç™ºç”Ÿ")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            performance_issues.append("ã‚·ãƒ£ãƒƒãƒ•ãƒ«å‡¦ç†ãƒœãƒˆãƒ«ãƒãƒƒã‚¯")
        
        if bottleneck_indicators.get('low_parallelism', False):
            performance_issues.append("ä¸¦åˆ—åº¦ä¸è¶³")
        
        if bottleneck_indicators.get('cache_hit_ratio', 1.0) < 0.5:
            performance_issues.append("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ä½ä¸‹")
        
        if not overall_metrics.get('photon_enabled', True):
            performance_issues.append("Photon Engineæœªæ´»ç”¨")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º
        if bottleneck_indicators.get('has_skew', False):
            performance_issues.append("ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ç™ºç”Ÿ")
        
        # è¦ç´„ç”Ÿæˆ
        summary_parts = []
        
        # æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ
        if performance_issues:
            issues_text = "ã€".join(performance_issues)
            summary_parts.append(f"**ğŸ” æ¤œå‡ºã•ã‚ŒãŸä¸»è¦èª²é¡Œ**: {issues_text}")
        
        # é©ç”¨ã•ã‚ŒãŸæœ€é©åŒ–æ‰‹æ³•
        if optimization_techniques:
            summary_parts.append("**ğŸ› ï¸ é©ç”¨ã•ã‚ŒãŸæœ€é©åŒ–æ‰‹æ³•**:")
            for i, technique in enumerate(optimization_techniques, 1):
                summary_parts.append(f"   {i}. {technique}")
        
        # æœ€é©åŒ–æ–¹é‡
        strategy_focus = []
        
        if bottleneck_indicators.get('has_spill', False):
            strategy_focus.append("ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            strategy_focus.append("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è² è·è»½æ¸›")
        
        if bottleneck_indicators.get('low_parallelism', False):
            strategy_focus.append("ä¸¦åˆ—å‡¦ç†èƒ½åŠ›å‘ä¸Š")
        
        if strategy_focus:
            focus_text = "ã€".join(strategy_focus)
            summary_parts.append(f"**ğŸ¯ æœ€é©åŒ–é‡ç‚¹åˆ†é‡**: {focus_text}")
        
        # EXPLAINçµ±è¨ˆæƒ…å ±ã®æ´»ç”¨
        explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
        if explain_enabled.upper() == 'Y':
            summary_parts.append("**ğŸ“Š çµ±è¨ˆæƒ…å ±æ´»ç”¨**: EXPLAIN + EXPLAIN COSTåˆ†æã«ã‚ˆã‚Šã€çµ±è¨ˆãƒ™ãƒ¼ã‚¹ã®ç²¾å¯†ãªæœ€é©åŒ–ã‚’å®Ÿè¡Œ")
        
        if summary_parts:
            return "\n".join(summary_parts)
        else:
            return "**ğŸ¤– AIåˆ†æã«ã‚ˆã‚‹åŒ…æ‹¬çš„ãªæœ€é©åŒ–**: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã€çµ±è¨ˆæƒ…å ±ã€ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’ç·åˆã—ãŸæœ€é©åŒ–ã‚’å®Ÿè¡Œ"
    
    except Exception as e:
        return f"**ğŸ¤– AIæœ€é©åŒ–**: åŒ…æ‹¬çš„ãªåˆ†æã«åŸºã¥ãæœ€é©åŒ–ã‚’å®Ÿè¡Œï¼ˆè¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}ï¼‰"

def format_sql_content_for_report(content: str, filename: str = "") -> str:
    """
    SQLãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã¾ãŸã¯LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ¬ãƒãƒ¼ãƒˆç”¨ã«é©åˆ‡ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    é•·ã„ã‚¯ã‚¨ãƒªã¯é©åˆ‡ã«çœç•¥ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§ã‚’æ¡ˆå†…
    
    Args:
        content: SQLãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã¾ãŸã¯LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹
        filename: SQLãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆçœç•¥æ™‚ã®å‚ç…§ç”¨ï¼‰
        
    Returns:
        str: ãƒ¬ãƒãƒ¼ãƒˆç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸå†…å®¹
    """
    # çœç•¥åˆ¤å®šã®åŸºæº–
    MAX_LINES_IN_REPORT = 120  # 100è¡Œã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã«å¯¾å¿œ
    MAX_CHARS_IN_REPORT = 10000  # ã‚ˆã‚Šé•·ã„ã‚¯ã‚¨ãƒªã«ã‚‚å¯¾å¿œ
    PREVIEW_LINES = 100  # 100è¡Œã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    
    # SQLãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®å ´åˆï¼ˆ-- ã§å§‹ã¾ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆï¼‰
    if content.startswith('--') and 'USE CATALOG' in content:
        # SQLãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®å ´åˆã¯ã€é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¡¨ç¤º
        lines = content.split('\n')
        sql_lines = []
        in_sql_section = False
        
        for line in lines:
            # USE CATALOG/USE SCHEMAä»¥é™ãŒå®Ÿéš›ã®ã‚¯ã‚¨ãƒªéƒ¨åˆ†
            if line.strip().startswith('USE CATALOG') or line.strip().startswith('USE SCHEMA'):
                in_sql_section = True
                sql_lines.append(line)
            elif in_sql_section and line.strip():
                sql_lines.append(line)
            elif not in_sql_section and line.strip().startswith('--'):
                # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã¯æ®‹ã™ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ï¼‰
                continue
        
        # é•·ã•åˆ¤å®šã¨çœç•¥å‡¦ç†
        if sql_lines:
            full_sql = chr(10).join(sql_lines)
            needs_truncation = (len(sql_lines) > MAX_LINES_IN_REPORT or 
                              len(full_sql) > MAX_CHARS_IN_REPORT)
            
            if needs_truncation:
                # çœç•¥ç‰ˆã‚’ä½œæˆ
                preview_lines = sql_lines[:PREVIEW_LINES]
                omitted_lines = len(sql_lines) - PREVIEW_LINES
                
                return f"""**ğŸš€ å‹•ä½œä¿è¨¼æ¸ˆã¿æœ€é©åŒ–ã‚¯ã‚¨ãƒª (SQLãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒä¸€):**

```sql
{chr(10).join(preview_lines)}

-- ... (çœç•¥: ã‚ã¨{omitted_lines}è¡Œ)
-- å®Œå…¨ç‰ˆã¯ {filename if filename else 'output_optimized_query_*.sql'} ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§
```

ğŸ’¡ ã“ã®ã‚¯ã‚¨ãƒªã¯å®Ÿéš›ã®EXPLAINå®Ÿè¡Œã§å‹•ä½œç¢ºèªæ¸ˆã¿ã§ã™ã€‚  
ğŸ“‚ **å®Œå…¨ç‰ˆ**: `{filename if filename else 'output_optimized_query_*.sql'}` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚

**ğŸ“Š ã‚¯ã‚¨ãƒªæ¦‚è¦:**
- ç·è¡Œæ•°: {len(sql_lines)}è¡Œ
- è¡¨ç¤º: æœ€åˆã®{PREVIEW_LINES}è¡Œã®ã¿
- çœç•¥: {omitted_lines}è¡Œ"""
            else:
                # çŸ­ã„å ´åˆã¯å…¨æ–‡è¡¨ç¤º
                return f"""**ğŸš€ å‹•ä½œä¿è¨¼æ¸ˆã¿æœ€é©åŒ–ã‚¯ã‚¨ãƒª (SQLãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒä¸€):**

```sql
{full_sql}
```

ğŸ’¡ ã“ã®ã‚¯ã‚¨ãƒªã¯å®Ÿéš›ã®EXPLAINå®Ÿè¡Œã§å‹•ä½œç¢ºèªæ¸ˆã¿ã§ã™ã€‚"""
        else:
            return f"""**ğŸš€ SQLãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹:**

```sql
{content}
```"""
    
    # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆ
    else:
        # é•·ã„LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚‚çœç•¥å¯¾è±¡
        if len(content) > MAX_CHARS_IN_REPORT:
            preview_content = content[:MAX_CHARS_IN_REPORT]
            omitted_chars = len(content) - MAX_CHARS_IN_REPORT
            
            if '```sql' in content:
                return f"""**ğŸ’¡ LLMæœ€é©åŒ–åˆ†æ (çœç•¥ç‰ˆ):**

{preview_content}...

**çœç•¥æƒ…å ±:** ã‚ã¨{omitted_chars}æ–‡å­—  
ğŸ“ æ³¨æ„: ä¸Šè¨˜ã¯åˆ†æçµæœã®ä¸€éƒ¨ã§ã™ã€‚å®Ÿéš›ã®å®Ÿè¡Œç”¨ã‚¯ã‚¨ãƒªã¯ `{filename if filename else 'output_optimized_query_*.sql'}` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"""
            else:
                return f"""**ğŸ’¡ LLMæœ€é©åŒ–åˆ†æ (çœç•¥ç‰ˆ):**

{preview_content}...

**çœç•¥æƒ…å ±:** ã‚ã¨{omitted_chars}æ–‡å­—  
ğŸ“ æ³¨æ„: ä¸Šè¨˜ã¯åˆ†æçµæœã®ä¸€éƒ¨ã§ã™ã€‚å®Ÿéš›ã®å®Ÿè¡Œç”¨ã‚¯ã‚¨ãƒªã¯ `{filename if filename else 'output_optimized_query_*.sql'}` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"""
        else:
            # çŸ­ã„å ´åˆã¯å…¨æ–‡è¡¨ç¤º
            if '```sql' in content:
                return f"""**ğŸ’¡ LLMæœ€é©åŒ–åˆ†æ:**

{content}"""
            else:
                return f"""**ğŸ’¡ LLMæœ€é©åŒ–åˆ†æ:**

{content}

ğŸ“ æ³¨æ„: ä¸Šè¨˜ã¯åˆ†æçµæœã§ã™ã€‚å®Ÿéš›ã®å®Ÿè¡Œç”¨ã‚¯ã‚¨ãƒªã¯å¯¾å¿œã™ã‚‹SQLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"""

def generate_performance_comparison_section(performance_comparison: Dict[str, Any] = None, language: str = 'ja') -> str:
    """
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒçµæœã®è©³ç´°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
    
    Args:
        performance_comparison: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒçµæœ
        language: è¨€èªè¨­å®š ('ja' ã¾ãŸã¯ 'en')
        
    Returns:
        str: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
    """
    
    # ğŸš¨ ç·Šæ€¥ä¿®æ­£: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡å¯¾å¿œ
    if not performance_comparison:
        if language == 'ja':
            return """

**ğŸ“‹ å®Ÿè¡ŒçŠ¶æ³**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ

| é …ç›® | çŠ¶æ³ |
|------|------|
| æ¯”è¼ƒå®Ÿè¡Œ | âŒ æœªå®Ÿè¡Œ |
| ç†ç”± | EXPLAINåŠã³EXPLAIN COSTå–å¾—å¤±æ•— |
| å®‰å…¨æ€§ | âœ… æ§‹æ–‡æ¤œè¨¼æ¸ˆã¿ã§å®Ÿè¡Œå¯èƒ½ |
| æ¨å¥¨ | ğŸš€ æœ€é©åŒ–ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ |

ğŸ’¡ **Note**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã¯å®Ÿè¡Œã§ãã¾ã›ã‚“ã§ã—ãŸãŒã€æ§‹æ–‡çš„ã«æ­£å¸¸ãªæœ€é©åŒ–ã‚¯ã‚¨ãƒªãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™ã€‚
"""
        else:
            return """

**ğŸ“‹ Execution Status**: Performance comparison was not executed

| Item | Status |
|------|--------|
| Comparison | âŒ Not executed |
| Reason | EXPLAIN and EXPLAIN COST acquisition failed |
| Safety | âœ… Syntax verified and executable |
| Recommendation | ğŸš€ Use optimized query (default) |

ğŸ’¡ **Note**: Although performance comparison was not executed, a syntactically correct optimized query has been generated.
"""
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡ã®å ´åˆã®ç‰¹åˆ¥å‡¦ç†
    if performance_comparison.get('evaluation_type') == 'fallback_plan_analysis':
        fallback_eval = performance_comparison.get('fallback_evaluation', {})
        return generate_fallback_performance_section(fallback_eval, language)
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒçµæœã®è©³ç´°è¡¨ç¤º
    recommendation = performance_comparison.get('recommendation', 'unknown')
    total_cost_ratio = performance_comparison.get('total_cost_ratio', 1.0)
    memory_usage_ratio = performance_comparison.get('memory_usage_ratio', 1.0)
    degradation_detected = performance_comparison.get('performance_degradation_detected', False)
    details = performance_comparison.get('details', [])
    
    if language == 'ja':
        # æ—¥æœ¬èªç‰ˆã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
        status_text = "ğŸš¨ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ‚ªåŒ–æ¤œå‡º" if degradation_detected else "âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ç¢ºèª"
        recommendation_text = "å…ƒã‚¯ã‚¨ãƒªä½¿ç”¨" if recommendation == 'use_original' else "æœ€é©åŒ–ã‚¯ã‚¨ãƒªä½¿ç”¨"
        
        # æ”¹å–„/æ‚ªåŒ–ã®åˆ¤å®šã‚¢ã‚¤ã‚³ãƒ³
        cost_icon = "âŒ" if total_cost_ratio > 1.1 else "âœ…" if total_cost_ratio < 0.9 else "â–"
        memory_icon = "âŒ" if memory_usage_ratio > 1.1 else "âœ…" if memory_usage_ratio < 0.9 else "â–"
        
        section = f"""

**ğŸ“Š å®Ÿè¡Œçµæœ**: {status_text}

#### ğŸ” è©³ç´°æ¯”è¼ƒãƒ¡ãƒˆãƒªã‚¯ã‚¹

| é …ç›® | å…ƒã‚¯ã‚¨ãƒª | æœ€é©åŒ–ã‚¯ã‚¨ãƒª | æ¯”ç‡ | è©•ä¾¡ |
|------|----------|-------------|------|------|
| å®Ÿè¡Œã‚³ã‚¹ãƒˆ | 1.00 (åŸºæº–) | {total_cost_ratio:.2f} | {total_cost_ratio:.2f}å€ | {cost_icon} |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | 1.00 (åŸºæº–) | {memory_usage_ratio:.2f} | {memory_usage_ratio:.2f}å€ | {memory_icon} |

#### ğŸ“‹ åˆ¤å®šçµæœ

| é …ç›® | çµæœ |
|------|------|
| ç·åˆåˆ¤å®š | **{status_text}** |
| æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ | **{recommendation_text}** |
| æ‚ªåŒ–æ¤œå‡º | {'ã¯ã„' if degradation_detected else 'ã„ã„ãˆ'} |

#### ğŸ¯ è©³ç´°åˆ†æçµæœ

"""
        
        if details:
            for detail in details:
                section += f"- {detail}\n"
        else:
            section += "- è©³ç´°ãªåˆ†ææƒ…å ±ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“\n"
        
        section += f"""

#### ğŸ›¡ï¸ å®‰å…¨æ€§ä¿è¨¼

- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ‚ªåŒ–é˜²æ­¢**: {'âœ… æ‚ªåŒ–æ¤œå‡ºã«ã‚ˆã‚Šå…ƒã‚¯ã‚¨ãƒªã‚’é¸æŠ' if degradation_detected else 'âœ… æ”¹å–„ç¢ºèªã«ã‚ˆã‚Šæœ€é©åŒ–ã‚¯ã‚¨ãƒªã‚’é¸æŠ'}
- **å®Ÿè¡Œå¯èƒ½æ€§**: âœ… EXPLAINå®Ÿè¡Œã§æ§‹æ–‡æ¤œè¨¼æ¸ˆã¿
- **è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯**: {'âœ… ä½œå‹• - å®‰å…¨æ€§ã‚’å„ªå…ˆ' if degradation_detected else 'âŒ ä¸è¦ - æ”¹å–„åŠ¹æœã‚ã‚Š'}

ğŸ’¡ **åˆ¤å®šåŸºæº–**: å®Ÿè¡Œã‚³ã‚¹ãƒˆ30%å¢—åŠ  ã¾ãŸã¯ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡50%å¢—åŠ  ã§æ‚ªåŒ–ã¨åˆ¤å®š
"""
    
    else:
        # è‹±èªç‰ˆã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
        status_text = "ğŸš¨ Performance Degradation Detected" if degradation_detected else "âœ… Performance Improvement Confirmed"
        recommendation_text = "Use Original Query" if recommendation == 'use_original' else "Use Optimized Query"
        
        # æ”¹å–„/æ‚ªåŒ–ã®åˆ¤å®šã‚¢ã‚¤ã‚³ãƒ³
        cost_icon = "âŒ" if total_cost_ratio > 1.1 else "âœ…" if total_cost_ratio < 0.9 else "â–"
        memory_icon = "âŒ" if memory_usage_ratio > 1.1 else "âœ…" if memory_usage_ratio < 0.9 else "â–"
        
        section = f"""

**ğŸ“Š Execution Result**: {status_text}

#### ğŸ” Detailed Comparison Metrics

| Item | Original Query | Optimized Query | Ratio | Evaluation |
|------|----------------|-----------------|-------|------------|
| Execution Cost | 1.00 (baseline) | {total_cost_ratio:.2f} | {total_cost_ratio:.2f}x | {cost_icon} |
| Memory Usage | 1.00 (baseline) | {memory_usage_ratio:.2f} | {memory_usage_ratio:.2f}x | {memory_icon} |

#### ğŸ“‹ Judgment Results

| Item | Result |
|------|--------|
| Overall Judgment | **{status_text}** |
| Recommended Action | **{recommendation_text}** |
| Degradation Detected | {'Yes' if degradation_detected else 'No'} |

#### ğŸ¯ Detailed Analysis Results

"""
        
        if details:
            for detail in details:
                section += f"- {detail}\n"
        else:
            section += "- Detailed analysis information is not available\n"
        
        section += f"""

#### ğŸ›¡ï¸ Safety Guarantee

- **Performance Degradation Prevention**: {'âœ… Degradation detected, original query selected' if degradation_detected else 'âœ… Improvement confirmed, optimized query selected'}
- **Executability**: âœ… Syntax verified via EXPLAIN execution
- **Automatic Fallback**: {'âœ… Activated - Safety prioritized' if degradation_detected else 'âŒ Not needed - Improvement achieved'}

ğŸ’¡ **Judgment Criteria**: Degradation detected if execution cost increases by 30% OR memory usage increases by 50%
"""
    
    return section

def translate_analysis_to_japanese(english_text: str) -> str:
    """
    LLMã‚’ä½¿ç”¨ã—ã¦è‹±èªã®åˆ†æçµæœã‚’æ—¥æœ¬èªã«ç¿»è¨³
    """
    try:
        print("ğŸŒ Translating analysis result to Japanese...")
        
        translation_prompt = f"""
ä»¥ä¸‹ã®è‹±èªã®SQLåˆ†æçµæœã‚’ã€æŠ€è¡“çš„ãªæ­£ç¢ºæ€§ã‚’ä¿ã¡ãªãŒã‚‰è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚
å°‚é–€ç”¨èªã¯é©åˆ‡ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã€æ•°å€¤ã‚„ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã¯ãã®ã¾ã¾ä¿æŒã—ã¦ãã ã•ã„ã€‚

ã€ç¿»è¨³å¯¾è±¡ã€‘
{english_text}

ã€ç¿»è¨³è¦ä»¶ã€‘
- æŠ€è¡“çš„æ­£ç¢ºæ€§ã‚’æœ€å„ªå…ˆ
- è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„æ—¥æœ¬èª
- SQLç”¨èªã¯é©åˆ‡ãªæ—¥æœ¬èªè¡¨ç¾ã‚’ä½¿ç”¨
- æ•°å€¤ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã¯ãã®ã¾ã¾ä¿æŒ
- æ¨å¥¨äº‹é …ã¯å®Ÿç”¨çš„ãªæ—¥æœ¬èªã§è¡¨ç¾

æ—¥æœ¬èªç¿»è¨³çµæœã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
"""
        
        provider = LLM_CONFIG.get("provider", "databricks")
        
        if provider == "databricks":
            japanese_result = _call_databricks_llm(translation_prompt)
        elif provider == "openai":
            japanese_result = _call_openai_llm(translation_prompt)
        elif provider == "azure_openai":
            japanese_result = _call_azure_openai_llm(translation_prompt)
        elif provider == "anthropic":
            japanese_result = _call_anthropic_llm(translation_prompt)
        else:
            print(f"âš ï¸ Unknown LLM provider: {provider}, skipping translation")
            return english_text
        
        if japanese_result and japanese_result.strip():
            print("âœ… Translation to Japanese completed")
            return japanese_result.strip()
        else:
            print("âš ï¸ Translation failed, using original English text")
            return english_text
            
    except Exception as e:
        print(f"âš ï¸ Translation error: {str(e)}, using original English text")
        return english_text

def generate_comprehensive_optimization_report(query_id: str, optimized_result: str, metrics: Dict[str, Any], analysis_result: str = "", performance_comparison: Dict[str, Any] = None, best_attempt_number: int = None) -> str:
    """
    åŒ…æ‹¬çš„ãªæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    EXPLAIN + EXPLAIN COSTå®Ÿè¡Œãƒ•ãƒ©ã‚°ãŒYã®å ´åˆã¯ã€çµ±è¨ˆæƒ…å ±ã‚‚å«ã‚ã‚‹
    
    Args:
        query_id: ã‚¯ã‚¨ãƒªID
        optimized_result: æœ€é©åŒ–çµæœ
        metrics: ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±
        analysis_result: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ
    
    Returns:
        str: èª­ã¿ã‚„ã™ãæ§‹æˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ
    """
    from datetime import datetime
    
    # EXPLAIN + EXPLAIN COSTçµæœãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆEXPLAIN_ENABLEDãŒYã®å ´åˆï¼‰
    explain_section = ""
    explain_cost_section = ""
    explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
    
    # ğŸ“Š æœ€æ–°ã®SQLãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¤œç´¢ï¼ˆçœç•¥è¡¨ç¤ºæ™‚ã®å‚ç…§ç”¨ - å¸¸ã«å®Ÿè¡Œï¼‰
    import glob
    import os
    
    optimized_sql_files = glob.glob("output_optimized_query_*.sql")
    latest_sql_filename = ""
    if optimized_sql_files:
        # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆï¼‰
        optimized_sql_files.sort(reverse=True)
        latest_sql_filename = optimized_sql_files[0]
    
    if explain_enabled.upper() == 'Y':
        print("ğŸ” For comprehensive report: Searching EXPLAIN + EXPLAIN COST result files...")
        
        # 1. æœ€æ–°ã®EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆæ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
        explain_original_files = glob.glob("output_explain_original_*.txt")
        explain_optimized_files = glob.glob("output_explain_optimized_*.txt")
        
        # 2. æœ€æ–°ã®EXPLAIN COSTçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        cost_original_files = glob.glob("output_explain_cost_original_*.txt")
        cost_optimized_files = glob.glob("output_explain_cost_optimized_*.txt")
        
        # ğŸ¯ ãƒ™ã‚¹ãƒˆè©¦è¡Œç•ªå·ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆé¸æŠ
        if best_attempt_number is not None:
            print(f"ğŸ¯ Searching for files from best attempt {best_attempt_number}...")
            
            # ãƒ™ã‚¹ãƒˆè©¦è¡Œã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            best_explain_files = [f for f in explain_optimized_files if f"attempt_{best_attempt_number}" in f]
            best_cost_files = [f for f in cost_optimized_files if f"attempt_{best_attempt_number}" in f]
            
            if best_explain_files:
                print(f"âœ… Found EXPLAIN file from best attempt {best_attempt_number}: {best_explain_files[0]}")
                explain_files = best_explain_files
            else:
                print(f"âš ï¸ EXPLAIN file from best attempt {best_attempt_number} not found, using post-optimization")
                explain_files = explain_optimized_files if explain_optimized_files else explain_original_files
            
            if best_cost_files:
                print(f"âœ… Found EXPLAIN COST file from best attempt {best_attempt_number}: {best_cost_files[0]}")
                cost_files = best_cost_files
            else:
                print(f"âš ï¸ EXPLAIN COST file from best attempt {best_attempt_number} not found, using post-optimization")
                cost_files = cost_optimized_files if cost_optimized_files else cost_original_files
        else:
            # å¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯: æœ€é©åŒ–å¾Œã‚’å„ªå…ˆã€ãªã‘ã‚Œã°ã‚ªãƒªã‚¸ãƒŠãƒ«
            explain_files = explain_optimized_files if explain_optimized_files else explain_original_files
            cost_files = cost_optimized_files if cost_optimized_files else cost_original_files
        
        # ğŸ“Š EXPLAIN + EXPLAIN COSTçµæœã‚’è¦ç´„ã—ã¦ã‹ã‚‰ãƒ¬ãƒãƒ¼ãƒˆã«çµ„ã¿è¾¼ã¿
        explain_content = ""
        explain_cost_content = ""
        query_type = "optimized" if (explain_optimized_files or cost_optimized_files) else "original"
        # EXPLAIN ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        if explain_files:
            latest_explain_file = max(explain_files, key=os.path.getctime)
            try:
                with open(latest_explain_file, 'r', encoding='utf-8') as f:
                    explain_content = f.read()
                print(f"âœ… Loaded EXPLAIN results: {latest_explain_file}")
            except Exception as e:
                print(f"âš ï¸ Failed to load EXPLAIN results: {str(e)}")
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ãƒã‚§ãƒƒã‚¯
            old_explain_files = glob.glob("output_explain_plan_*.txt")
            if old_explain_files:
                latest_explain_file = max(old_explain_files, key=os.path.getctime)
                try:
                    with open(latest_explain_file, 'r', encoding='utf-8') as f:
                        explain_content = f.read()
                        print(f"âœ… Loaded legacy format EXPLAIN results: {latest_explain_file}")
                except Exception as e:
                    print(f"âš ï¸ Failed to load legacy format EXPLAIN results: {str(e)}")
        
        # EXPLAIN COST ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        if cost_files:
            latest_cost_file = max(cost_files, key=os.path.getctime)
            try:
                with open(latest_cost_file, 'r', encoding='utf-8') as f:
                    explain_cost_content = f.read()
                    print(f"ğŸ’° Loaded EXPLAIN COST results for comprehensive report: {latest_cost_file}")
            except Exception as e:
                print(f"âš ï¸ Failed to load EXPLAIN COST results: {str(e)}")
        
        # ğŸ“Š è¦ç´„æ©Ÿèƒ½ã‚’ä½¿ã£ã¦ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã«å¯¾å¿œ
        summary_results = summarize_explain_results_with_llm(explain_content, explain_cost_content, query_type)
        
        # è¦ç´„çµæœã‚’ä½¿ã£ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰
        if summary_results['summarized']:
            print(f"ğŸ“Š Generating summary report sections (total size reduction)")
        
        if OUTPUT_LANGUAGE == 'ja':
            explain_section = f"""

## ğŸ” 6. EXPLAIN + EXPLAIN COSTçµ±åˆåˆ†æçµæœ

### ğŸ“Š è¦ç´„ã•ã‚ŒãŸå®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒ»çµ±è¨ˆæƒ…å ±

**åˆ†æå¯¾è±¡**: {query_type}ã‚¯ã‚¨ãƒª
**è¦ç´„å®Ÿè¡Œ**: {'ã¯ã„ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾å¿œï¼‰' if summary_results['summarized'] else 'ã„ã„ãˆï¼ˆã‚µã‚¤ã‚ºå°ï¼‰'}

{summary_results['explain_summary']}

### ğŸ’° çµ±è¨ˆãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ã®åŠ¹æœ

çµ±è¨ˆæƒ…å ±ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ä»¥ä¸‹ã®æ”¹å–„åŠ¹æœãŒæœŸå¾…ã§ãã¾ã™ï¼š

| é …ç›® | å¾“æ¥ï¼ˆæ¨æ¸¬ãƒ™ãƒ¼ã‚¹ï¼‰ | çµ±è¨ˆãƒ™ãƒ¼ã‚¹ | æ”¹å–„åŠ¹æœ |
|------|-------------------|-----------|----------|
| BROADCASTåˆ¤å®šç²¾åº¦ | ç´„60% | ç´„95% | **+35%** |
| ã‚¹ãƒ”ãƒ«äºˆæ¸¬ç²¾åº¦ | ç´„40% | ç´„85% | **+45%** |
| ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æœ€é©åŒ– | ç´„50% | ç´„90% | **+40%** |
| å…¨ä½“æœ€é©åŒ–åŠ¹æœ | å¹³å‡30%æ”¹å–„ | å¹³å‡60%æ”¹å–„ | **+30%** |

### ğŸ¯ çµ±è¨ˆæƒ…å ±æ¦‚è¦

çµ±è¨ˆæƒ…å ±ã«ã‚ˆã‚‹æœ€é©åŒ–ãŒå®Ÿè¡Œã•ã‚Œã¾ã—ãŸï¼ˆè©³ç´°ã¯DEBUG_ENABLED='Y'ã§ç¢ºèªå¯èƒ½ï¼‰ã€‚

"""
            explain_cost_section = ""  # çµ±åˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ãªã®ã§å€‹åˆ¥ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ä¸è¦
        else:
            explain_section = f"""

## ğŸ” 6. EXPLAIN + EXPLAIN COST Integrated Analysis Results

### ğŸ“Š Summarized Execution Plan & Statistical Information

**Analysis Target**: {query_type} query
**Summarization**: {'Yes (Token Limit Adaptation)' if summary_results['summarized'] else 'No (Small Size)'}

{summary_results['explain_summary']}

### ğŸ’° Effects of Statistics-Based Optimization

The following improvement effects can be expected by leveraging statistical information:

| Item | Traditional (Guess-based) | Statistics-based | Improvement |
|------|---------------------------|------------------|-------------|
| BROADCAST Judgment Accuracy | ~60% | ~95% | **+35%** |
| Spill Prediction Accuracy | ~40% | ~85% | **+45%** |
| Partition Optimization | ~50% | ~90% | **+40%** |
| Overall Optimization Effect | Average 30% improvement | Average 60% improvement | **+30%** |

### ğŸ¯ Statistical Information Overview

Statistical optimization has been executed (details available with DEBUG_ENABLED='Y').

"""
            explain_cost_section = ""  # Integrated section, so no separate section needed
    else:
        if OUTPUT_LANGUAGE == 'ja':
            explain_section = "\n\n## ğŸ” 6. EXPLAIN + EXPLAIN COSTçµ±åˆåˆ†æçµæœ\n\nâš ï¸ EXPLAIN_ENABLED = 'N' ã®ãŸã‚ã€EXPLAINåˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚\n"
            explain_cost_section = ""
        else:
            explain_section = "\n\n## ğŸ” 6. EXPLAIN + EXPLAIN COST Integrated Analysis Results\n\nâš ï¸ EXPLAIN analysis was skipped because EXPLAIN_ENABLED = 'N'.\n"
            explain_cost_section = ""
    
    # åŸºæœ¬æƒ…å ±ã®å–å¾—
    # åŸºæœ¬æƒ…å ±ã®å–å¾—
    overall_metrics = metrics.get('overall_metrics', {})
    bottleneck_indicators = metrics.get('bottleneck_indicators', {})
    liquid_analysis = metrics.get('liquid_clustering_analysis', {})
    
    # thinking_enabledå¯¾å¿œ: analysis_resultãŒãƒªã‚¹ãƒˆã®å ´åˆã®å‡¦ç†
    if isinstance(analysis_result, list):
        analysis_result_str = format_thinking_response(analysis_result)
    else:
        analysis_result_str = str(analysis_result)
    
    # signatureæƒ…å ±ã®é™¤å»
    import re
    signature_pattern = r"'signature':\s*'[A-Za-z0-9+/=]{100,}'"
    analysis_result_str = re.sub(signature_pattern, "'signature': '[REMOVED]'", analysis_result_str)
    
    # æ—¥æœ¬èªå‡ºåŠ›ã®å ´åˆã€analysis_result_strã‚’LLMã§æ—¥æœ¬èªã«ç¿»è¨³
    if OUTPUT_LANGUAGE == 'ja' and analysis_result_str and analysis_result_str.strip():
        analysis_result_str = translate_analysis_to_japanese(analysis_result_str)
    
    # ãƒ¬ãƒãƒ¼ãƒˆã®æ§‹æˆ
    if OUTPUT_LANGUAGE == 'ja':
        report = f"""# ğŸ“Š SQLæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ

**ã‚¯ã‚¨ãƒªID**: {query_id}  
**ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ¯ 1. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ

### ğŸ¤– AIã«ã‚ˆã‚‹è©³ç´°åˆ†æ

{analysis_result_str}

### ğŸ“Š ä¸»è¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™

| æŒ‡æ¨™ | å€¤ | è©•ä¾¡ |
|------|-----|------|
| å®Ÿè¡Œæ™‚é–“ | {overall_metrics.get('total_time_ms', 0):,} ms | {'âœ… è‰¯å¥½' if overall_metrics.get('total_time_ms', 0) < 60000 else 'âš ï¸ æ”¹å–„å¿…è¦'} |
| Photonæœ‰åŠ¹ | {'ã¯ã„' if overall_metrics.get('photon_enabled', False) else 'ã„ã„ãˆ'} | {'âœ… è‰¯å¥½' if overall_metrics.get('photon_enabled', False) else 'âŒ æœªæœ‰åŠ¹'} |
| ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ | {bottleneck_indicators.get('cache_hit_ratio', 0) * 100:.1f}% | {'âœ… è‰¯å¥½' if bottleneck_indicators.get('cache_hit_ratio', 0) > 0.8 else 'âš ï¸ æ”¹å–„å¿…è¦'} |
| ãƒ•ã‚£ãƒ«ã‚¿ç‡ | {bottleneck_indicators.get('data_selectivity', 0) * 100:.2f}% | {'âœ… è‰¯å¥½' if bottleneck_indicators.get('data_selectivity', 0) > 0.5 else 'âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã‚’ç¢ºèª'} |
| ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ | {bottleneck_indicators.get('shuffle_operations_count', 0)}å› | {'âœ… è‰¯å¥½' if bottleneck_indicators.get('shuffle_operations_count', 0) < 5 else 'âš ï¸ å¤šæ•°'} |
| ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ | {'ã¯ã„' if bottleneck_indicators.get('has_spill', False) else 'ã„ã„ãˆ'} | {'âŒ å•é¡Œã‚ã‚Š' if bottleneck_indicators.get('has_spill', False) else 'âœ… è‰¯å¥½'} |
| ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º | {'AQEã§æ¤œå‡ºãƒ»å¯¾å¿œæ¸ˆ' if bottleneck_indicators.get('has_skew', False) else 'æ½œåœ¨çš„ãªã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§ã‚ã‚Š' if bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False) else 'æœªæ¤œå‡º'} | {'ğŸ”§ AQEå¯¾å¿œæ¸ˆ' if bottleneck_indicators.get('has_skew', False) else 'âš ï¸ æ”¹å–„å¿…è¦' if bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False) else 'âœ… è‰¯å¥½'} |

### ğŸš¨ ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯

"""
        
        # ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®è©³ç´°
        bottlenecks = []
        
        if bottleneck_indicators.get('has_spill', False):
            spill_gb = bottleneck_indicators.get('spill_bytes', 0) / 1024 / 1024 / 1024
            bottlenecks.append(f"**ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«**: {spill_gb:.2f}GB - ãƒ¡ãƒ¢ãƒªä¸è¶³ã«ã‚ˆã‚‹æ€§èƒ½ä½ä¸‹")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            bottlenecks.append("**ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: JOIN/GROUP BYå‡¦ç†ã§ã®å¤§é‡ãƒ‡ãƒ¼ã‚¿è»¢é€")
        
        if bottleneck_indicators.get('has_skew', False):
            bottlenecks.append("**ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼**: AQEã§æ¤œå‡ºãƒ»å¯¾å¿œæ¸ˆ - SparkãŒè‡ªå‹•çš„ã«æœ€é©åŒ–å®Ÿè¡Œ")
        elif bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False):
            bottlenecks.append("**ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼**: æ½œåœ¨çš„ãªã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§ã‚ã‚Š - ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºãŒ512MBä»¥ä¸Š")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            bottlenecks.append("**ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ä½ä¸‹**: ãƒ‡ãƒ¼ã‚¿å†åˆ©ç”¨åŠ¹ç‡ãŒä½ã„")
        
        if not overall_metrics.get('photon_enabled', False):
            bottlenecks.append("**Photonæœªæœ‰åŠ¹**: é«˜é€Ÿå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã•ã‚Œã¦ã„ãªã„")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.2:
            bottlenecks.append("**ãƒ•ã‚£ãƒ«ã‚¿åŠ¹ç‡ä½ä¸‹**: å¿…è¦ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã‚‹")
        
        if bottlenecks:
            for i, bottleneck in enumerate(bottlenecks, 1):
                report += f"{i}. {bottleneck}\n"
        else:
            report += "ä¸»è¦ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯è¨­å®šãªã—ã€‚\n"
        
        report += "\n"
        
        # Add Liquid Clustering analysis results
        if liquid_analysis:
            performance_context = liquid_analysis.get('performance_context', {})
            llm_analysis = liquid_analysis.get('llm_analysis', '')
            
            report += f"""

## ğŸ—‚ï¸ 3. Liquid Clusteringåˆ†æçµæœ

### ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦

| é …ç›® | å€¤ |
|------|-----|
| å®Ÿè¡Œæ™‚é–“ | {performance_context.get('total_time_sec', 0):.1f}ç§’ |
| ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ | {performance_context.get('read_gb', 0):.2f}GB |
| å‡ºåŠ›è¡Œæ•° | {performance_context.get('rows_produced', 0):,}è¡Œ |
| èª­ã¿è¾¼ã¿è¡Œæ•° | {performance_context.get('rows_read', 0):,}è¡Œ |
| ãƒ•ã‚£ãƒ«ã‚¿ç‡ | {performance_context.get('data_selectivity', 0):.4f} |

### ğŸ¤– AIåˆ†æçµæœ

{llm_analysis}

"""
        
        # æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10ã‚’çµ±åˆ
        report += f"""
## ğŸŒ 2. æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10

### ğŸ“Š è©³ç´°ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ

ä»¥ä¸‹ã®ãƒˆãƒ”ãƒƒã‚¯ã«åŸºã¥ã„ã¦å‡¦ç†ã‚’åˆ†æã—ã¾ã™ï¼š

#### ğŸ” åˆ†æå¯¾è±¡ãƒˆãƒ”ãƒƒã‚¯
- **â±ï¸ å®Ÿè¡Œæ™‚é–“**: å…¨ä½“ã«å ã‚ã‚‹å‡¦ç†æ™‚é–“ã®å‰²åˆ
- **ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼
- **ğŸ”§ ä¸¦åˆ—åº¦**: ã‚¿ã‚¹ã‚¯æ•°ã¨ä¸¦åˆ—å®Ÿè¡ŒåŠ¹ç‡
- **ğŸ’¿ ã‚¹ãƒ”ãƒ«æ¤œå‡º**: ãƒ¡ãƒ¢ãƒªä¸è¶³ã«ã‚ˆã‚‹ãƒ‡ã‚£ã‚¹ã‚¯ã‚¹ãƒ”ãƒ«
- **âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º**: AQEãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿åˆ†æ•£ä¸å‡ç­‰æ¤œå‡º
- **ğŸ”„ Shuffleå±æ€§**: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å†åˆ†æ•£ã®æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆ
- **ğŸš€ å‡¦ç†åŠ¹ç‡**: è¡Œ/ç§’ã§ã®å‡¦ç†åŠ¹ç‡æŒ‡æ¨™

"""
        
        # TOP10ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã¨çµ±åˆ
        try:
            top10_report = generate_top10_time_consuming_processes_report(metrics, 10)
            # ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤å»ã—ã¦çµ±åˆ
            top10_lines = top10_report.split('\n')
            # "## ğŸŒ æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10"ã®è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            filtered_lines = []
            skip_header = True
            for line in top10_lines:
                if skip_header and line.startswith("## ğŸŒ"):
                    skip_header = False
                    continue
                if not skip_header:
                    filtered_lines.append(line)
            
            report += '\n'.join(filtered_lines)
            
        except Exception as e:
            report += f"âš ï¸ TOP10å‡¦ç†æ™‚é–“åˆ†æã®ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\n"
        
        # SQLæœ€é©åŒ–åˆ†æçµæœã®è¿½åŠ 
        # ğŸš€ SQLãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®å ´åˆã¯é©åˆ‡ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆçœç•¥æ©Ÿèƒ½ä»˜ãï¼‰
        formatted_sql_content = format_sql_content_for_report(optimized_result, latest_sql_filename)
        
        # ğŸ¯ æœ€é©åŒ–æ–¹é‡è¦ç´„ã‚’ç”Ÿæˆ
        optimization_strategy = generate_optimization_strategy_summary(optimized_result, metrics, analysis_result_str)
        
        report += f"""

## ğŸš€ 4. SQLæœ€é©åŒ–åˆ†æçµæœ

### ğŸ¯ æœ€é©åŒ–å®Ÿè¡Œæ–¹é‡

{optimization_strategy}

### ğŸ’¡ æœ€é©åŒ–ææ¡ˆ

{formatted_sql_content}

### ğŸ” 5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼çµæœ

{generate_performance_comparison_section(performance_comparison)}

### ğŸ“ˆ 6. æœŸå¾…ã•ã‚Œã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„åŠ¹æœ

#### ğŸ¯ äºˆæƒ³ã•ã‚Œã‚‹æ”¹å–„ç‚¹

"""
        
        # æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„åŠ¹æœã‚’è¨ˆç®—
        expected_improvements = []
        
        if bottleneck_indicators.get('has_spill', False):
            expected_improvements.append("**ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«è§£æ¶ˆ**: æœ€å¤§50-80%ã®æ€§èƒ½æ”¹å–„ãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            expected_improvements.append("**ã‚·ãƒ£ãƒƒãƒ•ãƒ«æœ€é©åŒ–**: 20-60%ã®å®Ÿè¡Œæ™‚é–“çŸ­ç¸®ãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            expected_improvements.append("**ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡å‘ä¸Š**: 30-70%ã®èª­ã¿è¾¼ã¿æ™‚é–“çŸ­ç¸®ãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if not overall_metrics.get('photon_enabled', False):
            expected_improvements.append("**Photonæœ‰åŠ¹åŒ–**: 2-10å€ã®å‡¦ç†é€Ÿåº¦å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.2:
            expected_improvements.append("**ãƒ•ã‚£ãƒ«ã‚¿åŠ¹ç‡æ”¹å–„**: 40-90%ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é‡å‰Šæ¸›ãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if expected_improvements:
            for i, improvement in enumerate(expected_improvements, 1):
                report += f"{i}. {improvement}\n"
            
            # ç·åˆçš„ãªæ”¹å–„åŠ¹æœ
            total_time_ms = overall_metrics.get('total_time_ms', 0)
            if total_time_ms > 0:
                improvement_ratio = min(0.8, len(expected_improvements) * 0.15)  # æœ€å¤§80%æ”¹å–„
                expected_time = total_time_ms * (1 - improvement_ratio)
                report += f"\n**ç·åˆæ”¹å–„åŠ¹æœ**: å®Ÿè¡Œæ™‚é–“ {total_time_ms:,}ms â†’ {expected_time:,.0f}msï¼ˆç´„{improvement_ratio*100:.0f}%æ”¹å–„ï¼‰\n"
        else:
            report += "ç¾åœ¨ã®ã‚¯ã‚¨ãƒªã¯æ—¢ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚å¤§å¹…ãªæ”¹å–„ã¯æœŸå¾…ã•ã‚Œã¾ã›ã‚“ã€‚\n"
        
        report += f"""

#### ğŸ”§ å®Ÿè£…å„ªå…ˆåº¦

1. **é«˜å„ªå…ˆåº¦**: Photonæœ‰åŠ¹åŒ–ã€ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«è§£æ¶ˆ
2. **ä¸­å„ªå…ˆåº¦**: Liquid Clusteringã€ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæœ€é©åŒ–
3. **ä½å„ªå…ˆåº¦**: çµ±è¨ˆæƒ…å ±æ›´æ–°ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥

{explain_section}

{explain_cost_section}

---

*ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
    else:
        # è‹±èªç‰ˆï¼ˆåŒæ§˜ã®æ§‹æˆï¼‰
        report = f"""# ğŸ“Š SQL Optimization Report

**Query ID**: {query_id}  
**Report Generation Time**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ¯ 1. Bottleneck Analysis Results

### ğŸ¤– AI-Powered Analysis

{analysis_result_str}

### ğŸ“Š Key Performance Indicators

| Metric | Value | Status |
|--------|-------|--------|
| Execution Time | {overall_metrics.get('total_time_ms', 0):,} ms | {'âœ… Good' if overall_metrics.get('total_time_ms', 0) < 60000 else 'âš ï¸ Needs Improvement'} |
| Photon Enabled | {'Yes' if overall_metrics.get('photon_enabled', False) else 'No'} | {'âœ… Good' if overall_metrics.get('photon_enabled', False) else 'âŒ Not Enabled'} |
| Cache Efficiency | {bottleneck_indicators.get('cache_hit_ratio', 0) * 100:.1f}% | {'âœ… Good' if bottleneck_indicators.get('cache_hit_ratio', 0) > 0.8 else 'âš ï¸ Needs Improvement'} |
| Filter Rate | {bottleneck_indicators.get('data_selectivity', 0) * 100:.2f}% | {'âœ… Good' if bottleneck_indicators.get('data_selectivity', 0) > 0.5 else 'âš ï¸ Check Filter Conditions'} |
| Shuffle Operations | {bottleneck_indicators.get('shuffle_operations_count', 0)} times | {'âœ… Good' if bottleneck_indicators.get('shuffle_operations_count', 0) < 5 else 'âš ï¸ High'} |
| Spill Occurrence | {'Yes' if bottleneck_indicators.get('has_spill', False) else 'No'} | {'âŒ Issues' if bottleneck_indicators.get('has_spill', False) else 'âœ… Good'} |
| Skew Detection | {'AQE Detected & Handled' if bottleneck_indicators.get('has_skew', False) else 'Not Detected'} | {'ğŸ”§ AQE Handled' if bottleneck_indicators.get('has_skew', False) else 'âœ… Good'} |

### ğŸš¨ Key Bottlenecks

"""
        
        # ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®è©³ç´°ï¼ˆè‹±èªç‰ˆï¼‰
        bottlenecks = []
        
        if bottleneck_indicators.get('has_spill', False):
            spill_gb = bottleneck_indicators.get('spill_bytes', 0) / 1024 / 1024 / 1024
            bottlenecks.append(f"**Memory Spill**: {spill_gb:.2f}GB - Performance degradation due to memory shortage")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            bottlenecks.append("**Shuffle Bottleneck**: Large data transfer in JOIN/GROUP BY operations")
        
        if bottleneck_indicators.get('has_skew', False):
            bottlenecks.append("**Data Skew**: AQE Detected & Handled - Spark automatically optimized execution")
        elif bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False):
            bottlenecks.append("**Data Skew**: Potential skew possibility - Partition size â‰¥ 512MB")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            bottlenecks.append("**Cache Inefficiency**: Low data reuse efficiency")
        
        if not overall_metrics.get('photon_enabled', False):
            bottlenecks.append("**Photon Not Enabled**: High-speed processing engine not utilized")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.2:
            bottlenecks.append("**Poor Filter Efficiency**: Reading more data than necessary")
        
        if bottlenecks:
            for i, bottleneck in enumerate(bottlenecks, 1):
                report += f"{i}. {bottleneck}\n"
        else:
            report += "No major bottlenecks detected.\n"
        
        report += "\n"
        
        # æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10ã‚’çµ±åˆï¼ˆè‹±èªç‰ˆï¼‰
        report += f"""
## ğŸŒ 2. Top 10 Most Time-Consuming Processes

### ğŸ“Š Detailed Bottleneck Analysis

The following topics are analyzed for process evaluation:

#### ğŸ” Analysis Topics
- **â±ï¸ Execution Time**: Percentage of total processing time
- **ğŸ’¾ Memory Usage**: Peak memory usage and memory pressure
- **ğŸ”§ Parallelism**: Number of tasks and parallel execution efficiency
- **ğŸ’¿ Spill Detection**: Disk spill due to memory shortage
- **âš–ï¸ Skew Detection**: AQE-based data distribution imbalance detection
- **ğŸ”„ Shuffle Attributes**: Optimization points for partition redistribution
- **ğŸš€ Processing Efficiency**: Processing efficiency metrics in rows/second

"""
        
        # TOP10ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã¨çµ±åˆï¼ˆè‹±èªç‰ˆï¼‰
        try:
            top10_report = generate_top10_time_consuming_processes_report(metrics, 10)
            # ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤å»ã—ã¦çµ±åˆ
            top10_lines = top10_report.split('\n')
            # "## ğŸŒ æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10"ã®è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            filtered_lines = []
            skip_header = True
            for line in top10_lines:
                if skip_header and line.startswith("## ğŸŒ"):
                    skip_header = False
                    continue
                if not skip_header:
                    filtered_lines.append(line)
            
            report += '\n'.join(filtered_lines)
            
        except Exception as e:
            report += f"âš ï¸ Error generating TOP10 analysis: {str(e)}\n"
        
        # Add Liquid Clustering analysis results (English version)
        if liquid_analysis:
            performance_context = liquid_analysis.get('performance_context', {})
            llm_analysis = liquid_analysis.get('llm_analysis', '')
            
            report += f"""

## ğŸ—‚ï¸ 3. Liquid Clustering Analysis Results

### ğŸ“Š Performance Overview

| Item | Value |
|------|-------|
| Execution Time | {performance_context.get('total_time_sec', 0):.1f}s |
| Data Read | {performance_context.get('read_gb', 0):.2f}GB |
| Output Rows | {performance_context.get('rows_produced', 0):,} |
| Read Rows | {performance_context.get('rows_read', 0):,} |
| Filter Rate | {performance_context.get('data_selectivity', 0):.4f} |

### ğŸ¤– AI Analysis Results

{llm_analysis}

"""
        
        # SQLæœ€é©åŒ–åˆ†æçµæœã®è¿½åŠ ï¼ˆè‹±èªç‰ˆï¼‰
        # ğŸš€ SQLãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®å ´åˆã¯é©åˆ‡ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆçœç•¥æ©Ÿèƒ½ä»˜ãï¼‰
        formatted_sql_content = format_sql_content_for_report(optimized_result, latest_sql_filename)
        
        # ğŸ¯ æœ€é©åŒ–æ–¹é‡è¦ç´„ã‚’ç”Ÿæˆï¼ˆè‹±èªç‰ˆï¼‰
        optimization_strategy = generate_optimization_strategy_summary(optimized_result, metrics, analysis_result_str)
        
        # æ—¥æœ¬èªã‹ã‚‰è‹±èªã¸ã®ç¿»è¨³ãƒãƒƒãƒ”ãƒ³ã‚°
        translation_map = {
            "ğŸ” æ¤œå‡ºã•ã‚ŒãŸä¸»è¦èª²é¡Œ": "ğŸ” Key Issues Identified",
            "ğŸ› ï¸ é©ç”¨ã•ã‚ŒãŸæœ€é©åŒ–æ‰‹æ³•": "ğŸ› ï¸ Applied Optimization Techniques",
            "ğŸ¯ æœ€é©åŒ–é‡ç‚¹åˆ†é‡": "ğŸ¯ Optimization Focus Areas",
            "ğŸ“Š çµ±è¨ˆæƒ…å ±æ´»ç”¨": "ğŸ“Š Statistical Analysis Utilization",
            "EXPLAIN + EXPLAIN COSTåˆ†æã«ã‚ˆã‚Šã€çµ±è¨ˆãƒ™ãƒ¼ã‚¹ã®ç²¾å¯†ãªæœ€é©åŒ–ã‚’å®Ÿè¡Œ": "Statistical-based precise optimization through EXPLAIN + EXPLAIN COST analysis",
            "ğŸ¤– AIåˆ†æã«ã‚ˆã‚‹åŒ…æ‹¬çš„ãªæœ€é©åŒ–": "ğŸ¤– Comprehensive AI-driven Optimization",
            "ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã€çµ±è¨ˆæƒ…å ±ã€ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’ç·åˆã—ãŸæœ€é©åŒ–ã‚’å®Ÿè¡Œ": "Comprehensive optimization integrating bottleneck analysis, statistical data, and best practices",
            "ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«ç™ºç”Ÿ": "Memory Spill Occurrence",
            "ã‚·ãƒ£ãƒƒãƒ•ãƒ«å‡¦ç†ãƒœãƒˆãƒ«ãƒãƒƒã‚¯": "Shuffle Processing Bottleneck",
            "ä¸¦åˆ—åº¦ä¸è¶³": "Insufficient Parallelism",
            "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ä½ä¸‹": "Low Cache Hit Rate",
            "Photon Engineæœªæ´»ç”¨": "Photon Engine Not Utilized",
            "ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ç™ºç”Ÿ": "Data Skew Occurrence",
            "ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–": "Memory Efficiency",
            "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è² è·è»½æ¸›": "Network Load Reduction",
            "ä¸¦åˆ—å‡¦ç†èƒ½åŠ›å‘ä¸Š": "Parallel Processing Enhancement"
        }
        
        optimization_strategy_en = optimization_strategy
        for jp_text, en_text in translation_map.items():
            optimization_strategy_en = optimization_strategy_en.replace(jp_text, en_text)
        
        # EXPLAINè¦ç´„ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨è¿½åŠ ï¼ˆå‹•çš„ã«æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼‰
        explain_summary_section = ""
        try:
            # è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§EXPLAINè¦ç´„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆoptimized/originalä¸¡æ–¹å¯¾å¿œï¼‰
            optimized_files = glob.glob("output_explain_summary_optimized_*.md")
            original_files = glob.glob("output_explain_summary_original_*.md")
            all_explain_files = optimized_files + original_files
            
            if all_explain_files:
                # ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæ™‚åˆ»ã§æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠï¼ˆã‚ˆã‚Šç¢ºå®Ÿï¼‰
                import os
                latest_explain_summary = max(all_explain_files, key=os.path.getctime)
                file_age = os.path.getctime(latest_explain_summary)
                
                print(f"ğŸ” Found {len(all_explain_files)} EXPLAIN summary files:")
                for f in sorted(all_explain_files, key=os.path.getctime, reverse=True):
                    age = os.path.getctime(f)
                    status = "ğŸ“ SELECTED" if f == latest_explain_summary else "  "
                    print(f"   {status} {f} (created: {os.path.getctime(f)})")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿è¾¼ã¿
                with open(latest_explain_summary, 'r', encoding='utf-8') as f:
                    explain_content = f.read()
                
                # è‹±èªç‰ˆã«ç¿»è¨³
                explain_content_en = translate_explain_summary_to_english(explain_content)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®šï¼ˆoptimized/originalï¼‰
                file_type = "Optimized" if "optimized" in latest_explain_summary else "Original"
                
                explain_summary_section = f"""
### ğŸ“‹ Current Query Explain Output ({file_type} Query)

> **Source File**: `{latest_explain_summary}`  
> **Analysis Type**: {file_type} query execution plan analysis

{explain_content_en}

"""
                print(f"âœ… EXPLAIN summary integrated: {latest_explain_summary} ({file_type})")
            else:
                print("âš ï¸ No EXPLAIN summary files found (searched: output_explain_summary_*.md)")
                # EXPLAINå®Ÿè¡ŒãŒç„¡åŠ¹ãªå ´åˆã®èª¬æ˜ã‚’è¿½åŠ 
                explain_summary_section = f"""
### ğŸ“‹ Current Query Explain Output

âš ï¸ **EXPLAIN analysis not available**

No EXPLAIN summary files were found. This could be due to:
- EXPLAIN_ENABLED setting is 'N' (disabled)
- EXPLAIN execution failed or was skipped
- Files haven't been generated yet for this query

To enable EXPLAIN analysis, set `EXPLAIN_ENABLED = 'Y'` before running the analysis.

"""
        except Exception as e:
            print(f"âš ï¸ Error loading EXPLAIN summary: {str(e)}")
            explain_summary_section = f"""
### ğŸ“‹ Current Query Explain Output

âŒ **Error loading EXPLAIN analysis**

An error occurred while loading EXPLAIN summary files: `{str(e)}`

Please check:
- File permissions and accessibility
- EXPLAIN_ENABLED setting
- Query execution status

"""

        report += f"""
## ğŸš€ 4. SQL Optimization Analysis Results

### ğŸ¯ Optimization Strategy

{optimization_strategy_en}

### ğŸ’¡ Optimization Recommendations

{formatted_sql_content}

{explain_summary_section}### ğŸ” 5. Performance Verification Results

{generate_performance_comparison_section(performance_comparison, language='en')}

### ğŸ“ˆ 6. Expected Performance Improvement

#### ğŸ¯ Anticipated Improvements

"""
        
        # æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„åŠ¹æœã‚’è¨ˆç®—ï¼ˆè‹±èªç‰ˆï¼‰
        expected_improvements = []
        
        if bottleneck_indicators.get('has_spill', False):
            expected_improvements.append("**Memory Spill Resolution**: Up to 50-80% performance improvement expected")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            expected_improvements.append("**Shuffle Optimization**: 20-60% execution time reduction expected")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            expected_improvements.append("**Cache Efficiency**: 30-70% read time reduction expected")
        
        if not overall_metrics.get('photon_enabled', False):
            expected_improvements.append("**Photon Enablement**: 2-10x processing speed improvement expected")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.2:
            expected_improvements.append("**Filter Efficiency**: 40-90% data read volume reduction expected")
        
        if expected_improvements:
            for i, improvement in enumerate(expected_improvements, 1):
                report += f"{i}. {improvement}\n"
            
            # ç·åˆçš„ãªæ”¹å–„åŠ¹æœ
            total_time_ms = overall_metrics.get('total_time_ms', 0)
            if total_time_ms > 0:
                improvement_ratio = min(0.8, len(expected_improvements) * 0.15)  # æœ€å¤§80%æ”¹å–„
                expected_time = total_time_ms * (1 - improvement_ratio)
                report += f"\n**Overall Improvement**: Execution time {total_time_ms:,}ms â†’ {expected_time:,.0f}ms (approx. {improvement_ratio*100:.0f}% improvement)\n"
        else:
            report += "Current query is already optimized. No significant improvements expected.\n"
        
        report += f"""

#### ğŸ”§ Implementation Priority

1. **High Priority**: Photon enablement, Memory spill resolution
2. **Medium Priority**: Liquid Clustering, Data layout optimization
3. **Low Priority**: Statistics update, Cache strategy

{explain_section}

{explain_cost_section}

---

*Report generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    return report

def refine_report_with_llm(raw_report: str, query_id: str) -> str:
    """
    LLMã‚’ä½¿ã£ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’æ¨æ•²ã—ã€èª­ã¿ã‚„ã™ã„æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    
    Args:
        raw_report: åˆæœŸç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ
        query_id: ã‚¯ã‚¨ãƒªID
        
    Returns:
        str: LLMã§æ¨æ•²ã•ã‚ŒãŸèª­ã¿ã‚„ã™ã„ãƒ¬ãƒãƒ¼ãƒˆ
    """
    
    print("ğŸ¤– Executing LLM-based report refinement...")
    
    # ğŸš¨ ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾ç­–: ãƒ¬ãƒãƒ¼ãƒˆã‚µã‚¤ã‚ºåˆ¶é™
    MAX_REPORT_SIZE = 50000  # 50KBåˆ¶é™
    original_size = len(raw_report)
    
    if original_size > MAX_REPORT_SIZE:
        print(f"âš ï¸ Report size too large: {original_size:,} characters â†’ truncated to {MAX_REPORT_SIZE:,} characters")
        # é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å„ªå…ˆçš„ã«ä¿æŒ
        truncated_report = raw_report[:MAX_REPORT_SIZE]
        truncated_report += f"\n\nâš ï¸ ãƒ¬ãƒãƒ¼ãƒˆãŒå¤§ãã™ãã‚‹ãŸã‚ã€{MAX_REPORT_SIZE:,} æ–‡å­—ã«åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸï¼ˆå…ƒã‚µã‚¤ã‚º: {original_size:,} æ–‡å­—ï¼‰"
        raw_report = truncated_report
    else:
        print(f"ğŸ“Š Report size: {original_size:,} characters (executing refinement)")
    
    # è¨€èªã«å¿œã˜ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ‡ã‚Šæ›¿ãˆ
    if OUTPUT_LANGUAGE == 'ja':
        refinement_prompt = f"""
æŠ€è¡“æ–‡æ›¸ã®ç·¨é›†è€…ã¨ã—ã¦ã€Databricks SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦æ¨æ•²ã—ã¦ãã ã•ã„ã€‚

ã€çµ¶å¯¾ã«å®ˆã‚‹ã¹ãè¦‹å‡ºã—æ§‹é€ ã€‘
```
# ğŸ“Š SQLæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ

## ğŸ” 1. åˆ†æã‚µãƒãƒªãƒ¼

### çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æè¡¨
ä¸»è¦èª²é¡Œã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚’ä»¥ä¸‹ã®çµ±åˆè¡¨å½¢å¼ã§ã¾ã¨ã‚ã¦ãã ã•ã„ï¼š

ğŸ” åˆ†æã‚µãƒãƒªãƒ¼
ã‚¯ã‚¨ãƒªå®Ÿè¡Œæ™‚é–“ã¯[X.X]ç§’ã¨[è©•ä¾¡]ã§ã™ãŒã€ä»¥ä¸‹ã®æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆãŒç‰¹å®šã•ã‚Œã¾ã—ãŸï¼š

| é …ç›® | ç¾åœ¨ã®çŠ¶æ³ | è©•ä¾¡ | å„ªå…ˆåº¦ |
|------|-----------|------|--------|
| å®Ÿè¡Œæ™‚é–“ | [X.X]ç§’ | âœ… è‰¯å¥½ / âš ï¸ æ”¹å–„å¿…è¦ | - |
| ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Šé‡ | [X.XX]GB | âœ… è‰¯å¥½ / âš ï¸ å¤§å®¹é‡ | - |
| Photonæœ‰åŠ¹åŒ– | ã¯ã„/ã„ã„ãˆ | âœ… è‰¯å¥½ / âŒ æœªæœ‰åŠ¹ | - |
| ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ | [N]å› | âœ… è‰¯å¥½ / âš ï¸ å¤šã„ | ğŸš¨ é«˜ / âš ï¸ ä¸­ |
| ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ | ãªã—/ã‚ã‚Š | âœ… è‰¯å¥½ / âŒ å•é¡Œ | ğŸš¨ é«˜ / - |
| ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ | [X.X]% | âœ… è‰¯å¥½ / âš ï¸ ä½åŠ¹ç‡ | âš ï¸ ä¸­ |
| ãƒ•ã‚£ãƒ«ã‚¿åŠ¹ç‡ | [X.X]% | âœ… è‰¯å¥½ / âš ï¸ ä½åŠ¹ç‡ | âš ï¸ ä¸­ |
| ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ | AQEå¯¾å¿œæ¸ˆ / æœªæ¤œå‡º | âœ… å¯¾å¿œæ¸ˆ / âœ… è‰¯å¥½ | - |

## ğŸ“Š 2. TOP10æ™‚é–“æ¶ˆè²»ãƒ—ãƒ­ã‚»ã‚¹åˆ†æ

### â±ï¸ å®Ÿè¡Œæ™‚é–“ãƒ©ãƒ³ã‚­ãƒ³ã‚°

## ğŸ—‚ï¸ 3. Liquid Clusteringåˆ†æçµæœ

### ğŸ“‹ æ¨å¥¨ãƒ†ãƒ¼ãƒ–ãƒ«åˆ†æ

## ğŸš€ 4. æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒª

### ğŸ’¡ å…·ä½“çš„ãªæœ€é©åŒ–å†…å®¹ã¨ã‚³ã‚¹ãƒˆåŠ¹æœ
æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒªã®å‰ã«ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’å¿…ãšå«ã‚ã¦ãã ã•ã„ï¼š

**ğŸ¯ é©ç”¨ã•ã‚ŒãŸæœ€é©åŒ–æ‰‹æ³•:**
- [å®Ÿéš›ã«é©ç”¨ã•ã‚ŒãŸæœ€é©åŒ–æ‰‹æ³•ã®ã¿ã‚’ãƒªã‚¹ãƒˆ]
- âŒ å®Ÿæ–½ã•ã‚Œã¦ã„ãªã„æ‰‹æ³•ã¯è¨˜è¼‰ã—ãªã„ï¼ˆä¾‹: ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„å ´åˆã¯REPARTITIONé©ç”¨ã‚’è¨˜è¼‰ã—ãªã„ï¼‰

**ğŸ’° EXPLAIN COSTãƒ™ãƒ¼ã‚¹ã®åŠ¹æœåˆ†æ:**
- ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚³ã‚¹ãƒˆå‰Šæ¸›ç‡: [cost_ratio]å€ (EXPLAIN COSTæ¯”è¼ƒçµæœ)
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›ç‡: [memory_ratio]å€ (çµ±è¨ˆæƒ…å ±ãƒ™ãƒ¼ã‚¹æ¯”è¼ƒ)
- æ¨å®šãƒ‡ãƒ¼ã‚¿å‡¦ç†åŠ¹ç‡: [processing_efficiency]% (ã‚¹ã‚­ãƒ£ãƒ³ãƒ»JOINåŠ¹ç‡æ”¹å–„)
```

ã€ğŸš¨ REPARTITIONã«é–¢ã™ã‚‹é‡è¦ãªä¿®æ­£æŒ‡ç¤ºã€‘
- **ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„å ´åˆ**: ã€ŒREPARTITIONã®é©ç”¨ã€ã‚’æ¨å¥¨æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«å«ã‚ãªã„
- **å®Ÿéš›ã«é©ç”¨ã•ã‚Œã¦ã„ãªã„æœ€é©åŒ–æ‰‹æ³•**: ã€Œç·Šæ€¥å¯¾å¿œã€ã‚„ã€Œæ¨å¥¨æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€ã«è¨˜è¼‰ã—ãªã„
- **äº‹å®Ÿãƒ™ãƒ¼ã‚¹ã®è¨˜è¼‰**: å®Ÿéš›ã«æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã¨é©ç”¨ã•ã‚ŒãŸå¯¾ç­–ã®ã¿ã‚’è¨˜è¼‰

ã€ğŸ’° ã‚³ã‚¹ãƒˆåŠ¹æœåˆ†æã§ã®å¿…é ˆä½¿ç”¨ãƒ‡ãƒ¼ã‚¿ã€‘
- **performance_comparisonçµæœã‚’å¿…ãšä½¿ç”¨**: cost_ratioã€memory_ratioç­‰ã®å®Ÿéš›ã®æ¯”è¼ƒå€¤
- **å®Ÿè¡Œæ™‚é–“äºˆæ¸¬ã¯ä½¿ç”¨ç¦æ­¢**: ä¸æ­£ç¢ºãªãŸã‚è¨˜è¼‰ã—ãªã„
- **EXPLAIN COSTãƒ™ãƒ¼ã‚¹ã®æ•°å€¤ã®ã¿**: æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ä¸­ã®å®Ÿéš›ã®è¨ˆç®—çµæœã‚’ä½¿ç”¨

ã€å³æ ¼ãªç¦æ­¢äº‹é …ã€‘
- TOP10ã‚’çµ¶å¯¾ã«TOP5ã«å¤‰æ›´ã—ãªã„
- "=========="ç­‰ã®åŒºåˆ‡ã‚Šæ–‡å­—ã‚’å‰Šé™¤ï¼ˆãŸã ã—çµµæ–‡å­—ã«ã‚ˆã‚‹è¦–è¦šçš„è¡¨ç¤ºã¯ä¿æŒï¼‰
- ç•ªå·ä»˜ããƒªã‚¹ãƒˆã§åŒã˜ç•ªå·ã‚’é‡è¤‡ã•ã›ãªã„
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤ã‚„æŠ€è¡“æƒ…å ±ã‚’å‰Šé™¤ã—ãªã„
- å®Ÿæ–½ã•ã‚Œã¦ã„ãªã„æœ€é©åŒ–æ‰‹æ³•ã‚’ã€Œå®Ÿæ–½æ¸ˆã¿ã€ã¨ã—ã¦è¨˜è¼‰ã—ãªã„

ã€ğŸš¨ é‡è¦ãªæƒ…å ±ä¿æŒã®å¿…é ˆè¦ä»¶ã€‘
- **ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±**: å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã€Œç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: XXã€æƒ…å ±ã¯å¿…ãšä¿æŒ
- **ãƒ•ã‚£ãƒ«ã‚¿ç‡æƒ…å ±**: ã€Œãƒ•ã‚£ãƒ«ã‚¿ç‡: X.X% (èª­ã¿è¾¼ã¿: XX.XXGB, ãƒ—ãƒ«ãƒ¼ãƒ³: XX.XXGB)ã€å½¢å¼ã®æƒ…å ±ã¯å¿…ãšä¿æŒ
- **ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—**: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã®ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ï¼ˆå…¨ä½“ã®â—‹â—‹%ï¼‰ã¯æ­£ç¢ºãªå€¤ã‚’ä¿æŒ
- **æ¨å¥¨vsç¾åœ¨ã®æ¯”è¼ƒ**: æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã¨ç¾åœ¨ã®ã‚­ãƒ¼ã®æ¯”è¼ƒæƒ…å ±ã¯å‰Šé™¤ç¦æ­¢
- **æ•°å€¤ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: å®Ÿè¡Œæ™‚é–“ã€ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é‡ã€ã‚¹ãƒ”ãƒ«é‡ç­‰ã®æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã¯å‰Šé™¤ç¦æ­¢
- **SQLå®Ÿè£…ä¾‹**: ALTER TABLEæ–‡ã‚„CLUSTER BYæ§‹æ–‡ã®å…·ä½“ä¾‹ã¯å‰Šé™¤ç¦æ­¢

ã€å‡¦ç†è¦ä»¶ã€‘
1. ä¸Šè¨˜ã®è¦‹å‡ºã—æ§‹é€ ã‚’å¿…ãšä½¿ç”¨
2. ä¸»è¦èª²é¡Œã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚’çµ±åˆè¡¨å½¢å¼ã§ã¾ã¨ã‚ã‚‹
3. å®Ÿéš›ã«é©ç”¨ã•ã‚ŒãŸæœ€é©åŒ–æ‰‹æ³•ã®ã¿ã‚’è¨˜è¼‰ï¼ˆå®Ÿæ–½ã•ã‚Œã¦ã„ãªã„æ‰‹æ³•ã¯è¨˜è¼‰ã—ãªã„ï¼‰
4. å…·ä½“çš„ãªã‚³ã‚¹ãƒˆåŠ¹æœã‚’æ•°å€¤ã§ç¤ºã™
5. æŠ€è¡“æƒ…å ±ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å®Œå…¨ä¿æŒï¼ˆç‰¹ã«ä¸Šè¨˜ã®é‡è¦æƒ…å ±ï¼‰
6. TOP10è¡¨ç¤ºã‚’ç¶­æŒ
7. çµµæ–‡å­—ã«ã‚ˆã‚‹è¦–è¦šçš„è¡¨ç¤ºã‚’ä¿æŒï¼ˆğŸš¨ CRITICALã€âš ï¸ HIGHã€âœ…è‰¯å¥½ç­‰ï¼‰
8. ä¸è¦ãªåŒºåˆ‡ã‚Šæ–‡å­—ï¼ˆ========ç­‰ï¼‰ã®ã¿å‰Šé™¤
9. ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã¨ãƒ•ã‚£ãƒ«ã‚¿ç‡æƒ…å ±ã¯çµ¶å¯¾ã«ä¿æŒ

ã€ç¾åœ¨ã®ãƒ¬ãƒãƒ¼ãƒˆã€‘
```
{raw_report}
```

ä¸Šè¨˜ã®è¦‹å‡ºã—æ§‹é€ ã«å¾“ã£ã¦æ¨æ•²ã—ã€æŠ€è¡“æƒ…å ±ã‚’å®Œå…¨ã«ä¿æŒã—ãŸãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
    else:
        refinement_prompt = f"""
As a technical document editor, please refine the following Databricks SQL performance analysis report according to these rules.

ã€Required Heading Structureã€‘
```
# ğŸ“Š SQL Optimization Report

## ğŸ” 1. Analysis Summary

### Integrated Performance Analysis Table
Merge major issues and performance indicators into the following integrated table format:

ğŸ” Analysis Summary
Query execution time is [X.X] seconds, which is [evaluation], but the following optimization points were identified:

| Item | Current Status | Evaluation | Priority |
|------|---------------|------------|----------|
| Execution Time | [X.X]s | âœ… Good / âš ï¸ Needs Improvement | - |
| Data Read Volume | [X.XX]GB | âœ… Good / âš ï¸ Large Volume | - |
| Photon Enabled | Yes/No | âœ… Good / âŒ Not Enabled | - |
| Shuffle Operations | [N] times | âœ… Good / âš ï¸ High | ğŸš¨ High / âš ï¸ Medium |
| Spill Occurrence | None/Present | âœ… Good / âŒ Issues | ğŸš¨ High / - |
| Cache Efficiency | [X.X]% | âœ… Good / âš ï¸ Low Efficiency | âš ï¸ Medium |
| Filter Efficiency | [X.X]% | âœ… Good / âš ï¸ Low Efficiency | âš ï¸ Medium |
| Data Skew | AQE Handled / Not Detected | âœ… Handled / âœ… Good | - |

## ğŸ“Š 2. TOP10 Time-Consuming Processes Analysis

### â±ï¸ Execution Time Ranking

## ğŸ—‚ï¸ 3. Liquid Clustering Analysis Results

### ğŸ“‹ Recommended Table Analysis

## ğŸš€ 4. Optimized SQL Query

### ğŸ’¡ Specific Optimization Details and Cost Effects
Before the optimized SQL query, must include the following information:

**ğŸ¯ Applied Optimization Techniques:**
- [List only actually applied optimization techniques]
- âŒ Do not list techniques that were not implemented (e.g., do not mention REPARTITION application if no spill was detected)

**ğŸ’° EXPLAIN COST-Based Effect Analysis:**
- Query execution cost reduction: [cost_ratio]x (EXPLAIN COST comparison result)
- Memory usage reduction: [memory_ratio]x (statistics-based comparison)
- Estimated data processing efficiency: [processing_efficiency]% (scan/JOIN efficiency improvement)
```

ã€ğŸš¨ Critical REPARTITION Correction Instructionsã€‘
- **When no spill is detected**: Do not include "REPARTITION application" in recommended improvement actions
- **Actually non-applied optimization techniques**: Do not list in "Emergency Response" or "Recommended Improvement Actions"
- **Fact-based reporting**: Only list actually detected problems and applied countermeasures

ã€ğŸ’° Required Data for Cost Effect Analysisã€‘
- **Must use performance_comparison results**: cost_ratio, memory_ratio and other actual comparison values
- **Execution time prediction is prohibited**: Do not include due to inaccuracy
- **EXPLAIN COST-based numbers only**: Use actual calculation results from optimization process

ã€Strict Prohibitionsã€‘
- Never change TOP10 to TOP5
- Remove separator characters like "==========" (but keep emoji visual displays)
- Do not duplicate numbered list items
- Do not delete metric values or technical information
- Do not report non-implemented optimization techniques as "implemented"

ã€ğŸš¨ Critical Information Preservation Requirementsã€‘
- **Current clustering key information**: Must preserve each table's "Current clustering key: XX" information
- **Filter rate information**: Must preserve "Filter rate: X.X% (read: XX.XXGB, pruned: XX.XXGB)" format
- **Percentage calculations**: Preserve accurate percentage values in bottleneck analysis (XX% of total)
- **Recommended vs current comparison**: Do not delete comparison information between recommended and current clustering keys
- **Numerical metrics**: Do not delete execution time, data read volume, spill volume, etc.
- **SQL implementation examples**: Do not delete specific examples of ALTER TABLE and CLUSTER BY syntax

ã€Processing Requirementsã€‘
1. Must use the above heading structure
2. Merge major issues and performance indicators into integrated table format
3. List only actually applied optimization techniques (do not list non-implemented techniques)
4. Show specific cost effects with numerical values
5. Completely preserve technical information and metrics (especially the important information above)
6. Maintain TOP10 display
7. Keep emoji visual displays (ğŸš¨ CRITICAL, âš ï¸ HIGH, âœ… Good, etc.)
8. Remove only unnecessary separator characters (======== etc.)
9. Absolutely preserve current clustering key information and filter rate information

ã€Current Reportã€‘
```
{raw_report}
```

Please refine according to the above heading structure and output a report that completely preserves technical information.
"""
    
    try:
        provider = LLM_CONFIG.get("provider", "databricks")
        
        if provider == "databricks":
            refined_report = _call_databricks_llm(refinement_prompt)
        elif provider == "openai":
            refined_report = _call_openai_llm(refinement_prompt)
        elif provider == "azure_openai":
            refined_report = _call_azure_openai_llm(refinement_prompt)
        elif provider == "anthropic":
            refined_report = _call_anthropic_llm(refinement_prompt)
        else:
            raise ValueError(f"Unsupported LLM provider: {provider}")
        
        # ğŸš¨ LLMã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ¤œå‡ºï¼ˆç²¾å¯†åŒ–ï¼‰
        if isinstance(refined_report, str):
            # ã‚ˆã‚Šç²¾å¯†ãªã‚¨ãƒ©ãƒ¼æ¤œå‡ºï¼ˆãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã®çµµæ–‡å­—ã¨åŒºåˆ¥ï¼‰
            actual_error_indicators = [
                "APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰",
                "Input is too long for requested model",
                "Bad Request",
                "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼:",
                "APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼:",
                'ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {"error_code":',
                "âŒ APIã‚¨ãƒ©ãƒ¼:",
                "âš ï¸ APIã‚¨ãƒ©ãƒ¼:"
            ]
            
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®é–‹å§‹éƒ¨åˆ†ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šå³å¯†ï¼‰
            is_error_response = any(
                refined_report.strip().startswith(indicator) or 
                f"\n{indicator}" in refined_report[:500]  # å…ˆé ­500æ–‡å­—ä»¥å†…ã§ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                for indicator in actual_error_indicators
            )
            
            if is_error_response:
                print(f"âŒ Error detected in LLM report refinement: {refined_report[:200]}...")
                print("ğŸ“„ Returning original report")
                return raw_report
        
        # thinking_enabledå¯¾å¿œ
        if isinstance(refined_report, list):
            refined_report = format_thinking_response(refined_report)
        
        # signatureæƒ…å ±ã®é™¤å»
        import re
        signature_pattern = r"'signature':\s*'[A-Za-z0-9+/=]{100,}'"
        refined_report = re.sub(signature_pattern, "'signature': '[REMOVED]'", refined_report)
        
        print(f"âœ… LLM-based report refinement completed (Query ID: {query_id})")
        return refined_report
        
    except Exception as e:
        print(f"âš ï¸ Error occurred during LLM-based report refinement: {str(e)}")
        print("ğŸ“„ Returning original report")
        return raw_report

def validate_and_fix_sql_syntax(sql_query: str) -> str:
    """
    SQLæ§‹æ–‡ã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯ã¨ä¿®æ­£ã‚’è¡Œã†ï¼ˆæ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ï¼‰
    
    ä¸»è¦ãƒã‚§ãƒƒã‚¯é …ç›®ï¼š
    1. BROADCASTãƒ’ãƒ³ãƒˆã®é…ç½®ä½ç½®æ¤œè¨¼
    2. å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆSELECTã€FROMã€WHEREç­‰ã®åŸºæœ¬æ§‹æ–‡ï¼‰
    3. åŸºæœ¬çš„ãªæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£
    4. ã‚³ãƒ¡ãƒ³ãƒˆã‚„ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã®é™¤å»
    
    Args:
        sql_query: ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®SQLã‚¯ã‚¨ãƒª
        
    Returns:
        str: ä¿®æ­£ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒª
    """
    import re
    
    if not sql_query or not sql_query.strip():
        return ""
    
    # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    sql_query = sql_query.strip()
    
    # 1. BROADCASTãƒ’ãƒ³ãƒˆã®é…ç½®ä½ç½®ãƒã‚§ãƒƒã‚¯
    sql_query = fix_broadcast_hint_placement(sql_query)
    
    # 2. ä¸å®Œå…¨ãªSQLæ§‹æ–‡ã®æ¤œå‡ºã¨ä¿®æ­£
    sql_query = fix_incomplete_sql_syntax(sql_query)
    
    # 3. ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚„çœç•¥è¨˜å·ã®é™¤å»
    sql_query = remove_sql_placeholders(sql_query)
    
    # 4. åŸºæœ¬çš„ãªæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£
    sql_query = fix_basic_syntax_errors(sql_query)
    
    return sql_query

def fix_broadcast_hint_placement(sql_query: str) -> str:
    """
    BROADCASTãƒ’ãƒ³ãƒˆã®é…ç½®ä½ç½®ã‚’ä¿®æ­£ï¼ˆã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨é…ç½®ã‚’ç¦æ­¢ï¼‰
    
    ä¿®æ­£å†…å®¹ï¼š
    - ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã®BROADCASTãƒ’ãƒ³ãƒˆã‚’ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã«ç§»å‹•
    - FROMå¥ã€JOINå¥ã€WHEREå¥å†…ã®ãƒ’ãƒ³ãƒˆã‚’å‰Šé™¤
    - è¤‡æ•°ã®BROADCASTãƒ’ãƒ³ãƒˆã‚’çµ±åˆ
    - DISTINCTå¥ã®ä¿æŒã‚’ç¢ºä¿
    """
    import re
    
    # ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã®BROADCASTãƒ’ãƒ³ãƒˆã‚’æ¤œå‡ºã¨å‰Šé™¤
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: LEFT JOIN (SELECT /*+ BROADCAST(...) */ ... ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    subquery_broadcast_pattern = r'JOIN\s*\(\s*SELECT\s*/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
    sql_query = re.sub(subquery_broadcast_pattern, 'JOIN (\n  SELECT', sql_query, flags=re.IGNORECASE)
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: WITHå¥ã‚„ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã®BROADCASTãƒ’ãƒ³ãƒˆ
    cte_broadcast_pattern = r'(WITH\s+\w+\s+AS\s*\(\s*SELECT\s*)/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
    sql_query = re.sub(cte_broadcast_pattern, r'\1', sql_query, flags=re.IGNORECASE)
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: FROMå¥å†…ã®BROADCASTãƒ’ãƒ³ãƒˆ
    from_broadcast_pattern = r'FROM\s+\w+\s*/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
    sql_query = re.sub(from_broadcast_pattern, 'FROM', sql_query, flags=re.IGNORECASE)
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³4: WHEREå¥å†…ã®BROADCASTãƒ’ãƒ³ãƒˆ
    where_broadcast_pattern = r'WHERE\s*/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
    sql_query = re.sub(where_broadcast_pattern, 'WHERE', sql_query, flags=re.IGNORECASE)
    
    # DISTINCTå¥ã®å­˜åœ¨ç¢ºèªï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ï¼‰
    distinct_pattern = r'^\s*SELECT\s*(/\*\+[^*]*\*/)?\s*DISTINCT\b'
    has_distinct = bool(re.search(distinct_pattern, sql_query, re.IGNORECASE))
    
    # BROADCASTãƒ’ãƒ³ãƒˆãŒãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTç›´å¾Œã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    main_select_pattern = r'^\s*SELECT\s*(/\*\+[^*]*\*/)?\s*(DISTINCT\s*)?'
    if not re.search(main_select_pattern, sql_query, re.IGNORECASE):
        # ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTç›´å¾Œã«BROADCASTãƒ’ãƒ³ãƒˆãŒãªã„å ´åˆã®å‡¦ç†
        # å‰Šé™¤ã•ã‚ŒãŸBROADCASTãƒ’ãƒ³ãƒˆã‚’å¾©å…ƒã—ã¦ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã«é…ç½®
        broadcast_tables = extract_broadcast_tables_from_sql(sql_query)
        if broadcast_tables:
            broadcast_hint = f"/*+ BROADCAST({', '.join(broadcast_tables)}) */"
            if has_distinct:
                # DISTINCTå¥ãŒã‚ã‚‹å ´åˆï¼šSELECT /*+ BROADCAST(...) */ DISTINCT ã®å½¢å¼ã«ã™ã‚‹
                sql_query = re.sub(r'^\s*SELECT\s*', f'SELECT {broadcast_hint} ', sql_query, flags=re.IGNORECASE)
            else:
                # DISTINCTå¥ãŒãªã„å ´åˆï¼šå¾“æ¥ã®å½¢å¼
                sql_query = re.sub(r'^\s*SELECT\s*', f'SELECT {broadcast_hint}\n  ', sql_query, flags=re.IGNORECASE)
    else:
        # æ—¢ã«ãƒ’ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆã€DISTINCTå¥ãŒæ­£ã—ã„ä½ç½®ã«ã‚ã‚‹ã‹ç¢ºèª
        # é–“é•ã£ãŸé †åºï¼ˆSELECT DISTINCT /*+ BROADCAST(...) */ ï¼‰ã‚’ä¿®æ­£
        wrong_order_pattern = r'^\s*SELECT\s*DISTINCT\s*(/\*\+[^*]*\*/)'
        if re.search(wrong_order_pattern, sql_query, re.IGNORECASE):
            # é–“é•ã£ãŸé †åºã‚’ä¿®æ­£ï¼šSELECT DISTINCT /*+ HINT */ â†’ SELECT /*+ HINT */ DISTINCT
            sql_query = re.sub(wrong_order_pattern, lambda m: f'SELECT {m.group(1)} DISTINCT', sql_query, flags=re.IGNORECASE)
    
    return sql_query


def fix_join_broadcast_hint_placement(sql_query: str) -> str:
    """
    JOINå¥å†…ã®BROADCASTãƒ’ãƒ³ãƒˆé…ç½®ã‚¨ãƒ©ãƒ¼ã‚’å¼·åˆ¶ä¿®æ­£ï¼ˆPARSE_SYNTAX_ERRORå¯¾ç­–ï¼‰
    ãƒ¦ãƒ¼ã‚¶ãƒ¼å ±å‘Šã®ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ï¼š join /*+ BROADCAST(i) */ item i ON ...
    """
    import re
    
    try:
        # JOINå¥å†…ã®BROADCASTãƒ’ãƒ³ãƒˆã‚’æ¤œå‡ºãƒ»æŠ½å‡º
        join_broadcast_pattern = r'JOIN\s+/\*\+\s*BROADCAST\(([^)]+)\)\s*\*/\s*(\w+)'
        join_broadcast_matches = re.findall(join_broadcast_pattern, sql_query, re.IGNORECASE | re.MULTILINE)
        
        if not join_broadcast_matches:
            # JOINå¥å†…ã®BROADCASTãƒ’ãƒ³ãƒˆãŒãªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™
            return sql_query
        
        print(f"ğŸ”§ Detected BROADCAST hints in JOIN clauses: {len(join_broadcast_matches)} instances")
        
        # æŠ½å‡ºã•ã‚ŒãŸBROADCASTå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«å/ã‚¨ã‚¤ãƒªã‚¢ã‚¹åã‚’åé›†
        broadcast_tables = []
        for table_name, table_alias in join_broadcast_matches:
            # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®å ´åˆã‚‚è€ƒæ…®
            tables = [t.strip() for t in table_name.split(',')]
            broadcast_tables.extend(tables)
            # ã‚¨ã‚¤ãƒªã‚¢ã‚¹åã‚‚è¿½åŠ ï¼ˆé‡è¤‡å‰Šé™¤ã¯å¾Œã§è¡Œã†ï¼‰
            if table_alias.strip():
                broadcast_tables.append(table_alias.strip())
        
        # é‡è¤‡å‰Šé™¤
        broadcast_tables = list(set(broadcast_tables))
        print(f"ğŸ“‹ BROADCAST targets: {', '.join(broadcast_tables)}")
        
        # JOINå¥å†…ã®BROADCASTãƒ’ãƒ³ãƒˆã‚’å‰Šé™¤
        fixed_query = re.sub(
            r'JOIN\s+/\*\+\s*BROADCAST\([^)]+\)\s*\*/\s*',
            'JOIN ',
            sql_query,
            flags=re.IGNORECASE | re.MULTILINE
        )
        
        # ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®æœ€åˆã®SELECTæ–‡ã‚’æ¤œå‡º
        select_pattern = r'^(\s*SELECT)\s+'
        select_match = re.search(select_pattern, fixed_query, re.IGNORECASE | re.MULTILINE)
        
        if select_match:
            # æ—¢å­˜ã®ãƒ’ãƒ³ãƒˆå¥ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            existing_hint_pattern = r'^(\s*SELECT)\s+(/\*\+[^*]*\*/)\s+'
            existing_hint_match = re.search(existing_hint_pattern, fixed_query, re.IGNORECASE | re.MULTILINE)
            
            if existing_hint_match:
                # æ—¢å­˜ã®ãƒ’ãƒ³ãƒˆå¥ã«BROADCASTã‚’è¿½åŠ 
                existing_hint = existing_hint_match.group(2)
                
                # æ—¢å­˜ã®BROADCASTæŒ‡å®šã‚’ç¢ºèª
                existing_broadcast_pattern = r'BROADCAST\(([^)]+)\)'
                existing_broadcast_match = re.search(existing_broadcast_pattern, existing_hint, re.IGNORECASE)
                
                if existing_broadcast_match:
                    # æ—¢å­˜ã®BROADCASTæŒ‡å®šã«è¿½åŠ 
                    existing_broadcast_tables = [t.strip() for t in existing_broadcast_match.group(1).split(',')]
                    all_broadcast_tables = list(set(existing_broadcast_tables + broadcast_tables))
                    new_broadcast = f"BROADCAST({', '.join(all_broadcast_tables)})"
                    new_hint = re.sub(
                        r'BROADCAST\([^)]+\)',
                        new_broadcast,
                        existing_hint,
                        flags=re.IGNORECASE
                    )
                else:
                    # æ—¢å­˜ã®ãƒ’ãƒ³ãƒˆå¥ã«BROADCASTã‚’è¿½åŠ 
                    broadcast_hint = f"BROADCAST({', '.join(broadcast_tables)})"
                    # ãƒ’ãƒ³ãƒˆå¥ã®æœ«å°¾ã® */ ã®å‰ã«è¿½åŠ 
                    new_hint = existing_hint.replace('*/', f', {broadcast_hint} */')
                
                # ãƒ’ãƒ³ãƒˆå¥ã‚’ç½®æ›
                fixed_query = re.sub(
                    r'^(\s*SELECT)\s+(/\*\+[^*]*\*/)\s+',
                    f'{select_match.group(1)} {new_hint} ',
                    fixed_query,
                    flags=re.IGNORECASE | re.MULTILINE
                )
            else:
                # æ–°ã—ããƒ’ãƒ³ãƒˆå¥ã‚’è¿½åŠ 
                broadcast_hint = f"/*+ BROADCAST({', '.join(broadcast_tables)}) */"
                fixed_query = re.sub(
                    r'^(\s*SELECT)\s+',
                    f'{select_match.group(1)} {broadcast_hint} ',
                    fixed_query,
                    flags=re.IGNORECASE | re.MULTILINE
                )
            
            print(f"âœ… Completed moving BROADCAST hints to correct positions")
            return fixed_query
        else:
            print("âš ï¸ Main query SELECT statement not found, returning original query")
            return sql_query
            
    except Exception as e:
        print(f"âš ï¸ Error in JOIN BROADCAST placement correction: {str(e)}")
        print("ğŸ”„ Returning original query")
        return sql_query


def enhance_error_correction_with_syntax_validation(corrected_query: str, original_query: str, error_info: str) -> str:
    """
    ã‚¨ãƒ©ãƒ¼ä¿®æ­£å¾Œã®ã‚¯ã‚¨ãƒªã‚’æ¤œè¨¼ã—ã€PARSE_SYNTAX_ERRORãŒè§£æ±ºã•ã‚Œã¦ã„ãªã„å ´åˆã¯å…ƒã‚¯ã‚¨ãƒªã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    """
    
    try:
        # ä¿®æ­£ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®å¾Œå‡¦ç†
        print("ğŸ”§ Executing post-processing of corrected query...")
        
        # JOINå¥å†…ã®BROADCASTé…ç½®ã®å¼·åˆ¶ä¿®æ­£
        final_query = fix_join_broadcast_hint_placement(corrected_query)
        
        # åŸºæœ¬çš„ãªæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
        if "/*+" in error_info and "PARSE_SYNTAX_ERROR" in error_info:
            # PARSE_SYNTAX_ERRORã®å ´åˆã¯ç‰¹ã«å³æ ¼ã«ãƒã‚§ãƒƒã‚¯
            
            # JOINå¥å†…ã®BROADCASTãƒ’ãƒ³ãƒˆãŒæ®‹ã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            import re
            join_broadcast_pattern = r'JOIN\s+/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
            if re.search(join_broadcast_pattern, final_query, re.IGNORECASE | re.MULTILINE):
                print("ğŸš¨ BROADCAST hints still remain in JOIN clauses after correction, using original query")
                return f"""-- âŒ PARSE_SYNTAX_ERRORä¿®æ­£å¤±æ•—ã®ãŸã‚ã€å…ƒã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨
-- ğŸ“‹ ã‚¨ãƒ©ãƒ¼å†…å®¹: {error_info[:200]}
-- ğŸ’¡ æ¨å¥¨: æ‰‹å‹•ã§BROADCASTãƒ’ãƒ³ãƒˆã®é…ç½®ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„

{original_query}"""
        
        print("âœ… Corrected query validation completed")
        return final_query
        
    except Exception as e:
        print(f"âš ï¸ Error in post-correction validation: {str(e)}")
        print("ğŸ”„ Using original query for safety")
        return f"""-- âŒ ã‚¨ãƒ©ãƒ¼ä¿®æ­£æ¤œè¨¼ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã€å…ƒã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨
-- ğŸ“‹ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}
-- ğŸ“‹ å…ƒã®ã‚¨ãƒ©ãƒ¼: {error_info[:200]}

{original_query}"""


def fallback_performance_evaluation(original_explain: str, optimized_explain: str) -> Dict[str, Any]:
    """
    EXPLAIN COSTå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
    EXPLAINçµæœã®ãƒ—ãƒ©ãƒ³è¤‡é›‘åº¦ã¨Photonåˆ©ç”¨åº¦ã§ç°¡æ˜“æ¯”è¼ƒ
    """
    
    try:
        import re
        
        # ãƒ—ãƒ©ãƒ³è¤‡é›‘åº¦ã®è©•ä¾¡
        def analyze_plan_complexity(explain_text):
            metrics = {
                'join_count': 0,
                'scan_count': 0,
                'exchange_count': 0,
                'photon_ops': 0,
                'plan_depth': 0,
                'total_operations': 0
            }
            
            # JOINæ“ä½œã‚«ã‚¦ãƒ³ãƒˆ
            metrics['join_count'] = len(re.findall(r'\bJoin\b|\bBroadcastHashJoin\b|\bSortMergeJoin\b', explain_text, re.IGNORECASE))
            
            # SCANæ“ä½œã‚«ã‚¦ãƒ³ãƒˆ
            metrics['scan_count'] = len(re.findall(r'\bScan\b|\bFileScan\b|\bTableScan\b', explain_text, re.IGNORECASE))
            
            # Exchangeæ“ä½œã‚«ã‚¦ãƒ³ãƒˆï¼ˆShuffleï¼‰
            metrics['exchange_count'] = len(re.findall(r'\bExchange\b|\bShuffle\b', explain_text, re.IGNORECASE))
            
            # Photonæ“ä½œã‚«ã‚¦ãƒ³ãƒˆ
            metrics['photon_ops'] = len(re.findall(r'\bPhoton\w*\b', explain_text, re.IGNORECASE))
            
            # ãƒ—ãƒ©ãƒ³æ·±åº¦ã®æ¨å®šï¼ˆã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆæ•°ã®æœ€å¤§å€¤ï¼‰
            lines = explain_text.split('\n')
            max_indent = 0
            for line in lines:
                if line.strip():
                    indent_level = (len(line) - len(line.lstrip(' +'))) // 2
                    max_indent = max(max_indent, indent_level)
            metrics['plan_depth'] = max_indent
            
            # ç·æ“ä½œæ•°
            metrics['total_operations'] = metrics['join_count'] + metrics['scan_count'] + metrics['exchange_count']
            
            return metrics
        
        original_metrics = analyze_plan_complexity(original_explain)
        optimized_metrics = analyze_plan_complexity(optimized_explain)
        
        # æ”¹å–„ãƒã‚¤ãƒ³ãƒˆã®è©•ä¾¡
        improvements = []
        concerns = []
        
        # JOINåŠ¹ç‡åŒ–
        if optimized_metrics['join_count'] < original_metrics['join_count']:
            improvements.append(f"JOINåŠ¹ç‡åŒ–: {original_metrics['join_count']} â†’ {optimized_metrics['join_count']}æ“ä½œ")
        elif optimized_metrics['join_count'] > original_metrics['join_count']:
            concerns.append(f"JOINæ“ä½œå¢—åŠ : {original_metrics['join_count']} â†’ {optimized_metrics['join_count']}æ“ä½œ")
        
        # Photonæ´»ç”¨åº¦
        if optimized_metrics['photon_ops'] > original_metrics['photon_ops']:
            improvements.append(f"Photonæ´»ç”¨æ‹¡å¤§: {original_metrics['photon_ops']} â†’ {optimized_metrics['photon_ops']}æ“ä½œ")
        elif optimized_metrics['photon_ops'] < original_metrics['photon_ops']:
            concerns.append(f"Photonæ´»ç”¨æ¸›å°‘: {original_metrics['photon_ops']} â†’ {optimized_metrics['photon_ops']}æ“ä½œ")
        
        # Exchange/ShuffleåŠ¹ç‡åŒ–
        if optimized_metrics['exchange_count'] < original_metrics['exchange_count']:
            improvements.append(f"Shuffleå‰Šæ¸›: {original_metrics['exchange_count']} â†’ {optimized_metrics['exchange_count']}æ“ä½œ")
        elif optimized_metrics['exchange_count'] > original_metrics['exchange_count']:
            concerns.append(f"Shuffleå¢—åŠ : {original_metrics['exchange_count']} â†’ {optimized_metrics['exchange_count']}æ“ä½œ")
        
        # ãƒ—ãƒ©ãƒ³è¤‡é›‘åº¦
        if optimized_metrics['plan_depth'] < original_metrics['plan_depth']:
            improvements.append(f"ãƒ—ãƒ©ãƒ³ç°¡ç´ åŒ–: æ·±åº¦{original_metrics['plan_depth']} â†’ {optimized_metrics['plan_depth']}")
        elif optimized_metrics['plan_depth'] > original_metrics['plan_depth']:
            concerns.append(f"ãƒ—ãƒ©ãƒ³è¤‡é›‘åŒ–: æ·±åº¦{original_metrics['plan_depth']} â†’ {optimized_metrics['plan_depth']}")
        
        # ç·åˆè©•ä¾¡
        improvement_score = len(improvements)
        concern_score = len(concerns)
        
        if improvement_score > concern_score:
            overall_status = "improvement_likely"
            recommendation = "use_optimized"
            summary = "âœ… å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æã«ã‚ˆã‚Šãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®å¯èƒ½æ€§ãŒé«˜ã„"
        elif concern_score > improvement_score:
            overall_status = "degradation_possible"
            recommendation = "use_original"
            summary = "âš ï¸ å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æã«ã‚ˆã‚Šãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ‚ªåŒ–ã®å¯èƒ½æ€§ã‚ã‚Š"
        else:
            overall_status = "neutral"
            recommendation = "use_optimized"
            summary = "â– å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æã§ã¯å¤§ããªå¤‰åŒ–ãªã—ï¼ˆæœ€é©åŒ–ã‚¯ã‚¨ãƒªã‚’æ¨å¥¨ï¼‰"
        
        return {
            'evaluation_type': 'fallback_plan_analysis',
            'original_metrics': original_metrics,
            'optimized_metrics': optimized_metrics,
            'improvements': improvements,
            'concerns': concerns,
            'overall_status': overall_status,
            'recommendation': recommendation,
            'summary': summary,
            'confidence': 'medium',
            'details': improvements + concerns if improvements or concerns else ["å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã«å¤§ããªå¤‰åŒ–ãªã—"]
        }
        
    except Exception as e:
        return {
            'evaluation_type': 'fallback_error',
            'error': str(e),
            'overall_status': 'unknown',
            'recommendation': 'use_optimized',
            'summary': f"âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼: {str(e)}ï¼ˆæœ€é©åŒ–ã‚¯ã‚¨ãƒªã‚’æ¨å¥¨ï¼‰",
            'confidence': 'low',
            'details': [f"è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}", "ä¿å®ˆçš„ã«æœ€é©åŒ–ã‚¯ã‚¨ãƒªã‚’æ¨å¥¨"]
        }


def generate_fallback_performance_section(fallback_evaluation: Dict[str, Any], language: str = 'ja') -> str:
    """
    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ã®ãƒ¬ãƒãƒ¼ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ
    """
    
    if not fallback_evaluation:
        return ""
    
    if language == 'ja':
        section = f"""

### ğŸ” 5. ç°¡æ˜“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡çµæœï¼ˆEXPLAIN COSTä»£æ›¿ï¼‰

**ğŸ“Š è©•ä¾¡çµæœ**: {fallback_evaluation['summary']}

#### ğŸ¯ å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æã«ã‚ˆã‚‹è©•ä¾¡

**ä¿¡é ¼åº¦**: {fallback_evaluation['confidence'].upper()}ï¼ˆEXPLAINçµæœãƒ™ãƒ¼ã‚¹ï¼‰

**æ¨å¥¨**: {'**æœ€é©åŒ–ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨**' if fallback_evaluation['recommendation'] == 'use_optimized' else '**å…ƒã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨**'}

#### ğŸ“‹ æ¤œå‡ºã•ã‚ŒãŸå¤‰åŒ–

"""
        
        if fallback_evaluation.get('details'):
            for detail in fallback_evaluation['details']:
                section += f"- {detail}\n"
        else:
            section += "- å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã«å¤§ããªå¤‰åŒ–ãªã—\n"
        
        if fallback_evaluation.get('original_metrics') and fallback_evaluation.get('optimized_metrics'):
            orig = fallback_evaluation['original_metrics']
            opt = fallback_evaluation['optimized_metrics']
            
            section += f"""

#### ğŸ“Š ãƒ—ãƒ©ãƒ³è¤‡é›‘åº¦æ¯”è¼ƒ

| é …ç›® | å…ƒã®ã‚¯ã‚¨ãƒª | æœ€é©åŒ–ã‚¯ã‚¨ãƒª | å¤‰åŒ– |
|------|------------|-------------|------|
| JOINæ“ä½œæ•° | {orig['join_count']} | {opt['join_count']} | {'âœ…æ”¹å–„' if opt['join_count'] < orig['join_count'] else 'âŒå¢—åŠ ' if opt['join_count'] > orig['join_count'] else 'â–åŒç­‰'} |
| Photonæ“ä½œæ•° | {orig['photon_ops']} | {opt['photon_ops']} | {'âœ…æ”¹å–„' if opt['photon_ops'] > orig['photon_ops'] else 'âŒæ¸›å°‘' if opt['photon_ops'] < orig['photon_ops'] else 'â–åŒç­‰'} |
| Shuffleæ“ä½œæ•° | {orig['exchange_count']} | {opt['exchange_count']} | {'âœ…æ”¹å–„' if opt['exchange_count'] < orig['exchange_count'] else 'âŒå¢—åŠ ' if opt['exchange_count'] > orig['exchange_count'] else 'â–åŒç­‰'} |
| ãƒ—ãƒ©ãƒ³æ·±åº¦ | {orig['plan_depth']} | {opt['plan_depth']} | {'âœ…æ”¹å–„' if opt['plan_depth'] < orig['plan_depth'] else 'âŒå¢—åŠ ' if opt['plan_depth'] > orig['plan_depth'] else 'â–åŒç­‰'} |

"""
        
        section += f"""

#### âš ï¸ è©•ä¾¡ã®åˆ¶é™äº‹é …

- **EXPLAIN COSTæœªå–å¾—**: æ­£ç¢ºãªã‚³ã‚¹ãƒˆãƒ»ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¯”è¼ƒä¸å¯
- **å®Ÿè¡Œçµ±è¨ˆä¸æ˜**: å®Ÿéš›ã®å®Ÿè¡Œæ™‚é–“ã‚„ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã¯ä¸æ˜
- **æ¨å®šãƒ™ãƒ¼ã‚¹**: å®Ÿè¡Œãƒ—ãƒ©ãƒ³æ§‹é€ ã‹ã‚‰ã®æ¨å®šè©•ä¾¡ã®ã¿
- **æ¨å¥¨**: å¯èƒ½ã§ã‚ã‚Œã°å®Ÿéš›ã®å®Ÿè¡Œãƒ†ã‚¹ãƒˆã§ç¢ºèªã™ã‚‹ã“ã¨ã‚’æ¨å¥¨

ğŸ’¡ **ã‚ˆã‚Šæ­£ç¢ºãªè©•ä¾¡ã®ãŸã‚**: AMBIGUOUS_REFERENCEç­‰ã®ã‚¨ãƒ©ãƒ¼ã‚’è§£æ±ºã—ã¦EXPLAIN COSTã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’æ¨å¥¨
"""
        
    return section


def fix_common_ambiguous_references(sql_query: str) -> str:
    """
    ã€å»ƒæ­¢ã€‘æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹ä¿®æ­£ã¯å»ƒæ­¢ - LLMã«ã‚ˆã‚‹é«˜åº¦ãªä¿®æ­£ã«å®Œå…¨ä¾å­˜
    """
    print("ğŸš« Regex-based pre-correction discontinued: Relying on advanced LLM-based correction")
    return sql_query


def fix_incomplete_sql_syntax(sql_query: str) -> str:
    """
    ä¸å®Œå…¨ãªSQLæ§‹æ–‡ã®æ¤œå‡ºã¨ä¿®æ­£
    """
    import re
    
    # åŸºæœ¬çš„ãªSQLã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    has_select = bool(re.search(r'\bSELECT\b', sql_query, re.IGNORECASE))
    has_from = bool(re.search(r'\bFROM\b', sql_query, re.IGNORECASE))
    
    # SELECTãŒãªã„å ´åˆã¯åŸºæœ¬çš„ãªSQLã§ã¯ãªã„å¯èƒ½æ€§ãŒé«˜ã„
    if not has_select:
        return sql_query
    
    # FROMãŒãªã„å ´åˆã¯ä¸å®Œå…¨ãªSQLã®å¯èƒ½æ€§
    if not has_from:
        # ä¸å®Œå…¨ãªSQLã®å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã§è­¦å‘Šã‚’è¿½åŠ 
        sql_query = f"-- âš ï¸ ä¸å®Œå…¨ãªSQLæ§‹æ–‡ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æ‰‹å‹•ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n{sql_query}"
    
    return sql_query

def remove_sql_placeholders(sql_query: str) -> str:
    """
    ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚„çœç•¥è¨˜å·ã®é™¤å»ï¼ˆSQLãƒ’ãƒ³ãƒˆã¯ä¿æŒï¼‰
    """
    import re
    
    # ä¸€èˆ¬çš„ãªãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆSQLãƒ’ãƒ³ãƒˆã¯é™¤å¤–ï¼‰
    placeholders = [
        r'\.\.\.',  # çœç•¥è¨˜å·
        r'\[çœç•¥\]',  # çœç•¥è¡¨è¨˜
        r'\[ã‚«ãƒ©ãƒ å\]',  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
        r'\[ãƒ†ãƒ¼ãƒ–ãƒ«å\]',  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
        r'column1, column2, \.\.\.',  # ã‚«ãƒ©ãƒ çœç•¥
        r'-- \.\.\.',  # ã‚³ãƒ¡ãƒ³ãƒˆå†…ã®çœç•¥
        r'column1, column2, \.\.\.',  # ã‚«ãƒ©ãƒ çœç•¥ãƒ‘ã‚¿ãƒ¼ãƒ³
        r', \.\.\.',  # æœ«å°¾ã®çœç•¥è¨˜å·
        r'å®Œå…¨ãªSQL - ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ .*?ã‚’çœç•¥ãªã—ã§è¨˜è¿°',  # æŒ‡ç¤ºæ–‡ã®é™¤å»
        r'\[å®Œå…¨ãªSQL.*?\]',  # å®Œå…¨ãªSQLæŒ‡ç¤ºã®é™¤å»
    ]
    
    for pattern in placeholders:
        sql_query = re.sub(pattern, '', sql_query, flags=re.IGNORECASE)
    
    # SQLãƒ’ãƒ³ãƒˆä»¥å¤–ã®è¤‡æ•°è¡Œã‚³ãƒ¡ãƒ³ãƒˆã‚’é™¤å»ï¼ˆãƒ’ãƒ³ãƒˆã¯ä¿æŒï¼‰
    # /*+ ... */ å½¢å¼ã®ãƒ’ãƒ³ãƒˆã¯ä¿æŒã—ã€ãã®ä»–ã® /* ... */ ã‚³ãƒ¡ãƒ³ãƒˆã®ã¿å‰Šé™¤
    sql_query = re.sub(r'/\*(?!\+).*?\*/', '', sql_query, flags=re.DOTALL)
    
    # ä¸å®Œå…¨ãªSQLæŒ‡ç¤ºã‚³ãƒ¡ãƒ³ãƒˆã‚’é™¤å»
    instruction_comments = [
        r'-- ğŸš¨ é‡è¦:.*',
        r'-- ä¾‹:.*',
        r'-- è¤‡æ•°ãƒ’ãƒ³ãƒˆä¾‹.*',
        r'-- ç„¡åŠ¹ãªä¾‹:.*',
        r'-- ğŸš¨ REPARTITIONãƒ’ãƒ³ãƒˆ.*',
    ]
    
    for pattern in instruction_comments:
        sql_query = re.sub(pattern, '', sql_query, flags=re.IGNORECASE)
    
    # ç©ºè¡Œã‚’æ­£è¦åŒ–
    sql_query = re.sub(r'\n\s*\n\s*\n+', '\n\n', sql_query)
    
    return sql_query.strip()

def fix_basic_syntax_errors(sql_query: str) -> str:
    """
    åŸºæœ¬çš„ãªæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£
    """
    import re
    
    # 1. NULLãƒªãƒ†ãƒ©ãƒ«ã®å‹ã‚­ãƒ£ã‚¹ãƒˆä¿®æ­£ - ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼ˆå†—é•·CASTç”Ÿæˆã®åŸå› ï¼‰
    # SELECT null as col01 â†’ SELECT cast(null as String) as col01
    # null_literal_pattern = r'\bnull\s+as\s+(\w+)'
    # sql_query = re.sub(null_literal_pattern, r'cast(null as String) as \1', sql_query, flags=re.IGNORECASE)
    
    # 2. é€£ç¶šã™ã‚‹ã‚«ãƒ³ãƒã®ä¿®æ­£
    sql_query = re.sub(r',\s*,', ',', sql_query)
    
    # 3. ä¸æ­£ãªç©ºç™½ã®ä¿®æ­£ï¼ˆè¡Œå†…ã®é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«ï¼‰
    sql_query = re.sub(r'[ \t]+', ' ', sql_query)
    
    # 4. è¡Œæœ«ã®ä¸è¦ãªæ–‡å­—å‰Šé™¤
    sql_query = re.sub(r'[,;]\s*$', '', sql_query.strip())
    
    # 5. ä¸å®Œå…¨ãªSELECTæ–‡ã®ä¿®æ­£
    # SELECTã®å¾Œã«ç›´æ¥FROMãŒæ¥ã‚‹å ´åˆã‚’ä¿®æ­£
    sql_query = re.sub(r'SELECT\s+FROM', 'SELECT *\nFROM', sql_query, flags=re.IGNORECASE)
    
    # 6. ä¸å®Œå…¨ãªJOINå¥ã®ä¿®æ­£
    # JOINã®å¾Œã«ONãŒæ¥ãªã„å ´åˆã®åŸºæœ¬çš„ãªä¿®æ­£
    lines = sql_query.split('\n')
    fixed_lines = []
    
    for line in lines:
        line = line.strip()
        if line:
            # JOINã®å¾Œã«ONãŒãªã„å ´åˆã®è­¦å‘Šã‚³ãƒ¡ãƒ³ãƒˆè¿½åŠ 
            if re.search(r'\bJOIN\s+\w+\s*$', line, re.IGNORECASE):
                fixed_lines.append(line)
                fixed_lines.append('  -- âš ï¸ JOINæ¡ä»¶ï¼ˆONå¥ï¼‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„')
            else:
                fixed_lines.append(line)
    
    sql_query = '\n'.join(fixed_lines)
    
    # 7. åŸºæœ¬çš„ãªæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
    sql_query = add_syntax_warnings(sql_query)
    
    return sql_query

def add_syntax_warnings(sql_query: str) -> str:
    """
    åŸºæœ¬çš„ãªæ§‹æ–‡ãƒã‚§ãƒƒã‚¯ã¨è­¦å‘Šã®è¿½åŠ 
    """
    import re
    
    warnings = []
    
    # åŸºæœ¬çš„ãªSQLã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    has_select = bool(re.search(r'\bSELECT\b', sql_query, re.IGNORECASE))
    has_from = bool(re.search(r'\bFROM\b', sql_query, re.IGNORECASE))
    
    # JOINãŒã‚ã‚‹ãŒONãŒãªã„å ´åˆ
    joins = re.findall(r'\b(LEFT|RIGHT|INNER|OUTER)?\s*JOIN\s+\w+', sql_query, re.IGNORECASE)
    ons = re.findall(r'\bON\b', sql_query, re.IGNORECASE)
    
    if len(joins) > len(ons):
        warnings.append('-- âš ï¸ JOINå¥ã®æ•°ã«å¯¾ã—ã¦ONå¥ãŒä¸è¶³ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™')
    
    # WITHå¥ãŒã‚ã‚‹å ´åˆã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯
    if re.search(r'\bWITH\s+\w+\s+AS\s*\(', sql_query, re.IGNORECASE):
        if not re.search(r'\)\s*SELECT\b', sql_query, re.IGNORECASE):
            warnings.append('-- âš ï¸ WITHå¥ã®å¾Œã®ãƒ¡ã‚¤ãƒ³SELECTæ–‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„')
    
    # è­¦å‘ŠãŒã‚ã‚‹å ´åˆã¯å…ˆé ­ã«è¿½åŠ 
    if warnings:
        sql_query = '\n'.join(warnings) + '\n\n' + sql_query
    
    return sql_query

def extract_broadcast_tables_from_sql(sql_query: str) -> list:
    """
    SQLã‚¯ã‚¨ãƒªã‹ã‚‰BROADCASTã•ã‚Œã‚‹ã¹ããƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
    """
    import re
    
    # å‰Šé™¤ã•ã‚ŒãŸBROADCASTãƒ’ãƒ³ãƒˆã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
    broadcast_pattern = r'BROADCAST\(([^)]+)\)'
    matches = re.findall(broadcast_pattern, sql_query, re.IGNORECASE)
    
    tables = []
    for match in matches:
        # ã‚«ãƒ³ãƒã§åŒºåˆ‡ã‚‰ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«åã‚’åˆ†å‰²
        table_names = [name.strip() for name in match.split(',')]
        tables.extend(table_names)
    
    return list(set(tables))  # é‡è¤‡ã‚’é™¤å»

def validate_final_sql_syntax(sql_query: str) -> bool:
    """
    æœ€çµ‚çš„ãªSQLæ§‹æ–‡ãƒã‚§ãƒƒã‚¯ï¼ˆä¿å­˜å‰ã®ç¢ºèªï¼‰
    
    Returns:
        bool: æ§‹æ–‡ãŒæ­£ã—ã„ã¨åˆ¤å®šã•ã‚ŒãŸå ´åˆTrueã€å•é¡ŒãŒã‚ã‚‹å ´åˆFalse
    """
    import re
    
    if not sql_query or not sql_query.strip():
        return False
    
    # åŸºæœ¬çš„ãªSQLã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    has_select = bool(re.search(r'\bSELECT\b', sql_query, re.IGNORECASE))
    
    # SELECTãŒãªã„å ´åˆã¯ä¸æ­£
    if not has_select:
        return False
    
    # æ˜ã‚‰ã‹ã«ä¸å®Œå…¨ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒƒã‚¯
    incomplete_patterns = [
        r'\.\.\.',  # çœç•¥è¨˜å·
        r'\[çœç•¥\]',  # çœç•¥è¡¨è¨˜
        r'\[ã‚«ãƒ©ãƒ å\]',  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
        r'\[ãƒ†ãƒ¼ãƒ–ãƒ«å\]',  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
        r'column1, column2, \.\.\.',  # ã‚«ãƒ©ãƒ çœç•¥
        r'å®Œå…¨ãªSQL.*?ã‚’.*?è¨˜è¿°',  # æŒ‡ç¤ºæ–‡
    ]
    
    for pattern in incomplete_patterns:
        if re.search(pattern, sql_query, re.IGNORECASE):
            return False
    
    # BROADCASTãƒ’ãƒ³ãƒˆé…ç½®ã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯
    broadcast_hints = re.findall(r'/\*\+\s*BROADCAST\([^)]+\)\s*\*/', sql_query, re.IGNORECASE)
    if broadcast_hints:
        # BROADCASTãƒ’ãƒ³ãƒˆãŒã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        subquery_broadcast = re.search(r'JOIN\s*\(\s*SELECT\s*/\*\+\s*BROADCAST', sql_query, re.IGNORECASE)
        if subquery_broadcast:
            return False
    
    # åŸºæœ¬çš„ãªæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
    # é€£ç¶šã™ã‚‹ã‚«ãƒ³ãƒ
    if re.search(r',\s*,', sql_query):
        return False
    
    # ä¸æ­£ãªç©ºç™½ãƒ‘ã‚¿ãƒ¼ãƒ³
    if re.search(r'\s{5,}', sql_query):  # 5å€‹ä»¥ä¸Šã®é€£ç¶šã™ã‚‹ç©ºç™½
        return False
    
    return True

def save_optimized_sql_files(original_query: str, optimized_result: str, metrics: Dict[str, Any], analysis_result: str = "", llm_response: str = "", performance_comparison: Dict[str, Any] = None, best_attempt_number: int = None) -> Dict[str, str]:
    """
    æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œå¯èƒ½ãªå½¢ã§ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    
    ç‰¹å¾´:
    - SQLãƒ•ã‚¡ã‚¤ãƒ«ã®æœ«å°¾ã«è‡ªå‹•ã§ã‚»ãƒŸã‚³ãƒ­ãƒ³(;)ã‚’ä»˜ä¸
    - ãã®ã¾ã¾Databricks Notebookã§å®Ÿè¡Œå¯èƒ½
    - %sql ãƒã‚¸ãƒƒã‚¯ã‚³ãƒãƒ³ãƒ‰ã§ã‚‚ç›´æ¥å®Ÿè¡Œå¯èƒ½
    - LLMã«ã‚ˆã‚‹ãƒ¬ãƒãƒ¼ãƒˆæ¨æ•²ã§èª­ã¿ã‚„ã™ã„æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    """
    
    import re
    from datetime import datetime
    
    # thinking_enabled: Trueã®å ´åˆã«optimized_resultãŒãƒªã‚¹ãƒˆã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚å¯¾å¿œ
    optimized_result_for_file = optimized_result
    optimized_result_main_content = optimized_result
    
    if isinstance(optimized_result, list):
        # Convert to human-readable format for file saving
        optimized_result_for_file = format_thinking_response(optimized_result)
        # SQLæŠ½å‡ºç”¨ã¯ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã¿ã‚’ä½¿ç”¨
        optimized_result_main_content = extract_main_content_from_thinking_response(optimized_result)
    
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    query_id = metrics.get('query_info', {}).get('query_id', 'unknown')
    
    # ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã¯é™¤å¤–ï¼ˆä¸è¦ï¼‰
    original_filename = None
    
    # æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®æŠ½å‡ºã¨ä¿å­˜
    optimized_filename = f"output_optimized_query_{timestamp}.sql"
    
    # æœ€é©åŒ–çµæœã‹ã‚‰SQLã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰æŠ½å‡ºï¼‰ - æ”¹å–„ç‰ˆ
    sql_pattern = r'```sql\s*(.*?)\s*```'
    sql_matches = re.findall(sql_pattern, optimized_result_main_content, re.DOTALL | re.IGNORECASE)
    
    optimized_sql = ""
    if sql_matches:
        # æœ€ã‚‚é•·ã„SQLãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½¿ç”¨ï¼ˆå®Œå…¨æ€§ã‚’å„ªå…ˆï¼‰
        optimized_sql = max(sql_matches, key=len).strip()
    else:
        # SQLãƒ–ãƒ­ãƒƒã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€SQLé–¢é€£ã®è¡Œã‚’æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰
        lines = optimized_result_main_content.split('\n')
        sql_lines = []
        in_sql_section = False
        
        for line in lines:
            line_stripped = line.strip()
            
            # SQLã®é–‹å§‹ã‚’æ¤œå‡º
            if any(keyword in line.upper() for keyword in ['SELECT', 'FROM', 'WHERE', 'WITH', 'CREATE', 'INSERT', 'UPDATE', 'DELETE']):
                in_sql_section = True
            
            if in_sql_section:
                # SQLã®çµ‚äº†ã‚’æ¤œå‡ºï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚„ãƒ¬ãƒãƒ¼ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
                if (line_stripped.startswith('#') or 
                    line_stripped.startswith('*') or 
                    line_stripped.startswith('##') or
                    line_stripped.startswith('**') or
                    line_stripped.startswith('---') or
                    line_stripped.startswith('===') or
                    'æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ' in line_stripped or
                    'æœŸå¾…åŠ¹æœ' in line_stripped or
                    'BROADCASTé©ç”¨æ ¹æ‹ ' in line_stripped):
                    in_sql_section = False
                else:
                    # ç©ºè¡Œã‚„æœ‰åŠ¹ãªSQLè¡Œã‚’è¿½åŠ 
                    sql_lines.append(line)
        
        optimized_sql = '\n'.join(sql_lines).strip()
    
    # SQLæ§‹æ–‡ã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯ï¼ˆå®Œå…¨æ€§ç¢ºèªï¼‰
    if optimized_sql:
        optimized_sql = validate_and_fix_sql_syntax(optimized_sql)
    
    # æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰
    try:
        with open(optimized_filename, 'w', encoding='utf-8') as f:
            f.write(f"-- æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒª\n")
            f.write(f"-- å…ƒã‚¯ã‚¨ãƒªID: {query_id}\n")
            f.write(f"-- æœ€é©åŒ–æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"-- ãƒ•ã‚¡ã‚¤ãƒ«: {optimized_filename}\n\n")
            
            
            # ğŸ¯ CATALOG/DATABASEè¨­å®šã®è‡ªå‹•è¿½åŠ 
            catalog_name = globals().get("CATALOG", "tpcds")
            database_name = globals().get("DATABASE", "tpcds_sf1000_delta_lc")
            
            f.write(f"-- ğŸ—‚ï¸ ã‚«ã‚¿ãƒ­ã‚°ãƒ»ã‚¹ã‚­ãƒ¼ãƒè¨­å®šï¼ˆè‡ªå‹•è¿½åŠ ï¼‰\n")
            f.write(f"USE CATALOG {catalog_name};\n")
            f.write(f"USE SCHEMA {database_name};\n\n")
                
            if optimized_sql:
                # SQLã®æœ«å°¾ã«ã‚»ãƒŸã‚³ãƒ­ãƒ³ã‚’ç¢ºå®Ÿã«è¿½åŠ 
                optimized_sql_clean = optimized_sql.strip()
                if optimized_sql_clean and not optimized_sql_clean.endswith(';'):
                    optimized_sql_clean += ';'
                
                # æœ€çµ‚çš„ãªæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
                if validate_final_sql_syntax(optimized_sql_clean):
                    f.write(optimized_sql_clean)
                else:
                    f.write("-- âš ï¸ æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æ‰‹å‹•ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n")
                    f.write(f"-- å…ƒã®SQL:\n{optimized_sql_clean}\n")
                    f.write("-- ä»¥ä¸‹ã¯æœ€é©åŒ–åˆ†æã®å…¨çµæœã§ã™:\n\n")
                    f.write(f"/*\n{optimized_result_main_content}\n*/")
            else:
                f.write("-- âš ï¸ SQLã‚³ãƒ¼ãƒ‰ã®è‡ªå‹•æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ\n")
                f.write("-- ä»¥ä¸‹ã¯æœ€é©åŒ–åˆ†æã®å…¨çµæœã§ã™:\n\n")
                f.write(f"/*\n{optimized_result_main_content}\n*/")
    except Exception as e:
        print(f"âš ï¸ Error occurred during SQL file saving: {str(e)}")
        # Generate basic file on error
        with open(optimized_filename, 'w', encoding='utf-8') as f:
            f.write(f"-- âš ï¸ Error occurred during SQL file saving: {str(e)}\n")
            f.write(f"-- Optimization result:\n{optimized_result_main_content}\n")
    
    # Save analysis report file (readable report refined by LLM)
    # Generate filename based on OUTPUT_LANGUAGE setting
    language_suffix = 'en' if OUTPUT_LANGUAGE == 'en' else 'jp'
    report_filename = f"output_optimization_report_{language_suffix}_{timestamp}.md"
    
    print("ğŸ¤– Executing LLM report refinement...")
    
    # ğŸš€ Load content of actually saved SQL file and use for report
    try:
        with open(optimized_filename, 'r', encoding='utf-8') as f:
            actual_sql_content = f.read()
        
        # Use actual SQL file content for report (guaranteed to work)
        print(f"âœ… Loaded SQL file content for report generation: {optimized_filename}")
        report_data = actual_sql_content
        
    except Exception as e:
        print(f"âš ï¸ SQL file loading failed, using initial response: {str(e)}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åˆå›ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½¿ç”¨
        report_data = llm_response if llm_response else optimized_result
    
    initial_report = generate_comprehensive_optimization_report(
        query_id, report_data, metrics, analysis_result, performance_comparison, best_attempt_number
    )
    
    # LLMã§ãƒ¬ãƒãƒ¼ãƒˆã‚’æ¨æ•²ï¼ˆè©³ç´°ãªæŠ€è¡“æƒ…å ±ã‚’ä¿æŒï¼‰
    refined_report = refine_report_with_llm(initial_report, query_id)
    
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(refined_report)
    
    print(f"âœ… Report file saving completed: {report_filename}")
    
    # Output file results (independent TOP10 files removed and integrated into optimization report)
    result = {
        'optimized_file': optimized_filename,
        'report_file': report_filename
    }
    
    return result

def demonstrate_execution_plan_size_extraction():
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‹ã‚‰ã®ã‚µã‚¤ã‚ºæ¨å®šæ©Ÿèƒ½ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    print("ğŸ§ª Demo of table size estimation feature from execution plan")
    print("-" * 50)
    
    # ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    sample_profiler_data = {
        "executionPlan": {
            "physicalPlan": {
                "nodes": [
                    {
                        "nodeName": "Scan Delta orders",
                        "id": "1",
                        "metrics": {
                            "estimatedSizeInBytes": 10485760,  # 10MB
                            "numFiles": 5,
                            "numPartitions": 2
                        },
                        "output": "[order_id#123, customer_id#124, amount#125] orders",
                        "details": "Table: catalog.database.orders"
                    },
                    {
                        "nodeName": "Scan Delta customers",
                        "id": "2", 
                        "metrics": {
                            "estimatedSizeInBytes": 52428800,  # 50MB
                            "numFiles": 10,
                            "numPartitions": 4
                        },
                        "output": "[customer_id#126, name#127, region#128] customers"
                    }
                ]
            }
        }
    }
    
    print("ğŸ“Š Sample execution plan:")
    print("  â€¢ orders table: estimatedSizeInBytes = 10,485,760 (10MB)")
    print("  â€¢ customers table: estimatedSizeInBytes = 52,428,800 (50MB)")
    print("")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæ¨å®šã®å®Ÿè¡Œ
    table_size_estimates = extract_table_size_estimates_from_plan(sample_profiler_data)
    
    print("ğŸ” Extracted table size estimations:")
    if table_size_estimates:
        for table_name, size_info in table_size_estimates.items():
            print(f"  ğŸ“‹ {table_name}:")
            print(f"    - Size: {size_info['estimated_size_mb']:.1f}MB")
            print(f"    - Confidence: {size_info['confidence']}")
            print(f"    - Source: {size_info['source']}")
            if 'num_files' in size_info:
                print(f"    - File count: {size_info['num_files']}")
            if 'num_partitions' in size_info:
                print(f"    - Partition count: {size_info['num_partitions']}")
            print("")
    else:
        print("  âš ï¸ Table size estimation information could not be extracted")
    
    print("ğŸ’¡ Impact on BROADCAST analysis:")
    if table_size_estimates:
        for table_name, size_info in table_size_estimates.items():
            size_mb = size_info['estimated_size_mb']
            if size_mb <= 30:
                print(f"  âœ… {table_name}: {size_mb:.1f}MB â‰¤ 30MB â†’ BROADCAST recommended")
            else:
                print(f"  âŒ {table_name}: {size_mb:.1f}MB > 30MB â†’ BROADCAST not recommended")
    
    print("")
    print("ğŸ¯ Comparison with conventional estimation methods:")
    print("  ğŸ“ˆ Conventional: Metrics-based indirect estimation (estimation accuracy: medium)")
    print("  âŒ New feature: Utilizing estimatedSizeInBytes from execution plan (disabled due to unavailability)")
    print("  â„¹ï¸ Current: Adopting conservative estimation with 3.0x compression ratio")
    
    return {}

print("âœ… Function definition completed: SQL optimization related functions (execution plan size estimation support)")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸš€ Original Query Extraction
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Extraction of original query from profiler data
# MAGIC - Detailed display of extracted query (up to 64KB)
# MAGIC - Fallback processing (sample query configuration)

# COMMAND ----------

# ğŸš€ SQLã‚¯ã‚¨ãƒªæœ€é©åŒ–ã®å®Ÿè¡Œ
print("\n" + "ğŸš€" * 20)
print("ğŸ”§ ã€SQL Query Optimization Executionã€‘")
print("ğŸš€" * 20)

# 1. ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®æŠ½å‡º
print("\nğŸ“‹ Step 1: Extract Original Query")
print("-" * 40)

original_query = extract_original_query_from_profiler_data(profiler_data)

if original_query:
    print(f"âœ… Original query extracted ({len(original_query)} characters)")
    print(f"ğŸ” Query preview:")
    # 64KB (65536æ–‡å­—) ã¾ã§è¡¨ç¤º
    max_display_chars = 65536
    if len(original_query) > max_display_chars:
        preview = original_query[:max_display_chars] + f"\n... (æ®‹ã‚Š {len(original_query) - max_display_chars} æ–‡å­—ã¯çœç•¥)"
    else:
        preview = original_query
    print(f"   {preview}")
else:
    print("âš ï¸ Original query not found")
    print("   Please set the query manually")
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã‚’è¨­å®š
    original_query = """
    -- ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªï¼ˆå®Ÿéš›ã®ã‚¯ã‚¨ãƒªã«ç½®ãæ›ãˆã¦ãã ã•ã„ï¼‰
    SELECT 
        customer_id,
        SUM(order_amount) as total_amount,
        COUNT(*) as order_count
    FROM orders 
    WHERE order_date >= '2023-01-01'
    GROUP BY customer_id
    ORDER BY total_amount DESC
    LIMIT 100
    """
    print(f"ğŸ“ Sample query has been set")

# ğŸ“ ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
print("\nğŸ“ Saving original query to file")
print("-" * 40)

from datetime import datetime

# ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
original_query_filename = f"output_original_query_{timestamp}.sql"

try:
    # ã‚«ã‚¿ãƒ­ã‚°ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®šã®å–å¾—
    catalog_name = globals().get('CATALOG', 'tpcds')
    database_name = globals().get('DATABASE', 'tpcds_sf1000_delta_lc')
    
    with open(original_query_filename, 'w', encoding='utf-8') as f:
        f.write(f"-- ğŸ“‹ ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æŠ½å‡ºï¼‰\n")
        f.write(f"-- æŠ½å‡ºæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"-- ãƒ•ã‚¡ã‚¤ãƒ«: {original_query_filename}\n")
        f.write(f"-- ã‚¯ã‚¨ãƒªæ–‡å­—æ•°: {len(original_query):,}\n\n")
        
        # ã‚«ã‚¿ãƒ­ã‚°ãƒ»ã‚¹ã‚­ãƒ¼ãƒè¨­å®šã®è¿½åŠ 
        f.write(f"-- ğŸ—‚ï¸ ã‚«ã‚¿ãƒ­ã‚°ãƒ»ã‚¹ã‚­ãƒ¼ãƒè¨­å®šï¼ˆè‡ªå‹•è¿½åŠ ï¼‰\n")
        f.write(f"USE CATALOG {catalog_name};\n")
        f.write(f"USE SCHEMA {database_name};\n\n")
        
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®æ›¸ãè¾¼ã¿
        f.write(f"-- ğŸ” ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒª\n")
        f.write(original_query)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æœ«å°¾ã«æ”¹è¡Œã‚’è¿½åŠ 
        if not original_query.endswith('\n'):
            f.write('\n')
    
    print(f"âœ… Original query saved: {original_query_filename}")
    print(f"ğŸ“Š Saved query character count: {len(original_query):,}")
    print(f"ğŸ’¾ File path: ./{original_query_filename}")
    print("ğŸ“Œ This file is retained as final output regardless of DEBUG_ENABLED setting")
    
except Exception as e:
    print(f"âŒ Failed to save original query file: {str(e)}")
    print("âš ï¸ Processing continues, but original query file was not created")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ” SQL Optimization Execution
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Retrieve original query extracted in Cell 43
# MAGIC - Generate and execute EXPLAIN statements in Databricks
# MAGIC - Output execution plan details to files
# MAGIC - Error handling and result verification

# COMMAND ----------

def extract_select_from_ctas(query: str) -> str:
    """
    CREATE TABLE AS SELECT (CTAS) ã‚¯ã‚¨ãƒªã‹ã‚‰ASä»¥é™ã®éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
    
    å¯¾å¿œãƒ‘ã‚¿ãƒ¼ãƒ³:
    - CREATE TABLE ... AS SELECT ...
    - CREATE OR REPLACE TABLE ... AS SELECT ...
    - CREATE TABLE ... AS WITH ... SELECT ...
    - AS ã®å¾Œã‚ã«æ‹¬å¼§ãŒãªã„å ´åˆ
    - è¤‡æ•°è¡Œã«ã¾ãŸãŒã‚‹å ´åˆ
    - ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ã®è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆUSINGã€PARTITIONED BYã€TBLPROPERTIESç­‰ï¼‰
    
    Args:
        query: å…ƒã®ã‚¯ã‚¨ãƒª
    
    Returns:
        str: ASä»¥é™ã®éƒ¨åˆ†ã®ã¿ã®ã‚¯ã‚¨ãƒªã€ã¾ãŸã¯CTASã§ãªã„å ´åˆã¯å…ƒã®ã‚¯ã‚¨ãƒª
    """
    import re
    
    # ã‚¯ã‚¨ãƒªã‚’æ­£è¦åŒ–ï¼ˆæ”¹è¡Œãƒ»ç©ºç™½ã‚’çµ±ä¸€ï¼‰
    normalized_query = re.sub(r'\s+', ' ', query.strip())
    
    # CTAS ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºï¼ˆåŒ…æ‹¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    # CREATE [OR REPLACE] TABLE ... AS ... ã®å½¢å¼ã‚’æ¤œå‡º
    # ASã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä½ç½®ã‚’æ­£ç¢ºã«ç‰¹å®šã™ã‚‹
    
    # CREATE [OR REPLACE] TABLEéƒ¨åˆ†ã®æ¤œå‡º
    create_patterns = [
        r'CREATE\s+OR\s+REPLACE\s+TABLE',
        r'CREATE\s+TABLE'
    ]
    
    for create_pattern in create_patterns:
        # CREATE TABLEéƒ¨åˆ†ã‚’æ¤œå‡º
        create_match = re.search(create_pattern, normalized_query, re.IGNORECASE)
        if create_match:
            # CREATE TABLEä»¥é™ã®éƒ¨åˆ†ã‚’å–å¾—
            after_create = normalized_query[create_match.end():].strip()
            
            # AS ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä½ç½®ã‚’æ¤œç´¢ï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ï¼‰
            # AS ã¯å˜èªå¢ƒç•Œã§åŒºåˆ‡ã‚‰ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚‹
            as_pattern = r'\bAS\b'
            as_match = re.search(as_pattern, after_create, re.IGNORECASE)
            
            if as_match:
                # ASä»¥é™ã®éƒ¨åˆ†ã‚’å–å¾—
                as_part = after_create[as_match.end():].strip()
                
                if as_part:
                    print(f"âœ… CTAS detected: Using part after AS for EXPLAIN statement")
                    print(f"ğŸ“Š Original query length: {len(query):,} characters")
                    print(f"ğŸ“Š Part after AS length: {len(as_part):,} characters")
                    
                    # WITHå¥ã§å§‹ã¾ã‚‹å ´åˆã‚„SELECTå¥ã§å§‹ã¾ã‚‹å ´åˆã‚’åˆ¤å®š
                    if as_part.upper().startswith('WITH'):
                        print("ğŸ“‹ Detected query starting with WITH clause")
                    elif as_part.upper().startswith('SELECT'):
                        print("ğŸ“‹ Detected query starting with SELECT clause")
                    else:
                        print("ğŸ“‹ Detected other query format")
                    
                    return as_part
    
    print("ğŸ“‹ Regular query: Use as is for EXPLAIN statement")
    return query

def generate_improved_query_for_performance_degradation(original_query: str, analysis_result: str, metrics: Dict[str, Any], degradation_analysis: Dict[str, Any], previous_optimized_query: str = "") -> str:
    """
    LLM optimization function specifically for performance degradation
    Apply specific improvement measures based on degradation cause analysis
    
    Args:
        original_query: Original query
        analysis_result: Bottleneck analysis result
        metrics: Metrics information
        degradation_analysis: Degradation cause analysis result
        previous_optimized_query: Previous optimized query
    """
    
    # æ‚ªåŒ–åˆ†æã®è©³ç´°æƒ…å ±ã‚’æŠ½å‡º
    primary_cause = degradation_analysis.get('primary_cause', 'unknown')
    cost_ratio = degradation_analysis.get('analysis_details', {}).get('cost_ratio', 1.0)
    specific_issues = degradation_analysis.get('specific_issues', [])
    fix_instructions = degradation_analysis.get('fix_instructions', [])
    confidence_level = degradation_analysis.get('confidence_level', 'low')
    
    # å‰å›ã‚¯ã‚¨ãƒªã®åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³
    previous_query_section = ""
    if previous_optimized_query:
        previous_query_section = f"""

ã€ğŸš¨ å‰å›ã®æœ€é©åŒ–ã‚¯ã‚¨ãƒªï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ‚ªåŒ–ï¼‰ã€‘
```sql
{previous_optimized_query}
```

**âŒ æ¤œå‡ºã•ã‚ŒãŸå•é¡Œç‚¹:**
- å®Ÿè¡Œã‚³ã‚¹ãƒˆæ¯”: {cost_ratio:.2f}å€ã®æ‚ªåŒ–
- ä¸»è¦åŸå› : {primary_cause}
- å…·ä½“çš„å•é¡Œ: {', '.join(specific_issues)}
"""

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ‚ªåŒ–ä¿®æ­£ã«ç‰¹åŒ–ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    performance_improvement_prompt = f"""
ã‚ãªãŸã¯Databricksã®SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®å°‚é–€å®¶ã§ã™ã€‚

å‰å›ã®æœ€é©åŒ–ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ‚ªåŒ–ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚æ‚ªåŒ–åŸå› åˆ†æã«åŸºã¥ã„ã¦ **æ ¹æœ¬çš„ãªæ”¹å–„** ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

ã€ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ‚ªåŒ–ã®è©³ç´°åˆ†æã€‘
- **æ‚ªåŒ–ç‡**: {cost_ratio:.2f}å€ï¼ˆ{(cost_ratio-1)*100:.1f}%å¢—åŠ ï¼‰
- **ä¸»è¦åŸå› **: {primary_cause}
- **ä¿¡é ¼åº¦**: {confidence_level}
- **å…·ä½“çš„å•é¡Œ**: {', '.join(specific_issues)}

ã€å…ƒã®åˆ†æå¯¾è±¡ã‚¯ã‚¨ãƒªã€‘
```sql
{original_query}
```
{previous_query_section}

ã€ğŸ”§ æ‚ªåŒ–åŸå› åˆ¥ã®å…·ä½“çš„ä¿®æ­£æŒ‡ç¤ºã€‘
{chr(10).join(f"- {instruction}" for instruction in fix_instructions)}

ã€ğŸ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®é‡è¦ãªæ–¹é‡ã€‘

1. **ğŸš¨ éå‰°æœ€é©åŒ–ã®æ˜¯æ­£**:
           - JOINé †åºã®åŠ¹ç‡åŒ–
           - åŠ¹ç‡çš„ã§ãªã„JOINé †åºã®è¦‹ç›´ã—
   - åŠ¹æœçš„ã§ãªã„ãƒ’ãƒ³ãƒˆã¯ç©æ¥µçš„ã«å‰Šé™¤

2. **âš¡ JOINåŠ¹ç‡åŒ–**:
   - JOINæ“ä½œæ•°ã®å¤§å¹…ãªå¢—åŠ ã‚’é¿ã‘ã‚‹
   - å…ƒã®JOINé †åºã‚’å°Šé‡
   - ä¸è¦ãªã‚µãƒ–ã‚¯ã‚¨ãƒªåŒ–ã«ã‚ˆã‚‹JOINé‡è¤‡ã‚’é˜²ã

3. **ğŸ¯ ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºæœ€é©åŒ–**:
   - ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ã‚’æœ€å¤§åŒ–
   - æ—©æœŸã®è¡Œæ•°å‰Šæ¸›ã‚’é‡è¦–
   - ä¸­é–“çµæœã®ã‚µã‚¤ã‚ºã‚’æœ€å°åŒ–

4. **ğŸ“Š çµ±è¨ˆæƒ…å ±ã«åŸºã¥ãåˆ¤æ–­**:
   - å°ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ<30MBï¼‰ã®ã¿BROADCASTé©ç”¨
   - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’é‡è¦–ã—ãŸJOINæˆ¦ç•¥
   - ã‚¹ãƒ”ãƒ«ç™ºç”Ÿã®æœ€å°åŒ–

ã€ğŸ”„ æ”¹å–„ã‚¯ã‚¨ãƒªç”Ÿæˆã®æŒ‡é‡ã€‘

**A. ä¿å®ˆçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆæ¨å¥¨ï¼‰:**
- å…ƒã‚¯ã‚¨ãƒªã®æ§‹é€ ã‚’æœ€å¤§é™ä¿æŒ
- ç¢ºå®Ÿã«åŠ¹æœçš„ãªæœ€é©åŒ–ã®ã¿é©ç”¨
- ãƒªã‚¹ã‚¯ã®é«˜ã„å¤‰æ›´ã¯é¿ã‘ã‚‹

**B. æ®µéšçš„æ”¹å–„:**
- æœ€ã‚‚å•é¡Œã¨ãªã£ã¦ã„ã‚‹ç®‡æ‰€ã®ã¿ä¿®æ­£
- ä¸€åº¦ã«å¤šãã®å¤‰æ›´ã‚’åŠ ãˆãªã„
- æ¸¬å®šå¯èƒ½ãªæ”¹å–„ã‚’é‡è¦–

**C. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥:**
- ä¸ç¢ºå®Ÿãªæœ€é©åŒ–ã¯å‰Šé™¤
- å…ƒã®ã‚¯ã‚¨ãƒªã«è¿‘ã„å½¢ã§ã®è»½å¾®ãªæ”¹å–„

ã€é‡è¦ãªåˆ¶ç´„ã€‘
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ‚ªåŒ–ã®ä¸»è¦åŸå› ã‚’ç¢ºå®Ÿã«è§£æ±º
- å…ƒã‚¯ã‚¨ãƒªã‚ˆã‚Šç¢ºå®Ÿã«é«˜é€Ÿãªã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
- æ©Ÿèƒ½æ€§ã‚’ä¸€åˆ‡æãªã‚ãªã„
- å®Œå…¨ã§å®Ÿè¡Œå¯èƒ½ãªSQLã®ã¿å‡ºåŠ›

ã€å‡ºåŠ›å½¢å¼ã€‘
## ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„SQL

**æ”¹å–„ã—ãŸå†…å®¹**:
- [å…·ä½“çš„ãªæ‚ªåŒ–åŸå› ã®ä¿®æ­£]
- [å‰Šé™¤/å¤‰æ›´ã—ãŸæœ€é©åŒ–è¦ç´ ]
- [æ–°ãŸã«é©ç”¨ã—ãŸæ”¹å–„ç­–]

```sql
[å®Œå…¨ãªSQL - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„æ¸ˆã¿]
```

## æ”¹å–„è©³ç´°
[æ‚ªåŒ–åŸå› ã®è§£æ±ºæ–¹æ³•ã¨æœŸå¾…ã•ã‚Œã‚‹æ€§èƒ½æ”¹å–„ã®èª¬æ˜]
"""

    # è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
    provider = LLM_CONFIG["provider"]
    
    try:
        if provider == "databricks":
            improved_result = _call_databricks_llm(performance_improvement_prompt)
        elif provider == "openai":
            improved_result = _call_openai_llm(performance_improvement_prompt)
        elif provider == "azure_openai":
            improved_result = _call_azure_openai_llm(performance_improvement_prompt)
        elif provider == "anthropic":
            improved_result = _call_anthropic_llm(performance_improvement_prompt)
        else:
            error_msg = "âš ï¸ Configured LLM provider is not recognized"
            print(f"âŒ LLM performance improvement error: {error_msg}")
            return f"LLM_ERROR: {error_msg}"
        
        # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        if isinstance(improved_result, str):
            error_indicators = [
                "APIã‚¨ãƒ©ãƒ¼:",
                "Input is too long", 
                "Bad Request",
                "âŒ",
                "âš ï¸",
                "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼:",
                "APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼:",
            ]
            
            for indicator in error_indicators:
                if indicator in improved_result:
                    print(f"âŒ Error detected in LLM performance improvement: {indicator}")
                    return f"LLM_ERROR: {improved_result}"
        
        return improved_result
        
    except Exception as e:
        error_msg = f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {str(e)}"
        print(f"âŒ {error_msg}")
        return f"LLM_ERROR: {error_msg}"


def generate_optimized_query_with_error_feedback(original_query: str, analysis_result: str, metrics: Dict[str, Any], error_info: str = "", previous_optimized_query: str = "") -> str:
    """
    Execute SQL optimization by LLM including error information
    Use prompts specialized for error correction
    
    Args:
        original_query: Original query
        analysis_result: Bottleneck analysis result
        metrics: Metrics information
        error_info: Error information
        previous_optimized_query: Initial optimized query (for hint retention)
    """
    
    # åˆå›æœ€é©åŒ–ã‚¯ã‚¨ãƒªã®æƒ…å ±ã‚’å«ã‚ã‚‹
    previous_query_section = ""
    if previous_optimized_query:
        previous_query_section = f"""

ã€ğŸš€ åˆå›ç”Ÿæˆã•ã‚ŒãŸæœ€é©åŒ–ã‚¯ã‚¨ãƒªï¼ˆã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼‰ã€‘
```sql
{previous_optimized_query}
```

**âš ï¸ é‡è¦**: ä¸Šè¨˜ã®æœ€é©åŒ–ã‚¯ã‚¨ãƒªã«å«ã¾ã‚Œã‚‹ä»¥ä¸‹ã®è¦ç´ ã¯å¿…ãšä¿æŒã—ã¦ãã ã•ã„ï¼š
- **REPARTITIONãƒ’ãƒ³ãƒˆ**: `/*+ REPARTITION(æ•°å€¤, ã‚«ãƒ©ãƒ å) */`
- **ãã®ä»–ã®æœ€é©åŒ–ãƒ’ãƒ³ãƒˆ**: COALESCEã€CACHEç­‰
- **æœ€é©åŒ–æ‰‹æ³•**: CTEæ§‹é€ ã€çµåˆé †åºã€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ç­‰
- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ç­–**: ã‚¹ãƒ”ãƒ«å¯¾ç­–ã€ä¸¦åˆ—åº¦æ”¹å–„ç­‰

**ğŸ¯ ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã®æ–¹é‡**: 
- ã‚¨ãƒ©ãƒ¼ç®‡æ‰€ã®ã¿ã‚’ä¿®æ­£ã—ã€æœ€é©åŒ–è¦ç´ ã¯å…¨ã¦ä¿æŒ
- ãƒ’ãƒ³ãƒˆå¥ã®é…ç½®ãƒ«ãƒ¼ãƒ«ã¯å³å®ˆï¼ˆREPARTITIONã¯ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªSELECTç›´å¾Œç­‰ï¼‰
"""

    # ğŸš¨ NEW: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è§£æã«ã‚ˆã‚‹è©³ç´°ä¿®æ­£æŒ‡ç¤ºç”Ÿæˆ
    def generate_specific_error_guidance(error_message: str) -> str:
        """Generate detailed correction instructions based on specific error messages"""
        guidance = ""
        
        if "AMBIGUOUS_REFERENCE" in error_message.upper():
            # AMBIGUOUS_REFERENCEã‚¨ãƒ©ãƒ¼ã®å…·ä½“çš„å¯¾å‡¦
            import re
            ambiguous_column_match = re.search(r'Reference `([^`]+)` is ambiguous', error_message)
            if ambiguous_column_match:
                ambiguous_column = ambiguous_column_match.group(1)
                guidance += f"""
ğŸ¯ **AMBIGUOUS_REFERENCE å°‚ç”¨ä¿®æ­£æŒ‡ç¤º**: 
- **å•é¡Œ**: ã‚«ãƒ©ãƒ  `{ambiguous_column}` ãŒè¤‡æ•°ãƒ†ãƒ¼ãƒ–ãƒ«ã«å­˜åœ¨
- **å¿…é ˆä¿®æ­£**: å…¨ã¦ã® `{ambiguous_column}` å‚ç…§ã«ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’æ˜ç¤º
- **ä¿®æ­£ä¾‹**: `{ambiguous_column}` â†’ `table_alias.{ambiguous_column}`
- **é‡è¦**: WHEREå¥ã€SELECTå¥ã€JOINå¥å…¨ã¦ã§æ˜ç¤ºçš„ä¿®é£¾ãŒå¿…è¦
"""
            
        if "UNRESOLVED_COLUMN" in error_message.upper():
            # UNRESOLVED_COLUMNã‚¨ãƒ©ãƒ¼ã®å…·ä½“çš„å¯¾å‡¦
            import re
            unresolved_match = re.search(r'column.*`([^`]+)`', error_message)
            if unresolved_match:
                unresolved_column = unresolved_match.group(1)
                guidance += f"""
ğŸ¯ **UNRESOLVED_COLUMN å°‚ç”¨ä¿®æ­£æŒ‡ç¤º**:
- **å•é¡Œ**: ã‚«ãƒ©ãƒ  `{unresolved_column}` ãŒè¦‹ã¤ã‹ã‚‰ãªã„
- **ç¢ºèªäº‹é …**: ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã€ã‚¹ãƒšãƒ«ãƒŸã‚¹ã€ã‚¹ã‚³ãƒ¼ãƒ—
- **ä¿®æ­£ä¾‹**: æ­£ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«ä¿®é£¾ã€å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ åã¸ã®å¤‰æ›´
"""
        
        if "PARSE_SYNTAX_ERROR" in error_message.upper():
            guidance += f"""
ğŸ¯ **PARSE_SYNTAX_ERROR å°‚ç”¨ä¿®æ­£æŒ‡ç¤º**:
- **é‡è¦**: æ§‹æ–‡ã‚¨ãƒ©ãƒ¼æœ€å„ªå…ˆä¿®æ­£ï¼ˆã‚«ãƒ³ãƒæŠœã‘ã€ã‚¨ã‚¤ãƒªã‚¢ã‚¹é‡è¤‡ç­‰ï¼‰
- **ç¢ºèª**: SELECTå¥ã®ã‚«ãƒ³ãƒã€FROMå¥ã®æ§‹æ–‡ã€ã‚¨ã‚¤ãƒªã‚¢ã‚¹å®šç¾©
"""
            
        return guidance
    
    specific_guidance = generate_specific_error_guidance(error_info)

    error_feedback_prompt = f"""
ã‚ãªãŸã¯Databricksã®SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã¨ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã®å°‚é–€å®¶ã§ã™ã€‚

ä»¥ä¸‹ã®æœ€é©åŒ–ã‚¯ã‚¨ãƒªã§EXPLAINå®Ÿè¡Œæ™‚ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚**æœ€é©åŒ–è¦ç´ ã‚’ä¿æŒã—ãªãŒã‚‰**ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’åŸºã«ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚

ã€ğŸš¨ ç™ºç”Ÿã—ãŸã‚¨ãƒ©ãƒ¼æƒ…å ±ã€‘
{error_info}
{specific_guidance}

ã€å…ƒã®åˆ†æå¯¾è±¡ã‚¯ã‚¨ãƒªã€‘
```sql
{original_query}
```
{previous_query_section}
ã€è©³ç´°ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœã€‘
{analysis_result}

ã€ğŸ”§ ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã®é‡è¦ãªæŒ‡é‡ã€‘
1. **ğŸš€ æœ€é©åŒ–è¦ç´ ã®çµ¶å¯¾ä¿æŒï¼ˆæœ€é‡è¦ï¼‰**:
   - **åˆå›ç”Ÿæˆã•ã‚ŒãŸJOINé †åºæœ€é©åŒ–ã‚’å¿…ãšä¿æŒ**
   - **åˆå›ç”Ÿæˆã•ã‚ŒãŸREPARTITIONãƒ’ãƒ³ãƒˆã‚’å¿…ãšä¿æŒ**: `/*+ REPARTITION(æ•°å€¤, ã‚«ãƒ©ãƒ ) */`
   - **ãã®ä»–ã®æœ€é©åŒ–ãƒ’ãƒ³ãƒˆã‚‚å…¨ã¦ä¿æŒ**: COALESCEã€CACHEç­‰
   - **CTEæ§‹é€ ã‚„çµåˆé †åºãªã©ã®æœ€é©åŒ–è¨­è¨ˆã‚’ç¶­æŒ**
   - **ã‚¹ãƒ”ãƒ«å¯¾ç­–ã‚„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ç­–ã‚’ä¿æŒ**

2. **ğŸš¨ è‡´å‘½çš„æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®æœ€å„ªå…ˆä¿®æ­£**:

   **A. ã‚«ãƒ³ãƒæŠœã‘ã‚¨ãƒ©ãƒ¼ (PARSE_SYNTAX_ERROR)**:
   - âŒ `i.i_item_sk ss.ss_item_sk` â†’ âœ… `i.i_item_sk, ss.ss_item_sk`
   - âŒ `SELECT col1 col2 FROM` â†’ âœ… `SELECT col1, col2 FROM`
   - **SELECTå¥å†…ã§ã®ã‚«ãƒ³ãƒæŠœã‘ã‚’æœ€å„ªå…ˆã§ä¿®æ­£**

   **B. äºŒé‡ãƒ»ä¸‰é‡ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚¨ãƒ©ãƒ¼**:
   - âŒ `iss.i.i_brand_id` â†’ âœ… `iss.i_brand_id` ã¾ãŸã¯ `i.i_brand_id`
   - âŒ `ss.ss.ss_item_sk` â†’ âœ… `ss.ss_item_sk`
   - **ä¸€ã¤ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«å¯¾ã™ã‚‹é‡è¤‡ã‚¨ã‚¤ãƒªã‚¢ã‚¹å‚ç…§ã‚’ä¿®æ­£**

   **C. å­˜åœ¨ã—ãªã„ãƒ†ãƒ¼ãƒ–ãƒ«/ã‚«ãƒ©ãƒ å‚ç…§**:
   - âŒ `this_year.i.i_brand_id` â†’ âœ… `this_year.i_brand_id`
   - **ã‚µãƒ–ã‚¯ã‚¨ãƒªã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¨å†…éƒ¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã®æ··åŒã‚’ä¿®æ­£**

   **D. FROMå¥æ§‹æ–‡ã‚¨ãƒ©ãƒ¼**:
   - âŒ `FROM table1, (SELECT ...) x WHERE` â†’ âœ… é©åˆ‡ãªJOINæ§‹æ–‡ã«å¤‰æ›
   - **å¤ã„ã‚«ãƒ³ãƒçµåˆã‚’æ˜ç¤ºçš„JOINæ§‹æ–‡ã«å¤‰æ›**

3. **ğŸ” AMBIGUOUS_REFERENCE ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£**: 
   - **å…¨ã¦ã®ã‚«ãƒ©ãƒ å‚ç…§ã§ãƒ†ãƒ¼ãƒ–ãƒ«åã¾ãŸã¯ã‚¨ã‚¤ãƒªã‚¢ã‚¹åã‚’æ˜ç¤ºçš„ã«æŒ‡å®š**
   - ä¾‹: `ss_item_sk` â†’ `store_sales.ss_item_sk` ã¾ãŸã¯ `ss.ss_item_sk`
   - **ã‚µãƒ–ã‚¯ã‚¨ãƒªã¨ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã§åŒåã‚«ãƒ©ãƒ ãŒã‚ã‚‹å ´åˆã¯ç‰¹ã«æ³¨æ„**

4. **ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã®ä¸€è²«ä½¿ç”¨**: 
   - å…¨ã¦ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«çŸ­ã„ã‚¨ã‚¤ãƒªã‚¢ã‚¹åã‚’ä»˜ä¸ï¼ˆä¾‹: store_sales â†’ ss, item â†’ iï¼‰
   - ã‚¯ã‚¨ãƒªå…¨ä½“ã§ä¸€è²«ã—ã¦ã‚¨ã‚¤ãƒªã‚¢ã‚¹åã‚’ä½¿ç”¨
   - ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…ã§ã‚‚åŒã˜ã‚¨ã‚¤ãƒªã‚¢ã‚¹åä½“ç³»ã‚’ç¶­æŒ

5. **ãã®ä»–ã®æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ä¿®æ­£**: 
   - **å‹å¤‰æ›ã‚¨ãƒ©ãƒ¼**: ä¸é©åˆ‡ãªã‚­ãƒ£ã‚¹ãƒˆä¿®æ­£
   - **ãƒ’ãƒ³ãƒˆå¥ã‚¨ãƒ©ãƒ¼**: æ§‹æ–‡ã«åˆã‚ã›ãŸé…ç½®ä¿®æ­£
   - **æ¨©é™ã‚¨ãƒ©ãƒ¼**: ä»£æ›¿ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•ææ¡ˆ

ã€ğŸš¨ BROADCASTãƒ’ãƒ³ãƒˆé…ç½®ã®å³æ ¼ãªãƒ«ãƒ¼ãƒ« - ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆã€‘
**âœ… æ­£ã—ã„é…ç½®ï¼ˆå¿…é ˆï¼‰:**
```sql
-- âœ… æ­£ã—ã„: ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTç›´å¾Œã®ã¿
SELECT /*+ BROADCAST(i, d) */
  ss.ss_item_sk, i.i_brand_id, d.d_year
FROM store_sales ss
  JOIN item i ON ss.ss_item_sk = i.i_item_sk
  JOIN date_dim d ON ss.ss_sold_date_sk = d.d_date_sk
```

**âŒ çµ¶å¯¾ã«ç¦æ­¢ã•ã‚Œã‚‹é…ç½®ï¼ˆæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®åŸå› ï¼‰:**
```sql
-- âŒ é–“é•ã„: JOINå¥å†…ã¸ã®é…ç½®ï¼ˆPARSE_SYNTAX_ERRORç™ºç”Ÿï¼‰
FROM store_sales ss
  JOIN /*+ BROADCAST(i) */ item i ON ss.ss_item_sk = i.i_item_sk  -- ã“ã‚ŒãŒæ§‹æ–‡ã‚¨ãƒ©ãƒ¼
  JOIN /*+ BROADCAST(d) */ date_dim d ON ss.ss_sold_date_sk = d.d_date_sk  -- ã“ã‚Œã‚‚æ§‹æ–‡ã‚¨ãƒ©ãƒ¼

-- âŒ é–“é•ã„: ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…ã¸ã®é…ç½®
SELECT ... FROM (
  SELECT /*+ BROADCAST(i) */ ...  -- ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…ã¯ç„¡åŠ¹
  FROM ...
)

-- âŒ é–“é•ã„: FROMå¥å†…ã¸ã®é…ç½®
FROM /*+ BROADCAST(i) */ item i  -- FROMå¥å†…ã¯æ§‹æ–‡ã‚¨ãƒ©ãƒ¼
```

**ğŸ”§ PARSE_SYNTAX_ERRORä¿®æ­£ã®å…·ä½“çš„æ‰‹é †:**
1. **JOINå¥å†…ã®BROADCASTãƒ’ãƒ³ãƒˆã‚’å…¨ã¦å‰Šé™¤**
2. **ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®æœ€åˆã®SELECTç›´å¾Œã«å…¨ã¦ã®BROADCASTãƒ’ãƒ³ãƒˆã‚’çµ±åˆ**
3. **ãƒ†ãƒ¼ãƒ–ãƒ«å/ã‚¨ã‚¤ãƒªã‚¢ã‚¹åã‚’æ­£ç¢ºã«æŒ‡å®š**

**ğŸ“ å…·ä½“çš„ä¿®æ­£ä¾‹ï¼ˆPARSE_SYNTAX_ERRORå¯¾å¿œï¼‰:**

âŒ **ä¿®æ­£å‰ï¼ˆã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼‰:**
```sql
SELECT ss.ss_item_sk, i.i_brand_id
FROM store_sales ss
  JOIN /*+ BROADCAST(i) */ item i ON ss.ss_item_sk = i.i_item_sk  -- PARSE_SYNTAX_ERROR
  JOIN /*+ BROADCAST(d) */ date_dim d ON ss.ss_sold_date_sk = d.d_date_sk  -- PARSE_SYNTAX_ERROR
```

âœ… **ä¿®æ­£å¾Œï¼ˆæ­£å¸¸ï¼‰:**
```sql
SELECT /*+ BROADCAST(i, d) */ ss.ss_item_sk, i.i_brand_id
FROM store_sales ss
  JOIN item i ON ss.ss_item_sk = i.i_item_sk
  JOIN date_dim d ON ss.ss_sold_date_sk = d.d_date_sk
```

**ğŸš¨ ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã®æœ€é‡è¦ãƒ«ãƒ¼ãƒ«:**
- **JOINå¥å†…ã®`/*+ BROADCAST(...) */`ã¯å³åº§ã«å‰Šé™¤**
- **å‰Šé™¤ã—ãŸBROADCASTå¯¾è±¡ã‚’ãƒ¡ã‚¤ãƒ³SELECTç›´å¾Œã«ç§»å‹•**
- **è¤‡æ•°ã®BROADCASTå¯¾è±¡ã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§çµ±åˆ: `/*+ BROADCAST(table1, table2, table3) */`**

ã€ğŸš¨ REPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ã®å³æ ¼ãªãƒ«ãƒ¼ãƒ« - ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆã€‘
- **ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã®SELECTæ–‡ç›´å¾Œã«é…ç½®**
- **ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã¨ã‚«ãƒ©ãƒ åã¯å¿…é ˆ**: `/*+ REPARTITION(200, column_name) */`
- **ã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿é©ç”¨**

ã€é‡è¦ãªåˆ¶ç´„ - ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆã€‘
- æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã‚’çµ¶å¯¾ã«ç™ºç”Ÿã•ã›ãªã„å®Œå…¨ãªSQLã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
- ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ åã€ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’å®Œå…¨ã«è¨˜è¿°
- ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆ...ã€[çœç•¥]ï¼‰ã¯ä¸€åˆ‡ä½¿ç”¨ç¦æ­¢
- å…ƒã®ã‚¯ã‚¨ãƒªã®DISTINCTå¥ã¯å¿…ãšä¿æŒ
- å®Ÿéš›ã«å®Ÿè¡Œã§ãã‚‹å®Œå…¨ãªSQLã‚¯ã‚¨ãƒªã®ã¿ã‚’å‡ºåŠ›

ã€å‡ºåŠ›å½¢å¼ã€‘
## ğŸ”§ ã‚¨ãƒ©ãƒ¼ä¿®æ­£æ¸ˆã¿æœ€é©åŒ–SQL

**ä¿®æ­£ã—ãŸå†…å®¹**:
- [å…·ä½“çš„ãªã‚¨ãƒ©ãƒ¼ä¿®æ­£ç®‡æ‰€]

**ä¿æŒã—ãŸæœ€é©åŒ–è¦ç´ **:
- [ä¿æŒã•ã‚ŒãŸREPARTITIONãƒ’ãƒ³ãƒˆ]
- [ä¿æŒã•ã‚ŒãŸJOINé †åºæœ€é©åŒ–]
- [ä¿æŒã•ã‚ŒãŸãã®ä»–ã®æœ€é©åŒ–æ‰‹æ³•]

```sql
[å®Œå…¨ãªSQL - ã‚¨ãƒ©ãƒ¼ä¿®æ­£æ¸ˆã¿ã€æœ€é©åŒ–è¦ç´ ä¿æŒ]
```

## ä¿®æ­£è©³ç´°
[ã‚¨ãƒ©ãƒ¼ã®åŸå› ã¨ä¿®æ­£æ–¹æ³•ã€ãŠã‚ˆã³æœ€é©åŒ–è¦ç´ ä¿æŒã®èª¬æ˜]
"""

    # è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
    provider = LLM_CONFIG["provider"]
    
    try:
        if provider == "databricks":
            optimized_result = _call_databricks_llm(error_feedback_prompt)
        elif provider == "openai":
            optimized_result = _call_openai_llm(error_feedback_prompt)
        elif provider == "azure_openai":
            optimized_result = _call_azure_openai_llm(error_feedback_prompt)
        elif provider == "anthropic":
            optimized_result = _call_anthropic_llm(error_feedback_prompt)
        else:
            error_msg = "âš ï¸ è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒèªè­˜ã§ãã¾ã›ã‚“"
            print(f"âŒ LLM error correction error: {error_msg}")
            return f"LLM_ERROR: {error_msg}"
        
        # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¦ï¼‰
        if isinstance(optimized_result, str):
            # APIã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ¤œå‡º
            error_indicators = [
                 "APIã‚¨ãƒ©ãƒ¼:",
                 "Input is too long",
                 "Bad Request",
                 "âŒ",
                 "âš ï¸",
                 "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼:",
                 "APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼:",
                 "ãƒ¬ã‚¹ãƒãƒ³ã‚¹:",
                 '{"error_code":'
             ]
            
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
            is_error_response = any(indicator in optimized_result for indicator in error_indicators)
            
            if is_error_response:
                print(f"âŒ Error occurred in LLM error correction API call: {optimized_result[:200]}...")
                return f"LLM_ERROR: {optimized_result}"
        
        # ğŸ”§ ä¿®æ­£å¾Œã®ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯å¾Œå‡¦ç†ã‚’é©ç”¨
        if isinstance(optimized_result, str) and not optimized_result.startswith("LLM_ERROR:"):
            print("ğŸ”§ Executing query validation and post-processing after error correction")
            final_corrected_query = enhance_error_correction_with_syntax_validation(optimized_result, original_query, error_info)
            return final_corrected_query
        
        return optimized_result
        
    except Exception as e:
        error_msg = f"âš ï¸ ã‚¨ãƒ©ãƒ¼ä¿®æ­£SQLç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        print(f"âŒ LLM error correction exception error: {error_msg}")
        return f"LLM_ERROR: {error_msg}"


def compare_query_performance(original_explain_cost: str, optimized_explain_cost: str) -> Dict[str, Any]:
    """
    Compare EXPLAIN COST results to detect performance degradation
    
    Args:
        original_explain_cost: EXPLAIN COST result of original query
        optimized_explain_cost: EXPLAIN COST result of optimized query
        
    Returns:
        Dict: Performance comparison results and recommendations
    """
    comparison_result = {
        'is_optimization_beneficial': True,
        'performance_degradation_detected': False,
        'significant_improvement_detected': False,  # ğŸš¨ æ˜ç¢ºãªæ”¹å–„æ¤œå‡ºãƒ•ãƒ©ã‚°è¿½åŠ ï¼ˆ1%ä»¥ä¸Šï¼‰
        'substantial_improvement_detected': False,  # ğŸš€ å¤§å¹…æ”¹å–„æ¤œå‡ºãƒ•ãƒ©ã‚°è¿½åŠ ï¼ˆ10%ä»¥ä¸Šï¼‰
        'total_cost_ratio': 1.0,
        'memory_usage_ratio': 1.0,
        'scan_cost_ratio': 1.0,
        'join_cost_ratio': 1.0,
        'recommendation': 'use_optimized',
        'details': []
    }
    
    try:
        import re
        
        # ã‚³ã‚¹ãƒˆæƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
        def extract_cost_metrics(explain_cost_text):
            metrics = {
                'total_size_bytes': 0,
                'total_rows': 0,
                'scan_operations': 0,
                'join_operations': 0,
                'memory_estimates': 0,
                'shuffle_partitions': 0
            }
            
            # ã‚µã‚¤ã‚ºã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æŠ½å‡º
            size_patterns = [
                r'size_bytes["\s]*[:=]\s*([0-9.]+)',
                r'sizeInBytes["\s]*[:=]\s*([0-9.]+)',
                r'(\d+\.?\d*)\s*[KMG]?iB',
                r'(\d+\.?\d*)\s*[KMG]?B'
            ]
            
            for pattern in size_patterns:
                matches = re.findall(pattern, explain_cost_text, re.IGNORECASE)
                for match in matches:
                    try:
                        size_val = float(match)
                        metrics['total_size_bytes'] += size_val
                    except:
                        continue
            
            # è¡Œæ•°ã‚’æŠ½å‡º
            row_patterns = [
                r'rows["\s]*[:=]\s*([0-9]+)',
                r'numRows["\s]*[:=]\s*([0-9]+)'
            ]
            
            for pattern in row_patterns:
                matches = re.findall(pattern, explain_cost_text, re.IGNORECASE)
                for match in matches:
                    try:
                        metrics['total_rows'] += int(match)
                    except:
                        continue
            
            # ã‚¹ã‚­ãƒ£ãƒ³ãƒ»JOINæ“ä½œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            metrics['scan_operations'] = len(re.findall(r'Scan|FileScan|TableScan', explain_cost_text, re.IGNORECASE))
            metrics['join_operations'] = len(re.findall(r'Join|HashJoin|SortMergeJoin', explain_cost_text, re.IGNORECASE))
            
            # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°
            shuffle_matches = re.findall(r'partitions?["\s]*[:=]\s*([0-9]+)', explain_cost_text, re.IGNORECASE)
            for match in shuffle_matches:
                try:
                    metrics['shuffle_partitions'] += int(match)
                except:
                    continue
                    
            return metrics
        
        # å…ƒã‚¯ã‚¨ãƒªã¨æœ€é©åŒ–ã‚¯ã‚¨ãƒªã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º
        original_metrics = extract_cost_metrics(original_explain_cost)
        optimized_metrics = extract_cost_metrics(optimized_explain_cost)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒï¼ˆå€¤ãŒ0ã®å ´åˆã¯1ã¨ã—ã¦è¨ˆç®—ï¼‰
        if original_metrics['total_size_bytes'] > 0:
            comparison_result['total_cost_ratio'] = optimized_metrics['total_size_bytes'] / original_metrics['total_size_bytes']
        
        if original_metrics['total_rows'] > 0:
            comparison_result['memory_usage_ratio'] = optimized_metrics['total_rows'] / original_metrics['total_rows']
        
        # ğŸš¨ å³æ ¼ãªåˆ¤å®šé–¾å€¤ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ï¼šä¿å®ˆçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
        COST_DEGRADATION_THRESHOLD = 1.01   # 1%ä»¥ä¸Šã®ã‚³ã‚¹ãƒˆå¢—åŠ ã§å…ƒã‚¯ã‚¨ãƒªæ¨å¥¨ï¼ˆå³æ ¼åŒ–ï¼‰
        MEMORY_DEGRADATION_THRESHOLD = 1.01 # 1%ä»¥ä¸Šã®ãƒ¡ãƒ¢ãƒªå¢—åŠ ã§å…ƒã‚¯ã‚¨ãƒªæ¨å¥¨ï¼ˆå³æ ¼åŒ–ï¼‰
        COST_IMPROVEMENT_THRESHOLD = 0.99   # 1%ä»¥ä¸Šã®å‰Šæ¸›ã§æœ€é©åŒ–ã‚¯ã‚¨ãƒªæ¨å¥¨ï¼ˆå³æ ¼åŒ–ï¼‰
        MEMORY_IMPROVEMENT_THRESHOLD = 0.99 # 1%ä»¥ä¸Šã®å‰Šæ¸›ã§æœ€é©åŒ–ã‚¯ã‚¨ãƒªæ¨å¥¨ï¼ˆå³æ ¼åŒ–ï¼‰
        
        # ğŸš€ å¤§å¹…æ”¹å–„ã®åˆ¤å®šé–¾å€¤ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ï¼š10%ä»¥ä¸Šæ”¹å–„ã§è©¦è¡Œçµ‚äº†ï¼‰
        SUBSTANTIAL_COST_IMPROVEMENT_THRESHOLD = 0.9   # 10%ä»¥ä¸Šã®ã‚³ã‚¹ãƒˆå‰Šæ¸›ã§å¤§å¹…æ”¹å–„èªå®š
        SUBSTANTIAL_MEMORY_IMPROVEMENT_THRESHOLD = 0.9 # 10%ä»¥ä¸Šã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã§å¤§å¹…æ”¹å–„èªå®š
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ‚ªåŒ–æ¤œå‡ºï¼ˆãƒãƒ¼ã‚¸ãƒ³ãªã—ã§æ˜ç¢ºãªåˆ¤å®šï¼‰
        degradation_factors = []
        
        # ğŸ¯ æ˜ç¢ºãªæ‚ªåŒ–åˆ¤å®šï¼ˆå¢ƒç•Œå€¤ã®æ›–æ˜§ã•ã‚’æ’é™¤ï¼‰
        if comparison_result['total_cost_ratio'] > COST_DEGRADATION_THRESHOLD:
            degradation_factors.append(f"Total execution cost degradation: {comparison_result['total_cost_ratio']:.2f}x (threshold: {COST_DEGRADATION_THRESHOLD:.2f})")
            
        if comparison_result['memory_usage_ratio'] > MEMORY_DEGRADATION_THRESHOLD:
            degradation_factors.append(f"Memory usage degradation: {comparison_result['memory_usage_ratio']:.2f}x (threshold: {MEMORY_DEGRADATION_THRESHOLD:.2f})")
        
        # Check for significant JOIN operations count increase
        if (optimized_metrics['join_operations'] > original_metrics['join_operations'] * 1.5):
            degradation_factors.append(f"JOIN operations count increase: {original_metrics['join_operations']} â†’ {optimized_metrics['join_operations']}")
        
        # æ‚ªåŒ–åˆ¤å®š
        if degradation_factors:
            comparison_result['performance_degradation_detected'] = True
            comparison_result['is_optimization_beneficial'] = False
            comparison_result['recommendation'] = 'use_original'
            comparison_result['details'] = degradation_factors
        else:
            # æ‚ªåŒ–ã§ã¯ãªã„ãŒã€æ”¹å–„/åŒç­‰ã®è©³ç´°åˆ¤å®š
            performance_factors = []
            
            # ğŸš¨ å³æ ¼ãªè©³ç´°åˆ¤å®šï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ï¼šä¿å®ˆçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
            # å®Ÿè¡Œã‚³ã‚¹ãƒˆã®è©³ç´°åˆ¤å®š
            if comparison_result['total_cost_ratio'] < COST_IMPROVEMENT_THRESHOLD:
                performance_factors.append(f"Execution cost improvement: {(1-comparison_result['total_cost_ratio'])*100:.1f}% reduction")
            elif comparison_result['total_cost_ratio'] > COST_DEGRADATION_THRESHOLD:  # 1%ä»¥ä¸Šã®å¢—åŠ ã§å³åº§ã«æ‚ªåŒ–åˆ¤å®š
                cost_increase_pct = (comparison_result['total_cost_ratio']-1)*100
                performance_factors.append(f"Execution cost increase: {cost_increase_pct:.1f}% increase (original query recommended)")
            else:
                performance_factors.append(f"Execution cost equivalent: {comparison_result['total_cost_ratio']:.2f}x (no change)")
                
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è©³ç´°åˆ¤å®š
            if comparison_result['memory_usage_ratio'] < MEMORY_IMPROVEMENT_THRESHOLD:
                performance_factors.append(f"Memory usage improvement: {(1-comparison_result['memory_usage_ratio'])*100:.1f}% reduction")
            elif comparison_result['memory_usage_ratio'] > MEMORY_DEGRADATION_THRESHOLD:  # 1%ä»¥ä¸Šã®å¢—åŠ ã§å³åº§ã«æ‚ªåŒ–åˆ¤å®š
                memory_increase_pct = (comparison_result['memory_usage_ratio']-1)*100
                performance_factors.append(f"Memory usage increase: {memory_increase_pct:.1f}% increase (original query recommended)")
            else:
                performance_factors.append(f"Memory usage equivalent: {comparison_result['memory_usage_ratio']:.2f}x (no change)")
            
            # JOINåŠ¹ç‡åŒ–ãƒã‚§ãƒƒã‚¯
            if optimized_metrics['join_operations'] < original_metrics['join_operations']:
                performance_factors.append(f"JOIN optimization: {original_metrics['join_operations']} â†’ {optimized_metrics['join_operations']} operations")
            elif optimized_metrics['join_operations'] > original_metrics['join_operations']:
                performance_factors.append(f"JOIN operations increase: {original_metrics['join_operations']} â†’ {optimized_metrics['join_operations']} operations (minor)")
            
            # ğŸš¨ å³æ ¼ãªç·åˆåˆ¤å®šï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ï¼šæ˜ç¢ºãªæ”¹å–„ã®ã¿æˆåŠŸï¼‰
            has_improvement = any("improvement" in factor for factor in performance_factors)
            has_cost_increase = any("cost increase" in factor for factor in performance_factors)
            has_memory_increase = any("memory increase" in factor for factor in performance_factors)
            
            # ğŸš¨ æ˜ç¢ºãªæ”¹å–„æ¤œå‡ºï¼ˆ1%ä»¥ä¸Šã®æ”¹å–„ã®ã¿ï¼‰
            has_significant_improvement = (
                comparison_result['total_cost_ratio'] < COST_IMPROVEMENT_THRESHOLD or
                comparison_result['memory_usage_ratio'] < MEMORY_IMPROVEMENT_THRESHOLD
            )
            
            # ğŸš€ å¤§å¹…æ”¹å–„æ¤œå‡ºï¼ˆ10%ä»¥ä¸Šã®æ”¹å–„ï¼‰
            has_substantial_improvement = (
                comparison_result['total_cost_ratio'] < SUBSTANTIAL_COST_IMPROVEMENT_THRESHOLD or
                comparison_result['memory_usage_ratio'] < SUBSTANTIAL_MEMORY_IMPROVEMENT_THRESHOLD
            )
            
            # ğŸš¨ å³æ ¼åˆ¤å®šï¼š1%ä»¥ä¸Šã®å¢—åŠ ã§ã‚‚å…ƒã‚¯ã‚¨ãƒªæ¨å¥¨
            if has_cost_increase or has_memory_increase:
                performance_factors.insert(0, "âŒ Performance degradation detected (original query recommended)")
                # ğŸš¨ å¢—åŠ æ¤œå‡ºæ™‚ã¯æ¨å¥¨ã‚‚å…ƒã‚¯ã‚¨ãƒªã«å¤‰æ›´
                comparison_result['performance_degradation_detected'] = True
                comparison_result['is_optimization_beneficial'] = False  
                comparison_result['recommendation'] = 'use_original'
                comparison_result['significant_improvement_detected'] = False
                comparison_result['substantial_improvement_detected'] = False
            elif has_substantial_improvement:
                # ğŸš€ å¤§å¹…æ”¹å–„ï¼ˆ10%ä»¥ä¸Šï¼‰ã‚’æ¤œå‡º
                cost_reduction = (1 - comparison_result['total_cost_ratio']) * 100
                memory_reduction = (1 - comparison_result['memory_usage_ratio']) * 100
                max_reduction = max(cost_reduction, memory_reduction)
                performance_factors.insert(0, f"ğŸš€ Significant performance improvement confirmed (max {max_reduction:.1f}% reduction - optimized query recommended)")
                comparison_result['significant_improvement_detected'] = True
                comparison_result['substantial_improvement_detected'] = True
            elif has_significant_improvement:
                performance_factors.insert(0, "âœ… Clear performance improvement confirmed (optimized query recommended)")
                comparison_result['significant_improvement_detected'] = True
                comparison_result['substantial_improvement_detected'] = False
            else:
                performance_factors.insert(0, "â– Performance equivalent (no clear improvement)")
                comparison_result['significant_improvement_detected'] = False
                comparison_result['substantial_improvement_detected'] = False
            
            comparison_result['details'] = performance_factors
        
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨å´ã«å€’ã—ã¦å…ƒã‚¯ã‚¨ãƒªã‚’æ¨å¥¨
        comparison_result['performance_degradation_detected'] = True
        comparison_result['is_optimization_beneficial'] = False
        comparison_result['recommendation'] = 'use_original'
        comparison_result['details'] = [f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã‚¨ãƒ©ãƒ¼ã®ãŸã‚å…ƒã‚¯ã‚¨ãƒªä½¿ç”¨: {str(e)}"]
    
    return comparison_result


def analyze_degradation_causes(performance_comparison: Dict[str, Any], original_explain_cost: str = "", optimized_explain_cost: str = "") -> Dict[str, str]:
    """
    Analyze causes of performance degradation and generate correction instructions
    """
    degradation_analysis = {
        'primary_cause': 'unknown',
        'specific_issues': [],
        'fix_instructions': [],
        'confidence_level': 'low',
        'analysis_details': {}
    }
    
    try:
        if not performance_comparison or not performance_comparison.get('performance_degradation_detected'):
            degradation_analysis['primary_cause'] = 'no_degradation'
            return degradation_analysis
        
        details = performance_comparison.get('details', [])
        cost_ratio = performance_comparison.get('total_cost_ratio', 1.0)
        memory_ratio = performance_comparison.get('memory_usage_ratio', 1.0)
        
        # ğŸ” ã‚³ã‚¹ãƒˆæ‚ªåŒ–ã®æ·±åˆ»åº¦åˆ†æ
        if cost_ratio > 1.5:  # 50%ä»¥ä¸Šã®æ‚ªåŒ–
            degradation_analysis['confidence_level'] = 'high'
            severity = 'critical'
        elif cost_ratio > 1.3:  # 30%ä»¥ä¸Šã®æ‚ªåŒ–
            degradation_analysis['confidence_level'] = 'medium'
            severity = 'significant'
        else:
            degradation_analysis['confidence_level'] = 'low'
            severity = 'minor'
        
        degradation_analysis['analysis_details']['cost_degradation_severity'] = severity
        degradation_analysis['analysis_details']['cost_ratio'] = cost_ratio
        degradation_analysis['analysis_details']['memory_ratio'] = memory_ratio
        
        # ğŸ¯ ä¸»è¦åŸå› ã®ç‰¹å®šã¨JOINæ“ä½œæ•°åˆ†æ
        for detail in details:
            detail_str = str(detail).lower()
            
            # Detect significant JOIN operations count increase
            if 'join operations count increase' in detail_str or 'join' in detail_str:
                degradation_analysis['primary_cause'] = 'excessive_joins'
                degradation_analysis['specific_issues'].append('Significant JOIN operations count increase')
                
                # JOINæ•°ã®å…·ä½“çš„ãªå¢—åŠ ã‚’è§£æ
                import re
                join_match = re.search(r'(\d+)\s*â†’\s*(\d+)', detail_str)
                if join_match:
                    original_joins = int(join_match.group(1))
                    optimized_joins = int(join_match.group(2))
                    join_increase_ratio = optimized_joins / original_joins if original_joins > 0 else float('inf')
                    
                    degradation_analysis['analysis_details']['original_joins'] = original_joins
                    degradation_analysis['analysis_details']['optimized_joins'] = optimized_joins
                    degradation_analysis['analysis_details']['join_increase_ratio'] = join_increase_ratio
                    
                    if join_increase_ratio > 1.5:  # 50%ä»¥ä¸Šã®JOINå¢—åŠ 
                        degradation_analysis['fix_instructions'].extend([
                            "JOINé †åºã®åŠ¹ç‡åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„",
                            "å…ƒã®JOINé †åºã‚’å°Šé‡ã—ã€å¤§å¹…ãªæ§‹é€ å¤‰æ›´ã‚’é¿ã‘ã¦ãã ã•ã„",
                            "ä¸è¦ãªã‚µãƒ–ã‚¯ã‚¨ãƒªåŒ–ã«ã‚ˆã‚‹JOINé‡è¤‡ã‚’é˜²ã„ã§ãã ã•ã„",
                            "CTEå±•é–‹ã«ã‚ˆã‚‹JOINå¢—åŠ ã‚’é¿ã‘ã€å…ƒã®æ§‹é€ ã‚’ä¿æŒã—ã¦ãã ã•ã„"
                        ])
                
            # Total execution cost degradation
            elif 'total execution cost degradation' in detail_str or 'cost' in detail_str:
                if degradation_analysis['primary_cause'] == 'unknown':
                    degradation_analysis['primary_cause'] = 'cost_increase'
                degradation_analysis['specific_issues'].append('Total execution cost degradation')
                degradation_analysis['fix_instructions'].extend([
                    "å°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åŠ¹ç‡çš„ã«JOINã§å‡¦ç†ã—ã¦ãã ã•ã„",
                                         "å¤§ããªãƒ†ãƒ¼ãƒ–ãƒ«ã®JOINé †åºã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„",
                    "REPARTITIONãƒ’ãƒ³ãƒˆã®é…ç½®ä½ç½®ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„"
                ])
            
            # Memory usage degradation
            elif 'memory usage degradation' in detail_str or 'memory' in detail_str:
                if degradation_analysis['primary_cause'] == 'unknown':
                    degradation_analysis['primary_cause'] = 'memory_increase'
                degradation_analysis['specific_issues'].append('Memory usage degradation')
                degradation_analysis['fix_instructions'].extend([
                    "å¤§ããªãƒ†ãƒ¼ãƒ–ãƒ«ã®BROADCASTé©ç”¨ã‚’å‰Šé™¤ã—ã¦ãã ã•ã„",
                    "ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªJOINæˆ¦ç•¥ã‚’é¸æŠã—ã¦ãã ã•ã„",
                    "ä¸­é–“çµæœã®ã‚µã‚¤ã‚ºã‚’å‰Šæ¸›ã—ã¦ãã ã•ã„"
                ])
        
        # ğŸ” EXPLAIN COSTåˆ†æã«ã‚ˆã‚‹è©³ç´°åŸå› ç‰¹å®šï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if original_explain_cost and optimized_explain_cost:
            cost_analysis = analyze_explain_cost_differences(original_explain_cost, optimized_explain_cost)
            degradation_analysis['analysis_details']['explain_cost_analysis'] = cost_analysis
            
            # BROADCASTé–¢é€£ã®å•é¡Œæ¤œå‡º
            if cost_analysis.get('broadcast_issues'):
                degradation_analysis['fix_instructions'].extend([
                    "æ¤œå‡ºã•ã‚ŒãŸBROADCASTå•é¡Œã‚’ä¿®æ­£ã—ã¦ãã ã•ã„",
                    "é©åˆ‡ãªã‚µã‚¤ã‚ºã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿BROADCASTå¯¾è±¡ã¨ã—ã¦ãã ã•ã„"
                ])
        
        # åŸå› ãŒç‰¹å®šã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if degradation_analysis['primary_cause'] == 'unknown':
            degradation_analysis['primary_cause'] = 'optimization_backfire'
            degradation_analysis['fix_instructions'].extend([
                "æœ€é©åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä¿å®ˆçš„ã«å¤‰æ›´ã—ã¦ãã ã•ã„",
                "å…ƒã®ã‚¯ã‚¨ãƒªæ§‹é€ ã‚’ã‚ˆã‚Šå¤šãä¿æŒã—ã¦ãã ã•ã„",
                "ãƒ’ãƒ³ãƒˆå¥ã®é©ç”¨ã‚’æœ€å°é™ã«æŠ‘ãˆã¦ãã ã•ã„"
            ])
        
        # é‡è¤‡ã™ã‚‹ä¿®æ­£æŒ‡ç¤ºã‚’å‰Šé™¤
        degradation_analysis['fix_instructions'] = list(set(degradation_analysis['fix_instructions']))
        
    except Exception as e:
        degradation_analysis['primary_cause'] = 'analysis_error'
        degradation_analysis['specific_issues'] = [f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"]
        degradation_analysis['fix_instructions'] = [
            "ä¿å®ˆçš„ãªæœ€é©åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„",
            "å…ƒã®ã‚¯ã‚¨ãƒªæ§‹é€ ã‚’æœ€å¤§é™ä¿æŒã—ã¦ãã ã•ã„"
        ]
    
    return degradation_analysis


def analyze_explain_cost_differences(original_cost: str, optimized_cost: str) -> Dict[str, Any]:
    """
    Identify degradation causes through differential analysis of EXPLAIN COST results
    """
    analysis = {
        'broadcast_issues': False,
        'join_strategy_changes': [],
        'size_estimation_problems': [],
        'plan_structure_changes': []
    }
    
    try:
        import re
        
        # BROADCASTé–¢é€£ã®å•é¡Œæ¤œå‡º
        original_broadcasts = len(re.findall(r'broadcast', original_cost.lower()))
        optimized_broadcasts = len(re.findall(r'broadcast', optimized_cost.lower()))
        
        if optimized_broadcasts > original_broadcasts * 2:  # BROADCASTä½¿ç”¨é‡ãŒ2å€ä»¥ä¸Šå¢—åŠ 
            analysis['broadcast_issues'] = True
            analysis['join_strategy_changes'].append(f"BROADCASTä½¿ç”¨ãŒå¤§å¹…å¢—åŠ : {original_broadcasts} â†’ {optimized_broadcasts}")
        
        # JOINæˆ¦ç•¥ã®å¤‰åŒ–æ¤œå‡º
        original_join_types = set(re.findall(r'(\w+)Join', original_cost))
        optimized_join_types = set(re.findall(r'(\w+)Join', optimized_cost))
        
        if optimized_join_types != original_join_types:
            analysis['join_strategy_changes'].append(f"JOINæˆ¦ç•¥å¤‰åŒ–: {original_join_types} â†’ {optimized_join_types}")
        
        # ãƒ—ãƒ©ãƒ³æ§‹é€ ã®è¤‡é›‘åŒ–æ¤œå‡º
        original_plan_depth = original_cost.count('+-')
        optimized_plan_depth = optimized_cost.count('+-')
        
        if optimized_plan_depth > original_plan_depth * 1.3:  # ãƒ—ãƒ©ãƒ³æ·±åº¦ãŒ30%ä»¥ä¸Šå¢—åŠ 
            analysis['plan_structure_changes'].append(f"å®Ÿè¡Œãƒ—ãƒ©ãƒ³è¤‡é›‘åŒ–: æ·±åº¦ {original_plan_depth} â†’ {optimized_plan_depth}")
        
    except Exception as e:
        analysis['analysis_error'] = str(e)
    
    return analysis


def execute_iterative_optimization_with_degradation_analysis(original_query: str, analysis_result: str, metrics: Dict[str, Any], max_optimization_attempts: int = 3) -> Dict[str, Any]:
    """
    Iterative optimization and performance degradation analysis
    Attempt re-optimization up to 3 times by analyzing degradation causes, use original query if no improvement
    """
    from datetime import datetime
    
    print(f"\nğŸš€ Starting iterative optimization process (maximum {max_optimization_attempts} improvement attempts)")
    print("ğŸ¯ Goal: Achieve 10%+ cost reduction | Select best result when maximum attempts reached")
    print("=" * 70)
    
    optimization_attempts = []
    original_query_for_explain = original_query  # å…ƒã‚¯ã‚¨ãƒªã®ä¿æŒ
    
    # ğŸš€ ãƒ™ã‚¹ãƒˆçµæœè¿½è·¡ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ï¼šæœ€å¤§è©¦è¡Œå›æ•°åˆ°é”æ™‚ã¯æœ€ã‚‚è‰¯ã„çµæœã‚’é¸æŠï¼‰
    best_result = {
        'attempt_num': 0,
        'query': original_query,
        'cost_ratio': 1.0,
        'memory_ratio': 1.0,
        'performance_comparison': None,
        'optimized_result': '',
        'status': 'baseline'
    }
    
    for attempt_num in range(1, max_optimization_attempts + 1):
        print(f"\nğŸ”„ Optimization attempt {attempt_num}/{max_optimization_attempts}")
        print("-" * 50)
        
        # å‰å›ã®è©¦è¡Œçµæœã«åŸºã¥ãä¿®æ­£æŒ‡ç¤ºã‚’ç”Ÿæˆ
        fix_instructions = ""
        if attempt_num > 1 and optimization_attempts:
            previous_attempt = optimization_attempts[-1]
            if previous_attempt.get('degradation_analysis'):
                degradation_analysis = previous_attempt['degradation_analysis']
                fix_instructions = "\n".join([
                    f"ã€å‰å›ã®æ‚ªåŒ–åŸå› : {degradation_analysis['primary_cause']}ã€‘",
                    f"ã€ä¿¡é ¼åº¦: {degradation_analysis['confidence_level']}ã€‘",
                    "ã€ä¿®æ­£æŒ‡ç¤ºã€‘"
                ] + degradation_analysis['fix_instructions'])
                
                print(f"ğŸ”§ Degradation cause analysis result: {degradation_analysis['primary_cause']}")
                print(f"ğŸ“Š Confidence level: {degradation_analysis['confidence_level']}")
                print(f"ğŸ’¡ Fix instructions: {len(degradation_analysis['fix_instructions'])} items")
        
        # æœ€é©åŒ–ã‚¯ã‚¨ãƒªç”Ÿæˆï¼ˆåˆå› or ä¿®æ­£ç‰ˆï¼‰
        if attempt_num == 1:
            print("ğŸ¤– Initial optimization query generation")
            optimized_query = generate_optimized_query_with_llm(original_query, analysis_result, metrics)
            # ğŸ› DEBUG: åˆå›è©¦è¡Œã‚¯ã‚¨ãƒªã‚’ä¿å­˜
            if isinstance(optimized_query, str) and not optimized_query.startswith("LLM_ERROR:"):
                save_debug_query_trial(optimized_query, attempt_num, "initial")
        else:
            print(f"ğŸ”§ Corrected optimization query generation (attempt {attempt_num})")
            # ğŸš¨ ä¿®æ­£: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ‚ªåŒ–å°‚ç”¨é–¢æ•°ã‚’ä½¿ç”¨
            previous_attempt = optimization_attempts[-1] if optimization_attempts else {}
            degradation_analysis = previous_attempt.get('degradation_analysis', {})
            optimized_query = generate_improved_query_for_performance_degradation(
                original_query, 
                analysis_result, 
                metrics, 
                degradation_analysis, 
                previous_attempt.get('optimized_query', '')
            )
            # ğŸ› DEBUG: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„è©¦è¡Œã‚¯ã‚¨ãƒªã‚’ä¿å­˜
            if isinstance(optimized_query, str) and not optimized_query.startswith("LLM_ERROR:"):
                degradation_cause = degradation_analysis.get('primary_cause', 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ‚ªåŒ–')
                save_debug_query_trial(optimized_query, attempt_num, "performance_improvement", 
                                     error_info=f"å‰å›æ‚ªåŒ–åŸå› : {degradation_cause}")
        
        # LLMã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        if isinstance(optimized_query, str) and optimized_query.startswith("LLM_ERROR:"):
            print(f"âŒ LLM error occurred in optimization attempt {attempt_num}")
            optimization_attempts.append({
                'attempt': attempt_num,
                'status': 'llm_error',
                'error': optimized_query[10:],
                'optimized_query': None
            })
            continue
        
        # ã‚¯ã‚¨ãƒªæŠ½å‡º
        if isinstance(optimized_query, list):
            optimized_query_str = extract_main_content_from_thinking_response(optimized_query)
        else:
            optimized_query_str = str(optimized_query)
        
        extracted_sql = extract_sql_from_llm_response(optimized_query_str)
        current_query = extracted_sql if extracted_sql else original_query
        
        # EXPLAINå®Ÿè¡Œã¨æ§‹æ–‡ãƒã‚§ãƒƒã‚¯
        explain_result = execute_explain_with_retry_logic(current_query, analysis_result, metrics, max_retries=MAX_RETRIES)
        
        if explain_result['final_status'] != 'success':
            print(f"âš ï¸ Attempt {attempt_num}: EXPLAIN execution failed")
            optimization_attempts.append({
                'attempt': attempt_num,
                'status': 'explain_failed',
                'error': explain_result.get('error_details', 'Unknown error'),
                'optimized_query': current_query
            })
            continue
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒå®Ÿè¡Œ
        print(f"ğŸ” Attempt {attempt_num}: Executing performance degradation detection")
        
        # ğŸ¯ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ï¼ˆé‡è¤‡å‡¦ç†é˜²æ­¢ï¼‰
        corrected_original_query = globals().get('original_query_corrected', original_query)
        if corrected_original_query != original_query:
            print("ğŸ’¾ Using cached original query: Preventing duplicate processing")
        
        # å…ƒã‚¯ã‚¨ãƒªã®EXPLAIN COSTå–å¾—
        original_explain_cost_result = execute_explain_and_save_to_file(corrected_original_query, "original_performance_check")
        
        # æœ€é©åŒ–ã‚¯ã‚¨ãƒªã®EXPLAIN COSTå–å¾—
        optimized_explain_cost_result = execute_explain_and_save_to_file(current_query, f"optimized_attempt_{attempt_num}")
        
        performance_comparison = None
        degradation_analysis = None
        
        # ğŸ” EXPLAIN COSTã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®æ”¹å–„
        original_cost_success = ('explain_cost_file' in original_explain_cost_result and 
                                'error_file' not in original_explain_cost_result)
        optimized_cost_success = ('explain_cost_file' in optimized_explain_cost_result and 
                                 'error_file' not in optimized_explain_cost_result)
        
        # ğŸš¨ ç·Šæ€¥ãƒ‡ãƒãƒƒã‚°: EXPLAIN COSTæˆåŠŸ/å¤±æ•—ã®è©³ç´°è¡¨ç¤º
        print(f"ğŸ” EXPLAIN COST success determination:")
        print(f"   ğŸ“Š Original query: {'âœ… Success' if original_cost_success else 'âŒ Failed'}")
        if not original_cost_success:
            print(f"      â€¢ explain_cost_file exists: {'explain_cost_file' in original_explain_cost_result}")
            print(f"      â€¢ error_file exists: {'error_file' in original_explain_cost_result}")
            print(f"      â€¢ Return keys: {list(original_explain_cost_result.keys())}")
        print(f"   ğŸ”§ Optimized query: {'âœ… Success' if optimized_cost_success else 'âŒ Failed'}")
        if not optimized_cost_success:
            print(f"      â€¢ explain_cost_file exists: {'explain_cost_file' in optimized_explain_cost_result}")
            print(f"      â€¢ error_file exists: {'error_file' in optimized_explain_cost_result}")
            print(f"      â€¢ Return keys: {list(optimized_explain_cost_result.keys())}")
        
        if not original_cost_success:
            print("âš ï¸ Original query EXPLAIN COST execution failed: Skipping performance comparison")
            if 'error_file' in original_explain_cost_result:
                print(f"ğŸ“„ Error details: {original_explain_cost_result['error_file']}")
        
        if not optimized_cost_success:
            print("âš ï¸ Optimized query EXPLAIN COST execution failed: Attempting error correction")
            if 'error_file' in optimized_explain_cost_result:
                print(f"ğŸ“„ Error details: {optimized_explain_cost_result['error_file']}")
                
                # ğŸš¨ CRITICAL FIX: ã‚¨ãƒ©ãƒ¼æ¤œå‡ºæ™‚ã¯å³åº§ã«LLMä¿®æ­£ã‚’å®Ÿè¡Œ
                print("ğŸ”§ Executing LLM-based error correction...")
                error_message = optimized_explain_cost_result.get('error_message', 'Unknown error')
                
                # ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã®ãŸã‚ã®LLMå‘¼ã³å‡ºã—
                corrected_query = generate_optimized_query_with_error_feedback(
                    original_query,
                    analysis_result, 
                    metrics,
                    error_message,
                    current_query  # ç¾åœ¨ã®ã‚¯ã‚¨ãƒªï¼ˆãƒ’ãƒ³ãƒˆä»˜ãï¼‰ã‚’æ¸¡ã™
                )
                
                # ğŸ› DEBUG: ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã‚¯ã‚¨ãƒªã‚’ä¿å­˜
                if isinstance(corrected_query, str) and not corrected_query.startswith("LLM_ERROR:"):
                    save_debug_query_trial(corrected_query, attempt_num, "error_correction", 
                                         error_info=f"ä¿®æ­£å¯¾è±¡ã‚¨ãƒ©ãƒ¼: {error_message[:100]}")
                
                # LLMã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
                if isinstance(corrected_query, str) and corrected_query.startswith("LLM_ERROR:"):
                    print("âŒ Error occurred in LLM correction: Executing fallback evaluation")
                else:
                    # thinking_enabledå¯¾å¿œ
                    if isinstance(corrected_query, list):
                        corrected_query_str = extract_main_content_from_thinking_response(corrected_query)
                    else:
                        corrected_query_str = str(corrected_query)
                    
                    # SQLã‚¯ã‚¨ãƒªéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
                    extracted_sql = extract_sql_from_llm_response(corrected_query_str)
                    if extracted_sql:
                        current_query = extracted_sql
                        print("âœ… LLM-based error correction completed, re-evaluating with corrected query")
                        
                        # ä¿®æ­£ã‚¯ã‚¨ãƒªã§å†åº¦EXPLAINå®Ÿè¡Œ
                        optimized_explain_cost_result = execute_explain_and_save_to_file(current_query, f"optimized_attempt_{attempt_num}_corrected")
                        optimized_cost_success = ('explain_cost_file' in optimized_explain_cost_result and 
                                                'error_file' not in optimized_explain_cost_result)
                        
                        if optimized_cost_success:
                            print("ğŸ¯ Corrected query EXPLAIN execution successful!")
                        else:
                            print("âš ï¸ Error occurred even with corrected query: Executing fallback evaluation")
                    else:
                        print("âŒ Failed to extract SQL query: Executing fallback evaluation")
            
            # ã‚¨ãƒ©ãƒ¼ä¿®æ­£å¾Œã‚‚ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡ã‚’å®Ÿè¡Œ
            if not optimized_cost_success:
                print("ğŸ”„ Executing fallback evaluation")
        
        # ğŸš¨ ç·Šæ€¥ä¿®æ­£: EXPLAIN COSTå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
        if not (original_cost_success and optimized_cost_success):
            print("ğŸ”„ Fallback: Executing simple performance evaluation using EXPLAIN results")
            
            # EXPLAINçµæœãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡
            original_explain_success = ('explain_file' in original_explain_cost_result and 
                                       'error_file' not in original_explain_cost_result)
            optimized_explain_success = ('explain_file' in optimized_explain_cost_result and 
                                        'error_file' not in optimized_explain_cost_result)
            
            if original_explain_success and optimized_explain_success:
                try:
                    # EXPLAINçµæœã‚’èª­ã¿è¾¼ã¿
                    with open(original_explain_cost_result['explain_file'], 'r', encoding='utf-8') as f:
                        original_explain_content = f.read()
                    
                    with open(optimized_explain_cost_result['explain_file'], 'r', encoding='utf-8') as f:
                        optimized_explain_content = f.read()
                    
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡å®Ÿè¡Œ
                    fallback_evaluation = fallback_performance_evaluation(original_explain_content, optimized_explain_content)
                    
                    print(f"ğŸ“Š Fallback evaluation result: {fallback_evaluation['summary']}")
                    print(f"   - Recommendation: {fallback_evaluation['recommendation']}")
                    print(f"   - Confidence: {fallback_evaluation['confidence']}")
                    
                    for detail in fallback_evaluation['details']:
                        print(f"   - {detail}")
                    
                    # performance_comparisonã®ä»£æ›¿ã¨ã—ã¦ä½¿ç”¨
                    performance_comparison = {
                        'is_optimization_beneficial': fallback_evaluation['recommendation'] == 'use_optimized',
                        'performance_degradation_detected': fallback_evaluation['overall_status'] == 'degradation_possible',
                        'significant_improvement_detected': fallback_evaluation['overall_status'] == 'clear_improvement',  # ğŸš¨ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡ã§ã‚‚æ˜ç¢ºæ”¹å–„æ¤œå‡º
                        'recommendation': fallback_evaluation['recommendation'],
                        'evaluation_type': 'fallback_plan_analysis',
                        'details': fallback_evaluation['details'],
                        'fallback_evaluation': fallback_evaluation,
                        'total_cost_ratio': 1.0,  # EXPLAIN COSTãªã—ã®ãŸã‚æœªçŸ¥
                        'memory_usage_ratio': 1.0  # EXPLAIN COSTãªã—ã®ãŸã‚æœªçŸ¥
                    }
                    
                    print("âœ… Fallback performance evaluation completed")
                    
                    # ğŸš¨ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡ã§ã‚‚å³æ ¼åˆ¤å®šé©ç”¨
                    if not performance_comparison.get('significant_improvement_detected', False):
                        if performance_comparison['performance_degradation_detected']:
                            print(f"ğŸš¨ Attempt {attempt_num}: Possibility of degradation in fallback evaluation")
                            status_reason = "fallback_degradation_detected"
                        else:
                            print(f"âš ï¸ Attempt {attempt_num}: Clear improvement not confirmed in fallback evaluation")
                            status_reason = "fallback_insufficient_improvement"
                        
                        optimization_attempts.append({
                            'attempt': attempt_num,
                            'status': status_reason,
                            'optimized_query': current_query,
                            'performance_comparison': performance_comparison,
                            'cost_ratio': performance_comparison['total_cost_ratio'],
                            'memory_ratio': performance_comparison['memory_usage_ratio']
                        })
                        
                        # ğŸš€ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡ã§ã‚‚ãƒ™ã‚¹ãƒˆçµæœè¿½è·¡
                        current_cost_ratio = performance_comparison.get('total_cost_ratio', 1.0)
                        current_memory_ratio = performance_comparison.get('memory_usage_ratio', 1.0)
                        
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡ã§ã¯æ”¹å–„ã®å ´åˆã®ã¿ãƒ™ã‚¹ãƒˆæ›´æ–°ï¼ˆä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ï¼‰
                        if performance_comparison.get('significant_improvement_detected', False):
                            is_better_than_best = (
                                current_cost_ratio < best_result['cost_ratio'] or 
                                (current_cost_ratio == best_result['cost_ratio'] and current_memory_ratio < best_result['memory_ratio'])
                            )
                            
                            if is_better_than_best:
                                print(f"ğŸ† Attempt {attempt_num}: New best result in fallback evaluation!")
                                best_result.update({
                                    'attempt_num': attempt_num,
                                    'query': current_query,
                                    'cost_ratio': current_cost_ratio,
                                    'memory_ratio': current_memory_ratio,
                                    'performance_comparison': performance_comparison,
                                    'optimized_result': optimized_query_str,
                                    'status': 'fallback_improved'
                                })
                        
                        optimization_attempts.append({
                            'attempt': attempt_num,
                            'status': status_reason,
                            'optimized_query': current_query,
                            'performance_comparison': performance_comparison,
                            'cost_ratio': current_cost_ratio,
                            'memory_ratio': current_memory_ratio
                        })
                        
                        # ğŸš€ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡ã§ã¯å¤§å¹…æ”¹å–„åˆ¤å®šãŒå›°é›£ãªãŸã‚ã€è©¦è¡Œç¶™ç¶š
                        if attempt_num < max_optimization_attempts:
                            print(f"ğŸ”„ Aiming for more reliable improvement in attempt {attempt_num + 1} (fallback evaluation)")
                            continue
                        else:
                            print(f"â° Maximum attempts ({max_optimization_attempts}) reached â†’ Selecting best result")
                            break
                    
                except Exception as e:
                    print(f"âŒ Error in fallback evaluation as well: {str(e)}")
                    print(f"   ğŸ“Š Error details: {type(e).__name__}")
                    if hasattr(e, '__traceback__'):
                        import traceback
                        print(f"   ğŸ“„ Stack trace: {traceback.format_exc()}")
                    performance_comparison = None
            else:
                print("âŒ EXPLAIN results also insufficient, performance evaluation impossible")
                performance_comparison = None
        
        # ğŸš¨ ç·Šæ€¥ä¿®æ­£: ãƒ­ã‚¸ãƒƒã‚¯é †åºã‚’ä¿®æ­£ï¼ˆEXPLAIN COSTæˆåŠŸåˆ¤å®šã‚’å…ˆã«å®Ÿè¡Œï¼‰
        if (original_cost_success and optimized_cost_success):
            
            try:
                print(f"ğŸ¯ Both EXPLAIN COST successful â†’ Executing performance comparison")
                
                # EXPLAIN COSTå†…å®¹ã‚’èª­ã¿è¾¼ã¿
                with open(original_explain_cost_result['explain_cost_file'], 'r', encoding='utf-8') as f:
                    original_cost_content = f.read()
                
                with open(optimized_explain_cost_result['explain_cost_file'], 'r', encoding='utf-8') as f:
                    optimized_cost_content = f.read()
                
                print(f"   ğŸ“Š Original query COST content length: {len(original_cost_content)} characters")
                print(f"   ğŸ”§ Optimized query COST content length: {len(optimized_cost_content)} characters")
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒå®Ÿè¡Œ
                print(f"ğŸ” Executing compare_query_performance...")
                performance_comparison = compare_query_performance(original_cost_content, optimized_cost_content)
                print(f"âœ… compare_query_performance completed: {performance_comparison is not None}")
                
                if performance_comparison:
                    print(f"   ğŸ“Š significant_improvement_detected: {performance_comparison.get('significant_improvement_detected', 'UNKNOWN')}")
                    print(f"   ğŸ“Š performance_degradation_detected: {performance_comparison.get('performance_degradation_detected', 'UNKNOWN')}")
                    print(f"   ğŸ“Š is_optimization_beneficial: {performance_comparison.get('is_optimization_beneficial', 'UNKNOWN')}")
                else:
                    print(f"âŒ performance_comparison is None!")
                
                # ğŸš€ ãƒ™ã‚¹ãƒˆçµæœæ›´æ–°åˆ¤å®šï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ï¼šå¸¸ã«æœ€è‰¯çµæœã‚’è¿½è·¡ï¼‰
                current_cost_ratio = performance_comparison['total_cost_ratio']
                current_memory_ratio = performance_comparison['memory_usage_ratio']
                
                # ç¾åœ¨ã®çµæœãŒãƒ™ã‚¹ãƒˆã‚’ä¸Šå›ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆã‚³ã‚¹ãƒˆæ¯”ç‡ãŒä½ã„ã»ã©è‰¯ã„ï¼‰
                is_better_than_best = (
                    current_cost_ratio < best_result['cost_ratio'] or 
                    (current_cost_ratio == best_result['cost_ratio'] and current_memory_ratio < best_result['memory_ratio'])
                )
                
                if is_better_than_best:
                    print(f"ğŸ† Attempt {attempt_num}: New best result recorded!")
                    print(f"   ğŸ“Š Cost ratio: {best_result['cost_ratio']:.3f} â†’ {current_cost_ratio:.3f}")
                    print(f"   ğŸ’¾ Memory ratio: {best_result['memory_ratio']:.3f} â†’ {current_memory_ratio:.3f}")
                    best_result.update({
                        'attempt_num': attempt_num,
                        'query': current_query,
                        'cost_ratio': current_cost_ratio,
                        'memory_ratio': current_memory_ratio,
                        'performance_comparison': performance_comparison,
                        'optimized_result': optimized_query_str,
                        'status': 'improved'
                    })
                
                # ğŸš€ å¤§å¹…æ”¹å–„ï¼ˆ10%ä»¥ä¸Šï¼‰é”æˆã§å³åº§ã«çµ‚äº†
                if performance_comparison.get('substantial_improvement_detected', False):
                    print(f"ğŸš€ Attempt {attempt_num}: Significant improvement achieved (10%+ reduction)! Optimization completed immediately")
                    optimization_attempts.append({
                        'attempt': attempt_num,
                        'status': 'substantial_success',
                        'optimized_query': current_query,
                        'performance_comparison': performance_comparison,
                        'cost_ratio': current_cost_ratio,
                        'memory_ratio': current_memory_ratio
                    })
                    
                    return {
                        'final_status': 'optimization_success',
                        'final_query': current_query,
                        'successful_attempt': attempt_num,
                        'total_attempts': attempt_num,
                        'optimization_attempts': optimization_attempts,
                        'performance_comparison': performance_comparison,
                        'optimized_result': optimized_query_str,
                        'saved_files': None,  # ãƒ¡ã‚¤ãƒ³å‡¦ç†ã§ä¿å­˜
                        'achievement_type': 'substantial_improvement'
                    }
                
                # ğŸš€ æ”¹å–„ã¯ã‚ã‚‹ãŒå¤§å¹…ã§ãªã„å ´åˆã®åˆ¤å®š
                elif performance_comparison.get('significant_improvement_detected', False):
                    print(f"âœ… Attempt {attempt_num}: Improvement confirmed (target 10% not reached)")
                    status_reason = "partial_improvement"
                else:
                    # æ”¹å–„ãªã—ã¾ãŸã¯æ‚ªåŒ–ã®å ´åˆ
                    if performance_comparison['performance_degradation_detected']:
                        print(f"ğŸš¨ Attempt {attempt_num}: Performance increase detected")
                        status_reason = "performance_degraded"
                    else:
                        print(f"âš ï¸ Attempt {attempt_num}: Clear improvement cannot be confirmed")
                        status_reason = "insufficient_improvement"
                
                # æ‚ªåŒ–åŸå› åˆ†æï¼ˆæ”¹å–„ä¸è¶³ã®å ´åˆã‚‚å®Ÿè¡Œï¼‰
                degradation_analysis = analyze_degradation_causes(performance_comparison, original_cost_content, optimized_cost_content)
                
                print(f"   Details: {', '.join(performance_comparison.get('details', []))}")
                
                optimization_attempts.append({
                    'attempt': attempt_num,
                    'status': status_reason,
                    'optimized_query': current_query,
                    'performance_comparison': performance_comparison,
                    'degradation_analysis': degradation_analysis,
                    'cost_ratio': current_cost_ratio,
                    'memory_ratio': current_memory_ratio
                })
                
                # ğŸš€ æ–°åˆ¤å®š: å¤§å¹…æ”¹å–„ï¼ˆ10%ä»¥ä¸Šï¼‰ã§ãªã„é™ã‚Šè©¦è¡Œç¶™ç¶š
                if attempt_num < max_optimization_attempts:
                    print(f"ğŸ”„ Aiming for significant improvement (10%+ reduction) in attempt {attempt_num + 1}")
                    continue
                else:
                    print(f"â° Maximum attempts ({max_optimization_attempts}) reached â†’ Selecting best result")
                    break
            
            except Exception as e:
                print(f"âŒ Attempt {attempt_num}: Error in performance comparison: {str(e)}")
                print(f"   ğŸ“Š Error type: {type(e).__name__}")
                if hasattr(e, '__traceback__'):
                    import traceback
                    print(f"   ğŸ“„ Stack trace: {traceback.format_exc()}")
                print(f"ğŸš¨ This error is the cause of 'Performance evaluation impossible'!")
                optimization_attempts.append({
                    'attempt': attempt_num,
                    'status': 'comparison_error',
                    'error': str(e),
                    'optimized_query': current_query
                })
                continue
        
        # ğŸš¨ ç·Šæ€¥ä¿®æ­£: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ãŒå®Œå…¨ã«å¤±æ•—ã—ãŸå ´åˆã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆãƒ­ã‚¸ãƒƒã‚¯é †åºä¿®æ­£å¾Œï¼‰
        elif performance_comparison is None:
            print(f"ğŸš¨ Attempt {attempt_num}: Performance evaluation impossible, proceeding to next attempt")
            
            optimization_attempts.append({
                'attempt': attempt_num,
                'status': 'performance_evaluation_failed',
                'optimized_query': current_query,
                'performance_comparison': None,
                'error': 'EXPLAIN COSTå®Ÿè¡Œå¤±æ•—ã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡å¤±æ•—',
                'cost_ratio': None,
                'memory_ratio': None
            })
            
            # æœ€å¾Œã®è©¦è¡Œã§ãªã„å ´åˆã¯æ¬¡ã®æ”¹å–„ã‚’è©¦è¡Œ
            if attempt_num < max_optimization_attempts:
                print(f"ğŸ”„ Will retry performance evaluation in attempt {attempt_num + 1}")
                continue
            else:
                print(f"âŒ Maximum attempts ({max_optimization_attempts}) reached, using original query")
                break
        
        else:
            print(f"âš ï¸ Attempt {attempt_num}: EXPLAIN COST acquisition failed, using syntactically normal optimized query")
            optimization_attempts.append({
                'attempt': attempt_num,
                'status': 'explain_cost_failed',
                'optimized_query': current_query,
                'note': 'EXPLAIN COST comparison skipped due to execution failure'
            })
            
            # ğŸš¨ ä¿®æ­£: EXPLAIN COSTãŒå–å¾—ã§ããªã„å ´åˆã‚‚é‡è¤‡ä¿å­˜ã‚’é˜²æ­¢
            # saved_files = save_optimized_sql_files(...)  # â† é‡è¤‡é˜²æ­¢ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
            
            return {
                'final_status': 'partial_success',
                'final_query': current_query,
                'successful_attempt': attempt_num,
                'total_attempts': attempt_num,
                'optimization_attempts': optimization_attempts,
                'optimized_result': optimized_query_str,  # ğŸ”§ ãƒ¡ã‚¤ãƒ³å‡¦ç†ã§ã®ä¿å­˜ç”¨ã«è¿½åŠ 
                'saved_files': None,  # ğŸ”§ ãƒ¡ã‚¤ãƒ³å‡¦ç†ã§ä¿å­˜ã™ã‚‹ãŸã‚None
                'note': 'Performance comparison unavailable but query is syntactically valid'
            }
    
    # ğŸš€ æœ€å¤§è©¦è¡Œå›æ•°åˆ°é”ï¼šãƒ™ã‚¹ãƒˆçµæœã‚’æœ€çµ‚ã‚¯ã‚¨ãƒªã¨ã—ã¦é¸æŠ
    print(f"\nâ° All {max_optimization_attempts} optimization attempts completed")
    print("ğŸ† Selecting best result as final query")
    print("=" * 60)
    
    # ãƒ™ã‚¹ãƒˆçµæœã®è©³ç´°è¡¨ç¤º
    if best_result['attempt_num'] > 0:
        print(f"ğŸ¥‡ Selected best result: Attempt {best_result['attempt_num']}")
        print(f"   ğŸ“Š Cost ratio: {best_result['cost_ratio']:.3f} (Improvement: {(1-best_result['cost_ratio'])*100:.1f}%)")
        print(f"   ğŸ’¾ Memory ratio: {best_result['memory_ratio']:.3f} (Improvement: {(1-best_result['memory_ratio'])*100:.1f}%)")
        
        final_query = best_result['query']
        final_optimized_result = best_result['optimized_result']
        final_performance_comparison = best_result['performance_comparison']
        final_status = 'optimization_success'
        achievement_type = 'best_of_trials'
        
        print(f"âœ… Adopting best result as optimized query")
        
    else:
        print(f"âš ï¸ Using original query due to errors or evaluation failures in all attempts")
        
        # è©¦è¡Œçµæœã‚µãƒãƒªãƒ¼
        failure_summary = []
        for attempt in optimization_attempts:
            if attempt['status'] == 'performance_degraded':
                failure_summary.append(f"è©¦è¡Œ{attempt['attempt']}: {attempt.get('degradation_analysis', {}).get('primary_cause', 'unknown')} (ã‚³ã‚¹ãƒˆæ¯”: {attempt.get('cost_ratio', 'N/A')})")
            elif attempt['status'] == 'llm_error':
                failure_summary.append(f"è©¦è¡Œ{attempt['attempt']}: LLMã‚¨ãƒ©ãƒ¼")
            elif attempt['status'] == 'explain_failed':
                failure_summary.append(f"è©¦è¡Œ{attempt['attempt']}: EXPLAINå®Ÿè¡Œå¤±æ•—")
            elif attempt['status'] == 'comparison_error':
                failure_summary.append(f"è©¦è¡Œ{attempt['attempt']}: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã‚¨ãƒ©ãƒ¼")
            else:
                failure_summary.append(f"è©¦è¡Œ{attempt['attempt']}: {attempt['status']}")
        
        failure_report = f"""# âš ï¸ å…¨æœ€é©åŒ–è©¦è¡Œå®Œäº†ã®ãŸã‚ã€å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨

## æœ€é©åŒ–è©¦è¡Œçµæœ

{chr(10).join(failure_summary) if failure_summary else "å…¨ã¦ã®è©¦è¡Œã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ"}

## æœ€çµ‚åˆ¤æ–­

{max_optimization_attempts}å›ã®æœ€é©åŒ–è©¦è¡Œã‚’å®Ÿè¡Œã—ã¾ã—ãŸãŒã€10%ä»¥ä¸Šã®å¤§å¹…æ”¹å–„ã«ã¯åˆ°é”ã›ãšã€
ãƒ™ã‚¹ãƒˆçµæœã‚‚å…ƒã‚¯ã‚¨ãƒªã‚’ä¸Šå›ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚

## å…ƒã®ã‚¯ã‚¨ãƒª

```sql
{original_query}
```

## æ¨å¥¨äº‹é …

- ãƒ‡ãƒ¼ã‚¿é‡ã‚„ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆæƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„
- ã‚ˆã‚Šè©³ç´°ãªEXPLAINæƒ…å ±ã‚’å–å¾—ã—ã¦æ‰‹å‹•æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„  
- Liquid Clusteringã‚„ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆã®æ›´æ–°ã‚’æ¤œè¨ã—ã¦ãã ã•ã„
"""
        
        final_query = original_query
        final_optimized_result = failure_report
        final_performance_comparison = None
        final_status = 'optimization_failed'
        achievement_type = 'no_improvement'
    
    return {
        'final_status': final_status,
        'final_query': final_query,
        'total_attempts': len(optimization_attempts),
        'optimization_attempts': optimization_attempts,
        'performance_comparison': final_performance_comparison,
        'optimized_result': final_optimized_result,
        'saved_files': None,  # ãƒ¡ã‚¤ãƒ³å‡¦ç†ã§ä¿å­˜
        'best_result': best_result,
        'achievement_type': achievement_type,
        'fallback_reason': f'Best result from {max_optimization_attempts} attempts selected' if best_result['attempt_num'] > 0 else 'All attempts failed or degraded'
    }


def execute_explain_with_retry_logic(original_query: str, analysis_result: str, metrics: Dict[str, Any], max_retries: int = 3) -> Dict[str, Any]:
    """
    EXPLAIN execution and error correction retry logic (syntax errors only)
    Attempts automatic correction up to max_retries times, uses original query on failure
    """
    from datetime import datetime
    
    print(f"\nğŸ”„ EXPLAIN execution and automatic error correction (max {max_retries} attempts)")
    print("=" * 60)
    
    # Initial optimization query generation
    print("ğŸ¤– Step 1: Initial optimization query generation")
    optimized_query = generate_optimized_query_with_llm(original_query, analysis_result, metrics)
    
    # ğŸ› DEBUG: å˜ä½“æœ€é©åŒ–ã‚¯ã‚¨ãƒªã‚’ä¿å­˜ï¼ˆåå¾©æœ€é©åŒ–ä»¥å¤–ã®ãƒ‘ã‚¹ï¼‰
    if isinstance(optimized_query, str) and not optimized_query.startswith("LLM_ERROR:"):
        save_debug_query_trial(optimized_query, 1, "single_optimization", query_id="direct_path")
    
    # LLMã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¦ï¼‰
    if isinstance(optimized_query, str) and optimized_query.startswith("LLM_ERROR:"):
        print("âŒ Error occurred in LLM optimization, using original query")
        print(f"ğŸ”§ Error details: {optimized_query[10:]}")  # Remove "LLM_ERROR:"
        
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ã—ã¦å³åº§ã«ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
        fallback_result = save_optimized_sql_files(
            original_query,
            f"# âŒ LLMæœ€é©åŒ–ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚ã€å…ƒã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨\n\n## ã‚¨ãƒ©ãƒ¼è©³ç´°\n{optimized_query[10:]}\n\n## å…ƒã®ã‚¯ã‚¨ãƒª\n```sql\n{original_query}\n```",
            metrics,
            analysis_result,
            "",  # llm_response
            None  # performance_comparison
        )
        
        return {
            'final_status': 'llm_error',
            'final_query': original_query,
            'total_attempts': 0,
            'all_attempts': [],
            'explain_result': None,
            'optimized_result': optimized_query,
            'error_details': optimized_query[10:]
        }
    
    # thinking_enabledå¯¾å¿œ: ãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆã¯ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
    if isinstance(optimized_query, list):
        optimized_query_str = extract_main_content_from_thinking_response(optimized_query)
    else:
        optimized_query_str = str(optimized_query)
    
    # SQLã‚¯ã‚¨ãƒªéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
    extracted_sql = extract_sql_from_llm_response(optimized_query_str)
    current_query = extracted_sql if extracted_sql else original_query
    
    retry_count = 0
    all_attempts = []  # å…¨è©¦è¡Œã®è¨˜éŒ²
    
    while retry_count <= max_retries:
        attempt_num = retry_count + 1
        print(f"\nğŸ” Attempt {attempt_num}/{max_retries + 1}: EXPLAIN execution")
        
        # EXPLAINå®Ÿè¡Œï¼ˆæœ€é©åŒ–å¾Œã‚¯ã‚¨ãƒªï¼‰
        explain_result = execute_explain_and_save_to_file(current_query, "optimized")
        
        # æˆåŠŸæ™‚ã®å‡¦ç†
        if 'explain_file' in explain_result and 'error_file' not in explain_result:
            print(f"âœ… Succeeded in attempt {attempt_num}!")
            
            # ğŸš¨ ä¿®æ­£ï¼šãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã¯åå¾©æœ€é©åŒ–é–¢æ•°ã§ä¸€å…ƒåŒ–
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã‚’ã“ã“ã§å®Ÿè¡Œã™ã‚‹ã¨äºŒé‡å®Ÿè¡Œã«ãªã‚‹ãŸã‚å‰Šé™¤
            
            # ğŸš¨ ä¿®æ­£ï¼šä»¥ä¸‹ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã‚’ç„¡åŠ¹åŒ–ï¼ˆäºŒé‡å®Ÿè¡Œé˜²æ­¢ï¼‰
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã¯ execute_iterative_optimization_with_degradation_analysis ã§ä¸€å…ƒåŒ–
            
            # ğŸ”§ æ§‹æ–‡ãƒã‚§ãƒƒã‚¯æˆåŠŸã®ãŸã‚ã€å³åº§ã« success ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã§æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸
            performance_comparison = None  # åå¾©æœ€é©åŒ–ã§è¨­å®šã•ã‚Œã‚‹
            
            if False:  # ğŸš¨ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ–ãƒ­ãƒƒã‚¯ã‚’ç„¡åŠ¹åŒ–
                
                try:
                    # EXPLAIN COSTãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿è¾¼ã¿
                    with open(original_explain_cost_result['explain_cost_file'], 'r', encoding='utf-8') as f:
                        original_cost_content = f.read()
                    
                    with open(optimized_explain_cost_result['explain_cost_file'], 'r', encoding='utf-8') as f:
                        optimized_cost_content = f.read()
                    
                    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒå®Ÿè¡Œ
                    performance_comparison = compare_query_performance(original_cost_content, optimized_cost_content)
                    
                    print(f"ğŸ“Š Performance comparison results:")
                    print(f"   - Execution cost ratio: {performance_comparison['total_cost_ratio']:.2f}x")
                    print(f"   - Memory usage ratio: {performance_comparison['memory_usage_ratio']:.2f}x")
                    print(f"   - Recommendation: {performance_comparison['recommendation']}")
                    
                    for detail in performance_comparison['details']:
                        print(f"   - {detail}")
                    
                    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ‚ªåŒ–ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆ
                    if performance_comparison['performance_degradation_detected']:
                        print("ğŸš¨ Performance degradation detected! Using original query")
                        
                        # å…ƒã‚¯ã‚¨ãƒªã§ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ‚ªåŒ–é˜²æ­¢ï¼‰
                        fallback_result = save_optimized_sql_files(
                            original_query,
                            f"# ğŸš¨ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ‚ªåŒ–æ¤œå‡ºã®ãŸã‚å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨\n\n## æ‚ªåŒ–è¦å› \n{'; '.join(performance_comparison['details'])}\n\n## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒçµæœ\n- å®Ÿè¡Œã‚³ã‚¹ãƒˆæ¯”: {performance_comparison['total_cost_ratio']:.2f}å€\n- ãƒ¡ãƒ¢ãƒªä½¿ç”¨æ¯”: {performance_comparison['memory_usage_ratio']:.2f}å€\n\n## å…ƒã®ã‚¯ã‚¨ãƒªï¼ˆæœ€é©åŒ–å‰ï¼‰\n```sql\n{original_query}\n```",
                            metrics,
                            analysis_result,
                            "",  # llm_response
                            performance_comparison  # ğŸ” è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒçµæœã‚’å«ã‚ã‚‹
                        )
                        
                        return {
                            'final_status': 'performance_degradation_detected',
                            'final_query': original_query,
                            'total_attempts': attempt_num,
                            'all_attempts': all_attempts,
                            'explain_result': original_explain_cost_result,
                            'optimized_result': optimized_query,
                            'performance_comparison': performance_comparison,
                            'fallback_reason': 'performance_degradation'
                        }
                    
                    else:
                        print("âœ… Performance improvement confirmed. Using optimized query")
                    
                except Exception as e:
                    print(f"âš ï¸ Error occurred in performance comparison: {str(e)}")
                    print("ğŸ”„ Using original query for safety")
                    
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚å®‰å…¨å´ã«å€’ã—ã¦å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨
                    fallback_result = save_optimized_sql_files(
                        original_query,
                        f"# âš ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã‚¨ãƒ©ãƒ¼ã®ãŸã‚å®‰å…¨æ€§ã‚’å„ªå…ˆã—ã¦å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨\n\n## ã‚¨ãƒ©ãƒ¼è©³ç´°\n{str(e)}\n\n## å…ƒã®ã‚¯ã‚¨ãƒª\n```sql\n{original_query}\n```",
                        metrics,
                        analysis_result,
                        "",  # llm_response
                        None  # performance_comparison
                    )
                    
                    return {
                        'final_status': 'performance_comparison_error',
                        'final_query': original_query,
                        'total_attempts': attempt_num,
                        'all_attempts': all_attempts,
                        'explain_result': explain_result,
                        'optimized_result': optimized_query,
                        'fallback_reason': 'performance_comparison_error',
                        'error_details': str(e)
                    }
            
            # ğŸš¨ ä¿®æ­£ï¼šelseéƒ¨åˆ†ã‚‚ç„¡åŠ¹åŒ–ï¼ˆäºŒé‡å®Ÿè¡Œé˜²æ­¢ï¼‰
            # else:
            #     print("âš ï¸ Skipping performance comparison due to EXPLAIN COST acquisition failure")
#     print("ğŸ”„ Using syntactically valid optimized query")
            
            # æˆåŠŸè¨˜éŒ²
            attempt_record = {
                'attempt': attempt_num,
                'status': 'success',
                'query': current_query,
                'explain_file': explain_result.get('explain_file'),
                'plan_lines': explain_result.get('plan_lines', 0),
                'performance_comparison': performance_comparison
            }
            all_attempts.append(attempt_record)
            
            # æœ€çµ‚çµæœï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ‚ªåŒ–ãªã—ã®å ´åˆï¼‰
            return {
                'final_status': 'success',
                'final_query': current_query,
                'total_attempts': attempt_num,
                'all_attempts': all_attempts,
                'explain_result': explain_result,
                'optimized_result': optimized_query,  # å…ƒã®å®Œå…¨ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹
                'performance_comparison': performance_comparison
            }
        
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®å‡¦ç†
        elif 'error_file' in explain_result:
            error_message = explain_result.get('error_message', 'Unknown error')
            print(f"âŒ Error occurred in attempt {attempt_num}: {error_message}")
            
            # ã‚¨ãƒ©ãƒ¼è¨˜éŒ²
            attempt_record = {
                'attempt': attempt_num,
                'status': 'error',
                'query': current_query,
                'error_message': error_message,
                'error_file': explain_result.get('error_file')
            }
            all_attempts.append(attempt_record)
            
            # æœ€å¤§è©¦è¡Œå›æ•°ã«é”ã—ãŸå ´åˆ
            if retry_count >= max_retries:
                print(f"ğŸš¨ Maximum number of attempts ({max_retries}) reached")
                print("ğŸ“‹ Using original working query")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…ƒã‚¯ã‚¨ãƒªã§ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
                fallback_result = save_optimized_sql_files(
                    original_query, 
                    f"# ğŸš¨ æœ€é©åŒ–ã‚¯ã‚¨ãƒªã®EXPLAINå®Ÿè¡ŒãŒ{max_retries}å›ã¨ã‚‚å¤±æ•—ã—ãŸãŸã‚ã€å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨\n\n## æœ€å¾Œã®ã‚¨ãƒ©ãƒ¼æƒ…å ±\n{error_message}\n\n## å…ƒã®ã‚¯ã‚¨ãƒª\n```sql\n{original_query}\n```",
                    metrics,
                    analysis_result,
                    "",  # llm_response
                    None  # performance_comparison
                )
                
                # å¤±æ•—æ™‚ã®ãƒ­ã‚°è¨˜éŒ²
                timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                log_filename = f"output_optimization_failure_log_{timestamp}.txt"
                
                try:
                    with open(log_filename, 'w', encoding='utf-8') as f:
                        f.write(f"# æœ€é©åŒ–ã‚¯ã‚¨ãƒªç”Ÿæˆå¤±æ•—ãƒ­ã‚°\n")
                        f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                        f.write(f"æœ€å¤§è©¦è¡Œå›æ•°: {max_retries}å›\n")
                        f.write(f"æœ€çµ‚çµæœ: å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨\n\n")
                        
                        f.write("=" * 80 + "\n")
                        f.write("å…¨è©¦è¡Œã®è©³ç´°è¨˜éŒ²:\n")
                        f.write("=" * 80 + "\n\n")
                        
                        for attempt in all_attempts:
                            f.write(f"ã€è©¦è¡Œ {attempt['attempt']}ã€‘\n")
                            f.write(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {attempt['status']}\n")
                            if attempt['status'] == 'error':
                                f.write(f"ã‚¨ãƒ©ãƒ¼: {attempt['error_message']}\n")
                                f.write(f"ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«: {attempt.get('error_file', 'N/A')}\n")
                            f.write(f"ä½¿ç”¨ã‚¯ã‚¨ãƒªé•·: {len(attempt['query'])} æ–‡å­—\n\n")
                        
                        f.write("=" * 80 + "\n")
                        f.write("å…ƒã®ã‚¯ã‚¨ãƒªï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½¿ç”¨ï¼‰:\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(original_query)
                    
                    print(f"ğŸ“„ Saved failure log: {log_filename}")
                    
                except Exception as log_error:
                                            print(f"âŒ Failed to save failure log as well: {str(log_error)}")
                
                return {
                    'final_status': 'fallback_to_original',
                    'final_query': original_query,
                    'total_attempts': attempt_num,
                    'all_attempts': all_attempts,
                    'fallback_files': fallback_result,
                    'failure_log': log_filename
                }
            
            # å†è©¦è¡Œã™ã‚‹å ´åˆã®ã‚¨ãƒ©ãƒ¼ä¿®æ­£
            retry_count += 1
            print(f"ğŸ”§ Correcting error for attempt {retry_count + 1}...")
            
            # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å«ã‚ã¦å†ç”Ÿæˆï¼ˆåˆå›æœ€é©åŒ–ã‚¯ã‚¨ãƒªã‚‚æ¸¡ã™ï¼‰
            corrected_query = generate_optimized_query_with_error_feedback(
                original_query, 
                analysis_result, 
                metrics, 
                error_message,
                current_query  # ğŸš€ åˆå›æœ€é©åŒ–ã‚¯ã‚¨ãƒªï¼ˆãƒ’ãƒ³ãƒˆä»˜ãï¼‰ã‚’æ¸¡ã™
            )
            
            # ğŸ› DEBUG: å†è©¦è¡Œæ™‚ã®ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã‚¯ã‚¨ãƒªã‚’ä¿å­˜
            if isinstance(corrected_query, str) and not corrected_query.startswith("LLM_ERROR:"):
                save_debug_query_trial(corrected_query, retry_count + 1, "retry_error_correction", 
                                     query_id=f"retry_{retry_count + 1}", 
                                     error_info=f"å†è©¦è¡Œ{retry_count + 1}ã®ã‚¨ãƒ©ãƒ¼ä¿®æ­£: {error_message[:100]}")
            
            # LLMã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¨ãƒ©ãƒ¼ä¿®æ­£æ™‚ï¼‰
            if isinstance(corrected_query, str) and corrected_query.startswith("LLM_ERROR:"):
                print("âŒ LLM error occurred even in error correction, using original query")
                print(f"ğŸ”§ Error details: {corrected_query[10:]}")  # Remove "LLM_ERROR:"
                
                # å¤±æ•—è¨˜éŒ²
                attempt_record = {
                    'attempt': retry_count + 1,
                    'status': 'llm_error_correction_failed',
                    'query': current_query,
                    'error_message': f"ã‚¨ãƒ©ãƒ¼ä¿®æ­£æ™‚LLMã‚¨ãƒ©ãƒ¼: {corrected_query[10:]}",
                    'error_file': None
                }
                all_attempts.append(attempt_record)
                
                # å…ƒã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
                fallback_result = save_optimized_sql_files(
                    original_query,
                    f"# âŒ ã‚¨ãƒ©ãƒ¼ä¿®æ­£æ™‚ã‚‚LLMã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚ã€å…ƒã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨\n\n## ã‚¨ãƒ©ãƒ¼ä¿®æ­£æ™‚ã®ã‚¨ãƒ©ãƒ¼è©³ç´°\n{corrected_query[10:]}\n\n## å…ƒã®ã‚¯ã‚¨ãƒª\n```sql\n{original_query}\n```",
                    metrics,
                    analysis_result,
                    "",  # llm_response
                    None  # performance_comparison
                )
                
                return {
                    'final_status': 'llm_error_correction_failed',
                    'final_query': original_query,
                    'total_attempts': retry_count + 1,
                    'all_attempts': all_attempts,
                    'explain_result': None,
                    'optimized_result': corrected_query,
                    'error_details': corrected_query[10:]
                }
            
            # thinking_enabledå¯¾å¿œ
            if isinstance(corrected_query, list):
                corrected_query_str = extract_main_content_from_thinking_response(corrected_query)
            else:
                corrected_query_str = str(corrected_query)
            
            # SQLã‚¯ã‚¨ãƒªéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
            extracted_sql = extract_sql_from_llm_response(corrected_query_str)
            current_query = extracted_sql if extracted_sql else current_query
            
            print(f"âœ… Generated error correction query ({len(current_query)} characters)")
    
    # ã“ã“ã«ã¯åˆ°é”ã—ãªã„ã¯ãšã ãŒã€å®‰å…¨ã®ãŸã‚
    return {
        'final_status': 'unexpected_error',
        'final_query': original_query,
        'total_attempts': retry_count + 1,
        'all_attempts': all_attempts
    }


def extract_sql_from_llm_response(llm_response: str) -> str:
    """
    Extract only SQL query part from LLM response
    """
    import re
    
    # SQLã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¤œç´¢ï¼ˆ```sql ... ```ï¼‰
    sql_pattern = r'```sql\s*(.*?)\s*```'
    matches = re.findall(sql_pattern, llm_response, re.DOTALL | re.IGNORECASE)
    
    if matches:
        # æœ€é•·ã®SQLãƒ–ãƒ­ãƒƒã‚¯ã‚’é¸æŠ
        sql_query = max(matches, key=len).strip()
        return sql_query
    
    # SQLã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€åˆ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
    # ```ã®ã¿ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯
    code_pattern = r'```\s*(.*?)\s*```'
    matches = re.findall(code_pattern, llm_response, re.DOTALL)
    
    for match in matches:
        match = match.strip()
        # SQLã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§å§‹ã¾ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if re.match(r'^(SELECT|WITH|CREATE|INSERT|UPDATE|DELETE|EXPLAIN)', match, re.IGNORECASE):
            return match
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã—ãªã„å ´åˆã¯å…ƒã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãã®ã¾ã¾è¿”ã™
    return llm_response.strip()


def execute_explain_and_save_to_file(original_query: str, query_type: str = "original") -> Dict[str, str]:
    """
    Execute EXPLAIN and EXPLAIN COST statements for queries and save results to file based on EXPLAIN_ENABLED setting
    For CTAS, extract only the SELECT part and pass it to EXPLAIN statement
    
    Args:
        original_query: Query to execute EXPLAIN on
        query_type: "original" or "optimized" to identify filename
    """
    from datetime import datetime
    import os
    
    if not original_query or not original_query.strip():
        print("âŒ Query is empty")
        return {}
    
    # EXPLAIN_ENABLEDè¨­å®šã‚’ç¢ºèª
    explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
    debug_enabled = globals().get('DEBUG_ENABLED', 'N')
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆï¼ˆEXPLAIN_ENABLED=Yã®å ´åˆã®ã¿ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    if explain_enabled.upper() == 'Y':
        explain_filename = f"output_explain_{query_type}_{timestamp}.txt"
        explain_cost_filename = f"output_explain_cost_{query_type}_{timestamp}.txt"
    else:
        explain_filename = None
        explain_cost_filename = None
    
    # CTASã®å ´åˆã¯SELECTéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
    query_for_explain = extract_select_from_ctas(original_query)
    
    # EXPLAINæ–‡ã¨EXPLAIN COSTæ–‡ã®ç”Ÿæˆ
    explain_query = f"EXPLAIN {query_for_explain}"
    explain_cost_query = f"EXPLAIN COST {query_for_explain}"
    
    # ã‚«ã‚¿ãƒ­ã‚°ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®è¨­å®šã‚’å–å¾—
    catalog = globals().get('CATALOG', 'main')
    database = globals().get('DATABASE', 'default')
    
    print(f"ğŸ“‚ Using catalog: {catalog}")
    print(f"ğŸ—‚ï¸ Using database: {database}")
    
    # ã‚«ã‚¿ãƒ­ã‚°ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’è¨­å®š
    try:
        spark.sql(f"USE CATALOG {catalog}")
        spark.sql(f"USE DATABASE {database}")
    except Exception as e:
        print(f"âš ï¸ Catalog/database configuration error: {str(e)}")
    
    # EXPLAINæ–‡ã¨EXPLAIN COSTæ–‡ã®å®Ÿè¡Œ
    try:
        print("ğŸ”„ Executing EXPLAIN and EXPLAIN COST statements...")
        
        # 1. é€šå¸¸ã®EXPLAINå®Ÿè¡Œ
        print("   ğŸ“Š Executing EXPLAIN...")
        explain_result_spark = spark.sql(explain_query)
        explain_result = explain_result_spark.collect()
        
        # EXPLAINçµæœã®å†…å®¹ã‚’ãƒã‚§ãƒƒã‚¯
        explain_content = ""
        for row in explain_result:
            explain_content += str(row[0]) + "\n"
        
        # 2. EXPLAIN COSTå®Ÿè¡Œ
        print("   ğŸ’° Executing EXPLAIN COST...")
        explain_cost_result_spark = spark.sql(explain_cost_query)
        explain_cost_result = explain_cost_result_spark.collect()
        
        # EXPLAIN COSTçµæœã®å†…å®¹ã‚’ãƒã‚§ãƒƒã‚¯
        explain_cost_content = ""
        for row in explain_cost_result:
            explain_cost_content += str(row[0]) + "\n"
        
        # ğŸš¨ ç·Šæ€¥ä¿®æ­£: ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å³å¯†åŒ–ï¼ˆèª¤æ¤œå‡ºé˜²æ­¢ï¼‰
        retryable_error_patterns = [
            "Error occurred during query planning",
            "error occurred during query planning", 
            "Query planning failed",
            "query planning failed",
            "Plan optimization failed",
            "plan optimization failed",
            "Failed to plan query",
            "failed to plan query",
            "Analysis exception",
            "analysis exception",
            "AMBIGUOUS_REFERENCE",
            "ambiguous_reference",
            "[AMBIGUOUS_REFERENCE]",
            # "Reference",  # ğŸš¨ é™¤å»: éåº¦ã«ä¸€èˆ¬çš„ã€æ­£å¸¸çµæœã‚‚èª¤æ¤œå‡º
            "reference is ambiguous",  # ã‚ˆã‚Šå…·ä½“çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¤‰æ›´
            # "is ambiguous",  # ğŸš¨ é™¤å»: éåº¦ã«ä¸€èˆ¬çš„
            "ambiguous reference",  # ã‚ˆã‚Šå…·ä½“çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¤‰æ›´
            # "Ambiguous",  # ğŸš¨ é™¤å»: éåº¦ã«ä¸€èˆ¬çš„
            "ParseException",
            "SemanticException", 
            "AnalysisException",
            "Syntax error",
            "syntax error",
            "PARSE_SYNTAX_ERROR",
            "INVALID_IDENTIFIER",
            "TABLE_OR_VIEW_NOT_FOUND",
            "COLUMN_NOT_FOUND",
            "UNRESOLVED_COLUMN",
            "[UNRESOLVED_COLUMN",
            "UNRESOLVED_COLUMN.WITH_SUGGESTION",
            "[UNRESOLVED_COLUMN.WITH_SUGGESTION]"
        ]
        
        # ğŸš¨ é‡è¦: EXPLAINçµæœã¨EXPLAIN COSTçµæœã®ä¸¡æ–¹ã‚’ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        detected_error = None
        error_source = None
        
        # ğŸš¨ ç·Šæ€¥ãƒ‡ãƒãƒƒã‚°: ã‚¨ãƒ©ãƒ¼æ¤œå‡ºãƒ—ãƒ­ã‚»ã‚¹ã®è©³ç´°è¡¨ç¤º
        print(f"ğŸ” Executing error pattern detection (patterns: {len(retryable_error_patterns)})")
        print(f"   ğŸ“Š EXPLAIN content length: {len(explain_content)} characters")
        print(f"   ğŸ’° EXPLAIN COST content length: {len(explain_cost_content)} characters")
        
        # 1. EXPLAINçµæœã®ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        for pattern in retryable_error_patterns:
            if pattern in explain_content.lower():
                detected_error = pattern
                error_source = "EXPLAIN"
                print(f"âŒ Error pattern detected in EXPLAIN result: '{pattern}'")
                break
        
        # 2. EXPLAIN COSTçµæœã®ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆEXPLAINã§ã‚¨ãƒ©ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã¿ï¼‰
        if not detected_error:
            for pattern in retryable_error_patterns:
                if pattern in explain_cost_content.lower():
                    detected_error = pattern
                    error_source = "EXPLAIN COST"
                    print(f"âŒ Error pattern detected in EXPLAIN COST result: '{pattern}'")
                    break
        
        if not detected_error:
            print("âœ… No error patterns detected: Processing as normal result")
        
        if detected_error:
            # ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦å‡¦ç†
            print(f"âŒ Error detected in {error_source} result: {detected_error}")
            
            # çµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºï¼ˆã‚¨ãƒ©ãƒ¼ç”¨ï¼‰
            print(f"\nğŸ“‹ {error_source} result preview:")
            print("-" * 50)
            if error_source == "EXPLAIN":
                preview_lines = min(10, len(explain_result))
                for i, row in enumerate(explain_result[:preview_lines]):
                    print(f"{i+1:2d}: {str(row[0])[:100]}...")
            else:
                preview_lines = min(10, len(explain_cost_result))
                for i, row in enumerate(explain_cost_result[:preview_lines]):
                    print(f"{i+1:2d}: {str(row[0])[:100]}...")
            
            # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ï¼ˆEXPLAIN_ENABLED=Yã®å ´åˆã®ã¿ï¼‰
            error_filename = None
            error_cost_filename = None
            if explain_enabled.upper() == 'Y':
                # EXPLAINçµæœã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«
                error_filename = f"output_explain_error_{query_type}_{timestamp}.txt"
                with open(error_filename, 'w', encoding='utf-8') as f:
                    f.write(f"# EXPLAINå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({query_type}ã‚¯ã‚¨ãƒª)\n")
                    f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—: {query_type}\n")
                    f.write(f"ã‚¨ãƒ©ãƒ¼æ¤œå‡ºå…ƒ: {error_source}\n")
                    f.write(f"æ¤œå‡ºã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³: {detected_error}\n")
                    f.write(f"ã‚¯ã‚¨ãƒªæ–‡å­—æ•°: {len(original_query):,}\n")
                    f.write("\n" + "=" * 80 + "\n")
                    f.write("EXPLAIN çµæœ:\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(explain_content)
                    f.write("\n" + "=" * 80 + "\n")
                    f.write("EXPLAIN COST çµæœ:\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(explain_cost_content)
                
                print(f"ğŸ“„ Saved error details: {error_filename}")
                if error_source == "EXPLAIN" and len(explain_result) > preview_lines:
                    print(f"... (Remaining {len(explain_result) - preview_lines} lines, see {error_filename})")
                elif error_source == "EXPLAIN COST" and len(explain_cost_result) > preview_lines:
                    print(f"... (Remaining {len(explain_cost_result) - preview_lines} lines, see {error_filename})")
            else:
                print("ğŸ’¡ Error file not saved because EXPLAIN_ENABLED=N")
                if error_source == "EXPLAIN" and len(explain_result) > preview_lines:
                    print(f"... (Remaining {len(explain_result) - preview_lines} lines)")
                elif error_source == "EXPLAIN COST" and len(explain_cost_result) > preview_lines:
                    print(f"... (Remaining {len(explain_cost_result) - preview_lines} lines)")
            
            print("-" * 50)
            
            result_dict = {
                'error_message': explain_content.strip() if error_source == "EXPLAIN" else explain_cost_content.strip(),
                'detected_pattern': detected_error,
                'error_source': error_source
            }
            if error_filename:
                result_dict['error_file'] = error_filename
            
            return result_dict
        
        # ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œãªã‹ã£ãŸå ´åˆã¯æˆåŠŸã¨ã—ã¦å‡¦ç†
        print(f"âœ… EXPLAIN & EXPLAIN COST execution successful")
        print(f"ğŸ“Š EXPLAIN execution plan lines: {len(explain_result):,}")
        print(f"ğŸ’° EXPLAIN COST statistics lines: {len(explain_cost_result):,}")
        
        # çµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
        print("\nğŸ“‹ EXPLAIN results preview:")
        print("-" * 50)
        preview_lines = min(10, len(explain_result))
        for i, row in enumerate(explain_result[:preview_lines]):
            print(f"{i+1:2d}: {str(row[0])[:100]}...")
        
        print("\nğŸ’° EXPLAIN COST results preview:")
        print("-" * 50)
        cost_preview_lines = min(10, len(explain_cost_result))
        for i, row in enumerate(explain_cost_result[:cost_preview_lines]):
            print(f"{i+1:2d}: {str(row[0])[:100]}...")
        
        # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆEXPLAIN_ENABLED=Yã®å ´åˆã®ã¿ï¼‰
        if explain_enabled.upper() == 'Y' and explain_filename and explain_cost_filename:
            # EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«
            with open(explain_filename, 'w', encoding='utf-8') as f:
                f.write(f"# EXPLAINå®Ÿè¡Œçµæœ ({query_type}ã‚¯ã‚¨ãƒª)\n")
                f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—: {query_type}\n")
                f.write(f"ã‚¯ã‚¨ãƒªæ–‡å­—æ•°: {len(original_query):,}\n")
                f.write("\n" + "=" * 80 + "\n")
                f.write("EXPLAINçµæœ:\n")
                f.write("=" * 80 + "\n\n")
                f.write(explain_content)
            
            # EXPLAIN COSTçµæœãƒ•ã‚¡ã‚¤ãƒ«
            with open(explain_cost_filename, 'w', encoding='utf-8') as f:
                f.write(f"# EXPLAIN COSTå®Ÿè¡Œçµæœ ({query_type}ã‚¯ã‚¨ãƒª)\n")
                f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—: {query_type}\n")
                f.write(f"ã‚¯ã‚¨ãƒªæ–‡å­—æ•°: {len(original_query):,}\n")
                f.write("\n" + "=" * 80 + "\n")
                f.write("EXPLAIN COSTçµæœï¼ˆçµ±è¨ˆæƒ…å ±ä»˜ãï¼‰:\n")
                f.write("=" * 80 + "\n\n")
                f.write(explain_cost_content)
            
            print(f"ğŸ“„ Saved EXPLAIN results: {explain_filename}")
            print(f"ğŸ’° Saved EXPLAIN COST results: {explain_cost_filename}")
            if len(explain_result) > preview_lines:
                print(f"... (Remaining {len(explain_result) - preview_lines} lines, see {explain_filename})")
            if len(explain_cost_result) > cost_preview_lines:
                print(f"... (Remaining {len(explain_cost_result) - cost_preview_lines} lines, see {explain_cost_filename})")
        else:
            print("ğŸ’¡ EXPLAIN result files not saved because EXPLAIN_ENABLED=N")
            if len(explain_result) > preview_lines:
                print(f"... (Remaining {len(explain_result) - preview_lines} lines)")
            if len(explain_cost_result) > cost_preview_lines:
                print(f"... (Remaining {len(explain_cost_result) - cost_preview_lines} lines)")
        
        print("-" * 50)
        
        result_dict = {
            'plan_lines': len(explain_result),
            'cost_lines': len(explain_cost_result)
        }
        if explain_filename and explain_enabled.upper() == 'Y':
            result_dict['explain_file'] = explain_filename
        if explain_cost_filename and explain_enabled.upper() == 'Y':
            result_dict['explain_cost_file'] = explain_cost_filename
        
        return result_dict
        
    except Exception as e:
        error_message = str(e)
        print(f"âŒ Failed to execute EXPLAIN or EXPLAIN COST statement: {error_message}")
        
        # çœŸã®è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ï¼ˆãƒªãƒˆãƒ©ã‚¤ä¸å¯èƒ½ãªã‚¨ãƒ©ãƒ¼ï¼‰ã®ãƒã‚§ãƒƒã‚¯
        truly_fatal_errors = [
            "Permission denied",
            "Access denied", 
            "Insufficient privileges",
            "Database not found",
            "Catalog not found",
            "Spark session is not active",
            "java.lang.OutOfMemoryError",
            "Driver killed",
            "Connection refused"
        ]
        
        # å†è©¦è¡Œå¯èƒ½ãªã‚¨ãƒ©ãƒ¼ï¼ˆLLMã§ä¿®æ­£å¯èƒ½ï¼‰
        retryable_error_patterns = [
            "Error occurred during query planning",
            "error occurred during query planning", 
            "Query planning failed",
            "query planning failed",
            "Plan optimization failed",
            "plan optimization failed",
            "Failed to plan query",
            "failed to plan query",
            "Analysis exception",
            "analysis exception",
            "AMBIGUOUS_REFERENCE",
            "ambiguous_reference",
            "[AMBIGUOUS_REFERENCE]",
            "Reference",
            "is ambiguous",
            "Ambiguous",
            "ParseException",
            "SemanticException",
            "AnalysisException",
            "Syntax error",
            "syntax error",
            "PARSE_SYNTAX_ERROR",
            "INVALID_IDENTIFIER",
            "TABLE_OR_VIEW_NOT_FOUND",
            "COLUMN_NOT_FOUND",
            "UNRESOLVED_COLUMN",
            "[UNRESOLVED_COLUMN",
            "UNRESOLVED_COLUMN.WITH_SUGGESTION",
            "[UNRESOLVED_COLUMN.WITH_SUGGESTION]"
        ]
        
        # çœŸã®è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ã‹ãƒã‚§ãƒƒã‚¯
        is_truly_fatal = any(pattern in error_message.lower() for pattern in truly_fatal_errors)
        
        # å†è©¦è¡Œå¯èƒ½ã‚¨ãƒ©ãƒ¼ã‹ãƒã‚§ãƒƒã‚¯
        is_retryable = any(pattern in error_message.lower() for pattern in retryable_error_patterns)
        
        if is_truly_fatal:
            print(f"ğŸš¨ FATAL: Unrecoverable error occurred")
            print(f"ğŸš¨ Error details: {error_message}")
            print(f"ğŸš¨ Terminating processing.")
            
            # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ï¼ˆEXPLAIN_ENABLED=Yã®å ´åˆã®ã¿ï¼‰
            if explain_enabled.upper() == 'Y':
                error_filename = f"output_explain_fatal_error_{query_type}_{timestamp}.txt"
                try:
                    with open(error_filename, 'w', encoding='utf-8') as f:
                        f.write(f"# FATAL EXPLAINå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ (å›å¾©ä¸å¯èƒ½, {query_type}ã‚¯ã‚¨ãƒª)\n")
                        f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                        f.write(f"ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—: {query_type}\n")
                        f.write(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {error_message}\n")
                        f.write(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: FATAL - Unrecoverable Error\n")
                        f.write("\n" + "=" * 80 + "\n")
                        f.write("å®Ÿè¡Œã—ã‚ˆã†ã¨ã—ãŸEXPLAINæ–‡:\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(explain_query)
                        f.write("\n\n" + "=" * 80 + "\n")
                        f.write("å®Ÿè¡Œã—ã‚ˆã†ã¨ã—ãŸEXPLAIN COSTæ–‡:\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(explain_cost_query)
                    
                    print(f"ğŸ“„ Saved Fatal error details: {error_filename}")
                    
                except Exception as file_error:
                    print(f"âŒ Failed to save Fatal error file: {str(file_error)}")
            else:
                print("ğŸ’¡ Fatal error file not saved because EXPLAIN_ENABLED=N")
            
            # ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†
            import sys
            sys.exit(1)
        
        elif is_retryable:
            print(f"ğŸ”„ Detected retryable error: {error_message}")
            print(f"ğŸ’¡ This error is a candidate for LLM automatic correction")
        
        # éè‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ã®å ´åˆã®å‡¦ç†
        error_filename = None
        if explain_enabled.upper() == 'Y':
            error_filename = f"output_explain_error_{query_type}_{timestamp}.txt"
            try:
                with open(error_filename, 'w', encoding='utf-8') as f:
                    f.write(f"# EXPLAINå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({query_type}ã‚¯ã‚¨ãƒª)\n")
                    f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—: {query_type}\n")
                    f.write(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {error_message}\n")
                    f.write("\n" + "=" * 80 + "\n")
                    f.write("å®Ÿè¡Œã—ã‚ˆã†ã¨ã—ãŸEXPLAINæ–‡:\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(explain_query)
                    f.write("\n\n" + "=" * 80 + "\n")
                    f.write("å®Ÿè¡Œã—ã‚ˆã†ã¨ã—ãŸEXPLAIN COSTæ–‡:\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(explain_cost_query)
                
                print(f"ğŸ“„ Saved error details: {error_filename}")
                
            except Exception as file_error:
                print(f"âŒ Failed to save error file: {str(file_error)}")
        else:
            print("ğŸ’¡ Error file not saved because EXPLAIN_ENABLED=N")
        
        result_dict = {
            'error_message': error_message
        }
        if error_filename:
            result_dict['error_file'] = error_filename
        
        return result_dict

# EXPLAINæ–‡å®Ÿè¡Œã®å®Ÿè¡Œ
print("\nğŸ” EXPLAIN statement execution processing")
print("-" * 40)

# ã‚»ãƒ«43ã§æŠ½å‡ºã—ãŸã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãŒå¤‰æ•°ã«æ®‹ã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
try:
    # original_queryãŒæ—¢ã«å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
    original_query_for_explain = original_query
    print(f"âœ… Retrieved original query ({len(original_query_for_explain)} characters)")
    
except NameError:
    print("âš ï¸ Original query not found")
    print("   Please execute Cell 43 (original query extraction) first")
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å†æŠ½å‡º
    try:
        print("ğŸ”„ Attempting re-extraction from profiler data...")
        original_query_for_explain = extract_original_query_from_profiler_data(profiler_data)
        
        if original_query_for_explain:
            print(f"âœ… Re-extraction successful ({len(original_query_for_explain)} characters)")
        else:
            print("âŒ Re-extraction failed")
            original_query_for_explain = None
            
    except Exception as e:
        print(f"âŒ Error during re-extraction: {str(e)}")
        original_query_for_explain = None

# EXPLAINå®Ÿè¡Œãƒ•ãƒ©ã‚°ã®ç¢ºèª
explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
print(f"ğŸ” EXPLAIN execution setting: {explain_enabled}")

if explain_enabled.upper() != 'Y':
    print("âš ï¸ EXPLAIN execution is disabled")
    print("   To execute EXPLAIN statements, set EXPLAIN_ENABLED = 'Y' in the first cell")
elif original_query_for_explain and original_query_for_explain.strip():
    print("\nğŸš€ Integrated SQL Optimization & EXPLAIN Execution (with automatic error correction)")
    
    # Sparkç’°å¢ƒã®ç¢ºèª
    try:
        spark_version = spark.version
        print(f"ğŸ“Š Spark environment: {spark_version}")
    except Exception as e:
        print(f"âŒ Failed to check Spark environment: {str(e)}")
        print("   Please execute in Databricks environment")
        spark = None
    
    if spark:
        # çµ±åˆå‡¦ç†: åˆ†æçµæœãŒå¿…è¦ãªã®ã§ç¢ºèª
        try:
            # analysis_resultãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if 'analysis_result' in globals():
                current_analysis_result = analysis_result
            else:
                print("âš ï¸ Analysis results not found. Executing simple analysis...")
                current_analysis_result = "åˆ†æçµæœãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€åŸºæœ¬çš„ãªæœ€é©åŒ–ã®ã¿å®Ÿè¡Œ"
            
            # extracted_metricsãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯  
            if 'extracted_metrics' in globals():
                current_metrics = extracted_metrics
            else:
                print("âš ï¸ Metrics not found. Executing with empty metrics...")
                current_metrics = {}
            
            # thinking_enabledå¯¾å¿œ
            if isinstance(current_analysis_result, list):
                analysis_result_str = extract_main_content_from_thinking_response(current_analysis_result)
            else:
                analysis_result_str = str(current_analysis_result)
            
            # ğŸ” Step 1: Original query EXPLAIN execution (with pre-correction)
            print("\nğŸ“‹ Step 1: Original query EXPLAIN execution (Photon compatibility analysis)")
            print("-" * 60)
            
            # ğŸ¯ Save the original query as-is (relying completely on LLM correction)
            print("ğŸ“‹ Using original query as-is: Relying on advanced LLM correction")
            original_query_validated = original_query_for_explain
            
            # ğŸ¯ å…ƒã‚¯ã‚¨ãƒªã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜ï¼ˆé‡è¤‡å‡¦ç†é˜²æ­¢ï¼‰
            globals()['original_query_corrected'] = original_query_validated
            print("ğŸ’¾ Caching original query: Preventing duplicate processing")
            
            original_explain_result = execute_explain_and_save_to_file(original_query_for_explain, "original")
            
            # ğŸš¨ å…ƒã‚¯ã‚¨ãƒªã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã®LLMä¿®æ­£
            if 'error_file' in original_explain_result:
                print(f"ğŸš¨ Detected syntax error in original query: {original_explain_result.get('error_file', 'unknown')}")
                print("ğŸ¤– Executing LLM-based original query correction...")
                
                # ã‚¨ãƒ©ãƒ¼å†…å®¹ã‚’èª­ã¿è¾¼ã¿
                error_message = ""
                if 'error_file' in original_explain_result:
                    try:
                        with open(original_explain_result['error_file'], 'r', encoding='utf-8') as f:
                            error_message = f.read()
                    except:
                        error_message = "ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—"
                
                # LLMã«ã‚ˆã‚‹å…ƒã‚¯ã‚¨ãƒªä¿®æ­£
                corrected_original_query = generate_optimized_query_with_error_feedback(
                    original_query_for_explain,
                    "å…ƒã®ã‚¯ã‚¨ãƒªã«æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ä¿®æ­£ãŒå¿…è¦ã§ã™ã€‚",
                    current_metrics,
                    error_message,
                    ""  # previous_optimized_queryã¯ç©º
                )
                
                # ğŸ› DEBUG: å…ƒã‚¯ã‚¨ãƒªã®ã‚¨ãƒ©ãƒ¼ä¿®æ­£çµæœã‚’ä¿å­˜
                if isinstance(corrected_original_query, str) and not corrected_original_query.startswith("LLM_ERROR:"):
                    save_debug_query_trial(corrected_original_query, 0, "original_query_correction", 
                                         query_id="original_corrected", 
                                         error_info=f"å…ƒã‚¯ã‚¨ãƒªæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ä¿®æ­£: {error_message[:100] if error_message else 'unknown error'}")
                
                # ä¿®æ­£çµæœã‚’ãƒã‚§ãƒƒã‚¯
                if isinstance(corrected_original_query, str) and not corrected_original_query.startswith("LLM_ERROR:"):
                    print("âœ… LLM-based original query correction completed")
                    
                    # ä¿®æ­£ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã‹ã‚‰SQLã‚’æŠ½å‡º
                    if isinstance(corrected_original_query, list):
                        corrected_query_str = extract_main_content_from_thinking_response(corrected_original_query)
                    else:
                        corrected_query_str = str(corrected_original_query)
                    
                    extracted_sql = extract_sql_from_llm_response(corrected_query_str)
                    if extracted_sql:
                        original_query_for_explain = extracted_sql
                        print("ğŸ”„ Re-executing EXPLAIN with corrected query")
                        
                        # ä¿®æ­£ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã§å†åº¦EXPLAINå®Ÿè¡Œ
                        original_explain_result = execute_explain_and_save_to_file(original_query_for_explain, "original_corrected")
                        
                        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚æ›´æ–°
                        globals()['original_query_corrected'] = original_query_for_explain
                        print("ğŸ’¾ Updating cache with corrected original query")
                    else:
                        print("âŒ Failed to extract SQL from corrected query")
                else:
                    print("âŒ LLM-based original query correction failed")
            
            if 'explain_file' in original_explain_result:
                print(f"âœ… Saved original query EXPLAIN result: {original_explain_result['explain_file']}")
            if 'plan_lines' in original_explain_result:
                print(f"ğŸ“Š Original query execution plan lines: {original_explain_result['plan_lines']:,}")
            
            # ğŸš€ Step 2: New iterative optimization process: up to 3 improvement attempts with degradation cause analysis
            print("\nğŸ“‹ Step 2: Iterative LLM optimization & performance degradation analysis (max 3 improvement attempts)")
            print("-" * 60)
            max_optimization_attempts = globals().get('MAX_OPTIMIZATION_ATTEMPTS', 3)
            retry_result = execute_iterative_optimization_with_degradation_analysis(
                original_query_for_explain, 
                analysis_result_str, 
                current_metrics, 
                max_optimization_attempts=max_optimization_attempts
            )            
            # çµæœã®è¡¨ç¤º
            print(f"\nğŸ“Š Final result: {retry_result['final_status']}")
            print(f"ğŸ”„ Total attempts: {retry_result['total_attempts']}")
            
            # åå¾©æœ€é©åŒ–ã®è©¦è¡Œè©³ç´°è¡¨ç¤º
            if 'optimization_attempts' in retry_result:
                attempts = retry_result['optimization_attempts']
                print(f"ğŸ“ˆ Optimization attempt details: {len(attempts)} times")
                for attempt in attempts:
                    status_icon = {
                        'success': 'âœ…',
                        'performance_degraded': 'ğŸš¨',
                        'llm_error': 'âŒ',
                        'explain_failed': 'âš ï¸',
                        'comparison_error': 'ğŸ”§'
                    }.get(attempt['status'], 'â“')
                    print(f"   {status_icon} Attempt {attempt['attempt']}: {attempt['status']}")
                    if 'cost_ratio' in attempt and attempt['cost_ratio'] is not None:
                        print(f"      ğŸ’° Cost ratio: {attempt['cost_ratio']:.2f}x")
            
            if retry_result['final_status'] in ['optimization_success', 'partial_success']:
                print("âœ… Successfully executed EXPLAIN for optimized query!")
                
                # æˆåŠŸæ™‚ã®ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±è¡¨ç¤º
                explain_result = retry_result.get('explain_result', {})
                if explain_result:
                    print("\nğŸ“ Generated files:")
                    if 'explain_file' in explain_result:
                        print(f"   ğŸ“„ EXPLAIN results: {explain_result['explain_file']}")
                    if 'plan_lines' in explain_result:
                        print(f"   ğŸ“Š Execution plan lines: {explain_result['plan_lines']:,}")
                
                # æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®ä¿å­˜
                optimized_result = retry_result.get('optimized_result', '')
                final_query = retry_result.get('final_query', original_query_for_explain)
                
                # File saving: final_query (successful query) to SQL file, optimized_result (original LLM response) to report
                performance_comparison = retry_result.get('performance_comparison')
                best_attempt_number = retry_result.get('best_result', {}).get('attempt_num')  # ğŸ¯ ãƒ™ã‚¹ãƒˆè©¦è¡Œç•ªå·ã‚’å–å¾—
                saved_files = save_optimized_sql_files(
                    original_query_for_explain,
                    final_query,  # ğŸš€ æˆåŠŸã—ãŸã‚¯ã‚¨ãƒªï¼ˆãƒ’ãƒ³ãƒˆä»˜ãï¼‰ã‚’ä¿å­˜
                    current_metrics,
                    analysis_result_str,
                    optimized_result,  # ğŸ“Š å…ƒã®LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆãƒ¬ãƒãƒ¼ãƒˆç”¨ï¼‰
                    performance_comparison,  # ğŸ” ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒçµæœ
                    best_attempt_number  # ğŸ¯ ãƒ™ã‚¹ãƒˆè©¦è¡Œç•ªå·ï¼ˆãƒ¬ãƒãƒ¼ãƒˆç”¨ï¼‰
                )
                
                print("\nğŸ“ Optimization files:")
                for file_type, filename in saved_files.items():
                    print(f"   ğŸ“„ {file_type}: {filename}")
                    
            elif retry_result['final_status'] == 'optimization_failed':
                print("ğŸš¨ Using original query due to failure or degradation in all optimization attempts")
                fallback_reason = retry_result.get('fallback_reason', 'Unknown reason')
                print(f"ğŸ”§ Failure reason: {fallback_reason}")
                
                # å¤±æ•—è©³ç´°ã®è¡¨ç¤º
                if 'optimization_attempts' in retry_result:
                    attempts = retry_result['optimization_attempts']
                    degraded_count = sum(1 for a in attempts if a['status'] == 'performance_degraded')
                    error_count = sum(1 for a in attempts if a['status'] in ['llm_error', 'explain_failed'])
                    
                    if degraded_count > 0:
                        print(f"ğŸ“Š Performance degradation: {degraded_count} times")
                    if error_count > 0:
                        print(f"âŒ Errors occurred: {error_count} times")
                
                print("ğŸ’¡ Recommendations:")
                print("   - Consider updating table statistics")
                print("   - Consider manual optimization with more detailed EXPLAIN information")
                print("   - Please check data volume and query complexity")
            
            elif retry_result['final_status'] == 'fallback_to_original':
                print("âš ï¸ Using original query due to persistent errors in optimized query")
            
            elif retry_result['final_status'] == 'llm_error':
                print("âŒ Using original query due to LLM API call error")
                error_details = retry_result.get('error_details', 'Unknown error')
                print(f"ğŸ”§ LLM error details: {error_details[:200]}...")
                print("ğŸ’¡ Solution: Reduce input data size or adjust LLM settings")
            
            elif retry_result['final_status'] == 'llm_error_correction_failed':
                print("âŒ Using original query due to LLM error even during error correction")
                error_details = retry_result.get('error_details', 'Unknown error')
                print(f"ğŸ”§ LLM error details: {error_details[:200]}...")
                print("ğŸ’¡ Solution: Execute manual SQL optimization or retry with simpler query")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã®ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±è¡¨ç¤º
                fallback_files = retry_result.get('fallback_files', {})
                failure_log = retry_result.get('failure_log', '')
                
                print("\nğŸ“ Generated files:")
                for file_type, filename in fallback_files.items():
                    print(f"   ğŸ“„ {file_type}: {filename}")
                if failure_log:
                    print(f"   ğŸ“„ Failure log: {failure_log}")
                    
            # å…¨è©¦è¡Œã®è©³ç´°è¡¨ç¤º
            print("\nğŸ“‹ Attempt details:")
            for attempt in retry_result.get('all_attempts', []):
                status_icon = "âœ…" if attempt['status'] == 'success' else "âŒ"
                print(f"   {status_icon} Attempt {attempt['attempt']}: {attempt['status']}")
                if attempt['status'] == 'error':
                    print(f"      Error: {attempt['error_message'][:100]}...")
                    
        except Exception as e:
            print(f"âŒ Error occurred during integrated processing: {str(e)}")
            print("ğŸš¨ Emergency error details:")
            import traceback
            traceback.print_exc()
            print("   Emergency fallback: Executing basic analysis and minimal file generation...")
            
            try:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®EXPLAINå®Ÿè¡Œï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªï¼‰
                explain_results = execute_explain_and_save_to_file(original_query_for_explain, "original")
                
                if explain_results:
                    print("\nğŸ“ EXPLAIN results:")
                    for file_type, filename in explain_results.items():
                        if file_type == 'explain_file':
                            print(f"   ğŸ“„ EXPLAIN results: {filename}")
                        elif file_type == 'error_file':
                            print(f"   ğŸ“„ Error log: {filename}")
                        elif file_type == 'plan_lines':
                            print(f"   ğŸ“Š Execution plan lines: {filename}")
                        elif file_type == 'error_message':
                            print(f"   âŒ Error message: {filename}")
                
                # ğŸš¨ ç·Šæ€¥ä¿®æ­£: ã‚¨ãƒ©ãƒ¼æ™‚ã§ã‚‚ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¼·åˆ¶ç”Ÿæˆ
                print("ğŸš¨ Executing emergency report generation...")
                emergency_saved_files = save_optimized_sql_files(
                    original_query_for_explain,
                    original_query_for_explain,  # æœ€é©åŒ–å¤±æ•—æ™‚ã¯å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨
                    current_metrics if 'current_metrics' in locals() else {},
                    "ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: çµ±åˆå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚ã€åŸºæœ¬åˆ†æã®ã¿å®Ÿè¡Œ",
                    f"ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†\n\nã‚¨ãƒ©ãƒ¼è©³ç´°:\n{str(e)}\n\nå…ƒã‚¯ã‚¨ãƒªã‚’ãã®ã¾ã¾ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚",
                    None  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒçµæœãªã—
                )
                
                print("\nğŸ“ Emergency generated files:")
                for file_type, filename in emergency_saved_files.items():
                    print(f"   ğŸ“„ {file_type}: {filename}")
                    
            except Exception as emergency_error:
                print(f"ğŸš¨ Error even in emergency fallback processing: {str(emergency_error)}")
                print("âš ï¸ Please verify query manually")
        
        print("\nâœ… Integrated SQL optimization processing completed")
        
    else:
        print("âŒ EXPLAIN statements cannot be executed because Spark environment is not available")
        print("   Please execute in Databricks environment")
        
else:
    print("âŒ No executable original query found")
print("   Please execute Cell 43 (original query extraction) first")

print()



# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“ Report Formatting Process
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Search and load optimization report files
# MAGIC - Refine and improve report content using LLM
# MAGIC - Save refinement results and generate final report

# COMMAND ----------
# 
# ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆæ¨æ•²å‡¦ç†ï¼ˆçµ±åˆå‡¦ç†ç”¨ï¼‰
print("\nğŸ“ Report refinement processing")
print("-" * 40)
# 
def find_latest_report_file() -> str:
    """Find the latest report file"""
    import os
    import glob
    
    # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ (è¨€èªåˆ¥å¯¾å¿œ)
    language_suffix = 'en' if OUTPUT_LANGUAGE == 'en' else 'jp'
    pattern = f"output_optimization_report_{language_suffix}_*.md"
    report_files = glob.glob(pattern)
    
    if not report_files:
        return None
    
    # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é †ï¼‰
    latest_file = max(report_files, key=os.path.getctime)
    return latest_file
# 
def refine_report_content_with_llm(report_content: str) -> str:
    """Refine report using LLM"""
    
    # LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è¨­å®šç¢ºèª
    if not LLM_CONFIG or not LLM_CONFIG.get('provider'):
        print("âŒ LLM provider is not configured")
        return report_content
    
    # ğŸš¨ ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾ç­–: ãƒ¬ãƒãƒ¼ãƒˆã‚µã‚¤ã‚ºåˆ¶é™
    MAX_CONTENT_SIZE = 50000  # 50KBåˆ¶é™
    original_size = len(report_content)
    
    if original_size > MAX_CONTENT_SIZE:
        print(f"âš ï¸ Report size too large: {original_size:,} characters â†’ truncated to {MAX_CONTENT_SIZE:,} characters")
        # é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å„ªå…ˆçš„ã«ä¿æŒ
        truncated_content = report_content[:MAX_CONTENT_SIZE]
        truncated_content += f"\n\nâš ï¸ ãƒ¬ãƒãƒ¼ãƒˆãŒå¤§ãã™ãã‚‹ãŸã‚ã€{MAX_CONTENT_SIZE:,} æ–‡å­—ã«åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸï¼ˆå…ƒã‚µã‚¤ã‚º: {original_size:,} æ–‡å­—ï¼‰"
        report_content = truncated_content
    else:
        print(f"ğŸ“Š Report size: {original_size:,} characters (executing refinement)")
    
    # Photonåˆ©ç”¨ç‡ã®æŠ½å‡ºã¨è©•ä¾¡åˆ¤å®š
    import re
    photon_pattern = r'åˆ©ç”¨ç‡[ï¼š:]\s*(\d+(?:\.\d+)?)%'
    photon_match = re.search(photon_pattern, report_content)
    
    photon_evaluation_instruction = ""
    if photon_match:
        photon_utilization = float(photon_match.group(1))
        if OUTPUT_LANGUAGE == 'ja':
            if photon_utilization <= 80:
                photon_evaluation_instruction = """
ã€Photonåˆ©ç”¨ç‡è©•ä¾¡æŒ‡ç¤ºã€‘
- Photonåˆ©ç”¨ç‡ãŒ80%ä»¥ä¸‹ã®å ´åˆã¯ã€Œè¦æ”¹å–„ã€ã¾ãŸã¯ã€Œä¸è‰¯ã€ã®è©•ä¾¡ã‚’æ˜ç¢ºã«è¡¨ç¤ºã—ã¦ãã ã•ã„
- 80%ä»¥ä¸‹ã®å ´åˆã¯ã€æ”¹å–„ã®å¿…è¦æ€§ã‚’å¼·èª¿ã—ã€å…·ä½“çš„ãªæ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æç¤ºã—ã¦ãã ã•ã„
- è©•ä¾¡ä¾‹: ã€ŒPhotonåˆ©ç”¨ç‡: XX% (è©•ä¾¡: è¦æ”¹å–„)ã€
"""
            else:
                photon_evaluation_instruction = """
ã€Photonåˆ©ç”¨ç‡è©•ä¾¡æŒ‡ç¤ºã€‘
- Photonåˆ©ç”¨ç‡ãŒ80%ä»¥ä¸Šã®å ´åˆã¯ã€Œè‰¯å¥½ã€ã®è©•ä¾¡ã‚’è¡¨ç¤ºã—ã¦ãã ã•ã„
- è©•ä¾¡ä¾‹: ã€ŒPhotonåˆ©ç”¨ç‡: XX% (è©•ä¾¡: è‰¯å¥½)ã€
"""
        else:
            if photon_utilization <= 80:
                photon_evaluation_instruction = """
ã€Photon Utilization Rate Evaluation Instructionsã€‘
- If Photon utilization rate is 80% or below, clearly display "Needs Improvement" or "Poor" evaluation
- For 80% or below, emphasize the need for improvement and provide specific improvement actions
- Example: "Photon Utilization Rate: XX% (Evaluation: Needs Improvement)"
"""
            else:
                photon_evaluation_instruction = """
ã€Photon Utilization Rate Evaluation Instructionsã€‘
- If Photon utilization rate is 80% or above, display "Good" evaluation
- Example: "Photon Utilization Rate: XX% (Evaluation: Good)"
"""
    
    # è¨€èªã«å¿œã˜ã¦æ¨æ•²ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ‡ã‚Šæ›¿ãˆ
    if OUTPUT_LANGUAGE == 'ja':
        refinement_prompt = f"""ã‚ãªãŸã¯æŠ€è¡“æ–‡æ›¸ç·¨é›†è€…ã§ã™ã€‚ä»¥ä¸‹ã®Databricks SQL ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿ã‚„ã™ãç°¡æ½”ã«æ¨æ•²ã—ã¦ãã ã•ã„ã€‚

ã€æ¨æ•²è¦ä»¶ã€‘
1. å…¨ä½“æ§‹æˆã‚’æ•´ç†ã—ã€è«–ç†çš„ã«æƒ…å ±ã‚’é…ç½®
2. å†—é•·ãªè¡¨ç¾ã‚’å‰Šé™¤ã—ã€ç°¡æ½”ã§ç†è§£ã—ã‚„ã™ã„è¡¨ç¾ã«ä¿®æ­£
3. é‡è¦ãªæƒ…å ±ãŒåŸ‹ã‚‚ã‚Œãªã„ã‚ˆã†é©åˆ‡ãªè¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«ã§æ§‹é€ åŒ–
4. æŠ€è¡“ç”¨èªã‚’ä¿æŒã—ã¤ã¤ã€ç†è§£ã—ã‚„ã™ã„èª¬æ˜ã‚’è¿½åŠ 
5. æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¿æŒ
6. å®Ÿç”¨çš„ãªæ¨å¥¨äº‹é …ã‚’æ˜ç¢ºã«æç¤º

ã€ğŸš¨ å‰Šé™¤ãƒ»ä¿®æ­£ã—ã¦ã¯ã„ã‘ãªã„é‡è¦æƒ…å ±ã€‘
- **ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±**: "ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: XX" ã¾ãŸã¯ "è¨­å®šãªã—" ã®è¡¨ç¤º
- **ãƒ•ã‚£ãƒ«ã‚¿ç‡æƒ…å ±**: "ãƒ•ã‚£ãƒ«ã‚¿ç‡: X.X% (èª­ã¿è¾¼ã¿: XX.XXGB, ãƒ—ãƒ«ãƒ¼ãƒ³: XX.XXGB)" ã®å½¢å¼
- **ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—**: å„ãƒ—ãƒ­ã‚»ã‚¹ã® "å…¨ä½“ã®XX%" è¡¨ç¤ºï¼ˆä¸¦åˆ—å®Ÿè¡Œã‚’è€ƒæ…®ã—ãŸæ­£ç¢ºãªè¨ˆç®—ï¼‰
- **æ¨å¥¨vsç¾åœ¨ã®æ¯”è¼ƒåˆ†æ**: æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã¨ç¾åœ¨ã®ã‚­ãƒ¼ã®æ¯”è¼ƒæƒ…å ±
- **å…·ä½“çš„ãªæ•°å€¤ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: å®Ÿè¡Œæ™‚é–“ã€ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é‡ã€ã‚¹ãƒ”ãƒ«é‡ã€åˆ©ç”¨ç‡ç­‰
- **SQLå®Ÿè£…ä¾‹**: ALTER TABLEæ§‹æ–‡ã€CLUSTER BYæ–‡ã€ãƒ’ãƒ³ãƒˆå¥ç­‰ã®å…·ä½“ä¾‹
- **ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥è©³ç´°æƒ…å ±**: å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒãƒ¼ãƒ‰æƒ…å ±ã€ãƒ•ã‚£ãƒ«ã‚¿åŠ¹ç‡ã€æ¨å¥¨äº‹é …

{photon_evaluation_instruction}

ã€ç¾åœ¨ã®ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã€‘
{report_content}

ã€å‡ºåŠ›è¦ä»¶ã€‘
- ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§æ¨æ•²ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›
- æŠ€è¡“æƒ…å ±ã‚’ä¿æŒã—ã¤ã¤å¯èª­æ€§ã‚’å‘ä¸Š
- é‡è¦ãƒã‚¤ãƒ³ãƒˆã®å¼·èª¿ã¨è¡Œå‹•è¨ˆç”»ã®æ˜ç¢ºåŒ–
- Photonåˆ©ç”¨ç‡è©•ä¾¡ã®æ˜ç¢ºãªè¡¨ç¤º
- **å¿…é ˆ**: ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã¨ãƒ•ã‚£ãƒ«ã‚¿ç‡æƒ…å ±ã®å®Œå…¨ä¿æŒ
- **å¿…é ˆ**: ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ã§ã¯å…ƒã®æ­£ç¢ºãªæ•°å€¤ã‚’ä½¿ç”¨
- **å¿…é ˆ**: ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥è©³ç´°åˆ†ææƒ…å ±ï¼ˆç¾åœ¨ã‚­ãƒ¼ã€æ¨å¥¨ã‚­ãƒ¼ã€ãƒ•ã‚£ãƒ«ã‚¿ç‡ï¼‰ã‚’å‰Šé™¤ã—ãªã„
- **å¿…é ˆ**: SQLå®Ÿè£…ä¾‹ï¼ˆALTER TABLEã€CLUSTER BYç­‰ï¼‰ã‚’å®Œå…¨ãªå½¢ã§ä¿æŒ
"""
    else:
        refinement_prompt = f"""You are a technical document editor. Please refine the following Databricks SQL performance analysis report to make it readable and concise.

ã€Refinement Requirementsã€‘
1. Organize the overall structure and arrange information logically
2. Remove redundant expressions and modify to concise, understandable expressions
3. Structure with appropriate heading levels so important information doesn't get buried
4. Keep technical terms while adding understandable explanations
5. Preserve numerical data and metrics
6. Clearly present practical recommendations

ã€ğŸš¨ Critical Information That Must NOT Be Deleted or Modifiedã€‘
- **Current clustering key information**: Display "Current clustering key: XX" or "Not configured"
- **Filter rate information**: Format "Filter rate: X.X% (read: XX.XXGB, pruned: XX.XXGB)"
- **Percentage calculations**: Display "XX% of total" for each process (accurate calculations considering parallel execution)
- **Recommended vs current comparison analysis**: Comparison information between recommended clustering keys and current keys
- **Specific numerical metrics**: Execution time, data read volume, spill volume, utilization rates, etc.
- **SQL implementation examples**: Specific examples of ALTER TABLE syntax, CLUSTER BY statements, hint clauses, etc.
- **Table-specific detailed information**: Node information, filter efficiency, and recommendations for each table

{photon_evaluation_instruction}

ã€Current Report Contentã€‘
{report_content}

ã€Output Requirementsã€‘
- Output refined report in markdown format
- Maintain technical information while improving readability
- Emphasize important points and clarify action plans
- Clearly display Photon utilization rate evaluation
- **Required**: Completely preserve current clustering key information and filter rate information
- **Required**: Use original accurate numerical values for percentage calculations
- **Required**: Do not delete detailed analysis information by table (current key, recommended key, filter rate)
- **Required**: Preserve SQL implementation examples (ALTER TABLE, CLUSTER BY, etc.) in complete form
"""
    
    try:
        # è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«åŸºã¥ã„ã¦æ¨æ•²ã‚’å®Ÿè¡Œ
        provider = LLM_CONFIG.get('provider', 'databricks')
        
        if provider == 'databricks':
            refined_content = _call_databricks_llm(refinement_prompt)
        elif provider == 'openai':
            refined_content = _call_openai_llm(refinement_prompt)
        elif provider == 'azure_openai':
            refined_content = _call_azure_openai_llm(refinement_prompt)
        elif provider == 'anthropic':
            refined_content = _call_anthropic_llm(refinement_prompt)
        else:
            print(f"âŒ Unsupported LLM provider: {provider}")
            return report_content
        
        # ğŸš¨ LLMã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ¤œå‡ºï¼ˆç²¾å¯†åŒ–ï¼‰
        if isinstance(refined_content, str):
            # ã‚ˆã‚Šç²¾å¯†ãªã‚¨ãƒ©ãƒ¼æ¤œå‡ºï¼ˆãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã®çµµæ–‡å­—ã¨åŒºåˆ¥ï¼‰
            actual_error_indicators = [
                "APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰",
                "Input is too long for requested model",
                "Bad Request",
                "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼:",
                "APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼:",
                'ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {"error_code":',
                "âŒ APIã‚¨ãƒ©ãƒ¼:",
                "âš ï¸ APIã‚¨ãƒ©ãƒ¼:"
            ]
            
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®é–‹å§‹éƒ¨åˆ†ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šå³å¯†ï¼‰
            is_error_response = any(
                refined_content.strip().startswith(indicator) or 
                f"\n{indicator}" in refined_content[:500]  # å…ˆé ­500æ–‡å­—ä»¥å†…ã§ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                for indicator in actual_error_indicators
            )
            
            if is_error_response:
                print(f"âŒ Error detected in LLM report refinement: {refined_content[:200]}...")
                print("ğŸ“„ Returning original report")
                return report_content
        
        # thinking_enabledå¯¾å¿œ: çµæœãŒãƒªã‚¹ãƒˆã®å ´åˆã®å‡¦ç†
        if isinstance(refined_content, list):
            refined_content = format_thinking_response(refined_content)
        
        print(f"âœ… LLM-based report refinement completed (Cell 46 independent processing)")
        return refined_content
        
    except Exception as e:
        print(f"âŒ Error occurred during LLM-based report refinement: {str(e)}")
        return report_content
# 
def save_refined_report(refined_content: str, original_filename: str) -> str:
    """Save refined report"""
    from datetime import datetime
    
    # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆè¨€èªåˆ¥å¯¾å¿œï¼‰
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    language_suffix = 'en' if OUTPUT_LANGUAGE == 'en' else 'jp'
    refined_filename = f"output_final_report_{language_suffix}_{timestamp}.md"
    
    try:
        with open(refined_filename, 'w', encoding='utf-8') as f:
            f.write(refined_content)
        
        print(f"âœ… Saved final report: {refined_filename}")
        return refined_filename
        
    except Exception as e:
        print(f"âŒ Error during refined report saving: {str(e)}")
        return None
# 
def finalize_report_files(original_filename: str, refined_filename: str) -> str:
    """Execute file processing based on DEBUG_ENABLED setting"""
    import os
    
    # DEBUG_ENABLEDè¨­å®šã‚’ç¢ºèª
    debug_enabled = globals().get('DEBUG_ENABLED', 'N')
    
    try:
        if debug_enabled.upper() == 'Y':
            # DEBUG_ENABLED=Y: å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åç§°å¤‰æ›´ã—ã¦ä¿æŒ
            if os.path.exists(original_filename):
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆå…ƒãƒ•ã‚¡ã‚¤ãƒ«åã« _raw ã‚’è¿½åŠ ï¼‰
                backup_filename = original_filename.replace('.md', '_raw.md')
                
                os.rename(original_filename, backup_filename)
                print(f"ğŸ“ Preserving original file: {original_filename} â†’ {backup_filename}")
            else:
                print(f"âš ï¸ Original file not found: {original_filename}")
        else:
            # DEBUG_ENABLED=N: å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            if os.path.exists(original_filename):
                os.remove(original_filename)
                print(f"ğŸ—‘ï¸ Deleted original file: {original_filename}")
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆoutput_final_report_*ï¼‰ã¯ãƒªãƒãƒ¼ãƒ ã›ãšãã®ã¾ã¾ä¿æŒ
        if os.path.exists(refined_filename):
            print(f"âœ… Preserving final report file: {refined_filename}")
            return refined_filename
        else:
            print(f"âŒ Refined version file not found: {refined_filename}")
            return None
            
    except Exception as e:
        print(f"âŒ Error during file operations: {str(e)}")
        return None
# 
# 
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
try:
    # æœ€æ–°ã®ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    latest_report = find_latest_report_file()
    
    if not latest_report:
        print("âŒ Report file not found")
        print("âš ï¸ Please execute Cell 43 (Integrated SQL Optimization Processing) first")
        print()
        print("ğŸ” Detailed troubleshooting:")
        print("1. Please confirm that Cell 43 completed normally")
        print("2. Please check if any error messages are displayed")
        print("3. Please check if variables like current_analysis_result and extracted_metrics are defined")
        print("4. Emergency fallback processing may have been executed")
        
        # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        import glob
        sql_files = glob.glob("output_optimized_query_*.sql")
        original_files = glob.glob("output_original_query_*.sql")
        all_reports = glob.glob("output_optimization_report*.md")
        
        # ç¾åœ¨ã®è¨€èªè¨­å®šã«å¯¾å¿œã™ã‚‹ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
        language_suffix = 'en' if OUTPUT_LANGUAGE == 'en' else 'jp'
        current_lang_reports = glob.glob(f"output_optimization_report_{language_suffix}_*.md")
        
        print(f"\nğŸ“ Current file status:")
        print(f"   ğŸ“„ Optimized query files: {len(sql_files)} files")
        print(f"   ğŸ“„ Original query files: {len(original_files)} files")
        print(f"   ğŸ“„ Report files ({language_suffix.upper()}): {len(current_lang_reports)} files")
        print(f"   ğŸ“„ Report files (total): {len(all_reports)} files")
        
        if all_reports:
            print(f"   ğŸ“‹ Detected report files:")
            for report in all_reports:
                print(f"      - {report}")
            print("   âš ï¸ Files exist but not detected by find_latest_report_file()")
            print("   ğŸ’¡ Please check filenames manually - possible pattern matching issue")
        
        if not sql_files and not original_files:
            print("   ğŸš¨ Important: Cell 43 processing may not have been executed at all")
            print("   ğŸ“‹ Solution: Re-execute Cell 43 from the beginning")
    else:
        print(f"ğŸ“„ Target report file: {latest_report}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿è¾¼ã¿
        with open(latest_report, 'r', encoding='utf-8') as f:
            original_content = f.read()
        
        print(f"ğŸ“Š Original report size: {len(original_content):,} characters")
        
        # ğŸš¨ é‡è¤‡æ¨æ•²é˜²æ­¢: æ—¢ã«æ¨æ•²æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
        refinement_indicators = [
            "ğŸ“Š **æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ**",  # æ¨æ•²å¾Œã®å…¸å‹çš„ãªãƒ˜ãƒƒãƒ€ãƒ¼
            "ğŸš€ **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„çµæœ**",  # æ¨æ•²å¾Œã®å…¸å‹çš„ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³
            "âœ… **æ¨å¥¨äº‹é …**",  # æ¨æ•²å¾Œã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            "LLMã«ã‚ˆã‚‹æ¨æ•²ã‚’å®Ÿè¡Œä¸­",  # æ¨æ•²ãƒ—ãƒ­ã‚»ã‚¹ä¸­ã«å«ã¾ã‚Œã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            "æ¨æ•²ç‰ˆãƒ¬ãƒãƒ¼ãƒˆ:",  # æ¨æ•²æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        ]
        
        already_refined = any(indicator in original_content for indicator in refinement_indicators)
        
        if already_refined:
            print(f"âœ… Report already refined (avoiding duplicate processing): {latest_report}")
            print("ğŸ“‹ Using refined report as is")
            refined_content = original_content
        else:
            print(f"ğŸ¤– Executing LLM-based refinement (target: {latest_report})...")
            refined_content = refine_report_content_with_llm(original_content)
        
        if refined_content != original_content:
            print(f"ğŸ“Š Post-refinement size: {len(refined_content):,} characters")
            
            # æ¨æ•²ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
            refined_filename = save_refined_report(refined_content, latest_report)
            
            if refined_filename:
                print(f"ğŸ“„ Refined report: {refined_filename}")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèª
                import os
                if os.path.exists(refined_filename):
                    file_size = os.path.getsize(refined_filename)
                    print(f"ğŸ“ Refined file size: {file_size:,} bytes")
                
                # å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã€æ¨æ•²ç‰ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«åã«ãƒªãƒãƒ¼ãƒ 
                final_filename = finalize_report_files(latest_report, refined_filename)
                
                if final_filename:
                    print(f"ğŸ“„ Final report file: {final_filename}")
                    
                    # æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèª
                    if os.path.exists(final_filename):
                        final_file_size = os.path.getsize(final_filename)
                        print(f"ğŸ“ Final file size: {final_file_size:,} bytes")
                
                print(f"âœ… Report refinement processing completed: {final_filename}")
                
                # æ¨æ•²ã®çµæœã‚’è¡¨ç¤ºï¼ˆæœ€åˆã®1000æ–‡å­—ï¼‰
                print("\nğŸ“‹ Refinement result preview:")
                print("-" * 50)
                preview = refined_content[:1000]
                print(preview)
                if len(refined_content) > 1000:
                    print(f"\n... (remaining {len(refined_content) - 1000} characters see {final_filename or latest_report})")
                print("-" * 50)
            else:
                print("âŒ Failed to save refined report")
        else:
            print("ğŸ“‹ Report is already in optimal state (refinement processing skipped)")
            print("âœ… Using existing report file as is")
            
            # æ—¢ã«æ¨æ•²æ¸ˆã¿ã®å ´åˆã‚‚ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤º
            print("\nğŸ“‹ Report content preview:")
            print("-" * 50)
            preview = refined_content[:1000]
            print(preview)
            if len(refined_content) > 1000:
                print(f"\n... (remaining {len(refined_content) - 1000} characters see {latest_report})")
            print("-" * 50)
            
except Exception as e:
    print(f"âŒ Error occurred during report refinement processing: {str(e)}")
    import traceback
    traceback.print_exc()
# 
print()
# 
# # ğŸ§¹ ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤å‡¦ç†ï¼ˆDEBUG_ENABLEDãƒ•ãƒ©ã‚°ã«åŸºã¥ãï¼‰
debug_enabled = globals().get('DEBUG_ENABLED', 'N')
explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')

if debug_enabled.upper() == 'Y':
    print("\nğŸ› Debug mode enabled: Preserving intermediate files")
    print("-" * 40)
    print("ğŸ’¡ All intermediate files are preserved because DEBUG_ENABLED=Y")
    print("ğŸ“ The following files are preserved:")
    
    import glob
    import os
    
    # ä¿æŒã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
    if explain_enabled.upper() == 'Y':
        original_files = glob.glob("output_explain_original_*.txt")
        optimized_files = glob.glob("output_explain_optimized_*.txt")
        cost_original_files = glob.glob("output_explain_cost_original_*.txt")
        cost_optimized_files = glob.glob("output_explain_cost_optimized_*.txt")
        error_files = glob.glob("output_explain_error_*.txt")
        all_files = original_files + optimized_files + cost_original_files + cost_optimized_files + error_files
        
        if all_files:
            print(f"   ğŸ” EXPLAIN result files:")
            print(f"      ğŸ“Š EXPLAIN: Original {len(original_files)} files, Post-optimization {len(optimized_files)} files")
            print(f"      ğŸ’° EXPLAIN COST: Original {len(cost_original_files)} files, Post-optimization {len(cost_optimized_files)} files")
            print(f"      âŒ Errors: {len(error_files)} files")
            for file_path in all_files[:3]:  # æœ€å¤§3å€‹ã¾ã§è¡¨ç¤º
                print(f"      ğŸ“„ {file_path}")
            if len(all_files) > 3:
                print(f"      ... and {len(all_files) - 3} other files")
    
    print("âœ… Debug mode: Skipped file deletion processing")
else:
    print("\nğŸ§¹ Intermediate file deletion processing")
    print("-" * 40)
    print("ğŸ’¡ Deleting intermediate files because DEBUG_ENABLED=N")
    language_suffix = 'en' if OUTPUT_LANGUAGE == 'en' else 'jp'
    print(f"ğŸ“ Files to be kept: output_original_query_*.sql, output_optimization_report_{language_suffix}_*.md, output_optimized_query_*.sql")
    
    import glob
    import os
    
    if explain_enabled.upper() == 'Y':
        # EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆæ–°ãƒ‘ã‚¿ãƒ¼ãƒ³ + æ—§ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        original_files = glob.glob("output_explain_original_*.txt")
        optimized_files = glob.glob("output_explain_optimized_*.txt")
        cost_original_files = glob.glob("output_explain_cost_original_*.txt")
        cost_optimized_files = glob.glob("output_explain_cost_optimized_*.txt")
        error_original_files = glob.glob("output_explain_error_original_*.txt")
        error_optimized_files = glob.glob("output_explain_error_optimized_*.txt")
        
        # æ—§ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å‰Šé™¤å¯¾è±¡ã«å«ã‚ã‚‹ï¼ˆä¸‹ä½äº’æ›æ€§ï¼‰
        old_explain_files = glob.glob("output_explain_plan_*.txt")
        old_error_files = glob.glob("output_explain_error_*.txt")
        
        # ğŸš¨ æ–°è¦è¿½åŠ : DEBUGç”¨ã®å®Œå…¨æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å‰Šé™¤å¯¾è±¡ã«å«ã‚ã‚‹
        full_plan_files = glob.glob("output_physical_plan_full_*.txt")
        full_stats_files = glob.glob("output_explain_cost_statistics_full_*.txt")
        extracted_stats_files = glob.glob("output_explain_cost_statistics_extracted_*.json")
        structured_plan_files = glob.glob("output_physical_plan_structured_*.json")
        structured_cost_files = glob.glob("output_explain_cost_structured_*.json")
        
        all_temp_files = (original_files + optimized_files + cost_original_files + cost_optimized_files + 
                         error_original_files + error_optimized_files + old_explain_files + old_error_files +
                         full_plan_files + full_stats_files + extracted_stats_files + 
                         structured_plan_files + structured_cost_files)
        
        explain_files = original_files + optimized_files + old_explain_files
        cost_files = cost_original_files + cost_optimized_files
        error_files = error_original_files + error_optimized_files + old_error_files
        debug_files = full_plan_files + full_stats_files + extracted_stats_files + structured_plan_files + structured_cost_files
        
        if all_temp_files:
            print(f"ğŸ“ Files to be deleted:")
            print(f"   ğŸ“Š EXPLAIN results: {len(explain_files)} files")
            print(f"   ğŸ’° EXPLAIN COST results: {len(cost_files)} files")
            print(f"   âŒ Error files: {len(error_files)} files")
            print(f"   ğŸ”§ DEBUG complete information: {len(debug_files)} files")
            print("ğŸ’¡ Note: These files should not have been created because DEBUG_ENABLED=N")
            
            # ğŸ”§ å¤‰æ•°ã®åˆæœŸåŒ–ã‚’ã‚ˆã‚Šå®‰å…¨ã«å®Ÿè¡Œ
            deleted_count = 0
            for file_path in all_temp_files:
                try:
                    os.remove(file_path)
                    print(f"âœ… Deletion completed: {file_path}")
                    deleted_count += 1
                except Exception as e:
                    print(f"âŒ Deletion failed: {file_path} - {str(e)}")
            
            print(f"ğŸ—‘ï¸ Deletion completed: {deleted_count}/{len(all_temp_files)} files")
            print("ğŸ’¡ EXPLAIN/EXPLAIN COST results and error files deleted as they were already used by LLM optimization processing")
        else:
            print("ğŸ“ No EXPLAIN/EXPLAIN COST results or error files found for deletion")
    else:
        print("âš ï¸ Skipped EXPLAIN result file deletion processing because EXPLAIN execution is disabled")

print()

print("ğŸ‰ All processing completed!")
print("ğŸ“ Please check the generated files and utilize the analysis results.")
