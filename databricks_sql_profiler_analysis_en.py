# Databricks notebook source
# MAGIC %md
# MAGIC # Databricks SQL Profiler Analysis Tool
# MAGIC
# MAGIC This notebook reads Databricks SQL profiler JSON log files and extracts metrics necessary for bottleneck identification and improvement recommendations.
# MAGIC
# MAGIC ## ðŸš€ Performance Optimization Updates (2024)
# MAGIC - **Fixed duplicate EXPLAIN execution** for original queries
# MAGIC - **Implemented caching mechanism** to prevent redundant database calls  
# MAGIC - **Optimized iterative optimization process** to avoid repeated EXPLAIN COST execution
# MAGIC - **Added global cache** for EXPLAIN results across multiple analysis functions
# MAGIC - **Reduced file I/O operations** through intelligent cache reuse
# MAGIC
# MAGIC ### Key Improvements:
# MAGIC 1. Original query EXPLAIN results are cached and reused across optimization attempts
# MAGIC 2. Fallback processing checks for existing cached results before re-execution  
# MAGIC 3. Analysis functions prioritize cached data over file system searches
# MAGIC 4. Performance degradation analysis no longer triggers duplicate EXPLAIN calls
# MAGIC
# MAGIC ### Expected Benefits:
# MAGIC - **3x faster execution** for iterative optimization (max 3 attempts)
# MAGIC - **Reduced database load** and network traffic
# MAGIC - **Elimination of duplicate output files**
# MAGIC - **More efficient resource utilization**
# MAGIC
# MAGIC ## Feature Overview
# MAGIC
# MAGIC 1. **SQL Profiler JSON File Loading**
# MAGIC    - Analysis of profiler logs output by Databricks
# MAGIC    - Extraction of execution plan metrics stored in the `graphs` key
# MAGIC
# MAGIC 2. **Key Metrics Extraction**
# MAGIC    - Query basic information (ID, status, execution time, etc.)
# MAGIC    - Overall performance (execution time, data volume, cache efficiency, etc.)
# MAGIC    - Stage and node detailed metrics
# MAGIC    - Bottleneck indicator calculation
# MAGIC
# MAGIC 3. **AI-powered Bottleneck Analysis**
# MAGIC    - Configurable LLM endpoints (Databricks, OpenAI, Azure OpenAI, Anthropic)
# MAGIC    - Bottleneck identification from extracted metrics
# MAGIC    - Specific improvement recommendations
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC **Prerequisites:**
# MAGIC - LLM endpoint configuration (Databricks Model Serving or external API)
# MAGIC - Required API key setup
# MAGIC - SQL profiler JSON file preparation (DBFS or FileStore)
# MAGIC
# MAGIC ---

# COMMAND ----------

# MAGIC %md
# MAGIC # ðŸ”§ Configuration & Setup Section
# MAGIC
# MAGIC **This section performs basic tool configuration**
# MAGIC
# MAGIC ðŸ“‹ **Configuration Contents:**
# MAGIC - Analysis target file specification
# MAGIC - LLM endpoint configuration
# MAGIC - Analysis function definitions
# MAGIC
# MAGIC âš ï¸ **Important:** Execute all cells in this section before running the main processing

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸ“ Analysis Target File Configuration
# MAGIC
# MAGIC **First, specify the SQL profiler JSON file to be analyzed.**
# MAGIC
# MAGIC This cell performs the following configurations:
# MAGIC - ðŸ“‚ SQL profiler JSON file path configuration
# MAGIC - ðŸ“‹ Examples of supported file path formats
# MAGIC - âš™ï¸ Basic environment configuration

# COMMAND ----------

# ðŸ“ SQL Profiler JSON File Path Configuration
# 
# Please change the JSON_FILE_PATH below to your actual file path:

# Notebook environment file path configuration (please select from the following options)

# Option 1: Pre-tuning plan file (recommended)
JSON_FILE_PATH = 'query-profile_01f0703c-c975-1f48-ad71-ba572cc57272.json'

# Option 2: To use other JSON files, uncomment and edit the following
# JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/nophoton.json'
# JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/POC1.json'
# JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/your_file.json'

# Command line environment (optional)
import sys
if len(sys.argv) > 1 and not sys.argv[1].startswith('-'):
    # Use only when command line argument is not a flag (doesn't start with -)
    JSON_FILE_PATH = sys.argv[1]

# ðŸŒ Output language setting (OUTPUT_LANGUAGE: 'ja' = Japanese, 'en' = English)
OUTPUT_LANGUAGE = 'en'

# ðŸ” EXPLAIN statement execution setting (EXPLAIN_ENABLED: 'Y' = execute, 'N' = do not execute)
EXPLAIN_ENABLED = 'Y'

# ðŸ› Debug mode setting (DEBUG_ENABLED: 'Y' = keep intermediate files, 'N' = keep final files only)
DEBUG_ENABLED = 'N'

# ðŸ” JSON Debug output setting (DEBUG_JSON_ENABLED: 'Y' = show JSON debug info, 'N' = hide JSON debug info)
DEBUG_JSON_ENABLED = 'N'

# ðŸ—‚ï¸ Catalog and database configuration (used when executing EXPLAIN statements)
CATALOG = 'tpcds'
DATABASE = 'tpcds_sf1000_delta_lc'

# === ðŸŽ¯ Query Optimization Points Extraction Functions ===

def extract_optimization_points_from_query(query: str, trial_type: str, attempt_num: int) -> str:
    """
    æˆåŠŸã—ãŸã‚¯ã‚¨ãƒªã‹ã‚‰æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆã‚’è»½é‡æŠ½å‡ºï¼ˆLLMä¸ä½¿ç”¨ï¼‰
    
    Args:
        query: æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª
        trial_type: è©¦è¡Œã‚¿ã‚¤ãƒ—
        attempt_num: è©¦è¡Œç•ªå·
    
    Returns:
        str: æŠ½å‡ºã•ã‚ŒãŸæœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆ
    """
    import re
    
    optimization_points = []
    
    # 1. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–¢é€£ã®æœ€é©åŒ–
    if re.search(r'CREATE\s+INDEX|USE\s+INDEX|FORCE\s+INDEX', query, re.IGNORECASE):
        optimization_points.append("ðŸ” Index optimization applied")
    
    # 2. JOINé †åºãƒ»æ–¹æ³•ã®æœ€é©åŒ–
    join_optimizations = []
    if re.search(r'BROADCAST\s+JOIN|SHUFFLE\s+JOIN', query, re.IGNORECASE):
        join_optimizations.append("JOIN strategy specification")
    if re.search(r'/\*\+\s*BROADCAST\s*\*/|/\*\+\s*SHUFFLE\s*\*/', query, re.IGNORECASE):
        join_optimizations.append("JOIN hint usage")
    if join_optimizations:
        optimization_points.append(f"ðŸ”— JOIN optimization: {', '.join(join_optimizations)}")
    
    # 3. çµ±è¨ˆæƒ…å ±ãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ’ãƒ³ãƒˆ
    if re.search(r'ANALYZE\s+TABLE|UPDATE\s+STATISTICS', query, re.IGNORECASE):
        optimization_points.append("ðŸ“Š Statistics update applied")
    if re.search(r'/\*\+[^*]*\*/|--\+', query, re.IGNORECASE):
        optimization_points.append("ðŸ’¡ Optimizer hints applied")
    
    # 4. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æœ€é©åŒ–
    if re.search(r'WHERE.*IN\s*\([^)]*SELECT|EXISTS\s*\(', query, re.IGNORECASE):
        optimization_points.append("ðŸŽ¯ Subquery filtering optimization")
    if re.search(r'PARTITION\s*\([^)]*\)', query, re.IGNORECASE):
        optimization_points.append("ðŸ“‚ Partition filtering applied")
    
    # 5. é›†ç´„ãƒ»ã‚½ãƒ¼ãƒˆæœ€é©åŒ–
    if re.search(r'GROUP\s+BY.*HAVING|WINDOW\s+FUNCTION', query, re.IGNORECASE):
        optimization_points.append("ðŸ“ˆ Aggregation optimization")
    
    # 6. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»ãƒžãƒ†ãƒªã‚¢ãƒ©ã‚¤ã‚º
    if re.search(r'CACHE\s+TABLE|MATERIALIZE', query, re.IGNORECASE):
        optimization_points.append("ðŸ’¾ Caching/Materialization applied")
    
    # 7. ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æœ€é©åŒ–
    if re.search(r'PARQUET|DELTA|COLUMNAR', query, re.IGNORECASE):
        optimization_points.append("ðŸ—ƒï¸ Storage format optimization")
    
    if not optimization_points:
        optimization_points.append("âš¡ General query structure optimization")
    
    return f"Trial {attempt_num} ({trial_type}): {'; '.join(optimization_points)}"

def save_optimization_points_summary(optimization_point: str) -> None:
    """
    æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆã‚’è¦ç´„ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜
    
    Args:
        optimization_point: æŠ½å‡ºã•ã‚ŒãŸæœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆ
    """
    try:
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        summary_filename = "optimization_points_summary.txt"
        
        with open(summary_filename, 'a', encoding='utf-8') as f:
            f.write(f"[{timestamp}] {optimization_point}\n")
        
        print(f"ðŸ“ Optimization points saved: {optimization_point}")
        
    except Exception as e:
        print(f"âš ï¸ Failed to save optimization points: {str(e)}")

def save_trial_log(optimization_point: str) -> None:
    """
    å€‹åˆ¥è©¦è¡Œãƒ­ã‚°ã‚’å°‚ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜
    
    Args:
        optimization_point: æŠ½å‡ºã•ã‚ŒãŸæœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆï¼ˆTrial X (type): pointså½¢å¼ï¼‰
    """
    try:
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        trial_log_filename = "trial_logs.txt"
        
        # Check if file exists and has header
        try:
            with open(trial_log_filename, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            file_exists_with_content = bool(content)
        except FileNotFoundError:
            file_exists_with_content = False
        
        # Create header if file doesn't exist or is empty
        if not file_exists_with_content:
            with open(trial_log_filename, 'w', encoding='utf-8') as f:
                f.write("Trial Logs - SQL Query Optimization\n")
                f.write("=====================================\n\n")
                f.write("Individual trial results from optimization attempts:\n\n")
        
        # Append the trial log entry
        with open(trial_log_filename, 'a', encoding='utf-8') as f:
            f.write(f"[{timestamp}] {optimization_point}\n")
        
        print(f"ðŸ“‹ Trial log saved: {optimization_point}")
        
    except Exception as e:
        print(f"âš ï¸ Failed to save trial log: {str(e)}")

def load_optimization_points_summary() -> str:
    """
    ä¿å­˜ã•ã‚ŒãŸæœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆè¦ç´„ã‚’èª­ã¿è¾¼ã¿
    
    Returns:
        str: æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆè¦ç´„ï¼ˆæœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”¨ï¼‰
    """
    try:
        summary_filename = "optimization_points_summary.txt"
        
        with open(summary_filename, 'r', encoding='utf-8') as f:
            content = f.read().strip()
        
        if not content:
            return ""
        
        # æœ€æ–°ã®5ã¤ã®ãƒã‚¤ãƒ³ãƒˆã«åˆ¶é™ï¼ˆãƒ¬ãƒãƒ¼ãƒˆã‚µã‚¤ã‚ºã‚’æŠ‘åˆ¶ï¼‰
        lines = content.split('\n')
        recent_points = lines[-5:] if len(lines) > 5 else lines
        
        summary = "## ðŸŽ¯ Query Optimization Points Summary\n\n"
        summary += "Recent successful optimization techniques applied:\n\n"
        
        for line in recent_points:
            if line.strip():
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’é™¤åŽ»ã—ã¦ãƒã‚¤ãƒ³ãƒˆã®ã¿ã‚’æŠ½å‡º
                point_content = line.split('] ', 1)[-1] if '] ' in line else line
                summary += f"- {point_content}\n"
        
        summary += "\n"
        return summary
        
    except FileNotFoundError:
        return ""
    except Exception as e:
        print(f"âš ï¸ Failed to load optimization points summary: {str(e)}")
        return ""

# === End of Query Optimization Points Extraction Functions ===

# COMMAND ----------

def save_debug_query_trial(query: str, attempt_num: int, trial_type: str, query_id: str = None, error_info: str = None) -> str:
    """
    Save queries under optimization attempt by attempt when DEBUG_ENABLED=Y
    
    Args:
        query: Generated query
        attempt_num: Trial number (1, 2, 3, ...)
        trial_type: Trial type ('initial', 'performance_improvement', 'error_correction')
        query_id: Query ID (optional)
        error_info: Error information (optional)
    
    Returns:
        Saved file path (empty string if not saved)
    """
    debug_enabled = globals().get('DEBUG_ENABLED', 'N')
    if debug_enabled.upper() != 'Y':
        return ""
    
    try:
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        
        # Generate query ID from trial number if not specified
        if not query_id:
            query_id = f"trial_{attempt_num}"
        
        # Generate filename: debug_trial_{attempt_num}_{trial_type}_{timestamp}.txt
        filename = f"debug_trial_{attempt_num:02d}_{trial_type}_{timestamp}.txt"
        
        # Prepare metadata information
        metadata_header = f"""-- ðŸ› DEBUG: Optimization trial query (DEBUG_ENABLED=Y)
-- ðŸ“‹ Trial number: {attempt_num}
-- ðŸŽ¯ Trial type: {trial_type}
-- ðŸ• Generated time: {timestamp}
-- ðŸ” Query ID: {query_id}
"""
        
        # Add error information if available
        if error_info:
            metadata_header += f"""-- âš ï¸  Error information: {error_info[:200]}{'...' if len(error_info) > 200 else ''}
"""
        
        metadata_header += f"""-- ðŸ“„ Generated file: {filename}
-- ================================================

"""
        
        # File saving
        full_content = metadata_header + query
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        print(f"ðŸ› DEBUG save completed: {filename} (attempt {attempt_num}: {trial_type})")
        return filename
        
    except Exception as e:
        print(f"âš ï¸ DEBUG save error: {str(e)}")
        return ""

# ðŸ§  Structured extraction settings (STRUCTURED_EXTRACTION_ENABLED: 'Y' = use structured extraction, 'N' = use traditional truncation)
# Controls the processing method for Physical Plan and EXPLAIN COST
# - 'Y': Structured extraction of important information only (recommended: high precision & high efficiency)
# - 'N': Traditional truncation based on character limits (for fallback)
STRUCTURED_EXTRACTION_ENABLED = 'Y'

# ðŸ”„ Maximum retry count settings for automatic error correction (MAX_RETRIES: default 2 times)
# Number of retries when EXPLAIN execution of LLM-generated optimized queries encounters errors
# - 1st attempt: EXPLAIN execution with initial generated query
# - 2nd attempt and beyond: Re-input error information to LLM to generate corrected query and re-execute
# - When maximum attempts reached: Use original working query for file generation
MAX_RETRIES = 3

# ðŸš€ Iterative optimization maximum attempt count settings (MAX_OPTIMIZATION_ATTEMPTS: default 3 times)
# Number of improvement attempts when performance degradation is detected
# - 1st attempt: Initial optimization query generation and performance verification
# - 2nd attempt and beyond: Corrected query generation and verification based on degradation cause analysis
# - When maximum attempts reached: Use original query
# Note: This is a separate parameter from syntax error correction (MAX_RETRIES)
MAX_OPTIMIZATION_ATTEMPTS = 3

# ðŸ’¡ Usage examples:
# OUTPUT_LANGUAGE = 'ja'  # Output files in Japanese
# OUTPUT_LANGUAGE = 'en'  # Output files in English

# ðŸŒ Multilingual message dictionary
MESSAGES = {
    'ja': {
        'bottleneck_title': 'Databricks SQL Profiler Bottleneck Analysis Results',
        'query_id': 'Query ID',
        'analysis_time': 'Analysis Date/Time',
        'execution_time': 'Execution Time',
        'sql_optimization_report': 'SQL Optimization Report',
        'optimization_time': 'Optimization Date/Time',
        'original_file': 'Original File',
        'optimized_file': 'Optimized File',
        'optimization_analysis': 'Optimization Analysis Results',
        'performance_metrics': 'Performance Metrics Reference Information',
        'read_data': 'Data Read',
        'spill': 'Spill',
        'top10_processes': 'TOP10 Most Time-Consuming Processes'
    },
    'en': {
        'bottleneck_title': 'Databricks SQL Profiler Bottleneck Analysis Results',
        'query_id': 'Query ID',
        'analysis_time': 'Analysis Time',
        'execution_time': 'Execution Time',
        'sql_optimization_report': 'SQL Optimization Report',
        'optimization_time': 'Optimization Time',
        'original_file': 'Original File',
        'optimized_file': 'Optimized File',
        'optimization_analysis': 'Optimization Analysis Results',
        'performance_metrics': 'Performance Metrics Reference',
        'read_data': 'Data Read',
        'spill': 'Spill',
        'top10_processes': 'Top 10 Most Time-Consuming Processes'
    }
}

def get_message(key: str) -> str:
    """Get multilingual message"""
    return MESSAGES.get(OUTPUT_LANGUAGE, MESSAGES['ja']).get(key, key)

# ðŸ“‹ Supported file path format examples:
# Unity Catalog Volumes:
# JSON_FILE_PATH = '/Volumes/catalog/schema/volume/profiler.json'
# 
# FileStore (recommended):
# JSON_FILE_PATH = '/FileStore/shared_uploads/your_username/profiler_log.json'
# 
# DBFS:
# JSON_FILE_PATH = '/dbfs/FileStore/shared_uploads/your_username/profiler_log.json'
# 
# DBFS URI:
# JSON_FILE_PATH = 'dbfs:/FileStore/shared_uploads/your_username/profiler_log.json'

print("ðŸ“ ã€Analysis Target File Configuration Completedã€‘")
print("=" * 50)
print(f"ðŸ“„ Target file: {JSON_FILE_PATH}")
print("=" * 50)

# âš™ï¸ Basic environment configuration
import json
try:
    import pandas as pd
except ImportError:
    print("Warning: pandas is not installed, some features may not work")
    pd = None
from typing import Dict, List, Any, Optional
from datetime import datetime

print("âœ… Basic library import completed")
print("ðŸš€ Please proceed to the next cell")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸ¤– LLM Endpoint Configuration
# MAGIC
# MAGIC This cell performs the following configurations:
# MAGIC - LLM provider selection (Databricks/OpenAI/Azure/Anthropic)
# MAGIC - Connection settings for each provider
# MAGIC - Required library imports

# COMMAND ----------

# ðŸ¤– LLM Endpoint Configuration
LLM_CONFIG = {
    # Endpoint type: 'databricks', 'openai', 'azure_openai', 'anthropic'
    "provider": "databricks",
    
    # Databricks Model Serving configuration (high-speed execution priority)
    "databricks": {
        "endpoint_name": "databricks-claude-3-7-sonnet",  # Model Serving endpoint name
        "max_tokens": 131072,  # 128K tokens (Claude 3.7 Sonnet maximum limit)
        "temperature": 0.0,    # For deterministic output (0.1â†’0.0)
        # "thinking_enabled": False,  # Extended thinking mode (default: disabled - high-speed execution priority) - Claude 3 Sonnet only
        # "thinking_budget_tokens": 65536  # Thinking token budget 64K tokens (used only when enabled) - Claude 3 Sonnet only
    },
    
    # OpenAI configuration (optimized for complete SQL generation)
    "openai": {
        "api_key": "",  # OpenAI API key (can also use environment variable OPENAI_API_KEY)
        "model": "gpt-4o",  # gpt-4o, gpt-4-turbo, gpt-3.5-turbo
        "max_tokens": 16000,  # Maximum within OpenAI limits
        "temperature": 0.0    # For deterministic output (0.1â†’0.0)
    },
    
    # Azure OpenAI configuration (optimized for complete SQL generation)
    "azure_openai": {
        "api_key": "",  # Azure OpenAI API key (can also use environment variable AZURE_OPENAI_API_KEY)
        "endpoint": "",  # https://your-resource.openai.azure.com/
        "deployment_name": "",  # Deployment name
        "api_version": "2024-02-01",
        "max_tokens": 16000,  # Maximum within Azure OpenAI limits
        "temperature": 0.0    # For deterministic output (0.1â†’0.0)
    },
    
    # Anthropic configuration (optimized for complete SQL generation)
    "anthropic": {
        "api_key": "",  # Anthropic API key (can also use environment variable ANTHROPIC_API_KEY)
        "model": "claude-3-5-sonnet-20241022",  # claude-3-5-sonnet-20241022, claude-3-opus-20240229
        "max_tokens": 16000,  # Maximum within Anthropic limits
        "temperature": 0.0    # For deterministic output (0.1â†’0.0)
    }
}

print("ðŸ¤– LLM endpoint configuration completed")
print(f"ðŸ¤– LLM Provider: {LLM_CONFIG['provider']}")

if LLM_CONFIG['provider'] == 'databricks':
    print(f"ðŸ”— Databricks endpoint: {LLM_CONFIG['databricks']['endpoint_name']}")
    thinking_status = "Enabled" if LLM_CONFIG['databricks'].get('thinking_enabled', False) else "Disabled"
    thinking_budget = LLM_CONFIG['databricks'].get('thinking_budget_tokens', 65536)
    max_tokens = LLM_CONFIG['databricks'].get('max_tokens', 131072)
    print(f"ðŸ§  Extended thinking mode: {thinking_status} (budget: {thinking_budget:,} tokens)")
    print(f"ðŸ“Š Maximum tokens: {max_tokens:,} tokens ({max_tokens//1024}K)")
    if not LLM_CONFIG['databricks'].get('thinking_enabled', False):
        print("âš¡ Fast execution mode: Skip thinking process for rapid result generation")
elif LLM_CONFIG['provider'] == 'openai':
    print(f"ðŸ”— OpenAI model: {LLM_CONFIG['openai']['model']}")
elif LLM_CONFIG['provider'] == 'azure_openai':
    print(f"ðŸ”— Azure OpenAI deployment: {LLM_CONFIG['azure_openai']['deployment_name']}")
elif LLM_CONFIG['provider'] == 'anthropic':
    print(f"ðŸ”— Anthropic model: {LLM_CONFIG['anthropic']['model']}")

print()
print("ðŸ’¡ LLM provider switching examples:")
print('   LLM_CONFIG["provider"] = "openai"      # Switch to OpenAI GPT-4')
print('   LLM_CONFIG["provider"] = "anthropic"   # Switch to Anthropic Claude')
print('   LLM_CONFIG["provider"] = "azure_openai" # Switch to Azure OpenAI')
print()
print("ðŸ§  Databricks extended thinking mode configuration examples:")
print('   LLM_CONFIG["databricks"]["thinking_enabled"] = False  # Disable extended thinking mode (default, fast execution)')
print('   LLM_CONFIG["databricks"]["thinking_enabled"] = True   # Enable extended thinking mode (detailed analysis only)')
print('   LLM_CONFIG["databricks"]["thinking_budget_tokens"] = 65536  # Thinking token budget (64K)')
print('   LLM_CONFIG["databricks"]["max_tokens"] = 131072  # Maximum tokens (128K)')
print()

# Import necessary libraries
try:
    import requests
except ImportError:
    print("Warning: requests is not installed, some features may not work")
    requests = None
import os
try:
    from pyspark.sql import SparkSession
except ImportError:
    print("Warning: pyspark is not installed")
    SparkSession = None
    print("âœ… Spark Version: Not available")

# Safely retrieve Databricks Runtime information
try:
    if spark is not None:
        runtime_version = spark.conf.get('spark.databricks.clusterUsageTags.sparkVersion')
    print(f"âœ… Databricks Runtime: {runtime_version}")
except Exception:
    try:
        # Retrieve DBR information using alternative method
        dbr_version = spark.conf.get('spark.databricks.clusterUsageTags.clusterName', 'Unknown')
        print(f"âœ… Databricks Cluster: {dbr_version}")
    except Exception:
        print("âœ… Databricks Environment: Skipped configuration information retrieval")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸ“‚ SQL Profiler JSON File Loading Function
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - SQL profiler JSON file loading
# MAGIC - Automatic detection of DBFS/FileStore/local paths
# MAGIC - File size and data information display

# COMMAND ----------

def load_profiler_json(file_path: str) -> Dict[str, Any]:
    """
    Load SQL profiler JSON file
    
    Args:
        file_path: JSON file path (DBFS or local path)
        
    Returns:
        Dict: Parsed JSON data
    """
    try:
        # Handle DBFS paths appropriately
        if file_path.startswith('/dbfs/'):
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        elif file_path.startswith('dbfs:/'):
            # Convert dbfs: prefix to /dbfs/
            local_path = file_path.replace('dbfs:', '/dbfs')
            with open(local_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        elif file_path.startswith('/FileStore/'):
            # Convert FileStore path to /dbfs/FileStore/
            local_path = '/dbfs' + file_path
            with open(local_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        else:
            # Local file
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        
        print(f"âœ… Successfully loaded JSON file: {file_path}")
        print(f"ðŸ“Š Data size: {len(str(data)):,} characters")
        return data
    except Exception as e:
        print(f"âŒ File loading error: {str(e)}")
        return {}

print("âœ… Function definition completed: load_profiler_json")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸ“Š Performance Metrics Extraction Function
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - Metrics extraction from SQL profiler data
# MAGIC - Query basic information retrieval
# MAGIC - Overall/stage/node-level performance indicator calculation
# MAGIC - Spill detection and bottleneck indicator analysis

# COMMAND ----------

def detect_data_format(profiler_data: Dict[str, Any]) -> str:
    """
    Detect JSON data format
    """
    # SQL profiler format detection
    if 'graphs' in profiler_data and isinstance(profiler_data['graphs'], list):
        if len(profiler_data['graphs']) > 0:
            return 'sql_profiler'
    
    # SQL query summary format detection (test2.json format)
    if 'query' in profiler_data and 'planMetadatas' in profiler_data:
        query_data = profiler_data.get('query', {})
        if 'metrics' in query_data:
            return 'sql_query_summary'
    
    return 'unknown'

def extract_performance_metrics_from_query_summary(profiler_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extract basic metrics from Databricks SQL query summary format JSON
    (supports test2.json format)
    """
    try:
        query_data = profiler_data.get('query', {})
        metrics_data = query_data.get('metrics', {})
        
        if not metrics_data:
            print("âš ï¸ No metrics data found")
            return {}
        
        print(f"âœ… Detected SQL query summary format metrics")
        print(f"   - Execution time: {metrics_data.get('totalTimeMs', 0):,} ms")
        print(f"   - Data read: {metrics_data.get('readBytes', 0) / 1024 / 1024 / 1024:.2f} GB")
        print(f"   - Rows processed: {metrics_data.get('rowsReadCount', 0):,} rows")
        
        # Extract basic metrics
        overall_metrics = {
            'total_time_ms': metrics_data.get('totalTimeMs', 0),
            'execution_time_ms': metrics_data.get('executionTimeMs', 0),
            'compilation_time_ms': metrics_data.get('compilationTimeMs', 0),
            'read_bytes': metrics_data.get('readBytes', 0),
            'read_remote_bytes': metrics_data.get('readRemoteBytes', 0),
            'read_cache_bytes': metrics_data.get('readCacheBytes', 0),
            'spill_to_disk_bytes': metrics_data.get('spillToDiskBytes', 0),
            'rows_produced_count': metrics_data.get('rowsProducedCount', 0),
            'rows_read_count': metrics_data.get('rowsReadCount', 0),
            'read_files_count': metrics_data.get('readFilesCount', 0),
            'read_partitions_count': metrics_data.get('readPartitionsCount', 0),
            'photon_total_time_ms': metrics_data.get('photonTotalTimeMs', 0),
            'task_total_time_ms': metrics_data.get('taskTotalTimeMs', 0),
            'network_sent_bytes': metrics_data.get('networkSentBytes', 0),
            'photon_enabled': metrics_data.get('photonTotalTimeMs', 0) > 0,
            'photon_utilization_ratio': 0
        }
        
        # Calculate Photon utilization rate
        if overall_metrics['task_total_time_ms'] > 0:
            overall_metrics['photon_utilization_ratio'] = min(
                overall_metrics['photon_total_time_ms'] / overall_metrics['task_total_time_ms'], 1.0
            )
        
        # Calculate cache hit rate
        cache_hit_ratio = 0
        if overall_metrics['read_bytes'] > 0:
            cache_hit_ratio = overall_metrics['read_cache_bytes'] / overall_metrics['read_bytes']
        
        # Calculate bottleneck indicators
        bottleneck_indicators = {
            'spill_bytes': overall_metrics['spill_to_disk_bytes'],
            'has_spill': overall_metrics['spill_to_disk_bytes'] > 0,
            'cache_hit_ratio': cache_hit_ratio,
            'has_cache_miss': cache_hit_ratio < 0.8,
            'photon_efficiency': overall_metrics['photon_utilization_ratio'],
            'has_shuffle_bottleneck': False,  # Cannot determine due to lack of detailed information
            'remote_read_ratio': 0,
            'has_memory_pressure': overall_metrics['spill_to_disk_bytes'] > 0,
            'max_task_duration_ratio': 1.0,  # Unknown
            'has_data_skew': False  # Cannot determine due to lack of detailed information
        }
        
        # Calculate remote read ratio
        if overall_metrics['read_bytes'] > 0:
            bottleneck_indicators['remote_read_ratio'] = overall_metrics['read_remote_bytes'] / overall_metrics['read_bytes']
        
        # Extract query information
        query_info = {
            'query_id': query_data.get('id', ''),
            'query_text': query_data.get('queryText', '')[:300] + "..." if len(query_data.get('queryText', '')) > 300 else query_data.get('queryText', ''),
            'status': query_data.get('status', ''),
            'query_start_time': query_data.get('queryStartTimeMs', 0),
            'query_end_time': query_data.get('queryEndTimeMs', 0),
            'spark_ui_url': query_data.get('sparkUiUrl', ''),
            'endpoint_id': query_data.get('endpointId', ''),
            'user': query_data.get('user', {}).get('displayName', ''),
            'statement_type': query_data.get('statementType', ''),
            'plans_state': query_data.get('plansState', '')
        }
        
        # Calculate detailed performance insights (recalculated later with node_metrics)
        performance_insights = calculate_performance_insights_from_metrics(overall_metrics, None)
        
        # Pseudo node metrics (generated from summary information)
        summary_node = {
            'node_id': 'summary_node',
            'name': f'Query Execution Summary ({query_data.get("statementType", "SQL")})',
            'tag': 'QUERY_SUMMARY',
            'key_metrics': {
                'durationMs': overall_metrics['total_time_ms'],
                'rowsNum': overall_metrics['rows_read_count'],
                'peakMemoryBytes': 0,  # Unknown
                'throughputMBps': performance_insights['parallelization']['throughput_mb_per_second'],
                'dataSelectivity': performance_insights['data_efficiency']['data_selectivity'],
                'cacheHitRatio': performance_insights['cache_efficiency']['cache_hit_ratio']
            },
            'detailed_metrics': {
                'Total Time': {'value': overall_metrics['total_time_ms'], 'display_name': 'Total Time'},
                'Read Bytes': {'value': overall_metrics['read_bytes'], 'display_name': 'Read Bytes'},
                'Spill Bytes': {'value': overall_metrics['spill_to_disk_bytes'], 'display_name': 'Spill to Disk'},
                'Photon Time': {'value': overall_metrics['photon_total_time_ms'], 'display_name': 'Photon Time'},
                'Rows Read': {'value': overall_metrics['rows_read_count'], 'display_name': 'Rows Read Count'},
                'Cache Hit Ratio': {'value': performance_insights['cache_efficiency']['cache_hit_ratio'], 'display_name': 'Cache Hit Ratio'},
                'Filter Rate': {'value': performance_insights['data_efficiency']['data_selectivity'], 'display_name': 'Filter Rate'},
                'Throughput': {'value': performance_insights['parallelization']['throughput_mb_per_second'], 'display_name': 'Throughput (MB/s)'}
            },
            'graph_index': 0,
            'performance_insights': performance_insights
        }
        
        # Recalculate performance_insights with complete metrics
        complete_metrics = {
            'overall_metrics': overall_metrics,
            'node_metrics': [summary_node]
        }
        performance_insights = calculate_performance_insights_from_metrics(overall_metrics, complete_metrics)
        
        return {
            'data_format': 'sql_query_summary',
            'query_info': query_info,
            'overall_metrics': overall_metrics,
            'bottleneck_indicators': bottleneck_indicators,
            'node_metrics': [summary_node],
            'stage_metrics': [],  # No detailed stage information
            'liquid_clustering_analysis': {},  # To be added later
            'raw_profiler_data': profiler_data,
            'performance_insights': performance_insights,  # Add detailed performance insights
            'analysis_capabilities': [
                'Metrics-based bottleneck analysis (cache efficiency, filter rate, Photon efficiency)',
                'Resource usage analysis (spill, parallelization efficiency, throughput)',
                'Performance metrics calculation (file efficiency, partition efficiency)',
                'Potential bottleneck identification (metrics-based)'
            ],
            'analysis_limitations': [
                'Detailed execution plan information (nodes, edges) is not available',
                'Stage-level metrics are not available', 
                'BROADCAST analysis is limited to basic estimation only',
                'Liquid Clustering analysis provides general recommendations only',
                'Data skew detection is based on average-value estimation only',
                'Detailed query structure analysis is not performed (metrics-focused approach)'
            ]
        }
        
    except Exception as e:
        print(f"âš ï¸ Error extracting SQL query summary format metrics: {str(e)}")
        return {}

def extract_performance_metrics(profiler_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extract bottleneck analysis metrics from SQL profiler data (supports multiple formats)
    """
    # Detect data format
    data_format = detect_data_format(profiler_data)
    
    print(f"ðŸ” Detected data format: {data_format}")
    
    if data_format == 'sql_query_summary':
        print("ðŸ“Š Processing as Databricks SQL query summary format...")
        result = extract_performance_metrics_from_query_summary(profiler_data)
        if result:
            # Add Liquid Clustering analysis (with limitations)
            try:
                result["liquid_clustering_analysis"] = analyze_liquid_clustering_opportunities(profiler_data, result)
            except Exception as e:
                print(f"âš ï¸ Skipping Liquid Clustering analysis: {str(e)}")
                result["liquid_clustering_analysis"] = {}
        return result
    elif data_format == 'sql_profiler':
        print("ðŸ“Š Processing as SQL profiler detailed format...")
        # Continue processing existing SQL profiler format
        pass
    else:
        print(f"âš ï¸ Unknown data format: {data_format}")
        return {}
    
    # Processing existing SQL profiler format
    metrics = {
        "query_info": {},
        "overall_metrics": {},
        "stage_metrics": [],
        "node_metrics": [],
        "bottleneck_indicators": {},
        "liquid_clustering_analysis": {},
        "raw_profiler_data": profiler_data  # Save raw data for plan analysis
    }
    
    # Basic query information
    if 'query' in profiler_data:
        query = profiler_data['query']
        metrics["query_info"] = {
            "query_id": query.get('id', ''),
            "status": query.get('status', ''),
            "query_start_time": query.get('queryStartTimeMs', 0),
            "query_end_time": query.get('queryEndTimeMs', 0),
            "user": query.get('user', {}).get('displayName', ''),
            "query_text": query.get('queryText', '')[:300] + "..." if len(query.get('queryText', '')) > 300 else query.get('queryText', '')
        }
        
        # Overall metrics
        if 'metrics' in query:
            query_metrics = query['metrics']
            metrics["overall_metrics"] = {
                "total_time_ms": query_metrics.get('totalTimeMs', 0),
                "compilation_time_ms": query_metrics.get('compilationTimeMs', 0),
                "execution_time_ms": query_metrics.get('executionTimeMs', 0),
                "read_bytes": query_metrics.get('readBytes', 0),
                "read_remote_bytes": query_metrics.get('readRemoteBytes', 0),
                "read_cache_bytes": query_metrics.get('readCacheBytes', 0),
                "rows_produced_count": query_metrics.get('rowsProducedCount', 0),
                "rows_read_count": query_metrics.get('rowsReadCount', 0),
                "spill_to_disk_bytes": query_metrics.get('spillToDiskBytes', 0),
                "read_files_count": query_metrics.get('readFilesCount', 0),
                "task_total_time_ms": query_metrics.get('taskTotalTimeMs', 0),
                "photon_total_time_ms": query_metrics.get('photonTotalTimeMs', 0),
                # Photon usage analysis (Photon execution time / total task time)
                "photon_enabled": query_metrics.get('photonTotalTimeMs', 0) > 0,
                "photon_utilization_ratio": min(query_metrics.get('photonTotalTimeMs', 0) / max(query_metrics.get('taskTotalTimeMs', 1), 1), 1.0)
            }
    
    # Extract stage and node metrics from graph data (supports multiple graphs)
    if 'graphs' in profiler_data and profiler_data['graphs']:
        # Analyze all graphs
        for graph_index, graph in enumerate(profiler_data['graphs']):
            print(f"ðŸ” Analyzing graph {graph_index}...")
            
            # Stage data
            if 'stageData' in graph:
                for stage in graph['stageData']:
                    stage_metric = {
                        "stage_id": stage.get('stageId', ''),
                        "status": stage.get('status', ''),
                        "duration_ms": stage.get('keyMetrics', {}).get('durationMs', 0),
                        "num_tasks": stage.get('numTasks', 0),
                        "num_failed_tasks": stage.get('numFailedTasks', 0),
                        "num_complete_tasks": stage.get('numCompleteTasks', 0),
                        "start_time_ms": stage.get('startTimeMs', 0),
                        "end_time_ms": stage.get('endTimeMs', 0),
                        "graph_index": graph_index  # Record which graph this originates from
                    }
                    metrics["stage_metrics"].append(stage_metric)
            
            # Node data (important ones only)
            if 'nodes' in graph:
                for node in graph['nodes']:
                    if not node.get('hidden', False):
                        # Use keyMetrics as-is (durationMs is already in milliseconds)
                        key_metrics = node.get('keyMetrics', {})
                        
                        node_metric = {
                            "node_id": node.get('id', ''),
                            "name": node.get('name', ''),
                            "tag": node.get('tag', ''),
                            "key_metrics": key_metrics,  # Unit-converted key_metrics
                            "metrics": node.get('metrics', []),  # Retain original metrics array
                            "metadata": node.get('metadata', []),  # Add metadata
                            "graph_index": graph_index  # Record which graph this originates from
                        }
                        
                        # Extract only important metrics in detail (added spill-related keywords, label support)
                        detailed_metrics = {}
                        for metric in node.get('metrics', []):
                            metric_key = metric.get('key', '')
                            metric_label = metric.get('label', '')
                            
                            # Check keywords in both key and label
                            key_keywords = ['TIME', 'MEMORY', 'ROWS', 'BYTES', 'DURATION', 'PEAK', 'CUMULATIVE', 'EXCLUSIVE', 
                                           'SPILL', 'DISK', 'PRESSURE', 'SINK']
                            
                            # Extract when metric_key or metric_label contains important keywords
                            is_important_metric = (
                                any(keyword in metric_key.upper() for keyword in key_keywords) or
                                any(keyword in metric_label.upper() for keyword in key_keywords)
                            )
                            
                            if is_important_metric:
                                                                  # Use label as metric name if valid, otherwise use key
                                metric_name = metric_label if metric_label and metric_label != 'UNKNOWN_KEY' else metric_key
                                detailed_metrics[metric_name] = {
                                    'value': metric.get('value', 0),
                                    'label': metric_label,
                                    'type': metric.get('metricType', ''),
                                    'original_key': metric_key,  # Save original key name
                                    'display_name': metric_name  # Display name
                                }
                        node_metric['detailed_metrics'] = detailed_metrics
                        metrics["node_metrics"].append(node_metric)
    
    # Calculate bottleneck indicators
    metrics["bottleneck_indicators"] = calculate_bottleneck_indicators(metrics)
    
    # Liquid Clustering analysis
    metrics["liquid_clustering_analysis"] = analyze_liquid_clustering_opportunities(profiler_data, metrics)
    
    return metrics

print("âœ… Function definition completed: extract_performance_metrics")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸ·ï¸ Node Name Analysis & Enhancement Function
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - Concretization of generic node names (Whole Stage Codegen, etc.)
# MAGIC - Related node search and optimal processing name selection
# MAGIC - Addition of Photon information and table information
# MAGIC - Semantic improvement of processing names

# COMMAND ----------

def get_meaningful_node_name(node: Dict[str, Any], extracted_metrics: Dict[str, Any]) -> str:
    """
    Function to get more meaningful node names
    Convert generic names (such as Whole Stage Codegen) to specific process names
    """
    original_name = node.get('name', '')
    node_id = node.get('node_id', node.get('id', ''))
    node_tag = node.get('tag', '')
    
    # Get detailed information from metadata
    metadata = node.get('metadata', [])
    metadata_info = {}
    for meta in metadata:
        key = meta.get('key', '')
        value = meta.get('value', '')
        label = meta.get('label', '')
        if value:
            metadata_info[key] = value
    
    # 1. Replace generic names with specific names
    if 'whole stage codegen' in original_name.lower():
        # Heuristic to infer more specific process names
        
        # Infer relevance based on node ID (adjacent IDs)
        node_id_num = None
        try:
            node_id_num = int(node_id) if node_id else None
        except:
            pass
        
        if node_id_num:
            # Look for specific processes with nearby IDs in the same file
            all_nodes = extracted_metrics.get('node_metrics', [])
            nearby_specific_nodes = []
            
            for other_node in all_nodes:
                other_id = other_node.get('node_id', '')
                other_name = other_node.get('name', '')
                
                try:
                    other_id_num = int(other_id) if other_id else None
                    if other_id_num and abs(other_id_num - node_id_num) <= 10:  # Within 10 nearby
                        if is_specific_process_name(other_name):
                            nearby_specific_nodes.append(other_name)
                except:
                    continue
            
            # Select the most specific process name
            if nearby_specific_nodes:
                specific_name = get_most_specific_process_name_from_list(nearby_specific_nodes)
                if specific_name and specific_name != original_name:
                    return f"{specific_name} (Whole Stage Codegen)"
        
        # Fallback: Extract more specific information from tag
        if 'CODEGEN' in node_tag:
            # Check child tag information from metadata
            child_tag = metadata_info.get('CHILD_TAG', '')
            if child_tag and child_tag != 'Child':
                return f"Whole Stage Codegen ({child_tag})"
    
    # 2. Reflect more specific tag information in node name
    tag_to_name_mapping = {
        'PHOTON_SHUFFLE_EXCHANGE_SINK_EXEC': 'Photon Shuffle Exchange',
        'PHOTON_GROUPING_AGG_EXEC': 'Photon Grouping Aggregate', 
        'UNKNOWN_DATA_SOURCE_SCAN_EXEC': 'Data Source Scan',
        'HASH_AGGREGATE_EXEC': 'Hash Aggregate',
        'WHOLE_STAGE_CODEGEN_EXEC': 'Whole Stage Codegen'
    }
    
    if node_tag in tag_to_name_mapping:
        mapped_name = tag_to_name_mapping[node_tag]
        if mapped_name != original_name and mapped_name != 'Whole Stage Codegen':
            # Use tag if it's more specific
            enhanced_name = mapped_name
        else:
            enhanced_name = original_name
    else:
        enhanced_name = original_name
    
    # 3. Add processing details from metadata
    
    # Add database and table information (enhanced version)
    table_name = None
    
    # Extract table name from multiple metadata keys (prioritize full path)
    for key_candidate in ['SCAN_TABLE', 'SCAN_IDENTIFIER', 'TABLE_NAME', 'RELATION', 'SCAN_RELATION']:
        if key_candidate in metadata_info:
            extracted_table = metadata_info[key_candidate]
            # Use as-is for full path (catalog.schema.table)
            if isinstance(extracted_table, str) and extracted_table.count('.') >= 2:
                table_name = extracted_table
                break
            elif isinstance(extracted_table, str) and extracted_table.count('.') == 1:
                # Use as-is for schema.table format as well
                table_name = extracted_table
                break
            elif not table_name:  # Set only if table name has not been found yet
                table_name = extracted_table
    
    # If table name cannot be extracted from metadata, infer from node name
    if not table_name and ('scan' in enhanced_name.lower() or 'data source' in enhanced_name.lower()):
        # Infer table name from node name
        import re
        
        # Format like "Scan tpcds.tpcds_sf1000_delta_lc.customer"
        table_patterns = [
            r'[Ss]can\s+([a-zA-Z_][a-zA-Z0-9_.]*[a-zA-Z0-9_])',
            r'[Tt]able\s+([a-zA-Z_][a-zA-Z0-9_.]*[a-zA-Z0-9_])',
            r'([a-zA-Z_][a-zA-Z0-9_]*\.)+([a-zA-Z_][a-zA-Z0-9_]*)',
        ]
        
        for pattern in table_patterns:
            match = re.search(pattern, original_name)
            if match:
                if '.' in match.group(0):
                    # Use full path for full table name (catalog.schema.table)
                    table_name = match.group(0)
                else:
                    table_name = match.group(1) if match.lastindex and match.lastindex >= 1 else match.group(0)
                break
    
            # Search for table name from metadata values field as well
    if not table_name:
        for meta in metadata:
            values = meta.get('values', [])
            if values:
                for value in values:
                    if isinstance(value, str) and '.' in value and len(value.split('.')) >= 2:
                        # For "catalog.schema.table" format
                        parts = value.split('.')
                        if len(parts) >= 2 and not any(part.isdigit() for part in parts[-2:]):
                            # Use full path (catalog.schema.table)
                            if len(parts) >= 3:
                                table_name = '.'.join(parts)  # Full path
                            else:
                                table_name = value  # Use as is
                            break
                if table_name:
                    break
    
    # Display table name for Data Source Scan
    if table_name and ('scan' in enhanced_name.lower() or 'data source' in enhanced_name.lower()):
        # Relax limits for full path display (up to 60 characters)
        if len(table_name) > 60:
            # Abbreviate middle part for catalog.schema.table format
            parts = table_name.split('.')
            if len(parts) >= 3:
                table_name = f"{parts[0]}.*.{parts[-1]}"
            else:
                table_name = table_name[:57] + "..."
        enhanced_name = f"Data Source Scan ({table_name})"
    elif 'scan' in enhanced_name.lower() and 'data source' in enhanced_name.lower():
        # Use clearer name even when table name is not found
        enhanced_name = "Data Source Scan"
    
    # Add Photon information
    if 'IS_PHOTON' in metadata_info and metadata_info['IS_PHOTON'] == 'true':
        if not enhanced_name.startswith('Photon'):
            enhanced_name = f"Photon {enhanced_name}"
    
    return enhanced_name

def find_related_specific_nodes(target_node_id: str, nodes: list, edges: list) -> list:
    """Search for specific processing nodes related to the specified node"""
    
    # Identify related nodes from edges
    related_node_ids = set()
    
    # Directly connected nodes
    for edge in edges:
        from_id = edge.get('fromId', '')
        to_id = edge.get('toId', '')
        
        if from_id == target_node_id:
            related_node_ids.add(to_id)
        elif to_id == target_node_id:
            related_node_ids.add(from_id)
    
    # Get details of related nodes
    related_nodes = []
    for node in nodes:
        node_id = node.get('id', '')
        if node_id in related_node_ids:
            node_name = node.get('name', '')
            # Select only nodes with specific process names
            if is_specific_process_name(node_name):
                related_nodes.append(node)
    
    return related_nodes

def is_specific_process_name(name: str) -> bool:
    """Determine if it's a specific processing name"""
    specific_keywords = [
        'columnar to row', 'row to columnar', 'filter', 'project', 'join',
        'aggregate', 'sort', 'exchange', 'broadcast', 'scan', 'union'
    ]
    
    generic_keywords = [
        'whole stage codegen', 'stage', 'query', 'result'
    ]
    
    name_lower = name.lower()
    
            # When containing specific keywords
    for keyword in specific_keywords:
        if keyword in name_lower:
            return True
    
            # Exclude cases with only generic keywords
    for keyword in generic_keywords:
        if keyword in name_lower and len(name_lower.split()) <= 3:
            return False
    
    return True

def get_most_specific_process_name(nodes: list) -> str:
    """Select the most specific processing name"""
    if not nodes:
        return ""
    
    # Priority: More specific and meaningful process names
    priority_keywords = [
        'columnar to row', 'row to columnar', 'filter', 'project',
        'hash join', 'broadcast join', 'sort merge join',
        'hash aggregate', 'sort aggregate', 'grouping aggregate'
    ]
    
    for keyword in priority_keywords:
        for node in nodes:
            node_name = node.get('name', '').lower()
            if keyword in node_name:
                return node.get('name', '')
    
    # Fallback: First specific node name
    for node in nodes:
        node_name = node.get('name', '')
        if is_specific_process_name(node_name):
            return node_name
    
    return ""

def get_most_specific_process_name_from_list(node_names: list) -> str:
    """Select the most specific processing name from a list of node names"""
    if not node_names:
        return ""
    
    # Priority: More specific and meaningful process names
    priority_keywords = [
        'columnar to row', 'row to columnar', 'filter', 'project',
        'hash join', 'broadcast join', 'sort merge join',
        'hash aggregate', 'sort aggregate', 'grouping aggregate'
    ]
    
    for keyword in priority_keywords:
        for name in node_names:
            if keyword in name.lower():
                return name
    
    # Fallback: First specific node name
    for name in node_names:
        if is_specific_process_name(name):
            return name
    
    return ""

def extract_shuffle_attributes(node: Dict[str, Any]) -> list:
    """
    Extract SHUFFLE_ATTRIBUTES from Shuffle node
    
    Args:
        node: Node information
        
    Returns:
        list: Detected Shuffle attributes
    """
    shuffle_attributes = []
    
    # Search for SHUFFLE_ATTRIBUTES from metadata
    metadata = node.get('metadata', [])
    if isinstance(metadata, list):
        for item in metadata:
            if isinstance(item, dict):
                item_key = item.get('key', '')
                item_label = item.get('label', '')
                item_values = item.get('values', [])
                
                # Check both key and label
                if (item_key == 'SHUFFLE_ATTRIBUTES' or 
                    item_label == 'Shuffle attributes'):
                    if isinstance(item_values, list):
                        shuffle_attributes.extend(item_values)
    
    # Search from raw_metrics as well (also check label)
    raw_metrics = node.get('metrics', [])
    if isinstance(raw_metrics, list):
        for metric in raw_metrics:
            if isinstance(metric, dict):
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_values = metric.get('values', [])
                
                if (metric_key == 'SHUFFLE_ATTRIBUTES' or 
                    metric_label == 'Shuffle attributes'):
                    if isinstance(metric_values, list):
                        shuffle_attributes.extend(metric_values)
    
    # Search from detailed_metrics as well
    detailed_metrics = node.get('detailed_metrics', {})
    if isinstance(detailed_metrics, dict):
        for key, info in detailed_metrics.items():
            if (key == 'SHUFFLE_ATTRIBUTES' or 
                (isinstance(info, dict) and info.get('label') == 'Shuffle attributes')):
                values = info.get('values', []) if isinstance(info, dict) else []
                if isinstance(values, list):
                    shuffle_attributes.extend(values)
    
    # Remove duplicates
    return list(set(shuffle_attributes))

def extract_cluster_attributes(node: Dict[str, Any]) -> list:
    """
    Extract clustering keys (SCAN_CLUSTERS) from scan node
    
    Args:
        node: Node information
        
    Returns:
        list: Detected clustering keys
    """
    cluster_attributes = []
    node_name = node.get('name', 'Unknown')
    
    # Check DEBUG_JSON_ENABLED setting for debug output
    debug_json_enabled = globals().get('DEBUG_JSON_ENABLED', 'N')
    
    if debug_json_enabled.upper() == 'Y':
        print(f"    ðŸ” Debug extract_cluster_attributes for: {node_name}")
    
    # Search for SCAN_CLUSTERS from metadata
    metadata = node.get('metadata', [])
    if debug_json_enabled.upper() == 'Y':
        print(f"      - metadata type: {type(metadata)}, length: {len(metadata) if isinstance(metadata, list) else 'N/A'}")
    
    if isinstance(metadata, list):
        for i, item in enumerate(metadata):
            if isinstance(item, dict):
                item_key = item.get('key', '')
                item_label = item.get('label', '')
                item_values = item.get('values', [])
                
                if debug_json_enabled.upper() == 'Y':
                    print(f"        metadata[{i}]: key='{item_key}', label='{item_label}', values={item_values}")
                
                # Check both key and label
                if (item_key == 'SCAN_CLUSTERS' or 
                    item_label == 'Cluster attributes'):
                    if debug_json_enabled.upper() == 'Y':
                        print(f"        *** FOUND SCAN_CLUSTERS in metadata: {item_values}")
                    if isinstance(item_values, list):
                        cluster_attributes.extend(item_values)
    
    # Search from raw_metrics as well (also check label)
    raw_metrics = node.get('metrics', [])
    if debug_json_enabled.upper() == 'Y':
        print(f"      - metrics type: {type(raw_metrics)}, length: {len(raw_metrics) if isinstance(raw_metrics, list) else 'N/A'}")
    
    if isinstance(raw_metrics, list):
        scan_clusters_found = False
        for i, metric in enumerate(raw_metrics):
            if isinstance(metric, dict):
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_values = metric.get('values', [])
                
                if (metric_key == 'SCAN_CLUSTERS' or 
                    metric_label == 'Cluster attributes'):
                    if debug_json_enabled.upper() == 'Y':
                        print(f"        *** FOUND SCAN_CLUSTERS in metrics[{i}]: key='{metric_key}', label='{metric_label}', values={metric_values}")
                    scan_clusters_found = True
                    if isinstance(metric_values, list):
                        cluster_attributes.extend(metric_values)
        
        if not scan_clusters_found and debug_json_enabled.upper() == 'Y':
            print(f"        No SCAN_CLUSTERS found in {len(raw_metrics)} metrics")
    
    # Search from detailed_metrics as well
    detailed_metrics = node.get('detailed_metrics', {})
    if debug_json_enabled.upper() == 'Y':
        print(f"      - detailed_metrics type: {type(detailed_metrics)}")
    
    if isinstance(detailed_metrics, dict):
        for key, info in detailed_metrics.items():
            if (key == 'SCAN_CLUSTERS' or 
                (isinstance(info, dict) and info.get('label') == 'Cluster attributes')):
                if debug_json_enabled.upper() == 'Y':
                    print(f"        *** FOUND SCAN_CLUSTERS in detailed_metrics: {key}")
                values = info.get('values', []) if isinstance(info, dict) else []
                if isinstance(values, list):
                    cluster_attributes.extend(values)
    
    # Remove duplicates
    final_result = list(set(cluster_attributes))
    if debug_json_enabled.upper() == 'Y':
        print(f"      â†’ Final clustering keys: {final_result}")
    return final_result

def extract_parallelism_metrics(node: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extract multiple Tasks total metrics and AQEShuffleRead metrics from node
    
    ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œãªã©ã§ã¯ä»¥ä¸‹ã®è¤‡æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå­˜åœ¨ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼š
    - Tasks total
    - Sink - Tasks total
    - Source - Tasks total
    - AQEShuffleRead - Number of partitions
    - AQEShuffleRead - Partition data size
    
    Args:
        node: Node information
        
    Returns:
        dict: æ¤œå‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹
            {
                "tasks_total": å€¤,
                "sink_tasks_total": å€¤,
                "source_tasks_total": å€¤,
                "all_tasks_metrics": [{"name": "Tasks total", "value": å€¤}, ...],
                "aqe_shuffle_partitions": å€¤,
                "aqe_shuffle_data_size": å€¤,
                "aqe_shuffle_avg_partition_size": å€¤,
                "aqe_shuffle_skew_warning": bool,
                "aqe_shuffle_metrics": [{"name": "AQE...", "value": å€¤}, ...]
            }
    """
    parallelism_metrics = {
        "tasks_total": 0,
        "sink_tasks_total": 0,
        "source_tasks_total": 0,
        "all_tasks_metrics": [],
        "aqe_shuffle_partitions": 0,
        "aqe_shuffle_data_size": 0,
        "aqe_shuffle_avg_partition_size": 0,
        "aqe_shuffle_skew_warning": False,
        "aqe_detected_and_handled": False,
        "aqe_shuffle_metrics": []
    }
    
    # Target Tasks total metric name patterns
    tasks_total_patterns = [
        "Tasks total",
        "Sink - Tasks total",
        "Source - Tasks total"
    ]
    
    # Target AQEShuffleRead metric name patterns
    aqe_shuffle_patterns = [
        "AQEShuffleRead - Number of partitions",
        "AQEShuffleRead - Partition data size"
    ]
    
    # 1. Search from detailed_metrics
    detailed_metrics = node.get('detailed_metrics', {})
    for metric_key, metric_info in detailed_metrics.items():
        metric_value = metric_info.get('value', 0)
        metric_label = metric_info.get('label', '')
        
        # å„Tasks totalãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        for pattern in tasks_total_patterns:
            if metric_key == pattern or metric_label == pattern:
                # ç‰¹å®šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ãƒžãƒƒãƒ”ãƒ³ã‚°
                if pattern == "Tasks total":
                    parallelism_metrics["tasks_total"] = metric_value
                elif pattern == "Sink - Tasks total":
                    parallelism_metrics["sink_tasks_total"] = metric_value
                elif pattern == "Source - Tasks total":
                    parallelism_metrics["source_tasks_total"] = metric_value
                
                # Add to all metrics list
                parallelism_metrics["all_tasks_metrics"].append({
                    "name": pattern,
                    "value": metric_value
                })
        
        # AQEShuffleReadãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
        for pattern in aqe_shuffle_patterns:
            if metric_key == pattern or metric_label == pattern:
                # ç‰¹å®šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ãƒžãƒƒãƒ”ãƒ³ã‚°
                if pattern == "AQEShuffleRead - Number of partitions":
                    parallelism_metrics["aqe_shuffle_partitions"] = metric_value
                elif pattern == "AQEShuffleRead - Partition data size":
                    parallelism_metrics["aqe_shuffle_data_size"] = metric_value
                
                # Add to all metrics list
                parallelism_metrics["aqe_shuffle_metrics"].append({
                    "name": pattern,
                    "value": metric_value
                })
    
    # 2. raw_metricsã‹ã‚‰æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    raw_metrics = node.get('metrics', [])
    if isinstance(raw_metrics, list):
        for metric in raw_metrics:
            if isinstance(metric, dict):
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_value = metric.get('value', 0)
                
                # å„Tasks totalãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
                for pattern in tasks_total_patterns:
                    if metric_key == pattern or metric_label == pattern:
                        # Skip if already found in detailed_metrics
                        if not any(m["name"] == pattern for m in parallelism_metrics["all_tasks_metrics"]):
                            # ç‰¹å®šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ãƒžãƒƒãƒ”ãƒ³ã‚°
                            if pattern == "Tasks total":
                                parallelism_metrics["tasks_total"] = metric_value
                            elif pattern == "Sink - Tasks total":
                                parallelism_metrics["sink_tasks_total"] = metric_value
                            elif pattern == "Source - Tasks total":
                                parallelism_metrics["source_tasks_total"] = metric_value
                            
                            # Add to all metrics list
                            parallelism_metrics["all_tasks_metrics"].append({
                                "name": pattern,
                                "value": metric_value
                            })
                
                # AQEShuffleReadãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
                for pattern in aqe_shuffle_patterns:
                    if metric_key == pattern or metric_label == pattern:
                        # Skip if already found in detailed_metrics
                        if not any(m["name"] == pattern for m in parallelism_metrics["aqe_shuffle_metrics"]):
                            # ç‰¹å®šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ãƒžãƒƒãƒ”ãƒ³ã‚°
                            if pattern == "AQEShuffleRead - Number of partitions":
                                parallelism_metrics["aqe_shuffle_partitions"] = metric_value
                            elif pattern == "AQEShuffleRead - Partition data size":
                                parallelism_metrics["aqe_shuffle_data_size"] = metric_value
                            
                            # Add to all metrics list
                            parallelism_metrics["aqe_shuffle_metrics"].append({
                                "name": pattern,
                                "value": metric_value
                            })
    
    # 3. key_metricsã‹ã‚‰æ¤œç´¢ï¼ˆæœ€å¾Œã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    key_metrics = node.get('key_metrics', {})
    if isinstance(key_metrics, dict):
        for metric_key, metric_value in key_metrics.items():
            for pattern in tasks_total_patterns:
                if metric_key == pattern:
                    # Skip if already found
                    if not any(m["name"] == pattern for m in parallelism_metrics["all_tasks_metrics"]):
                        # ç‰¹å®šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ãƒžãƒƒãƒ”ãƒ³ã‚°
                        if pattern == "Tasks total":
                            parallelism_metrics["tasks_total"] = metric_value
                        elif pattern == "Sink - Tasks total":
                            parallelism_metrics["sink_tasks_total"] = metric_value
                        elif pattern == "Source - Tasks total":
                            parallelism_metrics["source_tasks_total"] = metric_value
                        
                        # Add to all metrics list
                        parallelism_metrics["all_tasks_metrics"].append({
                            "name": pattern,
                            "value": metric_value
                        })
            
            # AQEShuffleReadãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
            for pattern in aqe_shuffle_patterns:
                if metric_key == pattern:
                    # Skip if already found
                    if not any(m["name"] == pattern for m in parallelism_metrics["aqe_shuffle_metrics"]):
                        # ç‰¹å®šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ãƒžãƒƒãƒ”ãƒ³ã‚°
                        if pattern == "AQEShuffleRead - Number of partitions":
                            parallelism_metrics["aqe_shuffle_partitions"] = metric_value
                        elif pattern == "AQEShuffleRead - Partition data size":
                            parallelism_metrics["aqe_shuffle_data_size"] = metric_value
                        
                        # Add to all metrics list
                        parallelism_metrics["aqe_shuffle_metrics"].append({
                            "name": pattern,
                            "value": metric_value
                        })
    
    # Calculate average partition size and set warnings
    if parallelism_metrics["aqe_shuffle_partitions"] > 0 and parallelism_metrics["aqe_shuffle_data_size"] > 0:
        avg_partition_size = parallelism_metrics["aqe_shuffle_data_size"] / parallelism_metrics["aqe_shuffle_partitions"]
        parallelism_metrics["aqe_shuffle_avg_partition_size"] = avg_partition_size
        
        # 512MB = 512 * 1024 * 1024 bytes
        threshold_512mb = 512 * 1024 * 1024
        if avg_partition_size >= threshold_512mb:
            parallelism_metrics["aqe_shuffle_skew_warning"] = True
        else:
            # When AQEShuffleRead metrics exist and average partition size is less than 512MB,
            # Determine that AQE has detected and handled skew
            parallelism_metrics["aqe_detected_and_handled"] = True
    
    return parallelism_metrics

def calculate_filter_rate(node: Dict[str, Any]) -> Dict[str, Any]:
    """
    ãƒŽãƒ¼ãƒ‰ã‹ã‚‰Size of files prunedã¨Size of files readãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã—ã¦ãƒ•ã‚£ãƒ«ã‚¿çŽ‡ã‚’è¨ˆç®—
    
    Args:
        node: ãƒŽãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿
        
    Returns:
        Dict: ãƒ•ã‚£ãƒ«ã‚¿çŽ‡è¨ˆç®—çµæžœ
    """
    import os
    debug_mode = os.environ.get('DEBUG_FILTER_ANALYSIS', 'false').lower() == 'true'
    
    filter_rate = None
    files_pruned_bytes = 0
    files_read_bytes = 0
    actual_io_bytes = 0  # å®Ÿéš›ã®I/Oèª­ã¿è¾¼ã¿é‡
    debug_info = []
    
    # Target metric names for search (prioritizing patterns confirmed in actual JSON files)
    pruned_metrics = [
        "Size of files pruned",  # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªæ¸ˆã¿
        "Size of files pruned before dynamic pruning",  # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªæ¸ˆã¿
        "Pruned files size", 
        "Files pruned size",
        "Num pruned files size"
    ]
    
    read_metrics = [
        "Size of files read",  # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªæ¸ˆã¿
        "Files read size",
        "Read files size",
        "Num files read size"
    ]
    
    # å®Ÿéš›ã®I/Oèª­ã¿è¾¼ã¿é‡ï¼ˆå„ªå…ˆçš„ã«ä½¿ç”¨ï¼‰
    actual_io_metrics = [
        "Size of data read with io requests",  # å®Ÿéš›ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰ã®èª­ã¿è¾¼ã¿é‡
        "Data read with io requests",
        "IO request data size",
        "Actual data read size"
    ]
    
    # detailed_metricsã‹ã‚‰æ¤œç´¢
    detailed_metrics = node.get('detailed_metrics', {})
    if debug_mode:
        debug_info.append(f"detailed_metrics keys: {list(detailed_metrics.keys())[:5]}")
    
    for metric_key, metric_info in detailed_metrics.items():
        metric_label = metric_info.get('label', '')
        metric_value = metric_info.get('value', 0)
        
        # Prunedé–¢é€£ï¼ˆlabelã‚’å„ªå…ˆçš„ã«ãƒã‚§ãƒƒã‚¯ï¼‰
        for target in pruned_metrics:
            if target in metric_label and metric_value > 0:
                files_pruned_bytes += metric_value  # è¤‡æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒã‚ã‚‹å ´åˆã¯åˆè¨ˆ
                if debug_mode:
                    debug_info.append(f"Found pruned metric: {metric_label} = {metric_value}")
                break
        
        # Readé–¢é€£ï¼ˆlabelã‚’å„ªå…ˆçš„ã«ãƒã‚§ãƒƒã‚¯ï¼‰
        for target in read_metrics:
            if target in metric_label and metric_value > 0:
                files_read_bytes += metric_value  # è¤‡æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒã‚ã‚‹å ´åˆã¯åˆè¨ˆ
                if debug_mode:
                    debug_info.append(f"Found read metric: {metric_label} = {metric_value}")
                break
        
        # å®Ÿéš›ã®I/Oèª­ã¿è¾¼ã¿é‡ï¼ˆæœ€å„ªå…ˆï¼‰
        for target in actual_io_metrics:
            if target in metric_label and metric_value > 0:
                actual_io_bytes += metric_value
                if debug_mode:
                    debug_info.append(f"Found actual IO metric: {metric_label} = {metric_value}")
                break
    
    # raw_metricsã‹ã‚‰æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    if files_pruned_bytes == 0 or files_read_bytes == 0:
        raw_metrics = node.get('metrics', [])
        if debug_mode:
            debug_info.append(f"Searching in {len(raw_metrics)} raw metrics")
        
        for metric in raw_metrics:
            metric_label = metric.get('label', '')
            metric_value = metric.get('value', 0)
            
            # Prunedé–¢é€£ï¼ˆlabelã‚’å„ªå…ˆçš„ã«ãƒã‚§ãƒƒã‚¯ï¼‰
            for target in pruned_metrics:
                if target in metric_label and metric_value > 0:
                    files_pruned_bytes += metric_value  # è¤‡æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒã‚ã‚‹å ´åˆã¯åˆè¨ˆ
                    if debug_mode:
                        debug_info.append(f"Found pruned metric in raw: {metric_label} = {metric_value}")
                    break
            
            # Readé–¢é€£ï¼ˆlabelã‚’å„ªå…ˆçš„ã«ãƒã‚§ãƒƒã‚¯ï¼‰
            for target in read_metrics:
                if target in metric_label and metric_value > 0:
                    files_read_bytes += metric_value  # è¤‡æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒã‚ã‚‹å ´åˆã¯åˆè¨ˆ
                    if debug_mode:
                        debug_info.append(f"Found read metric in raw: {metric_label} = {metric_value}")
                    break
            
            # å®Ÿéš›ã®I/Oèª­ã¿è¾¼ã¿é‡ï¼ˆraw_metricsã‹ã‚‰ã‚‚æ¤œç´¢ï¼‰
            for target in actual_io_metrics:
                if target in metric_label and metric_value > 0:
                    actual_io_bytes += metric_value
                    if debug_mode:
                        debug_info.append(f"Found actual IO metric in raw: {metric_label} = {metric_value}")
                    break
    
    # ãƒ•ã‚£ãƒ«ã‚¿çŽ‡è¨ˆç®—ï¼ˆI/Oå®Ÿç¸¾ã‚’å„ªå…ˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹çŽ‡ï¼‰
    if actual_io_bytes > 0 and files_read_bytes > 0:
        # æ–°ã—ã„è¨ˆç®—æ–¹å¼: å®Ÿéš›ã®I/OåŠ¹çŽ‡
        filter_rate = (files_read_bytes - actual_io_bytes) / files_read_bytes
        if debug_mode:
            debug_info.append(f"Using IO-based calculation: ({files_read_bytes/1024**3:.2f}GB - {actual_io_bytes/1024**3:.2f}GB) / {files_read_bytes/1024**3:.2f}GB = {filter_rate:.3f}")
    else:
        # å¾“æ¥ã®è¨ˆç®—æ–¹å¼: ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹çŽ‡
        total_available_bytes = files_read_bytes + files_pruned_bytes
        if total_available_bytes > 0:
            filter_rate = files_pruned_bytes / total_available_bytes
            if debug_mode:
                debug_info.append(f"Using pruning-based calculation: {files_pruned_bytes/1024**3:.2f}GB / {total_available_bytes/1024**3:.2f}GB = {filter_rate:.3f}")
        else:
            filter_rate = 0.0
            if debug_mode:
                debug_info.append("No filter metrics available, using 0.0")
    
    result = {
        "filter_rate": filter_rate,
        "files_pruned_bytes": files_pruned_bytes,
        "files_read_bytes": files_read_bytes,
        "actual_io_bytes": actual_io_bytes,  # å®Ÿéš›ã®I/Oèª­ã¿è¾¼ã¿é‡ã‚’è¿½åŠ 
        "has_filter_metrics": (files_read_bytes > 0 or files_pruned_bytes > 0),
        "calculation_method": "io_based" if (actual_io_bytes > 0 and files_read_bytes > 0) else "pruning_based"
    }
    
    if debug_mode:
        result["debug_info"] = debug_info
    
    return result

def format_filter_rate_display(filter_result: Dict[str, Any]) -> str:
    """
    ãƒ•ã‚£ãƒ«ã‚¿çŽ‡è¨ˆç®—çµæžœã‚’è¡¨ç¤ºç”¨æ–‡å­—åˆ—ã«å¤‰æ›
    
    Args:
        filter_result: calculate_filter_rate()ã®çµæžœ
        
    Returns:
        str: è¡¨ç¤ºç”¨æ–‡å­—åˆ—
    """
    if not filter_result["has_filter_metrics"] or filter_result["filter_rate"] is None:
        return None
    
    filter_rate = filter_result["filter_rate"]
    files_read_gb = filter_result["files_read_bytes"] / (1024 * 1024 * 1024)
    
    # è¨ˆç®—æ–¹å¼ã«å¿œã˜ã¦è¡¨ç¤ºã‚’èª¿æ•´
    if filter_result.get("calculation_method") == "io_based" and filter_result.get("actual_io_bytes", 0) > 0:
        actual_io_gb = filter_result["actual_io_bytes"] / (1024 * 1024 * 1024)
        effective_filtered_gb = files_read_gb - actual_io_gb
        return f"ðŸ“‚ Filter rate: {filter_rate:.1%} (read: {files_read_gb:.2f}GB, actual: {actual_io_gb:.2f}GB)"
    else:
        files_pruned_gb = filter_result["files_pruned_bytes"] / (1024 * 1024 * 1024)
        return f"ðŸ“‚ Filter rate: {filter_rate:.1%} (read: {files_read_gb:.2f}GB, pruned: {files_pruned_gb:.2f}GB)"

def extract_detailed_bottleneck_analysis(extracted_metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    Perform Cell 33-style detailed bottleneck analysis and return structured data
    
    ðŸš¨ Important: Prevention of percentage calculation degradation
    - Using the sum of parallel execution node times as total time is strictly prohibited
    - Prioritize using overall_metrics.total_time_ms (wall-clock time)
    - Use maximum node time during fallback (not sum)
    
    Args:
        extracted_metrics: Extracted metrics
        
    Returns:
        dict: Detailed bottleneck analysis results
    """
    detailed_analysis = {
        "top_bottleneck_nodes": [],
        "shuffle_optimization_hints": [],
        "spill_analysis": {
            "total_spill_gb": 0,
            "spill_nodes": [],
            "critical_spill_nodes": []
        },
        "skew_analysis": {
            "skewed_nodes": [],
            "total_skewed_partitions": 0
        },
        "performance_recommendations": []
    }
    
    # ãƒŽãƒ¼ãƒ‰ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆï¼ˆTOP10ï¼‰
    sorted_nodes = sorted(extracted_metrics.get('node_metrics', []), 
                         key=lambda x: x.get('key_metrics', {}).get('durationMs', 0), 
                         reverse=True)
    
    # æœ€å¤§10å€‹ã®ãƒŽãƒ¼ãƒ‰ã‚’å‡¦ç†
    final_sorted_nodes = sorted_nodes[:10]
    
    # ðŸš¨ é‡è¦: æ­£ã—ã„å…¨ä½“æ™‚é–“ã®è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
    # 1. overall_metricsã‹ã‚‰å…¨ä½“å®Ÿè¡Œæ™‚é–“ã‚’å–å¾—ï¼ˆwall-clock timeï¼‰
    overall_metrics = extracted_metrics.get('overall_metrics', {})
    total_duration = overall_metrics.get('total_time_ms', 0)
    
    # ðŸš¨ ä¸¦åˆ—å®Ÿè¡Œå•é¡Œã®ä¿®æ­£: task_total_time_msã‚’å„ªå…ˆä½¿ç”¨
    task_total_time_ms = overall_metrics.get('task_total_time_ms', 0)
    
    if task_total_time_ms > 0:
        total_duration = task_total_time_ms
    elif total_duration <= 0:
        # execution_time_msã‚’æ¬¡ã®å„ªå…ˆåº¦ã§ä½¿ç”¨
        execution_time_ms = overall_metrics.get('execution_time_ms', 0)
        if execution_time_ms > 0:
            total_duration = execution_time_ms
        else:
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            max_node_time = max([node.get('key_metrics', {}).get('durationMs', 0) for node in sorted_nodes], default=1)
            total_duration = int(max_node_time * 1.2)
    
    for i, node in enumerate(final_sorted_nodes):
        duration_ms = node.get('key_metrics', {}).get('durationMs', 0)
        memory_mb = node.get('key_metrics', {}).get('peakMemoryBytes', 0) / 1024 / 1024
        rows_num = node.get('key_metrics', {}).get('rowsNum', 0)
        
        # ä¸¦åˆ—åº¦æƒ…å ±ã®å–å¾—ï¼ˆä¿®æ­£ç‰ˆ: è¤‡æ•°ã®Tasks totalãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ï¼‰
        parallelism_data = extract_parallelism_metrics(node)
        
        # å¾“æ¥ã®å˜ä¸€å€¤ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
        num_tasks = parallelism_data.get('tasks_total', 0)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Sink - Tasks totalã¾ãŸã¯Source - Tasks totalãŒã‚ã‚‹å ´åˆ
        if num_tasks == 0:
            if parallelism_data.get('sink_tasks_total', 0) > 0:
                num_tasks = parallelism_data.get('sink_tasks_total', 0)
            elif parallelism_data.get('source_tasks_total', 0) > 0:
                num_tasks = parallelism_data.get('source_tasks_total', 0)
        
        # ã‚¹ãƒ”ãƒ«æ¤œå‡ºï¼ˆã‚»ãƒ«33ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        spill_detected = False
        spill_bytes = 0
        exact_spill_metrics = [
            "Num bytes spilled to disk due to memory pressure",
            "Sink - Num bytes spilled to disk due to memory pressure",
            "Sink/Num bytes spilled to disk due to memory pressure"
        ]
        
        # detailed_metricsã‹ã‚‰æ¤œç´¢
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            metric_value = metric_info.get('value', 0)
            metric_label = metric_info.get('label', '')
            
            if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                spill_detected = True
                spill_bytes = max(spill_bytes, metric_value)
                break
        
        # raw_metricsã‹ã‚‰æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not spill_detected:
            raw_metrics = node.get('metrics', [])
            for metric in raw_metrics:
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_value = metric.get('value', 0)
                
                if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                    spill_detected = True
                    spill_bytes = max(spill_bytes, metric_value)
                    break
        
        # ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºï¼ˆAQEãƒ™ãƒ¼ã‚¹ï¼‰
        skew_detected = False
        skewed_partitions = 0
        target_skew_metric = "AQEShuffleRead - Number of skewed partitions"
        
        for metric_key, metric_info in detailed_metrics.items():
            if metric_key == target_skew_metric:
                try:
                    skewed_partitions = int(metric_info.get('value', 0))
                    if skewed_partitions > 0:
                        skew_detected = True
                    break
                except (ValueError, TypeError):
                    continue
        
        node_name = get_meaningful_node_name(node, extracted_metrics)
        # ðŸš¨ é‡è¦: æ­£ã—ã„ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
        # wall-clock timeã«å¯¾ã™ã‚‹å„ãƒŽãƒ¼ãƒ‰ã®å®Ÿè¡Œæ™‚é–“ã®å‰²åˆ
        time_percentage = min((duration_ms / max(total_duration, 1)) * 100, 100.0)
        
        # ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®šï¼ˆAQEã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã¨AQEShuffleReadå¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã®ä¸¡æ–¹ã‚’è€ƒæ…®ï¼‰
        aqe_shuffle_skew_warning = parallelism_data.get('aqe_shuffle_skew_warning', False)
        combined_skew_detected = skew_detected or aqe_shuffle_skew_warning
        
        # ãƒŽãƒ¼ãƒ‰åˆ†æžçµæžœã‚’æ§‹é€ åŒ–
        node_analysis = {
            "rank": i + 1,
            "node_id": node.get('node_id', node.get('id', 'N/A')),
            "node_name": node_name,
            "duration_ms": duration_ms,
            "time_percentage": time_percentage,
            "memory_mb": memory_mb,
            "rows_processed": rows_num,
            "num_tasks": num_tasks,
            "parallelism_data": parallelism_data,  # è¤‡æ•°ã®Tasks totalãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±ã‚’è¿½åŠ 
            "spill_detected": spill_detected,
            "spill_bytes": spill_bytes,
            "spill_gb": spill_bytes / 1024 / 1024 / 1024 if spill_bytes > 0 else 0,
            "skew_detected": combined_skew_detected,  # AQEã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã¨AQEShuffleReadè­¦å‘Šã®ä¸¡æ–¹ã‚’è€ƒæ…®
            "aqe_skew_detected": skew_detected,  # å¾“æ¥ã®AQEã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã®ã¿
            "aqe_shuffle_skew_warning": aqe_shuffle_skew_warning,  # AQEShuffleReadå¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºè­¦å‘Š
            "skewed_partitions": skewed_partitions,
            "is_shuffle_node": "shuffle" in node_name.lower(),
            "severity": "CRITICAL" if duration_ms >= 10000 else "HIGH" if duration_ms >= 5000 else "MEDIUM" if duration_ms >= 1000 else "LOW"
        }
        
        # ShuffleãƒŽãƒ¼ãƒ‰ã®å ´åˆã€ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿REPARTITIONãƒ’ãƒ³ãƒˆã‚’è¿½åŠ 
        if node_analysis["is_shuffle_node"] and spill_detected and spill_bytes > 0:
            shuffle_attributes = extract_shuffle_attributes(node)
            if shuffle_attributes:
                suggested_partitions = max(num_tasks * 2, 200)
                
                # Shuffleå±žæ€§ã§æ¤œå‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ ã‚’å…¨ã¦ä½¿ç”¨ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
                repartition_columns = ", ".join(shuffle_attributes)
                
                repartition_hint = {
                    "node_id": node_analysis["node_id"],
                    "attributes": shuffle_attributes,
                    "suggested_sql": f"REPARTITION({suggested_partitions}, {repartition_columns})",
                    "reason": f"Spill({node_analysis['spill_gb']:.2f}GB) improvement",
                    "priority": "HIGH",
                    "estimated_improvement": "Significant performance improvement expected",

                }
                detailed_analysis["shuffle_optimization_hints"].append(repartition_hint)
                node_analysis["repartition_hint"] = repartition_hint
        

        # ãƒ•ã‚£ãƒ«ã‚¿çŽ‡è¨ˆç®—ã¨æƒ…å ±æ›´æ–°
        filter_result = calculate_filter_rate(node)
        node_analysis.update({
            "filter_rate": filter_result["filter_rate"],
            "files_pruned_bytes": filter_result["files_pruned_bytes"],
            "files_read_bytes": filter_result["files_read_bytes"],
            "has_filter_metrics": filter_result["has_filter_metrics"]
        })
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã®è¿½åŠ 
        cluster_attributes = extract_cluster_attributes(node)
        node_analysis.update({
            "cluster_attributes": cluster_attributes,
            "has_clustering": len(cluster_attributes) > 0
        })
        
        detailed_analysis["top_bottleneck_nodes"].append(node_analysis)
        
        # ã‚¹ãƒ”ãƒ«åˆ†æžã¸ã®è¿½åŠ 
        if spill_detected:
            detailed_analysis["spill_analysis"]["total_spill_gb"] += node_analysis["spill_gb"]
            detailed_analysis["spill_analysis"]["spill_nodes"].append({
                "node_id": node_analysis["node_id"],
                "node_name": node_name,
                "spill_gb": node_analysis["spill_gb"],
                "rank": i + 1
            })
            
            if node_analysis["spill_gb"] > 1.0:  # 1GBä»¥ä¸Šã¯é‡è¦
                detailed_analysis["spill_analysis"]["critical_spill_nodes"].append(node_analysis["node_id"])
        
        # ã‚¹ã‚­ãƒ¥ãƒ¼åˆ†æžã¸ã®è¿½åŠ 
        if skew_detected:
            detailed_analysis["skew_analysis"]["total_skewed_partitions"] += skewed_partitions
            detailed_analysis["skew_analysis"]["skewed_nodes"].append({
                "node_id": node_analysis["node_id"],
                "node_name": node_name,
                "skewed_partitions": skewed_partitions,
                "rank": i + 1
            })
    
    # å…¨ä½“çš„ãªæŽ¨å¥¨äº‹é …ã®ç”Ÿæˆ
    if detailed_analysis["spill_analysis"]["total_spill_gb"] > 5.0:
        detailed_analysis["performance_recommendations"].append({
            "type": "memory_optimization",
            "priority": "CRITICAL",
            "description": f"Large spill({detailed_analysis['spill_analysis']['total_spill_gb']:.1f}GB) detected: Memory configuration and partitioning strategy review required"
        })
    
    if len(detailed_analysis["shuffle_optimization_hints"]) > 0:
        detailed_analysis["performance_recommendations"].append({
            "type": "shuffle_optimization", 
            "priority": "HIGH",
            "description": f"Memory optimization required for {len(detailed_analysis['shuffle_optimization_hints'])} shuffle nodes with spill occurrence"
        })
    
    if detailed_analysis["skew_analysis"]["total_skewed_partitions"] > 10:
        detailed_analysis["performance_recommendations"].append({
            "type": "skew_optimization",
            "priority": "HIGH", 
            "description": f"Data skew ({detailed_analysis['skew_analysis']['total_skewed_partitions']} partitions) detected: Data distribution review required"
        })
    
    return detailed_analysis

print("âœ… Function definition completed: get_meaningful_node_name, extract_shuffle_attributes, extract_detailed_bottleneck_analysis")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸŽ¯ Bottleneck Indicator Calculation Function
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - Execution time and compilation time ratio analysis
# MAGIC - Cache efficiency and data processing efficiency calculation
# MAGIC - Photon utilization analysis
# MAGIC - Spill detection and shuffle/parallelism issue identification

# COMMAND ----------

def calculate_bottleneck_indicators(metrics: Dict[str, Any]) -> Dict[str, Any]:
    """Calculate bottleneck metrics"""
    indicators = {}
    
    overall = metrics.get('overall_metrics', {})
    total_time = overall.get('total_time_ms', 0)
    execution_time = overall.get('execution_time_ms', 0)
    compilation_time = overall.get('compilation_time_ms', 0)
    
    if total_time > 0:
        indicators['compilation_ratio'] = compilation_time / total_time
        indicators['execution_ratio'] = execution_time / total_time
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹çŽ‡
    read_bytes = overall.get('read_bytes', 0)
    cache_bytes = overall.get('read_cache_bytes', 0)
    if read_bytes > 0:
        indicators['cache_hit_ratio'] = cache_bytes / read_bytes
    
    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†åŠ¹çŽ‡ï¼ˆå®¹é‡ãƒ™ãƒ¼ã‚¹ï¼‰
    read_bytes = overall.get('read_bytes', 0)
    
    # å®¹é‡ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿çŽ‡ã‚’è¨ˆç®—ï¼ˆæ­£ã—ã„å®Ÿè£…ï¼‰
    data_selectivity = calculate_filter_rate_percentage(overall, metrics)
    
    indicators['data_selectivity'] = data_selectivity
    
    # Photonä½¿ç”¨çŽ‡ï¼ˆã‚¿ã‚¹ã‚¯å®Ÿè¡Œæ™‚é–“ã«å¯¾ã™ã‚‹å‰²åˆï¼‰
    task_time = overall.get('task_total_time_ms', 0)
    photon_time = overall.get('photon_total_time_ms', 0)
    if task_time > 0:
        indicators['photon_ratio'] = min(photon_time / task_time, 1.0)  # æœ€å¤§100%ã«åˆ¶é™
    else:
        indicators['photon_ratio'] = 0.0
    
    # ã‚¹ãƒ”ãƒ«æ¤œå‡ºï¼ˆè©³ç´°ç‰ˆï¼šSink - Num bytes spilled to disk due to memory pressure ãƒ™ãƒ¼ã‚¹ï¼‰
    spill_detected = False
    total_spill_bytes = 0
    spill_details = []
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹åï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
    target_spill_metrics = [
        "Sink - Num bytes spilled to disk due to memory pressure",
        "Num bytes spilled to disk due to memory pressure"
    ]
    
    # å„ãƒŽãƒ¼ãƒ‰ã§ã‚¹ãƒ”ãƒ«æ¤œå‡ºã‚’å®Ÿè¡Œ
    for node in metrics.get('node_metrics', []):
        node_spill_found = False
        
        # 1. Search from detailed_metrics
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            metric_value = metric_info.get('value', 0)
            metric_label = metric_info.get('label', '')
            
            # è¤‡æ•°ã®ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’ãƒã‚§ãƒƒã‚¯
            if ((metric_key in target_spill_metrics or 
                 metric_label in target_spill_metrics) and metric_value > 0):
                spill_detected = True
                node_spill_found = True
                total_spill_bytes += metric_value
                spill_details.append({
                    'node_id': node.get('node_id', ''),
                    'node_name': node.get('name', ''),
                    'spill_bytes': metric_value,
                    'spill_metric': metric_key if metric_key in target_spill_metrics else metric_label,
                    'source': 'detailed_metrics'
                })
                break
        
        # 2. raw_metricsã‹ã‚‰æ¤œç´¢ï¼ˆã“ã®ãƒŽãƒ¼ãƒ‰ã§ã¾ã è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆï¼‰
        if not node_spill_found:
            raw_metrics = node.get('metrics', [])
            for metric in raw_metrics:
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_value = metric.get('value', 0)
                
                # è¤‡æ•°ã®ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’ãƒã‚§ãƒƒã‚¯
                if ((metric_key in target_spill_metrics or 
                     metric_label in target_spill_metrics) and metric_value > 0):
                    spill_detected = True
                    node_spill_found = True
                    total_spill_bytes += metric_value
                    spill_details.append({
                        'node_id': node.get('node_id', ''),
                        'node_name': node.get('name', ''),
                        'spill_bytes': metric_value,
                        'spill_metric': metric_key if metric_key in target_spill_metrics else metric_label,
                        'source': 'raw_metrics'
                    })
                    break
        
        # 3. key_metricsã‹ã‚‰æ¤œç´¢ï¼ˆæœ€å¾Œã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not node_spill_found:
            key_metrics = node.get('key_metrics', {})
            for key_metric_name, key_metric_value in key_metrics.items():
                # è¤‡æ•°ã®ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’ãƒã‚§ãƒƒã‚¯
                if key_metric_name in target_spill_metrics and key_metric_value > 0:
                    spill_detected = True
                    node_spill_found = True
                    total_spill_bytes += key_metric_value
                    spill_details.append({
                        'node_id': node.get('node_id', ''),
                        'node_name': node.get('name', ''),
                        'spill_bytes': key_metric_value,
                        'spill_metric': key_metric_name,
                        'source': 'key_metrics'
                    })
                    break
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: overall_metricsã‹ã‚‰ã®ç°¡æ˜“æ¤œå‡º
    if not spill_detected:
        fallback_spill_bytes = overall.get('spill_to_disk_bytes', 0)
        if fallback_spill_bytes > 0:
            spill_detected = True
            total_spill_bytes = fallback_spill_bytes
            spill_details.append({
                'node_id': 'overall',
                'node_name': 'Overall Metrics',
                'spill_bytes': fallback_spill_bytes,
                'source': 'overall_metrics'
            })
    
    indicators['has_spill'] = spill_detected
    indicators['spill_bytes'] = total_spill_bytes
    indicators['spill_details'] = spill_details
    indicators['spill_nodes_count'] = len(spill_details)
    
    # æœ€ã‚‚æ™‚é–“ã®ã‹ã‹ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸
    stage_durations = [(s['stage_id'], s['duration_ms']) for s in metrics.get('stage_metrics', []) if s['duration_ms'] > 0]
    if stage_durations:
        slowest_stage = max(stage_durations, key=lambda x: x[1])
        indicators['slowest_stage_id'] = slowest_stage[0]
        indicators['slowest_stage_duration'] = slowest_stage[1]
    
    # æœ€ã‚‚ãƒ¡ãƒ¢ãƒªã‚’ä½¿ç”¨ã™ã‚‹ãƒŽãƒ¼ãƒ‰
    memory_usage = []
    for node in metrics.get('node_metrics', []):
        peak_memory = node.get('key_metrics', {}).get('peakMemoryBytes', 0)
        if peak_memory > 0:
            memory_usage.append((node['node_id'], node['name'], peak_memory))
    
    if memory_usage:
        highest_memory_node = max(memory_usage, key=lambda x: x[2])
        indicators['highest_memory_node_id'] = highest_memory_node[0]
        indicators['highest_memory_node_name'] = highest_memory_node[1]
        indicators['highest_memory_bytes'] = highest_memory_node[2]
    
    # ä¸¦åˆ—åº¦ã¨ã‚·ãƒ£ãƒƒãƒ•ãƒ«å•é¡Œã®æ¤œå‡º
    shuffle_nodes = []
    low_parallelism_stages = []
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒŽãƒ¼ãƒ‰ã®ç‰¹å®š
    for node in metrics.get('node_metrics', []):
        node_name = node.get('name', '').upper()
        if any(keyword in node_name for keyword in ['SHUFFLE', 'EXCHANGE']):
            shuffle_nodes.append({
                'node_id': node['node_id'],
                'name': node['name'],
                'duration_ms': node.get('key_metrics', {}).get('durationMs', 0),
                'rows': node.get('key_metrics', {}).get('rowsNum', 0)
            })
    
    # ä½Žä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸ã®æ¤œå‡º
    for stage in metrics.get('stage_metrics', []):
        num_tasks = stage.get('num_tasks', 0)
        duration_ms = stage.get('duration_ms', 0)
        
        # ä¸¦åˆ—åº¦ãŒä½Žã„ï¼ˆã‚¿ã‚¹ã‚¯æ•°ãŒå°‘ãªã„ï¼‰ã‹ã¤å®Ÿè¡Œæ™‚é–“ãŒé•·ã„ã‚¹ãƒ†ãƒ¼ã‚¸
        if num_tasks > 0 and num_tasks < 10 and duration_ms > 5000:  # 10ã‚¿ã‚¹ã‚¯æœªæº€ã€5ç§’ä»¥ä¸Š
            low_parallelism_stages.append({
                'stage_id': stage['stage_id'],
                'num_tasks': num_tasks,
                'duration_ms': duration_ms,
                'avg_task_duration': duration_ms / max(num_tasks, 1)
            })
    
    indicators['shuffle_operations_count'] = len(shuffle_nodes)
    indicators['low_parallelism_stages_count'] = len(low_parallelism_stages)
    indicators['has_shuffle_bottleneck'] = len(shuffle_nodes) > 0 and any(s['duration_ms'] > 10000 for s in shuffle_nodes)
    indicators['has_low_parallelism'] = len(low_parallelism_stages) > 0
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã®è©³ç´°æƒ…å ±
    if shuffle_nodes:
        total_shuffle_time = sum(s['duration_ms'] for s in shuffle_nodes)
        indicators['total_shuffle_time_ms'] = total_shuffle_time
        indicators['shuffle_time_ratio'] = total_shuffle_time / max(total_time, 1)
        
        # æœ€ã‚‚æ™‚é–“ã®ã‹ã‹ã‚‹ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ
        slowest_shuffle = max(shuffle_nodes, key=lambda x: x['duration_ms'])
        indicators['slowest_shuffle_duration_ms'] = slowest_shuffle['duration_ms']
        indicators['slowest_shuffle_node'] = slowest_shuffle['name']
    
    # ä½Žä¸¦åˆ—åº¦ã®è©³ç´°æƒ…å ±
    if low_parallelism_stages:
        indicators['low_parallelism_details'] = low_parallelism_stages
        avg_parallelism = sum(s['num_tasks'] for s in low_parallelism_stages) / len(low_parallelism_stages)
        indicators['average_low_parallelism'] = avg_parallelism
    
    # AQEShuffleReadè­¦å‘Šã®æ¤œå‡º
    aqe_shuffle_skew_warning_detected = False
    aqe_detected_and_handled = False
    
    for node in metrics.get('node_metrics', []):
        parallelism_data = extract_parallelism_metrics(node)
        if parallelism_data.get('aqe_shuffle_skew_warning', False):
            aqe_shuffle_skew_warning_detected = True
        if parallelism_data.get('aqe_detected_and_handled', False):
            aqe_detected_and_handled = True
    
    # å„ªå…ˆé †ä½: 512MBä»¥ä¸Šã®è­¦å‘ŠãŒã‚ã‚Œã°ã€ãã‚Œã‚’å„ªå…ˆ
    # è­¦å‘ŠãŒãªã„å ´åˆã®ã¿ã€AQEå¯¾å¿œæ¸ˆã¿ã¨åˆ¤å®š
    indicators['has_aqe_shuffle_skew_warning'] = aqe_shuffle_skew_warning_detected
    indicators['has_skew'] = aqe_detected_and_handled and not aqe_shuffle_skew_warning_detected
    
    return indicators

print("âœ… Function definition completed: calculate_bottleneck_indicators")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸ§¬ Liquid Clustering Analysis Function
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - Column information extraction from profiler data
# MAGIC - Filter, JOIN, and GROUP BY condition analysis
# MAGIC - Data skew and performance impact evaluation
# MAGIC - Clustering recommended column identification

# COMMAND ----------

def calculate_performance_insights_from_metrics(overall_metrics: Dict[str, Any], metrics: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±ã®ã¿ã‹ã‚‰è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ´žå¯Ÿã‚’è¨ˆç®—
    """
    insights = {}
    
    # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿
    total_time_ms = overall_metrics.get('total_time_ms', 0)
    read_bytes = overall_metrics.get('read_bytes', 0)
    read_cache_bytes = overall_metrics.get('read_cache_bytes', 0)
    read_remote_bytes = overall_metrics.get('read_remote_bytes', 0)
    rows_read = overall_metrics.get('rows_read_count', 0)
    rows_produced = overall_metrics.get('rows_produced_count', 0)
    read_files = overall_metrics.get('read_files_count', 0)
    read_partitions = overall_metrics.get('read_partitions_count', 0)
    photon_time = overall_metrics.get('photon_total_time_ms', 0)
    task_time = overall_metrics.get('task_total_time_ms', 0)
    spill_bytes = overall_metrics.get('spill_to_disk_bytes', 0)
    
    # 1. ãƒ‡ãƒ¼ã‚¿åŠ¹çŽ‡åˆ†æžï¼ˆå®¹é‡ãƒ™ãƒ¼ã‚¹ï¼‰
    # metricsãŒNoneã®å ´åˆã¯ç©ºã®è¾žæ›¸ã§åˆæœŸåŒ–
    if metrics is None:
        metrics = {'node_metrics': []}
    
    # å®¹é‡ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿çŽ‡ã‚’è¨ˆç®—ï¼ˆæ­£ã—ã„å®Ÿè£…ï¼‰
    filter_rate_capacity = calculate_filter_rate_percentage(overall_metrics, metrics)
    
    insights['data_efficiency'] = {
        'data_selectivity': filter_rate_capacity,
        'avg_bytes_per_file': read_bytes / max(read_files, 1),
        'avg_bytes_per_partition': read_bytes / max(read_partitions, 1),
        'avg_rows_per_file': rows_read / max(read_files, 1),
        'avg_rows_per_partition': rows_read / max(read_partitions, 1)
    }
    
    # 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹çŽ‡åˆ†æž
    cache_hit_ratio = read_cache_bytes / max(read_bytes, 1)
    insights['cache_efficiency'] = {
        'cache_hit_ratio': cache_hit_ratio,
        'cache_hit_percentage': cache_hit_ratio * 100,
        'remote_read_ratio': read_remote_bytes / max(read_bytes, 1),
        'cache_effectiveness': 'high' if cache_hit_ratio > 0.8 else 'medium' if cache_hit_ratio > 0.5 else 'low'
    }
    
    # 3. ä¸¦åˆ—åŒ–åŠ¹çŽ‡åˆ†æž
    insights['parallelization'] = {
        'files_per_second': read_files / max(total_time_ms / 1000, 1),
        'partitions_per_second': read_partitions / max(total_time_ms / 1000, 1),
        'throughput_mb_per_second': (read_bytes / 1024 / 1024) / max(total_time_ms / 1000, 1),
        'rows_per_second': rows_read / max(total_time_ms / 1000, 1)
    }
    
    # 4. PhotonåŠ¹çŽ‡åˆ†æž
    photon_efficiency = photon_time / max(task_time, 1)
    insights['photon_analysis'] = {
        'photon_enabled': photon_time > 0,
        'photon_efficiency': photon_efficiency,
        'photon_utilization_percentage': photon_efficiency * 100,
        'photon_effectiveness': 'high' if photon_efficiency > 0.8 else 'medium' if photon_efficiency > 0.5 else 'low'
    }
    
    # 5. ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³
    insights['resource_usage'] = {
        'memory_pressure': spill_bytes > 0,
        'spill_gb': spill_bytes / 1024 / 1024 / 1024,
        'data_processed_gb': read_bytes / 1024 / 1024 / 1024,
        'data_reduction_ratio': 1 - (rows_produced / max(rows_read, 1))
    }
    
    # 6. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™
    bottlenecks = []
    if cache_hit_ratio < 0.3:
        bottlenecks.append('Low cache efficiency')
    if read_remote_bytes / max(read_bytes, 1) > 0.8:
        bottlenecks.append('High remote read ratio')
    if photon_efficiency < 0.5 and photon_time > 0:
        bottlenecks.append('Low Photon efficiency')
    if spill_bytes > 0:
        bottlenecks.append('Memory spill occurring')
    if insights['data_efficiency']['data_selectivity'] < 0.2:
        bottlenecks.append('Low filter efficiency')
    
    insights['potential_bottlenecks'] = bottlenecks
    
    return insights

def calculate_filter_rate_percentage(overall_metrics: Dict[str, Any], metrics: Dict[str, Any]) -> float:
    """
    å®¹é‡ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿çŽ‡ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆoverall_metrics.read_bytesä½¿ç”¨ç‰ˆï¼‰
    
    âŒ ãƒ‡ã‚°ãƒ¬é˜²æ­¢æ³¨æ„: ã“ã®é–¢æ•°ã¯å¿…ãšoverall_metrics.read_bytesã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼
    âŒ files_read_bytesï¼ˆã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰é›†è¨ˆï¼‰ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ï¼
    
    Args:
        overall_metrics: å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆread_bytesã‚’ä½¿ç”¨ï¼‰
        metrics: å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆnode_metricsã‚’å«ã‚€ã€pruned_byteså–å¾—ç”¨ï¼‰
        
    Returns:
        float: ãƒ•ã‚£ãƒ«ã‚¿çŽ‡ï¼ˆ0.0-1.0ã€é«˜ã„å€¤ã»ã©åŠ¹çŽ‡çš„ï¼‰
               ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹çŽ‡ = files_pruned_bytes / (overall_read_bytes + files_pruned_bytes)
    """
    import os
    debug_mode = os.environ.get('DEBUG_FILTER_ANALYSIS', 'false').lower() == 'true'
    
    # âŒ ãƒ‡ã‚°ãƒ¬é˜²æ­¢: å¿…ãšoverall_metrics.read_bytesã‚’ä½¿ç”¨ï¼
    overall_read_bytes = overall_metrics.get('read_bytes', 0)
    
    if debug_mode:
        print(f"ðŸ” Filter rate calculation debug (using overall_metrics.read_bytes version):")
        print(f"   overall_read_bytes: {overall_read_bytes:,} ({overall_read_bytes / (1024**4):.2f} TB)")
    
    try:
        # pruned_bytesã®ã¿node_metricsã‹ã‚‰å–å¾—ï¼ˆread_bytesã¯ä½¿ç”¨ã—ãªã„ï¼‰
        node_metrics = metrics.get('node_metrics', [])
        total_files_pruned_bytes = 0
        filter_metrics_found = False
        
        # å…¨ã¦ã®ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰ã‹ã‚‰prunedæƒ…å ±ã®ã¿ã‚’é›†è¨ˆ
        for node in node_metrics:
            if node.get('tag') in ['FileScan', 'BatchScan', 'TableScan', 'UNKNOWN_DATA_SOURCE_SCAN_EXEC']:
                filter_result = calculate_filter_rate(node)
                if filter_result.get('has_filter_metrics', False):
                    files_pruned_bytes = filter_result.get('files_pruned_bytes', 0)
                    
                    if files_pruned_bytes > 0:
                        total_files_pruned_bytes += files_pruned_bytes
                        filter_metrics_found = True
                        
                        if debug_mode:
                            print(f"   Node {node.get('node_id', 'unknown')}: files_pruned_bytes = {files_pruned_bytes:,}")
        
        # âŒ ãƒ‡ã‚°ãƒ¬é˜²æ­¢: overall_read_bytes + pruned_bytes ã§è¨ˆç®—
        if filter_metrics_found and overall_read_bytes > 0:
            # æ­£ã—ã„è¨ˆç®—: ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹çŽ‡ = files_pruned / (overall_read + files_pruned)
            total_available_bytes = overall_read_bytes + total_files_pruned_bytes
            if total_available_bytes > 0:
                overall_filter_rate = total_files_pruned_bytes / total_available_bytes
            else:
                overall_filter_rate = 0.0
                
            if debug_mode:
                print(f"   âŒ Regression prevention version: using overall_read_bytes")
                print(f"     overall_read_bytes: {overall_read_bytes:,} ({overall_read_bytes / (1024**4):.2f} TB)")
                print(f"     total_files_pruned_bytes: {total_files_pruned_bytes:,} ({total_files_pruned_bytes / (1024**4):.2f} TB)")
                print(f"     total_available_bytes: {total_available_bytes:,} ({total_available_bytes / (1024**4):.2f} TB)")
                print(f"     Pruning efficiency: {overall_filter_rate*100:.2f}%")
            return overall_filter_rate
        
        if debug_mode:
            print(f"   Filter metrics: {'Detected' if filter_metrics_found else 'Not detected'}")
            print(f"   overall_read_bytes: {overall_read_bytes:,}")
            if not filter_metrics_found:
                print(f"   âš ï¸ Pruning information is not available")
            if overall_read_bytes == 0:
                print(f"   âš ï¸ No read data available")
        
        # ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°æƒ…å ±ãŒãªã„å ´åˆã¯0ã‚’è¿”ã™
        return 0.0
        
    except Exception as e:
        if debug_mode:
            print(f"   Filter rate calculation error: {e}")
        return 0.0

def extract_liquid_clustering_data(profiler_data: Dict[str, Any], metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extract data required for Liquid Clustering analysis from SQL profiler data (for LLM analysis)
    """
    # metrics ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åž‹ãƒã‚§ãƒƒã‚¯ï¼ˆé˜²å¾¡çš„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ï¼‰
    if not isinstance(metrics, dict):
        print(f"âš ï¸ Error: metrics parameter is not a dictionary (type: {type(metrics)})")
        print(f"   Expected: dict, Received: {type(metrics)}")
        return {
            "filter_columns": [],
            "join_columns": [],
            "groupby_columns": [],
            "aggregate_columns": [],
            "table_info": {},
            "scan_nodes": [],
            "join_nodes": [],
            "filter_nodes": [],
            "metadata_summary": {"error": f"Invalid metrics type: {type(metrics)}"}
        }
    
    extracted_data = {
        "filter_columns": [],
        "join_columns": [],
        "groupby_columns": [],
        "aggregate_columns": [],
        "table_info": {},
        "scan_nodes": [],
        "join_nodes": [],
        "filter_nodes": [],
        "metadata_summary": {}
    }
    
    print(f"ðŸ” Starting data extraction for Liquid Clustering analysis")
    
    # ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’ç¢ºèª
    data_format = metrics.get('data_format', '')
    if data_format == 'sql_query_summary':
        print("ðŸ“Š SQL query summary format: Limited Liquid Clustering analysis")
        # test2.jsonå½¢å¼ã®å ´åˆã¯åˆ¶é™ä»˜ãã®åˆ†æžã‚’è¡Œã†
        query_info = metrics.get('query_info', {})
        query_text = query_info.get('query_text', '')
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±ã®ã¿ã‹ã‚‰åŸºæœ¬çš„ãªãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’ç”Ÿæˆ
        # test2.jsonå½¢å¼ã§ã¯ planMetadatas ãŒç©ºã®ãŸã‚ã€graphs metadata ã¯åˆ©ç”¨ä¸å¯
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹é‡è¦–ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æžã‚’è¡Œã†
        
        # å…¨ä½“çš„ãªãƒ•ã‚£ãƒ«ã‚¿çŽ‡æƒ…å ±ã‚’è¨ˆç®—
        overall_metrics = metrics.get('overall_metrics', {})
        overall_filter_rate = calculate_filter_rate_percentage(overall_metrics, metrics)
        read_bytes = overall_metrics.get('read_bytes', 0)
        read_gb = read_bytes / (1024**3) if read_bytes > 0 else 0
        
        # ãƒ—ãƒ«ãƒ¼ãƒ³é‡ã‚’æŽ¨å®šï¼ˆãƒ•ã‚£ãƒ«ã‚¿çŽ‡ã‹ã‚‰é€†ç®—ï¼‰
        if overall_filter_rate > 0 and read_bytes > 0:
            pruned_bytes = (read_bytes * overall_filter_rate) / (1 - overall_filter_rate)
            pruned_gb = pruned_bytes / (1024**3)
        else:
            pruned_bytes = 0
            pruned_gb = 0
        
        extracted_data["table_info"]["metrics_summary"] = {
            "node_name": "Metrics-Based Analysis",
            "node_tag": "QUERY_SUMMARY", 
            "node_id": "summary",
            "files_count": overall_metrics.get('read_files_count', 0),
            "partitions_count": overall_metrics.get('read_partitions_count', 0),
            "data_size_gb": read_gb,
            "rows_read": overall_metrics.get('rows_read_count', 0),
            "rows_produced": overall_metrics.get('rows_produced_count', 0),
            "data_selectivity": overall_filter_rate,
            "avg_file_size_mb": (overall_metrics.get('read_bytes', 0) / 1024 / 1024) / max(overall_metrics.get('read_files_count', 1), 1),
            "avg_partition_size_mb": (overall_metrics.get('read_bytes', 0) / 1024 / 1024) / max(overall_metrics.get('read_partitions_count', 1), 1),
            "note": "Detailed table information is not available in SQL query summary format. Executing metrics-based analysis.",
            "current_clustering_keys": [],  # ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼
            "filter_info": {  # ãƒ•ã‚£ãƒ«ã‚¿çŽ‡æƒ…å ±ã‚’è¿½åŠ 
                "filter_rate": overall_filter_rate,
                "files_read_bytes": read_bytes,
                "files_pruned_bytes": pruned_bytes,
                "has_filter_metrics": read_bytes > 0
            }
        }
        
        # ã‚µãƒžãƒªãƒ¼ãƒŽãƒ¼ãƒ‰ã®æƒ…å ±ã‚’ä½¿ç”¨
        for node in metrics.get('node_metrics', []):
            node_name = node.get('name', '')
            extracted_data["scan_nodes"].append({
                "name": node_name,
                "type": node.get('tag', ''),
                "rows": node.get('key_metrics', {}).get('rowsNum', 0),
                "duration_ms": node.get('key_metrics', {}).get('durationMs', 0),
                "node_id": node.get('node_id', '')
            })
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚µãƒžãƒªãƒ¼ï¼ˆåˆ¶é™ä»˜ãï¼‰
        view_count = sum(1 for table in extracted_data["table_info"].values() if table.get('is_view', False))
        actual_table_count = sum(len(table.get('underlying_tables', [])) for table in extracted_data["table_info"].values())
        
        extracted_data["metadata_summary"] = {
            "total_nodes": len(metrics.get('node_metrics', [])),
            "total_graphs": 0,
            "filter_expressions_count": 0,
            "join_expressions_count": 0,
            "groupby_expressions_count": 0,
            "aggregate_expressions_count": 0,
            "tables_identified": len(extracted_data["table_info"]),
            "views_identified": view_count,
            "underlying_tables_estimated": actual_table_count,
            "scan_nodes_count": len(extracted_data["scan_nodes"]),
            "join_nodes_count": 0,
            "filter_nodes_count": 0,
            "analysis_limitation": "Detailed analysis is limited due to SQL query summary format"
        }
        
        print(f"âœ… Limited data extraction completed: {extracted_data['metadata_summary']}")
        
        # ãƒ“ãƒ¥ãƒ¼æƒ…å ±ã®è©³ç´°è¡¨ç¤º
        if view_count > 0:
            print(f"ðŸ” View information details:")
            for table_name, table_info in extracted_data["table_info"].items():
                if table_info.get('is_view', False):
                    print(f"  ðŸ“Š View: {table_name}")
                    print(f"     Alias: {table_info.get('alias', 'None')}")
                    print(f"     Table type: {table_info.get('table_type', 'unknown')}")
                    
                    underlying_tables = table_info.get('underlying_tables', [])
                    if underlying_tables:
                        print(f"     Estimated underlying table count: {len(underlying_tables)}")
                        for i, underlying_table in enumerate(underlying_tables[:3]):  # Display max 3
                            print(f"       - {underlying_table}")
                        if len(underlying_tables) > 3:
                            print(f"       ... and {len(underlying_tables) - 3} additional tables")
                    print()
        
        return extracted_data
    
    # é€šå¸¸ã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼å½¢å¼ã®å‡¦ç†
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿè¡Œã‚°ãƒ©ãƒ•æƒ…å ±ã‚’å–å¾—ï¼ˆè¤‡æ•°ã‚°ãƒ©ãƒ•å¯¾å¿œï¼‰
    graphs = profiler_data.get('graphs', [])
    if not graphs:
        print("âš ï¸ Graph data not found")
        return extracted_data

    # ã™ã¹ã¦ã®ã‚°ãƒ©ãƒ•ã‹ã‚‰ãƒŽãƒ¼ãƒ‰ã‚’åŽé›†
    all_nodes = []
    table_size_info = {}  # ãƒ†ãƒ¼ãƒ–ãƒ«å -> ã‚µã‚¤ã‚ºæƒ…å ±ã®ãƒžãƒƒãƒ”ãƒ³ã‚°
    
    for graph_index, graph in enumerate(graphs):
        nodes = graph.get('nodes', [])
        for node in nodes:
            node['graph_index'] = graph_index
            all_nodes.append(node)
            
            # ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºã‚’æŠ½å‡º
            node_name = node.get('name', '')
            if 'Scan' in node_name:
                # ãƒ†ãƒ¼ãƒ–ãƒ«åã®æŠ½å‡º
                table_name = node_name.replace('Scan ', '').strip()
                
                # Size of files readãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º
                metrics = node.get('metrics', [])
                files_read_bytes = 0
                files_pruned_bytes = 0
                io_read_bytes = 0
                
                for metric in metrics:
                    label = metric.get('label', '')
                    value = metric.get('value', 0)
                    
                    if 'Size of files read' in label:
                        files_read_bytes = value
                    elif 'Size of files pruned' in label:
                        files_pruned_bytes = value
                    elif 'Size of data read with io requests' in label:
                        io_read_bytes = value
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±ã‚’ä¿å­˜ï¼ˆæœ€å¤§å€¤ã‚’è¨˜éŒ²ï¼‰
                if table_name not in table_size_info or files_read_bytes > table_size_info[table_name]['files_read_bytes']:
                    table_size_info[table_name] = {
                        'files_read_bytes': files_read_bytes,
                        'files_read_gb': files_read_bytes / (1024**3),
                        'files_pruned_bytes': files_pruned_bytes,
                        'files_pruned_gb': files_pruned_bytes / (1024**3),
                        'io_read_bytes': io_read_bytes,
                        'io_read_gb': io_read_bytes / (1024**3),
                        'total_scan_gb': (files_read_bytes + files_pruned_bytes) / (1024**3)
                    }
    
    print(f"ðŸ” Processing {len(all_nodes)} nodes from {len(graphs)} graphs")
    print(f"ðŸ“Š Extracted table sizes from {len(table_size_info)} tables:")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
    for table_name, size_info in table_size_info.items():
        print(f"  - {table_name}: {size_info['files_read_gb']:.2f} GB (files read)")

    # ãƒŽãƒ¼ãƒ‰ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã‚’æŠ½å‡º
    for node in all_nodes:
        node_name = node.get('name', '')
        node_tag = node.get('tag', '')
        node_metadata = node.get('metadata', [])
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é‡è¦ãªæƒ…å ±ã‚’æŠ½å‡º
        for metadata_item in node_metadata:
            key = metadata_item.get('key', '')
            values = metadata_item.get('values', [])
            value = metadata_item.get('value', '')
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã®æŠ½å‡º
            if key == 'FILTERS' and values:
                for filter_expr in values:
                    extracted_data["filter_columns"].append({
                        "expression": filter_expr,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # GROUP BYå¼ã®æŠ½å‡º
            elif key == 'GROUPING_EXPRESSIONS' and values:
                for group_expr in values:
                    extracted_data["groupby_columns"].append({
                        "expression": group_expr,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # JOINæ¡ä»¶ã®æŠ½å‡º
            elif key in ['LEFT_KEYS', 'RIGHT_KEYS'] and values:
                for join_key in values:
                    extracted_data["join_columns"].append({
                        "expression": join_key,
                        "key_type": key,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # é›†ç´„é–¢æ•°ã®æŠ½å‡º
            elif key == 'AGGREGATE_EXPRESSIONS' and values:
                for agg_expr in values:
                    extracted_data["aggregate_columns"].append({
                        "expression": agg_expr,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®æŠ½å‡º
            elif key == 'SCAN_IDENTIFIER':
                table_name = value
                # ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰ã‹ã‚‰ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æƒ…å ±ã‚’æŠ½å‡º
                cluster_attributes = extract_cluster_attributes(node)
                print(f"    ðŸ“Š Table {table_name} clustering keys: {cluster_attributes}")
                
                extracted_data["table_info"][table_name] = {
                    "node_name": node_name,
                    "node_tag": node_tag,
                    "node_id": node.get('id', ''),
                    "current_clustering_keys": cluster_attributes  # æŠ½å‡ºã—ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã‚’è¨­å®š
                }

    # ãƒŽãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—åˆ¥ã®åˆ†é¡žã¨ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã®é–¢é€£ä»˜ã‘
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒè¾žæ›¸ã§ãªã„å ´åˆã¯node_metricsã‚’ç©ºãƒªã‚¹ãƒˆã¨ã—ã¦å‡¦ç†
    if isinstance(metrics, dict):
        node_metrics = metrics.get('node_metrics', [])
    else:
        print(f"âš ï¸ Warning: metrics is not a dictionary (type: {type(metrics)}), using empty node_metrics")
        node_metrics = []
    for node in node_metrics:
        node_name = node.get('name', '')
        node_type = node.get('tag', '')
        key_metrics = node.get('key_metrics', {})
        
        if any(keyword in node_name.upper() for keyword in ['SCAN', 'FILESCAN', 'PARQUET', 'DELTA']):
            # ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰ã‹ã‚‰ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã‚’æŠ½å‡º
            cluster_attributes = extract_cluster_attributes(node)
            
            # ãƒŽãƒ¼ãƒ‰ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡ºã—ã¦ãƒžãƒƒãƒ”ãƒ³ã‚°
            node_metadata = node.get('metadata', [])
            table_name_from_node = None
            
            for meta in node_metadata:
                meta_key = meta.get('key', '')
                meta_value = meta.get('value', '')
                if meta_key == 'SCAN_IDENTIFIER' and meta_value:
                    table_name_from_node = meta_value
                    break
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«åãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒŽãƒ¼ãƒ‰åã‹ã‚‰æŽ¨æ¸¬
            if not table_name_from_node:
                import re
                table_patterns = [
                    r'[Ss]can\s+([a-zA-Z_][a-zA-Z0-9_.]*[a-zA-Z0-9_])',
                    r'([a-zA-Z_][a-zA-Z0-9_]*\.)+([a-zA-Z_][a-zA-Z0-9_]*)',
                ]
                
                for pattern in table_patterns:
                    match = re.search(pattern, node_name)
                    if match:
                        if '.' in match.group(0):
                            table_name_from_node = match.group(0)
                        else:
                            table_name_from_node = match.group(1) if match.lastindex and match.lastindex >= 1 else match.group(0)
                        break
            
            # ãƒ•ã‚£ãƒ«ã‚¿çŽ‡æƒ…å ±ã‚’è¨ˆç®—
            filter_result = calculate_filter_rate(node)
            filter_rate_info = {
                "filter_rate": filter_result.get("filter_rate", 0),
                "files_read_bytes": filter_result.get("files_read_bytes", 0),
                "files_pruned_bytes": filter_result.get("files_pruned_bytes", 0),
                "has_filter_metrics": filter_result.get("has_filter_metrics", False)
            }
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã«ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã¨ãƒ•ã‚£ãƒ«ã‚¿çŽ‡ã‚’è¿½åŠ 
            if table_name_from_node:
                # æ—¢å­˜ã®ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’æ›´æ–°
                if table_name_from_node in extracted_data["table_info"]:
                    extracted_data["table_info"][table_name_from_node]["current_clustering_keys"] = cluster_attributes
                    extracted_data["table_info"][table_name_from_node]["filter_info"] = filter_rate_info
                else:
                    # æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’ä½œæˆ
                    extracted_data["table_info"][table_name_from_node] = {
                        "node_name": node_name,
                        "node_tag": node_type,
                        "node_id": node.get('node_id', ''),
                        "current_clustering_keys": cluster_attributes,
                        "filter_info": filter_rate_info
                    }
            
            extracted_data["scan_nodes"].append({
                "name": node_name,
                "type": node_type,
                "rows": key_metrics.get('rowsNum', 0),
                "duration_ms": key_metrics.get('durationMs', 0),
                "node_id": node.get('node_id', ''),
                "table_name": table_name_from_node,
                "current_clustering_keys": cluster_attributes
            })
        elif any(keyword in node_name.upper() for keyword in ['JOIN', 'HASH']):
            extracted_data["join_nodes"].append({
                "name": node_name,
                "type": node_type,
                "duration_ms": key_metrics.get('durationMs', 0),
                "node_id": node.get('node_id', '')
            })
        elif any(keyword in node_name.upper() for keyword in ['FILTER']):
            extracted_data["filter_nodes"].append({
                "name": node_name,
                "type": node_type,
                "duration_ms": key_metrics.get('durationMs', 0),
                "node_id": node.get('node_id', '')
            })

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚µãƒžãƒªãƒ¼
    extracted_data["metadata_summary"] = {
        "total_nodes": len(all_nodes),
        "total_graphs": len(graphs),
        "filter_expressions_count": len(extracted_data["filter_columns"]),
        "join_expressions_count": len(extracted_data["join_columns"]),
        "groupby_expressions_count": len(extracted_data["groupby_columns"]),
        "aggregate_expressions_count": len(extracted_data["aggregate_columns"]),
        "tables_identified": len(extracted_data["table_info"]),
        "scan_nodes_count": len(extracted_data["scan_nodes"]),
        "join_nodes_count": len(extracted_data["join_nodes"]),
        "filter_nodes_count": len(extracted_data["filter_nodes"])
    }
    
    print(f"âœ… Data extraction completed: {extracted_data['metadata_summary']}")
    
    # Display detailed current clustering key information
    clustering_info_found = False
    for table_name, table_info in extracted_data["table_info"].items():
        current_keys = table_info.get('current_clustering_keys', [])
        if current_keys:
            if not clustering_info_found:
                print(f"ðŸ” Current clustering key information:")
                clustering_info_found = True
            print(f"  ðŸ“Š Table: {table_name}")
            print(f"     Current keys: {', '.join(current_keys)}")
            print(f"     Node: {table_info.get('node_name', 'Unknown')}")
            print()
    
    if not clustering_info_found:
        print(f"â„¹ï¸ No current clustering keys detected")
    
    # ðŸš¨ é‡è¦: æŠ½å‡ºã—ãŸãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±ã‚’table_infoã«çµ±åˆ
    def normalize_table_name(table_name):
        """ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æ­£è¦åŒ–ï¼ˆãƒ•ãƒ«ãƒãƒ¼ãƒ ã¨çŸ­ç¸®åã®ä¸¡æ–¹ã‚’ãƒã‚§ãƒƒã‚¯ï¼‰"""
        if not table_name:
            return None
        # æ—¢å­˜ã®ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‹ã‚‰ãƒžãƒƒãƒã™ã‚‹ã‚‚ã®ã‚’æŽ¢ã™
        for existing_table in extracted_data["table_info"].keys():
            if (existing_table == table_name or 
                existing_table.endswith('.' + table_name) or
                table_name.endswith('.' + existing_table.split('.')[-1])):
                return existing_table
        return table_name
    
    for table_name, size_info in table_size_info.items():
        # ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æ­£è¦åŒ–ã—ã¦æ—¢å­˜ã‚¨ãƒ³ãƒˆãƒªã¨ãƒžãƒƒãƒ
        normalized_table_name = normalize_table_name(table_name)
        
        if normalized_table_name not in extracted_data["table_info"]:
            # æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æƒ…å ±ãªã—ï¼‰
            extracted_data["table_info"][normalized_table_name] = {
                "node_name": f"Scan {table_name}",
                "node_tag": "SCAN", 
                "node_id": f"scan_{table_name.replace('.', '_')}",
                "current_clustering_keys": [],  # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æƒ…å ±ã¯åˆ¥é€”æŠ½å‡ºæ¸ˆã¿
                "filter_info": {}
            }
        # æ—¢å­˜ã‚¨ãƒ³ãƒˆãƒªãŒã‚ã‚‹å ´åˆã¯ã€current_clustering_keysã¯ä¿æŒ
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±ã‚’è¿½åŠ ï¼ˆæ­£è¦åŒ–ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ä½¿ç”¨ï¼‰
        extracted_data["table_info"][normalized_table_name].update({
            "table_size_gb": size_info['files_read_gb'],
            "files_read_bytes": size_info['files_read_bytes'],
            "files_pruned_bytes": size_info['files_pruned_bytes'],
            "io_read_bytes": size_info['io_read_bytes'],
            "total_scan_gb": size_info['total_scan_gb'],
            "size_classification": (
                "large" if size_info['files_read_gb'] >= 50 else
                "medium" if size_info['files_read_gb'] >= 10 else
                "small"
            )
        })
    
    print(f"âœ… Table size integration completed: {len(table_size_info)} tables")
    
    return extracted_data

def analyze_liquid_clustering_opportunities(profiler_data: Dict[str, Any], metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    Generate Liquid Clustering analysis and recommendations using LLM
    """
    print(f"ðŸ¤– Starting LLM-based Liquid Clustering analysis")
    
    # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
    extracted_data = extract_liquid_clustering_data(profiler_data, metrics)
    
    # LLMåˆ†æžç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
    overall_metrics = metrics.get('overall_metrics', {})
    bottleneck_indicators = metrics.get('bottleneck_indicators', {})
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¦‚è¦
    total_time_sec = overall_metrics.get('total_time_ms', 0) / 1000
    read_gb = overall_metrics.get('read_bytes', 0) / 1024 / 1024 / 1024
    rows_produced = overall_metrics.get('rows_produced_count', 0)
    rows_read = overall_metrics.get('rows_read_count', 0)
    
    # æŠ½å‡ºã—ãŸã‚«ãƒ©ãƒ æƒ…å ±ã®ã‚µãƒžãƒªãƒ¼ä½œæˆï¼ˆä¸Šä½5å€‹ã¾ã§ï¼‰
    filter_summary = []
    for i, item in enumerate(extracted_data["filter_columns"][:5]):
        filter_summary.append(f"  {i+1}. {item['expression']} (node: {item['node_name']})")
    
    join_summary = []
    for i, item in enumerate(extracted_data["join_columns"][:5]):
        join_summary.append(f"  {i+1}. {item['expression']} (type: {item['key_type']}, node: {item['node_name']})")
    
    groupby_summary = []
    for i, item in enumerate(extracted_data["groupby_columns"][:5]):
        groupby_summary.append(f"  {i+1}. {item['expression']} (node: {item['node_name']})")
    
    aggregate_summary = []
    for i, item in enumerate(extracted_data["aggregate_columns"][:5]):
        aggregate_summary.append(f"  {i+1}. {item['expression']} (node: {item['node_name']})")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®é‡è¤‡ã‚¨ãƒ³ãƒˆãƒªã‚’çµ±åˆï¼ˆãƒ•ãƒ«ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’å„ªå…ˆï¼‰
    print(f"ðŸ” Debug: Table info consolidation starting...")
    print(f"   Original table_info keys: {list(extracted_data['table_info'].keys())}")
    
    # ã¾ãšãƒ•ãƒ«ãƒ†ãƒ¼ãƒ–ãƒ«åã®ã¿ã‚’å‡¦ç†
    consolidated_table_info = {}
    full_table_names = []
    short_table_names = []
    
    for table_name, table_info in extracted_data["table_info"].items():
        if '.' in table_name and table_name.count('.') >= 2:  # ãƒ•ãƒ«ãƒ†ãƒ¼ãƒ–ãƒ«åã®æ¡ä»¶ã‚’åŽ³å¯†åŒ–
            consolidated_table_info[table_name] = table_info
            full_table_names.append(table_name)
            print(f"   âœ… Added full table: {table_name}, clustering_keys: {table_info.get('current_clustering_keys', [])}")
        else:
            short_table_names.append((table_name, table_info))
    
    # æ¬¡ã«çŸ­ç¸®ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’å‡¦ç†ï¼ˆå¯¾å¿œã™ã‚‹ãƒ•ãƒ«ãƒ†ãƒ¼ãƒ–ãƒ«åãŒãªã„å ´åˆã®ã¿ï¼‰
    for table_name, table_info in short_table_names:
        # å¯¾å¿œã™ã‚‹ãƒ•ãƒ«ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŽ¢ã™
        matching_full_table = None
        for full_name in full_table_names:
            if full_name.endswith('.' + table_name):
                matching_full_table = full_name
                break
        
        if not matching_full_table:
            consolidated_table_info[table_name] = table_info
            print(f"   âš ï¸ Added short table (no full match): {table_name}, clustering_keys: {table_info.get('current_clustering_keys', [])}")
        else:
            print(f"   âŒ Skipped short table (has full match): {table_name} â†’ {matching_full_table}")
    
    print(f"   Final consolidated keys: {list(consolidated_table_info.keys())}")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®ã‚µãƒžãƒªãƒ¼ï¼ˆç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã¨ãƒ•ã‚£ãƒ«ã‚¿çŽ‡ã‚’å«ã‚€ï¼‰
    table_summary = []
    for table_name, table_info in consolidated_table_info.items():
        current_keys = table_info.get('current_clustering_keys', [])
        current_keys_str = ', '.join(current_keys) if current_keys else 'Not configured'
        
        # ãƒ•ã‚£ãƒ«ã‚¿çŽ‡æƒ…å ±ã‚’è¿½åŠ 
        filter_info = table_info.get('filter_info', {})
        filter_rate = filter_info.get('filter_rate', 0)
        files_read_bytes = filter_info.get('files_read_bytes', 0)
        files_pruned_bytes = filter_info.get('files_pruned_bytes', 0)
        
        # ãƒã‚¤ãƒˆæ•°ã‚’GBå˜ä½ã«å¤‰æ›
        read_gb = files_read_bytes / (1024**3) if files_read_bytes > 0 else 0
        pruned_gb = files_pruned_bytes / (1024**3) if files_pruned_bytes > 0 else 0
        
        if filter_info.get('has_filter_metrics', False):
            filter_str = f", filter rate: {filter_rate*100:.1f}% (read: {read_gb:.2f}GB, pruned: {pruned_gb:.2f}GB)"
        else:
            filter_str = ", filter rate: no information"
        
        # ðŸš¨ é‡è¦: å®Ÿéš›ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±ã‚’ä½¿ç”¨ã—ã¦æŽ¨å¥¨åˆ¤å®š
        table_size_gb = table_info.get('table_size_gb', 0)
        size_classification = table_info.get('size_classification', 'unknown')
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹æŽ¨å¥¨åˆ¤å®š
        recommendation_status = (
            "âŒæŽ¨å¥¨ã—ãªã„(å°è¦æ¨¡)" if size_classification == "small" else
            "âš ï¸æ¡ä»¶ä»˜ãæŽ¨å¥¨(ä¸­è¦æ¨¡)" if size_classification == "medium" else
            "âœ…å¼·ãæŽ¨å¥¨(å¤§è¦æ¨¡)" if size_classification == "large" else
            "âš ï¸è¦ç¢ºèª"
        )
        
        table_summary.append(f"  - {table_name} ({recommendation_status}, ã‚µã‚¤ã‚º: {table_size_gb:.2f}GB, node: {table_info['node_name']}, current clustering key: {current_keys_str}{filter_str})")
    
    # ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æƒ…å ±
    scan_performance = []
    for scan in extracted_data["scan_nodes"]:
        efficiency = scan['rows'] / max(scan['duration_ms'], 1)
        scan_performance.append(f"  - {scan['name']}: {scan['rows']:,} rows, {scan['duration_ms']:,}ms, efficiency={efficiency:.1f} rows/ms")

    clustering_prompt = f"""
You are a Databricks Liquid Clustering expert. Please analyze the following SQL profiler data and provide optimal Liquid Clustering recommendations.

ã€Query Performance Overviewã€‘
- Execution time: {total_time_sec:.1f} seconds
- Data read: {read_gb:.2f}GB
- Output rows: {rows_produced:,} rows
- Read rows: {rows_read:,} rows
- ãƒ•ã‚£ãƒ«ã‚¿çŽ‡: {calculate_filter_rate_percentage(overall_metrics, metrics):.4f}

ã€æŠ½å‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‘

ðŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ ({len(extracted_data["filter_columns"])}å€‹):
{chr(10).join(filter_summary)}

ðŸ”— JOINæ¡ä»¶ ({len(extracted_data["join_columns"])}å€‹):
{chr(10).join(join_summary)}

ðŸ“Š GROUP BY ({len(extracted_data["groupby_columns"])}å€‹):
{chr(10).join(groupby_summary)}

ðŸ“ˆ é›†ç´„é–¢æ•° ({len(extracted_data["aggregate_columns"])}å€‹) - âš ï¸å‚è€ƒæƒ…å ±ã®ã¿ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã«ã¯ä½¿ç”¨ç¦æ­¢ï¼‰:
{chr(10).join(aggregate_summary)}
âš ï¸ æ³¨æ„: ä¸Šè¨˜ã®é›†ç´„é–¢æ•°ã§ä½¿ç”¨ã•ã‚Œã‚‹ã‚«ãƒ©ãƒ ã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã®å€™è£œã‹ã‚‰é™¤å¤–ã—ã¦ãã ã•ã„ã€‚

ã€ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã€‘
ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(extracted_data["table_info"])}å€‹
{chr(10).join(table_summary)}

ã€ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã€‘
{chr(10).join(scan_performance)}

ã€ç¾åœ¨ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã€‘
- ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ: {'ã‚ã‚Š' if bottleneck_indicators.get('has_spill', False) else 'ãªã—'}
- ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ: {bottleneck_indicators.get('shuffle_operations_count', 0)}å›ž
- ä½Žä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸: {bottleneck_indicators.get('low_parallelism_stages_count', 0)}å€‹

ã€åˆ†æžè¦æ±‚ã€‘
1. å„ãƒ†ãƒ¼ãƒ–ãƒ«ã«å¯¾ã™ã‚‹æœ€é©ãªLiquid Clusteringã‚«ãƒ©ãƒ ã®æŽ¨å¥¨ï¼ˆæœ€å¤§4ã‚«ãƒ©ãƒ ï¼‰
2. ã‚«ãƒ©ãƒ é¸å®šã®æ ¹æ‹ ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€JOINã€GROUP BYã§ã®ä½¿ç”¨é »åº¦ã¨é‡è¦åº¦ï¼‰
   ðŸš¨ é‡è¦: é›†ç´„é–¢æ•°ï¼ˆSUM, AVG, COUNTç­‰ï¼‰ã®å¯¾è±¡ã‚«ãƒ©ãƒ ã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã«å«ã‚ãªã„
   âœ… æœ‰åŠ¹: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã€JOINæ¡ä»¶ã€GROUP BYæ¡ä»¶ã§ä½¿ç”¨ã•ã‚Œã‚‹ã‚«ãƒ©ãƒ ã®ã¿
3. ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã¨æŽ¨å¥¨ã‚­ãƒ¼ã®æ¯”è¼ƒåˆ†æž
4. å®Ÿè£…å„ªå…ˆé †ä½ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹å‘ä¸ŠåŠ¹æžœé †ï¼‰
5. å…·ä½“çš„ãªSQLå®Ÿè£…ä¾‹ï¼ˆæ­£ã—ã„Databricks SQLæ§‹æ–‡ã€ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã«æ˜Žè¨˜ï¼‰
6. æœŸå¾…ã•ã‚Œã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ”¹å–„åŠ¹æžœï¼ˆæ•°å€¤ã§ï¼‰

ã€ðŸš¨ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼é¸å®šã®é‡è¦ãªåˆ¶é™äº‹é …ã€‘
âŒ ç¦æ­¢: é›†ç´„é–¢æ•°ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚«ãƒ©ãƒ ï¼ˆä¾‹ï¼šSUM(sales_amount)ã®sales_amountï¼‰
âŒ ç¦æ­¢: è¨ˆç®—ã®ã¿ã«ä½¿ç”¨ã•ã‚Œã‚‹ã‚«ãƒ©ãƒ ï¼ˆä¾‹ï¼šAVG(quantity)ã®quantityï¼‰
âœ… æŽ¨å¥¨: WHEREå¥ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚«ãƒ©ãƒ 
âœ… æŽ¨å¥¨: JOIN ONå¥ã®ã‚­ãƒ¼ã‚«ãƒ©ãƒ   
âœ… æŽ¨å¥¨: GROUP BYå¥ã®ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã‚«ãƒ©ãƒ 
âœ… æŽ¨å¥¨: ORDER BYå¥ã®ã‚½ãƒ¼ãƒˆã‚­ãƒ¼ï¼ˆç¯„å›²æ¤œç´¢ãŒã‚ã‚‹å ´åˆï¼‰

ç†ç”±: é›†ç´„å¯¾è±¡ã‚«ãƒ©ãƒ ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã«å«ã‚ã¦ã‚‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹æžœã‚„JOINåŠ¹çŽ‡ã®æ”¹å–„ãŒæœŸå¾…ã§ããšã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®åŠ¹æžœã‚’è–„ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

ã€åˆ¶ç´„äº‹é …ã€‘
- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã‚„ZORDERã¯ææ¡ˆã—ãªã„ï¼ˆLiquid Clusteringã®ã¿ï¼‰
- æ­£ã—ã„Databricks SQLæ§‹æ–‡ã‚’ä½¿ç”¨ï¼š
  * æ–°è¦ãƒ†ãƒ¼ãƒ–ãƒ«: CREATE TABLE ... CLUSTER BY (col1, col2, ...)
  * æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«: ALTER TABLE table_name CLUSTER BY (col1, col2, ...)
- æœ€å¤§4ã‚«ãƒ©ãƒ ã¾ã§ã®æŽ¨å¥¨
- ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ã‚„ä¸¦åˆ—åº¦ã®å•é¡Œã‚‚è€ƒæ…®

ã€ðŸš¨ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ã®æŽ¨å¥¨åˆ¤å®šåŸºæº–ã€‘
âŒ æŽ¨å¥¨ã—ãªã„ï¼ˆåŠ¹æžœè–„ï¼‰: 10GBæœªæº€ã®ãƒ†ãƒ¼ãƒ–ãƒ«
  - å°è¦æ¨¡ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆdate_dim, itemãªã©ï¼‰ã¯é™¤å¤–
  - ç†ç”±: ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒå°‘ãªãã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åŠ¹æžœãŒé™å®šçš„
  - ä»£æ›¿ç­–: é©åˆ‡ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚„ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ´»ç”¨

âš ï¸ æ¡ä»¶ä»˜ãæŽ¨å¥¨: 10-50GBã®ãƒ†ãƒ¼ãƒ–ãƒ«  
  - ä¸­è¦æ¨¡ãƒ†ãƒ¼ãƒ–ãƒ«ã§ã€é »ç¹ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ãŒã‚ã‚‹å ´åˆã®ã¿æŽ¨å¥¨
  - ãƒ•ã‚£ãƒ«ã‚¿çŽ‡ã‚„ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è€ƒæ…®ã—ã¦åˆ¤å®š

âœ… å¼·ãæŽ¨å¥¨: 50GBä»¥ä¸Šã®ãƒ†ãƒ¼ãƒ–ãƒ«
  - å¤§è¦æ¨¡ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆstore_sales: 159GB, catalog_sales: 121GBç­‰ï¼‰
  - ç†ç”±: å¤§é‡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹æžœãŒå¤§ãã„
  
ã€ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥æŽ¨å¥¨å„ªå…ˆåº¦ã€‘
1. å¤§è¦æ¨¡ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ50GB+ï¼‰: æœ€å„ªå…ˆã§Liquid Clusteringé©ç”¨
2. ä¸­è¦æ¨¡ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ10-50GBï¼‰: ãƒ•ã‚£ãƒ«ã‚¿é »åº¦ã¨ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãåˆ¤å®š  
3. å°è¦æ¨¡ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ10GBæœªæº€ï¼‰: âŒ Liquid Clusteringã¯æŽ¨å¥¨ã—ãªã„

ã€ðŸš¨ CRITICAL: Liquid Clustering Column Order Rulesã€‘
- **NEVER suggest column reordering**: Column order in Liquid Clustering is MEANINGLESS for performance
- **NEVER generate reordering recommendations**: If current clustering exists, do NOT suggest changing the order
- **Technical Fact**: (col1, col2) and (col2, col1) have IDENTICAL performance in Liquid Clustering
- **Only suggest clustering IF**: Table currently has NO clustering OR completely different columns are needed

ã€ðŸš¨ Absolutely Prohibited Actionsã€‘
âŒ NEVER suggest "ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã®é †åºã‚’å…¥ã‚Œæ›¿ãˆ" (reordering current clustering keys)
âŒ NEVER recommend changing (cs_item_sk, cs_sold_date_sk) to (cs_sold_date_sk, cs_item_sk)
âŒ NEVER suggest "order changes for better performance"
âŒ NEVER mention "æ—¥ä»˜ã‚«ãƒ©ãƒ ã‚’å…ˆé ­ã«ã™ã‚‹ã“ã¨ã§åŠ¹çŽ‡ãŒå‘ä¸Š" (date column first for efficiency)

ã€âœ… ONLY Acceptable Recommendationsã€‘
âœ… Keep existing clustering unchanged if columns are appropriate
âœ… Suggest completely new clustering columns only if current ones are suboptimal
âœ… State "ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ãŒæœ€é©ãªãŸã‚å¤‰æ›´ä¸è¦" when current clustering is good

ç°¡æ½”ã§å®Ÿè·µçš„ãªåˆ†æžçµæžœã‚’æ—¥æœ¬èªžã§æä¾›ã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªå‡ºåŠ›å½¢å¼æŒ‡ç¤ºã€‘
å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®åˆ†æžã§ã¯ã€å¿…ãšä»¥ä¸‹ã®å½¢å¼ã§ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã¨ãƒ•ã‚£ãƒ«ã‚¿çŽ‡ã‚’å«ã‚ã¦ãã ã•ã„ï¼š

## ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥æŽ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°

### 1. [ãƒ†ãƒ¼ãƒ–ãƒ«å] ãƒ†ãƒ¼ãƒ–ãƒ« (æœ€å„ªå…ˆ/é«˜å„ªå…ˆåº¦/ä¸­å„ªå…ˆåº¦/âŒæŽ¨å¥¨ã—ãªã„)
**ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚º**: [æŽ¨å®šã‚µã‚¤ã‚º]GB
**ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼**: [ç¾åœ¨è¨­å®šã•ã‚Œã¦ã„ã‚‹ã‚­ãƒ¼ ã¾ãŸã¯ "è¨­å®šãªã—"]
**æŽ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚«ãƒ©ãƒ **: [æŽ¨å¥¨ã‚«ãƒ©ãƒ 1], [æŽ¨å¥¨ã‚«ãƒ©ãƒ 2], [æŽ¨å¥¨ã‚«ãƒ©ãƒ 3], [æŽ¨å¥¨ã‚«ãƒ©ãƒ 4] ã¾ãŸã¯ âŒ ã‚µã‚¤ã‚ºãŒå°ã•ã„ãŸã‚æŽ¨å¥¨ã—ãªã„

```sql
-- ðŸš¨ æ³¨æ„: 10GBæœªæº€ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®å ´åˆã¯ä»¥ä¸‹ã‚’å‡ºåŠ›
-- âŒ Liquid Clusteringã¯åŠ¹æžœãŒè–„ã„ãŸã‚æŽ¨å¥¨ã—ã¾ã›ã‚“
-- ðŸ’¡ ä»£æ›¿ç­–: CACHE TABLE [ãƒ†ãƒ¼ãƒ–ãƒ«å]; -- ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§é«˜é€Ÿã‚¢ã‚¯ã‚»ã‚¹
-- ðŸ’¡ ã¾ãŸã¯: OPTIMIZE [ãƒ†ãƒ¼ãƒ–ãƒ«å]; -- å°ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆã§ã‚¹ã‚­ãƒ£ãƒ³åŠ¹çŽ‡å‘ä¸Š

-- 10GBä»¥ä¸Šã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®å ´åˆã®ã¿ä»¥ä¸‹ã‚’å‡ºåŠ›  
ALTER TABLE [ãƒ†ãƒ¼ãƒ–ãƒ«å] 
CLUSTER BY ([æŽ¨å¥¨ã‚«ãƒ©ãƒ 1], [æŽ¨å¥¨ã‚«ãƒ©ãƒ 2], [æŽ¨å¥¨ã‚«ãƒ©ãƒ 3], [æŽ¨å¥¨ã‚«ãƒ©ãƒ 4]);
OPTIMIZE [ãƒ†ãƒ¼ãƒ–ãƒ«å] FULL;
```

**é¸å®šæ ¹æ‹ **:
- **ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºåˆ¤å®š**: [ã‚µã‚¤ã‚º]GB â†’ [æŽ¨å¥¨ã™ã‚‹/æŽ¨å¥¨ã—ãªã„]ç†ç”±
- [ã‚«ãƒ©ãƒ 1]: [ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨é‡è¦åº¦]
- [ã‚«ãƒ©ãƒ 2]: [ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨é‡è¦åº¦]
- [ä»¥ä¸‹åŒæ§˜...]
- ðŸš¨é‡è¦: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼é †åºå¤‰æ›´ã¯ãƒŽãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å±€æ‰€æ€§ã«å½±éŸ¿ã—ãªã„ï¼ˆLiquid Clusteringä»•æ§˜ï¼‰
- ðŸš¨æ³¨æ„: æ—¢å­˜ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ãŒé©åˆ‡ãªå ´åˆã¯é †åºå¤‰æ›´ã‚’æŽ¨å¥¨ã—ãªã„
- âœ…æ”¹å–„åŠ¹æžœ: ã‚¹ã‚­ãƒ£ãƒ³åŠ¹çŽ‡ã¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹æžœã®å‘ä¸Šï¼ˆé †åºç„¡é–¢ä¿‚ï¼‰

**æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„åŠ¹æžœ**:
- [å…·ä½“çš„ãªæ•°å€¤ã§ã®æ”¹å–„è¦‹è¾¼ã¿] ã¾ãŸã¯ âŒ å°è¦æ¨¡ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãŸã‚åŠ¹æžœè–„ã„

**ãƒ•ã‚£ãƒ«ã‚¿çŽ‡**: [X.X]% (èª­ã¿è¾¼ã¿: [XX.XX]GB, ãƒ—ãƒ«ãƒ¼ãƒ³: [XX.XX]GB)

ã“ã®å½¢å¼ã«ã‚ˆã‚Šã€ç¾åœ¨ã®è¨­å®šã€æŽ¨å¥¨è¨­å®šã€ãŠã‚ˆã³å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç¾åœ¨ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åŠ¹çŽ‡ã‚’æ˜Žç¢ºã«è¡¨ç¤ºã—ã¦ãã ã•ã„ã€‚ãƒ•ã‚£ãƒ«ã‚¿çŽ‡æƒ…å ±ã¯ä¸Šè¨˜ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‹ã‚‰æ­£ç¢ºãªæ•°å€¤ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
"""

    try:
        # LLMåˆ†æžã®å®Ÿè¡Œ
        provider = LLM_CONFIG["provider"]
        print(f"ðŸ¤– Analyzing Liquid Clustering using {provider}...")
        
        if provider == "databricks":
            llm_analysis = _call_databricks_llm(clustering_prompt)
        elif provider == "openai":
            llm_analysis = _call_openai_llm(clustering_prompt)
        elif provider == "azure_openai":
            llm_analysis = _call_azure_openai_llm(clustering_prompt)
        elif provider == "anthropic":
            llm_analysis = _call_anthropic_llm(clustering_prompt)
        else:
            llm_analysis = f"âŒ Unsupported LLM provider: {provider}"
        
        # Post-process and validate LLM analysis to remove inappropriate reordering recommendations
        if llm_analysis and not llm_analysis.startswith("âŒ"):
            llm_analysis = validate_and_filter_clustering_recommendations(llm_analysis, extracted_data)
        
        # åˆ†æžçµæžœã®æ§‹é€ åŒ–
        clustering_analysis = {
            "llm_analysis": llm_analysis,
            "extracted_data": extracted_data,
            "performance_context": {
                "total_time_sec": total_time_sec,
                "read_gb": read_gb,
                "rows_produced": rows_produced,
                "rows_read": rows_read,
                "data_selectivity": calculate_filter_rate_percentage(overall_metrics, metrics)
            },
            "summary": {
                "analysis_method": "LLM-based",
                "tables_identified": len(extracted_data["table_info"]),
                "total_filter_columns": len(extracted_data["filter_columns"]),
                "total_join_columns": len(extracted_data["join_columns"]),
                "total_groupby_columns": len(extracted_data["groupby_columns"]),
                "total_aggregate_columns": len(extracted_data["aggregate_columns"]),
                "scan_nodes_count": len(extracted_data["scan_nodes"]),
                "llm_provider": provider
            }
        }
        
        print("âœ… LLM Liquid Clustering analysis completed")
        return clustering_analysis
        
    except Exception as e:
        error_msg = f"LLM analysis error: {str(e)}"
        print(f"âŒ {error_msg}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªæŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’è¿”ã™
        return {
            "llm_analysis": f"âŒ LLMåˆ†æžã«å¤±æ•—ã—ã¾ã—ãŸ: {error_msg}",
            "extracted_data": extracted_data,
            "summary": {
                "analysis_method": "extraction-only",
                "tables_identified": len(extracted_data["table_info"]),
                "total_filter_columns": len(extracted_data["filter_columns"]),
                "error": error_msg
            }
        }

def validate_and_filter_clustering_recommendations(llm_analysis: str, extracted_data: Dict[str, Any]) -> str:
    """
    Post-process LLM analysis to remove any inappropriate clustering key reordering recommendations.
    
    This function serves as a safety net to ensure that even if the LLM generates reordering
    recommendations despite the prompt instructions, they will be filtered out.
    
    Args:
        llm_analysis: Raw LLM analysis text
        extracted_data: Extracted clustering data including current clustering keys
        
    Returns:
        str: Filtered and validated analysis text
    """
    import re
    
    # Get current clustering keys for each table
    current_clustering = {}
    table_info = extracted_data.get('table_info', {})
    for table_name, table_data in table_info.items():
        current_keys = table_data.get('current_clustering_keys', [])
        if current_keys:
            current_clustering[table_name] = current_keys
    
    print(f"ðŸ” Validating clustering recommendations for {len(current_clustering)} tables with existing clustering...")
    
    # Patterns that indicate problematic reordering recommendations
    problematic_patterns = [
        r'ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã®é †åºã‚’å…¥ã‚Œæ›¿ãˆ',
        r'ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã®é †åºã‚’å¤‰æ›´',
        r'æ—¥ä»˜ã‚«ãƒ©ãƒ ã‚’å…ˆé ­ã«ã™ã‚‹ã“ã¨ã§åŠ¹çŽ‡ãŒå‘ä¸Š',
        r'reorder.*current.*clustering.*key',
        r'changing.*order.*clustering.*key',
        r'clustering.*key.*order.*change',
        r'é †åº.*å…¥ã‚Œæ›¿ãˆ.*åŠ¹çŽ‡',
        r'å…¥ã‚Œæ›¿ãˆ.*æœ€é©',
        r'reorder.*for.*better.*performance'
    ]
    
    # Check if the analysis contains problematic recommendations
    found_problematic = []
    for pattern in problematic_patterns:
        matches = re.findall(pattern, llm_analysis, re.IGNORECASE | re.DOTALL)
        if matches:
            found_problematic.extend(matches)
    
    if found_problematic:
        print(f"âš ï¸ WARNING: Found {len(found_problematic)} problematic reordering recommendations in LLM response")
        for i, match in enumerate(found_problematic):
            print(f"   {i+1}. {match}")
        
        # Filter out problematic recommendations
        filtered_analysis = llm_analysis
        
        # Remove lines containing reordering recommendations
        lines = filtered_analysis.split('\n')
        filtered_lines = []
        
        for line in lines:
            is_problematic_line = False
            for pattern in problematic_patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    is_problematic_line = True
                    print(f"   ðŸš« Removed problematic line: {line.strip()}")
                    break
            
            if not is_problematic_line:
                filtered_lines.append(line)
        
        filtered_analysis = '\n'.join(filtered_lines)
        
        # Add validation notice
        validation_notice = """

ðŸ” **åˆ†æžçµæžœæ¤œè¨¼æ¸ˆã¿**: ã“ã®æŽ¨å¥¨äº‹é …ã¯ã€Liquid Clusteringã®æŠ€è¡“ä»•æ§˜ã«åŸºã¥ã„ã¦æ¤œè¨¼æ¸ˆã¿ã§ã™ã€‚
- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã®é †åºã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã«å½±éŸ¿ã—ã¾ã›ã‚“
- æ—¢å­˜ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ãŒé©åˆ‡ãªå ´åˆã¯å¤‰æ›´ä¸è¦ã§ã™
- é †åºå¤‰æ›´ã«ã‚ˆã‚‹æ€§èƒ½æ”¹å–„åŠ¹æžœã¯ã‚ã‚Šã¾ã›ã‚“

"""
        filtered_analysis += validation_notice
        
        print(f"âœ… Successfully filtered LLM response and added validation notice")
        return filtered_analysis
    
    else:
        print("âœ… No problematic reordering recommendations found in LLM response")
        return llm_analysis

def save_liquid_clustering_analysis(clustering_analysis: Dict[str, Any], output_dir: str = "/tmp") -> Dict[str, str]:
    """
    Liquid Clusteringåˆ†æžçµæžœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
    """
    import os
    import json
    from datetime import datetime
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    json_path = f"{output_dir}/liquid_clustering_analysis_{timestamp}.json"
    markdown_path = f"{output_dir}/liquid_clustering_analysis_{timestamp}.md"
    sql_path = f"{output_dir}/liquid_clustering_implementation_{timestamp}.sql"
    
    file_paths = {}
    
    try:
        # 1. JSONå½¢å¼ã§ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        # setåž‹ã‚’liståž‹ã«å¤‰æ›ã—ã¦JSON serializable ã«ã™ã‚‹
        json_data = convert_sets_to_lists(clustering_analysis)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        file_paths['json'] = json_path
        print(f"âœ… Saved detailed data in JSON format: {json_path}")
        
        # 2. Save analysis report in Markdown format
        markdown_content = generate_liquid_clustering_markdown_report(clustering_analysis)
        
        with open(markdown_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        file_paths['markdown'] = markdown_path
        print(f"âœ… Saved analysis report in Markdown format: {markdown_path}")
        
        # 3. Generate SQL implementation examples file
        sql_content = generate_liquid_clustering_sql_implementations(clustering_analysis)
        
        with open(sql_path, 'w', encoding='utf-8') as f:
            f.write(sql_content)
        
        file_paths['sql'] = sql_path
        print(f"âœ… Saved SQL implementation examples: {sql_path}")
        
        return file_paths
        
    except Exception as e:
        error_msg = f"ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}"
        print(f"âŒ {error_msg}")
        return {"error": error_msg}

def generate_liquid_clustering_markdown_report(clustering_analysis: Dict[str, Any]) -> str:
    """
    Liquid Clusteringåˆ†æžçµæžœã®Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    """
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # åŸºæœ¬æƒ…å ±ã®å–å¾—
    summary = clustering_analysis.get('summary', {})
    performance_context = clustering_analysis.get('performance_context', {})
    extracted_data = clustering_analysis.get('extracted_data', {})
    llm_analysis = clustering_analysis.get('llm_analysis', '')
    
    markdown_content = f"""# Liquid Clustering Analysis Report

**Generated Date**: {timestamp}  
**Analysis Method**: {summary.get('analysis_method', 'Unknown')}  
**LLM Provider**: {summary.get('llm_provider', 'Unknown')}

## ðŸ“Š Performance Overview

| Item | Value |
|------|-----|
| Execution Time | {performance_context.get('total_time_sec', 0):.1f} seconds |
| Data Read | {performance_context.get('read_gb', 0):.2f}GB |
| Output Rows | {performance_context.get('rows_produced', 0):,} rows |
| Read Rows | {performance_context.get('rows_read', 0):,} rows |
| Filter Rate | {performance_context.get('data_selectivity', 0):.4f} |

## ðŸ” Extracted Metadata

### Filter Conditions ({summary.get('total_filter_columns', 0)} items)
"""
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã®è©³ç´°
    filter_columns = extracted_data.get('filter_columns', [])
    for i, filter_item in enumerate(filter_columns[:10], 1):  # æœ€å¤§10å€‹ã¾ã§è¡¨ç¤º
        markdown_content += f"{i}. `{filter_item.get('expression', '')}` (ãƒŽãƒ¼ãƒ‰: {filter_item.get('node_name', '')})\n"
    
    if len(filter_columns) > 10:
        markdown_content += f"... ä»– {len(filter_columns) - 10}å€‹\n"
    
    markdown_content += f"""
### JOINæ¡ä»¶ ({summary.get('total_join_columns', 0)}å€‹)
"""
    
    # JOINæ¡ä»¶ã®è©³ç´°
    join_columns = extracted_data.get('join_columns', [])
    for i, join_item in enumerate(join_columns[:10], 1):
        markdown_content += f"{i}. `{join_item.get('expression', '')}` ({join_item.get('key_type', '')})\n"
    
    if len(join_columns) > 10:
        markdown_content += f"... ä»– {len(join_columns) - 10}å€‹\n"
    
    markdown_content += f"""
### GROUP BYæ¡ä»¶ ({summary.get('total_groupby_columns', 0)}å€‹)
"""
    
    # GROUP BYæ¡ä»¶ã®è©³ç´°
    groupby_columns = extracted_data.get('groupby_columns', [])
    for i, groupby_item in enumerate(groupby_columns[:10], 1):
        markdown_content += f"{i}. `{groupby_item.get('expression', '')}` (ãƒŽãƒ¼ãƒ‰: {groupby_item.get('node_name', '')})\n"
    
    if len(groupby_columns) > 10:
        markdown_content += f"... ä»– {len(groupby_columns) - 10}å€‹\n"
    
    markdown_content += f"""
### é›†ç´„é–¢æ•° ({summary.get('total_aggregate_columns', 0)}å€‹)
"""
    
    # é›†ç´„é–¢æ•°ã®è©³ç´°
    aggregate_columns = extracted_data.get('aggregate_columns', [])
    for i, agg_item in enumerate(aggregate_columns[:10], 1):
        markdown_content += f"{i}. `{agg_item.get('expression', '')}` (ãƒŽãƒ¼ãƒ‰: {agg_item.get('node_name', '')})\n"
    
    if len(aggregate_columns) > 10:
        markdown_content += f"... ä»– {len(aggregate_columns) - 10}å€‹\n"
    
    markdown_content += f"""
## ðŸ·ï¸ è­˜åˆ¥ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ« ({summary.get('tables_identified', 0)}å€‹)

"""
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®è©³ç´°ï¼ˆç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã‚’å«ã‚€ï¼‰
    table_info = extracted_data.get('table_info', {})
    for table_name, table_details in table_info.items():
        current_keys = table_details.get('current_clustering_keys', [])
        current_keys_str = ', '.join(current_keys) if current_keys else 'è¨­å®šãªã—'
        markdown_content += f"- **{table_name}** (ãƒŽãƒ¼ãƒ‰: {table_details.get('node_name', '')})\n"
        markdown_content += f"  - ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: `{current_keys_str}`\n"
    
    markdown_content += f"""
## ðŸ”Ž ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰åˆ†æž ({summary.get('scan_nodes_count', 0)}å€‹)

"""
    
    # ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰ã®è©³ç´°
    scan_nodes = extracted_data.get('scan_nodes', [])
    for scan in scan_nodes:
        efficiency = scan.get('rows', 0) / max(scan.get('duration_ms', 1), 1)
        markdown_content += f"- **{scan.get('name', '')}**: {scan.get('rows', 0):,}è¡Œ, {scan.get('duration_ms', 0):,}ms, åŠ¹çŽ‡={efficiency:.1f}è¡Œ/ms\n"
    
    markdown_content += f"""
## ðŸ¤– LLMåˆ†æžçµæžœ

{llm_analysis}

## ðŸ“‹ åˆ†æžã‚µãƒžãƒªãƒ¼

- **åˆ†æžå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°**: {summary.get('tables_identified', 0)}
- **ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶æ•°**: {summary.get('total_filter_columns', 0)}
- **JOINæ¡ä»¶æ•°**: {summary.get('total_join_columns', 0)}
- **GROUP BYæ¡ä»¶æ•°**: {summary.get('total_groupby_columns', 0)}
- **é›†ç´„é–¢æ•°æ•°**: {summary.get('total_aggregate_columns', 0)}
- **ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰æ•°**: {summary.get('scan_nodes_count', 0)}

---
*Report generation time: {timestamp}*
"""
    
    return markdown_content

def generate_liquid_clustering_sql_implementations(clustering_analysis: Dict[str, Any]) -> str:
    """
    Generate SQL examples for Liquid Clustering implementation
    """
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # åŸºæœ¬æƒ…å ±ã®å–å¾—
    extracted_data = clustering_analysis.get('extracted_data', {})
    table_info = extracted_data.get('table_info', {})
    
    sql_content = f"""-- =====================================================
-- Liquid Clustering å®Ÿè£…SQLä¾‹
-- ç”Ÿæˆæ—¥æ™‚: {timestamp}
-- =====================================================

-- ã€é‡è¦ã€‘
-- ä»¥ä¸‹ã®SQLä¾‹ã¯åˆ†æžçµæžœã«åŸºã¥ãæŽ¨å¥¨äº‹é …ã§ã™ã€‚
-- å®Ÿéš›ã®å®Ÿè£…å‰ã«ã€ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã‚„ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

"""
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã®SQLå®Ÿè£…ä¾‹ã‚’ç”Ÿæˆ
    for table_name, table_details in table_info.items():
        # ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã‚’å–å¾—
        current_keys = table_details.get('current_clustering_keys', [])
        current_keys_str = ', '.join(current_keys) if current_keys else 'è¨­å®šãªã—'
        
        sql_content += f"""
-- =====================================================
-- ãƒ†ãƒ¼ãƒ–ãƒ«: {table_name}
-- ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: {current_keys_str}
-- =====================================================

-- æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã«Liquid Clusteringã‚’é©ç”¨ã™ã‚‹å ´åˆ:
-- ALTER TABLE {table_name} CLUSTER BY (column1, column2, column3, column4);

-- æ–°è¦ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆæ™‚ã«Liquid Clusteringã‚’è¨­å®šã™ã‚‹å ´åˆ:
-- CREATE TABLE {table_name}_clustered
-- CLUSTER BY (column1, column2, column3, column4)
-- AS SELECT * FROM {table_name};

-- Delta Live Tablesã§ã®è¨­å®šä¾‹:
-- @dlt.table(
--   cluster_by=["column1", "column2", "column3", "column4"]
-- )
-- def {table_name.split('.')[-1]}_clustered():
--   return spark.table("{table_name}")

-- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çŠ¶æ³ã®ç¢ºèª:
-- DESCRIBE DETAIL {table_name};

-- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµ±è¨ˆã®ç¢ºèª:
-- ANALYZE TABLE {table_name} COMPUTE STATISTICS FOR ALL COLUMNS;

"""
    
    sql_content += f"""
-- =====================================================
-- ä¸€èˆ¬çš„ãªLiquid Clusteringå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³
-- =====================================================

-- ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é »åº¦ã®é«˜ã„ã‚«ãƒ©ãƒ ã‚’å„ªå…ˆ
-- æŽ¨å¥¨é †åº: 1) ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚«ãƒ©ãƒ  2) JOINæ¡ä»¶ã‚«ãƒ©ãƒ  3) GROUP BYã‚«ãƒ©ãƒ 

-- ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã‚’è€ƒæ…®ã—ãŸé †åº
-- ä½Žã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ â†’ é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã®é †ã§é…ç½®

-- ãƒ‘ã‚¿ãƒ¼ãƒ³3: ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãé…ç½®
-- ã‚ˆãä¸€ç·’ã«ä½¿ç”¨ã•ã‚Œã‚‹ã‚«ãƒ©ãƒ ã‚’è¿‘ã„ä½ç½®ã«é…ç½®

-- =====================================================
-- å®Ÿè£…å¾Œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¤œè¨¼SQL
-- =====================================================

-- 1. ã‚¯ã‚¨ãƒªå®Ÿè¡Œè¨ˆç”»ã®ç¢ºèª
-- EXPLAIN SELECT ... FROM table_name WHERE ...;

-- 2. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒƒãƒ—çµ±è¨ˆã®ç¢ºèª
-- SELECT * FROM table_name WHERE filter_column = 'value';
-- -- SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒƒãƒ—æ•°ã‚’ç¢ºèª

-- 3. ãƒ‡ãƒ¼ã‚¿é…ç½®ã®ç¢ºèª
-- SELECT 
--   file_path,
--   count(*) as row_count,
--   min(cluster_column1) as min_val,
--   max(cluster_column1) as max_val
-- FROM table_name
-- GROUP BY file_path
-- ORDER BY file_path;

-- =====================================================
-- æ³¨æ„äº‹é …
-- =====================================================

-- 1. Liquid Clusteringã¯æœ€å¤§4ã‚«ãƒ©ãƒ ã¾ã§æŒ‡å®šå¯èƒ½
-- 2. ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã¨ã¯ä½µç”¨ä¸å¯
-- 3. æ—¢å­˜ã®ZORDER BYã¯è‡ªå‹•çš„ã«ç„¡åŠ¹åŒ–ã•ã‚Œã‚‹
-- 4. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®åŠ¹æžœã¯æ™‚é–“ã¨ã¨ã‚‚ã«å‘ä¸Šã™ã‚‹ï¼ˆOPTIMIZEå®Ÿè¡Œã§æœ€é©åŒ–ï¼‰
-- 5. å®šæœŸçš„ãªOPTIMIZEå®Ÿè¡Œã‚’æŽ¨å¥¨
-- 6. **é‡è¦**: ã‚«ãƒ©ãƒ ã®æŒ‡å®šé †åºã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã«å½±éŸ¿ã—ã¾ã›ã‚“
--    * CLUSTER BY (col1, col2, col3) ã¨ CLUSTER BY (col3, col1, col2) ã¯åŒç­‰
--    * å¾“æ¥ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã‚„Z-ORDERã¨ã¯ç•°ãªã‚‹é‡è¦ãªç‰¹æ€§

-- OPTIMIZEå®Ÿè¡Œä¾‹:
-- OPTIMIZE table_name;

-- =====================================================
-- ç”Ÿæˆæƒ…å ±
-- =====================================================
-- ç”Ÿæˆæ—¥æ™‚: {timestamp}
-- åˆ†æžå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(table_info)}
-- åŸºã¥ã„ãŸåˆ†æž: LLMã«ã‚ˆã‚‹Liquid Clusteringåˆ†æž
"""
    
    return sql_content

print("âœ… Function definition completed: analyze_liquid_clustering_opportunities, save_liquid_clustering_analysis")

# COMMAND ----------

def translate_explain_summary_to_english(explain_content: str) -> str:
    """
    EXPLAINè¦ç´„ãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¥æœ¬èªžéƒ¨åˆ†ã‚’è‹±èªžã«ç¿»è¨³
    
    Args:
        explain_content: EXPLAINè¦ç´„ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹
    
    Returns:
        str: è‹±èªžç‰ˆEXPLAINè¦ç´„
    """
    # OUTPUT_LANGUAGEãŒ'en'ã®å ´åˆã¯ç¿»è¨³ã‚’ã‚¹ã‚­ãƒƒãƒ—
    output_language = globals().get('OUTPUT_LANGUAGE', 'ja')
    if output_language == 'en':
        return explain_content
    # æ—¥æœ¬èªžã‹ã‚‰è‹±èªžã¸ã®ç¿»è¨³ãƒžãƒƒãƒ”ãƒ³ã‚°
    translation_map = {
        # ãƒ˜ãƒƒãƒ€ãƒ¼éƒ¨åˆ†
        "# EXPLAIN + EXPLAIN COSTè¦ç´„çµæžœ (optimized)": "# EXPLAIN + EXPLAIN COST Summary Results (optimized)",
        "## ðŸ“Š åŸºæœ¬æƒ…å ±": "## ðŸ“Š Basic Information", 
        "ç”Ÿæˆæ—¥æ™‚": "Generated",
        "ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—": "Query Type",
        "å…ƒã‚µã‚¤ã‚º": "Original Size",
        "è¦ç´„å¾Œã‚µã‚¤ã‚º": "Summary Size",
        "åœ§ç¸®çŽ‡": "Compression Ratio",
        "æ–‡å­—": "characters",
        
        # LLMè¦ç´„çµæžœ
        "## ðŸ§  LLMè¦ç´„çµæžœ": "## ðŸ§  LLM Summary Results",
        "# Databricks SQLã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åˆ†æž": "# Databricks SQL Query Performance Analysis",
        "## ðŸ“Š Physical Planè¦ç´„": "## ðŸ“Š Physical Plan Summary",
        "### ä¸»è¦ãªå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—": "### Key Processing Steps",
        "è¤‡æ•°ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—": "Data retrieval from multiple tables",
        "ã‚µãƒ–ã‚¯ã‚¨ãƒªå®Ÿè¡Œ": "Subquery execution",
        "å¹³å‡å£²ä¸Šã‚’è¨ˆç®—ã™ã‚‹ã‚µãƒ–ã‚¯ã‚¨ãƒª": "Subquery calculating average sales",
        "ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°": "Filtering",
        "å¹³å‡å£²ä¸Šã‚’è¶…ãˆã‚‹å•†å“ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°": "Filtering products exceeding average sales",
        "é›†è¨ˆå‡¦ç†": "Aggregation processing",
        "ãƒ–ãƒ©ãƒ³ãƒ‰ã€ã‚¯ãƒ©ã‚¹ã€ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®å£²ä¸Šé›†è¨ˆ": "Sales aggregation by brand, class, category",
        "JOINå‡¦ç†": "JOIN processing",
        "è¤‡æ•°ã®JOINæ“ä½œ": "Multiple JOIN operations",
        "ãŒå¤šç”¨": "is frequently used",
        "ã‚½ãƒ¼ãƒˆ": "Sorting",
        "ã§ã®ã‚½ãƒ¼ãƒˆ": "sorting by",
        "æœ€çµ‚çµæžœã‚’": "Final results to",
        "è¡Œã«åˆ¶é™": "rows limit",
        
        # JOINæ–¹å¼ã¨ãƒ‡ãƒ¼ã‚¿ç§»å‹•
        "### JOINæ–¹å¼ã¨ãƒ‡ãƒ¼ã‚¿ç§»å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³": "### JOIN Methods and Data Movement Patterns",
        "ä¸»è¦JOINæ–¹å¼": "Primary JOIN Method",
        "ãƒ‡ãƒ¼ã‚¿ç§»å‹•": "Data Movement",
        "ã«ã‚ˆã‚‹åŠ¹çŽ‡çš„ãªãƒ‡ãƒ¼ã‚¿ç§»å‹•": "for efficient data movement",
        "ã«ã‚ˆã‚‹é›†ç´„å‡¦ç†": "for aggregation processing",
        "ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿åˆ†æ•£": "for data distribution",
        "ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³": "partitions",
        
        # Photonåˆ©ç”¨çŠ¶æ³
        "### Photonåˆ©ç”¨çŠ¶æ³": "### Photon Usage Status",
        "é«˜åº¦ãªPhotonæ´»ç”¨": "Advanced Photon utilization",
        "ãªã©å¤šæ•°ã®Photonæœ€é©åŒ–æ¼”ç®—å­ã‚’ä½¿ç”¨": "and many other Photon optimization operators in use",
        "å®Ÿè¡Œæ™‚ã®æœ€é©åŒ–ãŒæœ‰åŠ¹": "Runtime optimization enabled",
        
        # çµ±è¨ˆæƒ…å ±ã‚µãƒžãƒªãƒ¼
        "## ðŸ’° çµ±è¨ˆæƒ…å ±ã‚µãƒžãƒªãƒ¼": "## ðŸ’° Statistics Summary",
        "### ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºã¨è¡Œæ•°": "### Table Size and Row Count",
        "ç´„": "approximately",
        "å„„è¡Œ": "billion rows",
        "æœ€çµ‚çµæžœã‚»ãƒƒãƒˆ": "Final result set",
        "é©ç”¨å¾Œ": "after application",
        "ä¸­é–“çµæžœ": "Intermediate results",
        "ä¸‡è¡Œ": "thousand rows",
        "ã‚½ãƒ¼ãƒˆå‰": "before sorting",
        
        # JOINé¸æŠžçŽ‡ã¨ãƒ•ã‚£ãƒ«ã‚¿åŠ¹çŽ‡
        "### JOINé¸æŠžçŽ‡ã¨ãƒ•ã‚£ãƒ«ã‚¿åŠ¹çŽ‡": "### JOIN Selectivity and Filter Efficiency",
        "ãƒ•ã‚£ãƒ«ã‚¿": "filter",
        "å¹´åº¦æ¡ä»¶": "Year condition",
        "ã«ã‚ˆã‚Šã€": "resulted in",
        "è¡Œã«çµžã‚Šè¾¼ã¿": "rows filtered",
        "é«˜åŠ¹çŽ‡": "high efficiency",
        "ã‚µãƒ–ã‚¯ã‚¨ãƒªçµæžœ": "Subquery result",
        "å¹³å‡å£²ä¸Šè¨ˆç®—ã®ã‚µãƒ–ã‚¯ã‚¨ãƒªã¯å˜ä¸€è¡Œã‚’è¿”å´": "Average sales calculation subquery returns single row",
        "ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªãƒ•ã‚£ãƒ«ã‚¿": "Main query filter",
        "å¹³å‡å£²ä¸Šã‚’è¶…ãˆã‚‹å•†å“ã«çµžã‚Šè¾¼ã¿": "Filtered to products exceeding average sales",
        "è¡Œã«å‰Šæ¸›": "rows reduced to",
        
        # ã‚«ãƒ©ãƒ çµ±è¨ˆ
        "### ã‚«ãƒ©ãƒ çµ±è¨ˆ": "### Column Statistics",
        "ç¨®é¡žã®ç•°ãªã‚‹å€¤": "distinct values",
        "ã®ç¯„å›²": "range",
        "æ•°é‡": "quantity",
        
        # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†æ•£çŠ¶æ³
        "### ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†æ•£çŠ¶æ³": "### Partition Distribution Status",
        "ãƒãƒƒã‚·ãƒ¥ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°": "Hash partitioning",
        "ã«åŸºã¥ã": "based on",
        "ã‚·ãƒ³ã‚°ãƒ«ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³": "Single partition",
        "é›†ç´„å‡¦ç†ã‚„æœ€çµ‚çµæžœã®åŽé›†ã«ä½¿ç”¨": "Used for aggregation processing and final result collection",
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åˆ†æž
        "## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åˆ†æž": "## âš¡ Performance Analysis",
        "### å®Ÿè¡Œã‚³ã‚¹ãƒˆã®å†…è¨³": "### Execution Cost Breakdown",
        "æœ€ã‚‚ã‚³ã‚¹ãƒˆãŒé«˜ã„æ“ä½œ": "Most expensive operation",
        "ã‹ã‚‰ã®ã‚¹ã‚­ãƒ£ãƒ³": "table scan",
        "ã‚µãƒ–ã‚¯ã‚¨ãƒªã‚³ã‚¹ãƒˆ": "Subquery cost",
        "ã‹ã‚‰ã®UNION ALLå‡¦ç†": "UNION ALL processing from",
        "ã«ã‚ˆã‚‹é›†è¨ˆã‚³ã‚¹ãƒˆ": "aggregation cost by",
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æž
        "### ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚Šãã†ãªæ“ä½œ": "### Operations Likely to Become Bottlenecks",
        "å¤§è¦æ¨¡ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³": "Large table scan",
        "ã®ã‚¹ã‚­ãƒ£ãƒ³ãŒæœ€å¤§ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯": "scan is the biggest bottleneck",
        "è¤‡æ•°ãƒ†ãƒ¼ãƒ–ãƒ«UNION": "Multiple table UNION",
        "ã§ã®3ã¤ã®è²©å£²ãƒ†ãƒ¼ãƒ–ãƒ«": "3 sales tables in",
        "ã®çµ±åˆ": "integration",
        "ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ": "Shuffle operations",
        "ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿å†åˆ†æ•£": "data redistribution by",
        
        # æœ€é©åŒ–ã®ä½™åœ°
        "### æœ€é©åŒ–ã®ä½™åœ°ãŒã‚ã‚‹ç®‡æ‰€": "### Areas with Optimization Potential",
        "ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°": "Partition pruning",
        "ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯åŠ¹æžœçš„ã ãŒã€ã•ã‚‰ã«": "filtering is effective, but further",
        "ã®è²©å£²ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æœ€é©åŒ–ãŒå¯èƒ½": "sales table partition optimization is possible",
        "JOINé †åº": "JOIN order",
        "ã®é †åºæœ€é©åŒ–": "order optimization",
        "ãƒ•ã‚£ãƒ«ã‚¿ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³": "Filter pushdown",
        "ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãŒã€ã•ã‚‰ã«æœ€é©åŒ–ã®ä½™åœ°ã‚ã‚Š": "is used, but further optimization potential exists",
        "ã‚«ãƒ©ãƒ é¸æŠž": "Column selection",
        "å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿ã‚’æ—©æœŸã«é¸æŠžã™ã‚‹ã“ã¨ã§ãƒ‡ãƒ¼ã‚¿ç§»å‹•é‡ã‚’å‰Šæ¸›å¯èƒ½": "Data movement can be reduced by early selection of only necessary columns",
        "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡": "Memory usage",
        "ã®ãƒ“ãƒ«ãƒ‰å´ã®ã‚µã‚¤ã‚ºæœ€é©åŒ–": "build-side size optimization for",
        
        # ç‰¹è¨˜äº‹é …
        "### ç‰¹è¨˜äº‹é …": "### Notable Points",
        "æ´»ç”¨": "utilization",
        "ã‚¯ã‚¨ãƒªå…¨ä½“ã§": "Throughout the query",
        "æœ€é©åŒ–ãŒåŠ¹æžœçš„ã«é©ç”¨ã•ã‚Œã¦ã„ã‚‹": "optimization is effectively applied",
        "çµ±è¨ˆæƒ…å ±": "Statistical information",
        "ãŒé©åˆ‡ã«åŽé›†ã•ã‚Œã¦ãŠã‚Šã€ã‚ªãƒ—ãƒ†ã‚£ãƒžã‚¤ã‚¶ã®åˆ¤æ–­ã«è²¢çŒ®": "is properly collected and contributes to optimizer decisions",
        "å‹•çš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°": "Dynamic filtering",
        "ãŒé©ç”¨ã•ã‚Œã€ä¸è¦ãªãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚’å›žé¿": "is applied to avoid unnecessary data reading",
        "ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–å®Ÿè¡Œ": "Adaptive execution",
        "ãŒæœ‰åŠ¹ã§ã€å®Ÿè¡Œæ™‚ã®æœ€é©åŒ–ãŒæœŸå¾…ã§ãã‚‹": "is enabled, runtime optimization can be expected",
        
        # çµè«–
        "ã“ã®ã‚¯ã‚¨ãƒªã¯è¤‡é›‘ãªJOINã¨é›†è¨ˆã‚’å«ã‚€ãŒ": "This query includes complex JOINs and aggregations, but",
        "ã®åŠ¹æžœçš„ãªä½¿ç”¨ã«ã‚ˆã‚Šã€æ¯”è¼ƒçš„åŠ¹çŽ‡çš„ã«å®Ÿè¡Œã•ã‚Œã‚‹ã¨äºˆæ¸¬ã•ã‚Œã¾ã™": "effective use is expected to execute relatively efficiently",
        "æœ€å¤§ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯å¤§è¦æ¨¡ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¹ã‚­ãƒ£ãƒ³ã¨ãƒ‡ãƒ¼ã‚¿ç§»å‹•ã«ã‚ã‚Šã¾ã™": "The biggest bottlenecks are large table scans and data movement",
        
        # çµ±è¨ˆæƒ…å ±æŠ½å‡º
        "## ðŸ’° çµ±è¨ˆæƒ…å ±æŠ½å‡º": "## ðŸ’° Statistics Extraction",
        "## ðŸ“Š çµ±è¨ˆæƒ…å ±ã‚µãƒžãƒªãƒ¼ï¼ˆç°¡æ½”ç‰ˆï¼‰": "## ðŸ“Š Statistics Summary (Concise Version)",
        "ç·çµ±è¨ˆé …ç›®æ•°": "Total statistics items",
        "å€‹": "items",
        "ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆ": "Table statistics", 
        "ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±": "Partition information",
        "### ðŸŽ¯ ä¸»è¦çµ±è¨ˆ": "### ðŸŽ¯ Key Statistics",
        "ðŸ“Š ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚º": "ðŸ“Š Table Size",
        "ðŸ’¡ è©³ç´°ãªçµ±è¨ˆæƒ…å ±ã¯": "ðŸ’¡ Detailed statistics available with",
        "ã§ç¢ºèªã§ãã¾ã™": "setting"
    }
    
    # ç¿»è¨³ã‚’é©ç”¨
    translated_content = explain_content
    for jp_text, en_text in translation_map.items():
        translated_content = translated_content.replace(jp_text, en_text)
    
    return translated_content

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸ¤– LLM-powered Bottleneck Analysis Function
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - LLM analysis formatting of extracted metrics
# MAGIC - Multiple LLM provider support (Databricks/OpenAI/Azure/Anthropic)
# MAGIC - Detailed analysis report generation in English
# MAGIC - Error handling and fallback analysis

# COMMAND ----------

def analyze_bottlenecks_with_llm(metrics: Dict[str, Any]) -> str:
    """
    Generate comprehensive performance analysis report
    Integrates information from Cell 33 (TOP10 processes), Cell 35 (Liquid Clustering), Cell 43 (integrated optimization execution)
    Also leverages EXPLAIN + EXPLAIN COST results for more precise analysis
    
    ðŸš¨ Important: Prevention of percentage calculation degradation
    - Using the sum of parallel execution node times as total time is strictly prohibited
    - Prioritize using overall_metrics.total_time_ms (wall-clock time)
    - Use maximum node time during fallback (not sum)
    """
    from datetime import datetime
    
    print("ðŸ“Š Generating comprehensive performance analysis report (EXPLAIN+EXPLAIN COST integration)...")
    
    # === EXPLAIN + EXPLAIN COSTçµæžœã®èª­ã¿è¾¼ã¿ ===
    explain_content = ""
    explain_cost_content = ""
    physical_plan = ""
    photon_explanation = ""
    cost_statistics = ""
    
    explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
    if explain_enabled.upper() == 'Y':
        import glob
        import os
        
        print("ðŸ” For bottleneck analysis: Searching EXPLAIN + EXPLAIN COST result files...")
        
        # æœ€æ–°ã®EXPLAINçµæžœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        explain_original_files = glob.glob("output_explain_original_*.txt")
        explain_optimized_files = glob.glob("output_explain_optimized_*.txt")
        explain_files = explain_original_files if explain_original_files else explain_optimized_files
        
        if explain_files:
            latest_explain_file = max(explain_files, key=os.path.getctime)
            try:
                with open(latest_explain_file, 'r', encoding='utf-8') as f:
                    explain_content = f.read()
                    print(f"âœ… Loaded EXPLAIN results for bottleneck analysis: {latest_explain_file}")
                
                # Physical Planã®æŠ½å‡º
                if "== Physical Plan ==" in explain_content:
                    physical_plan_start = explain_content.find("== Physical Plan ==")
                    physical_plan_end = explain_content.find("== Photon", physical_plan_start)
                    if physical_plan_end == -1:
                        physical_plan_end = len(explain_content)
                    physical_plan = explain_content[physical_plan_start:physical_plan_end].strip()
                
                # Photon Explanationã®æŠ½å‡º
                if "== Photon Explanation ==" in explain_content:
                    photon_start = explain_content.find("== Photon Explanation ==")
                    photon_explanation = explain_content[photon_start:].strip()
                    
            except Exception as e:
                print(f"âš ï¸ Failed to load EXPLAIN results for bottleneck analysis: {str(e)}")
        
        # ðŸš€ EXPLAIN COSTçµæžœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
        cached_cost_result = globals().get('cached_original_explain_cost_result')
        explain_cost_content = ""
        cost_statistics = ""
        
        if cached_cost_result and 'explain_cost_file' in cached_cost_result:
            try:
                with open(cached_cost_result['explain_cost_file'], 'r', encoding='utf-8') as f:
                    explain_cost_content = f.read()
                    print(f"ðŸ’¾ Using cached EXPLAIN COST results for bottleneck analysis: {cached_cost_result['explain_cost_file']}")
                
                # çµ±è¨ˆæƒ…å ±ã®æŠ½å‡º
                cost_statistics = extract_cost_statistics_from_explain_cost(explain_cost_content)
                print(f"ðŸ“Š Extracted statistics for bottleneck analysis: {len(cost_statistics)} characters")
                    
            except Exception as e:
                print(f"âš ï¸ Failed to load cached EXPLAIN COST results: {str(e)}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
                cached_cost_result = None
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯å¾“æ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
        if not cached_cost_result:
            cost_original_files = glob.glob("output_explain_cost_original_*.txt")
            cost_optimized_files = glob.glob("output_explain_cost_optimized_*.txt")
            cost_files = cost_original_files if cost_original_files else cost_optimized_files
            
            if cost_files:
                latest_cost_file = max(cost_files, key=os.path.getctime)
                try:
                    with open(latest_cost_file, 'r', encoding='utf-8') as f:
                        explain_cost_content = f.read()
                        print(f"ðŸ’° Loaded EXPLAIN COST results for bottleneck analysis: {latest_cost_file}")
                    
                    # çµ±è¨ˆæƒ…å ±ã®æŠ½å‡º
                    cost_statistics = extract_cost_statistics_from_explain_cost(explain_cost_content)
                    print(f"ðŸ“Š Extracted statistics for bottleneck analysis: {len(cost_statistics)} characters")
                        
                except Exception as e:
                    print(f"âš ï¸ Failed to load EXPLAIN COST results for bottleneck analysis: {str(e)}")
        
        if not explain_files and not cost_files:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ãƒã‚§ãƒƒã‚¯
            old_explain_files = glob.glob("output_explain_plan_*.txt")
            if old_explain_files:
                latest_explain_file = max(old_explain_files, key=os.path.getctime)
                try:
                    with open(latest_explain_file, 'r', encoding='utf-8') as f:
                        explain_content = f.read()
                        print(f"âœ… Loaded legacy format EXPLAIN results: {latest_explain_file}")
                        
                    if "== Physical Plan ==" in explain_content:
                        physical_plan_start = explain_content.find("== Physical Plan ==")
                        physical_plan_end = explain_content.find("== Photon", physical_plan_start)
                        if physical_plan_end == -1:
                            physical_plan_end = len(explain_content)
                        physical_plan = explain_content[physical_plan_start:physical_plan_end].strip()
                        
                    if "== Photon Explanation ==" in explain_content:
                        photon_start = explain_content.find("== Photon Explanation ==")
                        photon_explanation = explain_content[photon_start:].strip()
                except Exception as e:
                    print(f"âš ï¸ Failed to load legacy format EXPLAIN results: {str(e)}")
            else:
                print("âš ï¸ Bottleneck analysis: EXPLAINãƒ»EXPLAIN COST result files not found")
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ™‚åˆ»
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # === 1. åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å–å¾— ===
    overall_metrics = metrics.get('overall_metrics', {})
    bottleneck_indicators = metrics.get('bottleneck_indicators', {})
    
    total_time_sec = overall_metrics.get('total_time_ms', 0) / 1000
    read_gb = overall_metrics.get('read_bytes', 0) / 1024 / 1024 / 1024
    cache_hit_ratio = bottleneck_indicators.get('cache_hit_ratio', 0) * 100
    data_selectivity = bottleneck_indicators.get('data_selectivity', 0) * 100
    
    # Photonæƒ…å ±
    photon_enabled = overall_metrics.get('photon_enabled', False)
    photon_utilization = min(overall_metrics.get('photon_utilization_ratio', 0) * 100, 100.0)
    
    # ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«æƒ…å ±
    shuffle_count = bottleneck_indicators.get('shuffle_operations_count', 0)
    has_shuffle_bottleneck = bottleneck_indicators.get('has_shuffle_bottleneck', False)
    has_low_parallelism = bottleneck_indicators.get('has_low_parallelism', False)
    low_parallelism_count = bottleneck_indicators.get('low_parallelism_stages_count', 0)
    
    # ã‚¹ãƒ”ãƒ«æƒ…å ±
    has_spill = bottleneck_indicators.get('has_spill', False)
    spill_bytes = bottleneck_indicators.get('spill_bytes', 0)
    spill_gb = spill_bytes / 1024 / 1024 / 1024 if spill_bytes > 0 else 0
    
    # ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºæƒ…å ±
    has_skew = bottleneck_indicators.get('has_skew', False)
    has_aqe_shuffle_skew_warning = bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False)
    
    # === 2. ã‚»ãƒ«33: TOP10ãƒ—ãƒ­ã‚»ã‚¹åˆ†æžæƒ…å ±ã®å–å¾— ===
    # å…¨ãƒŽãƒ¼ãƒ‰ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ç”¨ï¼‰
    all_sorted_nodes = sorted(metrics['node_metrics'], 
                             key=lambda x: x['key_metrics'].get('durationMs', 0), 
                             reverse=True)
    
    # TOP5ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŠ½å‡ºç”¨
    sorted_nodes = all_sorted_nodes[:5]
    
    # ðŸš¨ é‡è¦: æ­£ã—ã„å…¨ä½“æ™‚é–“ã®è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
    # 1. overall_metrics.total_time_msã‚’å„ªå…ˆä½¿ç”¨ï¼ˆwall-clock timeï¼‰
    total_time_ms = overall_metrics.get('total_time_ms', 0)
    
    # ðŸš¨ ä¸¦åˆ—å®Ÿè¡Œå•é¡Œã®ä¿®æ­£: task_total_time_msã‚’å„ªå…ˆä½¿ç”¨
    # å€‹åˆ¥ãƒŽãƒ¼ãƒ‰æ™‚é–“ã¯ä¸¦åˆ—ã‚¿ã‚¹ã‚¯ã®ç´¯ç©æ™‚é–“ã®ãŸã‚ã€åŒã˜ãç´¯ç©æ™‚é–“ã§ã‚ã‚‹task_total_time_msã¨æ¯”è¼ƒ
    task_total_time_ms = overall_metrics.get('task_total_time_ms', 0)
    
    if task_total_time_ms > 0:
        total_time_ms = task_total_time_ms
        print(f"âœ… Debug: Parallel execution support - using task_total_time_ms: {total_time_ms:,} ms ({total_time_ms/3600000:.1f} hours)")
    elif total_time_ms <= 0:
        # execution_time_msã‚’æ¬¡ã®å„ªå…ˆåº¦ã§ä½¿ç”¨
        execution_time_ms = overall_metrics.get('execution_time_ms', 0)
        if execution_time_ms > 0:
            total_time_ms = execution_time_ms
            print(f"âš ï¸ Debug: task_total_time_ms unavailable, using execution_time_ms: {total_time_ms} ms")
        else:
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨ãƒŽãƒ¼ãƒ‰ã®åˆè¨ˆæ™‚é–“
            max_node_time = max([node['key_metrics'].get('durationMs', 0) for node in all_sorted_nodes], default=1)
            total_time_ms = int(max_node_time * 1.2)
            print(f"âš ï¸ Debug: Final fallback - using estimated time: {total_time_ms} ms")
    
    print(f"ðŸ“Š Debug: Total time used for percentage calculation: {total_time_ms:,} ms ({total_time_ms/1000:.1f} sec)")
    
    critical_processes = []
    for i, node in enumerate(sorted_nodes):
        duration_ms = node['key_metrics'].get('durationMs', 0)
        duration_sec = duration_ms / 1000
        
        # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ï¼ˆ100%ã‚’ä¸Šé™ã¨ã™ã‚‹ï¼‰
        percentage = min((duration_ms / max(total_time_ms, 1)) * 100, 100.0)
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®é‡è¦åº¦åˆ¤å®š
        severity = "CRITICAL" if duration_ms >= 10000 else "HIGH" if duration_ms >= 5000 else "MEDIUM"
        
        # æ„å‘³ã®ã‚ã‚‹ãƒŽãƒ¼ãƒ‰åã‚’å–å¾—
        node_name = get_meaningful_node_name(node, metrics)
        short_name = node_name[:80] + "..." if len(node_name) > 80 else node_name
        
        critical_processes.append({
            'rank': i + 1,
            'name': short_name,
            'duration_sec': duration_sec,
            'percentage': percentage,
            'severity': severity
        })
    
    # === 3. ã‚»ãƒ«35: Liquid Clusteringåˆ†æžæƒ…å ±ã®å–å¾— ===
    liquid_analysis = metrics.get('liquid_clustering_analysis', {})
    extracted_data = liquid_analysis.get('extracted_data', {})
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±
    table_info = extracted_data.get('table_info', {})
    identified_tables = list(table_info.keys())[:5]  # TOP5ãƒ†ãƒ¼ãƒ–ãƒ«
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ»JOINãƒ»GROUP BYæƒ…å ±
    filter_columns = extracted_data.get('filter_columns', [])[:10]
    join_columns = extracted_data.get('join_columns', [])[:10]
    groupby_columns = extracted_data.get('groupby_columns', [])[:10]
    
    # === 4. ã‚»ãƒ«43: çµ±åˆæœ€é©åŒ–å‡¦ç†ã§ã®è©³ç´°ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æžã®å–å¾— ===
    try:
        detailed_bottleneck = extract_detailed_bottleneck_analysis(metrics)
    except Exception as e:
        print(f"âš ï¸ Error in detailed bottleneck analysis: {e}")
        detailed_bottleneck = {
            'top_bottleneck_nodes': [],
            'performance_recommendations': []
        }
    
    # === 5. åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ ===
    
    report_lines = []
    
    # ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚µãƒžãƒªãƒ¼
    report_lines.append("# ðŸ“Š Databricks SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åŒ…æ‹¬åˆ†æžãƒ¬ãƒãƒ¼ãƒˆ")
    report_lines.append(f"**ç”Ÿæˆæ—¥æ™‚**: {timestamp}")
    report_lines.append("")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¦‚è¦
    report_lines.append("## 1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¦‚è¦")
    report_lines.append("")
    report_lines.append("### ä¸»è¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æŒ‡æ¨™")
    report_lines.append("")
    report_lines.append("| æŒ‡æ¨™ | å€¤ | è©•ä¾¡ |")
    report_lines.append("|------|-----|------|")
    report_lines.append(f"| Execution Time | {total_time_sec:.1f}s | {'âœ… Good' if total_time_sec < 60 else 'âš ï¸ Needs Improvement'} |")
    report_lines.append(f"| Data Read | {read_gb:.2f}GB | {'âœ… Good' if read_gb < 10 else 'âš ï¸ Large Volume'} |")
    report_lines.append(f"| Photon Enabled | {'Yes' if photon_enabled else 'No'} | {'âœ… Good' if photon_enabled else 'âŒ Not Enabled'} |")
    report_lines.append(f"| Cache Efficiency | {cache_hit_ratio:.1f}% | {'âœ… Good' if cache_hit_ratio > 80 else 'âš ï¸ Needs Improvement'} |")
    report_lines.append(f"| Filter Rate | {data_selectivity:.1f}% | {'âœ… Good' if data_selectivity > 50 else 'âš ï¸ Check Filter Conditions'} |")
    report_lines.append(f"| Shuffle Operations | {shuffle_count} times | {'âœ… Good' if shuffle_count < 5 else 'âš ï¸ Many'} |")
    report_lines.append(f"| Spill Occurred | {'Yes' if has_spill else 'No'} | {'âŒ Problem' if has_spill else 'âœ… Good'} |")
    
    # ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã®åˆ¤å®š
    if has_skew:
        skew_status = "Detected & handled by AQE"
        skew_evaluation = "ðŸ”§ AQE handled"
    elif has_aqe_shuffle_skew_warning:
        skew_status = "Potential skew possibility"
        skew_evaluation = "âš ï¸ Improvement needed"
    else:
        skew_status = "Not detected"
        skew_evaluation = "âœ… Good"
    
    report_lines.append(f"| Skew Detection | {skew_status} | {skew_evaluation} |")
    report_lines.append("")
    
    # ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æž
    report_lines.append("## 2. ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æž")
    report_lines.append("")
    
    # Photonåˆ†æž
    photon_status = "æœ‰åŠ¹" if photon_enabled else "ç„¡åŠ¹"
    photon_recommendation = ""
    if not photon_enabled:
        photon_recommendation = " â†’ **Photonæœ‰åŠ¹åŒ–ã‚’å¼·ãæŽ¨å¥¨**"
    elif photon_utilization < 50:
        photon_recommendation = " â†’ **Photonåˆ©ç”¨çŽ‡å‘ä¸ŠãŒå¿…è¦**"
    elif photon_utilization < 80:
        photon_recommendation = " â†’ **Photonè¨­å®šã®æœ€é©åŒ–ã‚’æŽ¨å¥¨**"
    else:
        photon_recommendation = " â†’ **æœ€é©åŒ–æ¸ˆã¿**"
    
    report_lines.append("### Photonã‚¨ãƒ³ã‚¸ãƒ³")
    report_lines.append(f"- **çŠ¶æ…‹**: {photon_status} (åˆ©ç”¨çŽ‡: {photon_utilization:.1f}%){photon_recommendation}")
    report_lines.append("")
    
    # ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«åˆ†æž
    report_lines.append("### ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«")
    shuffle_status = "âŒ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚ã‚Š" if has_shuffle_bottleneck else "âœ… è‰¯å¥½"
    parallelism_status = "âŒ ä½Žä¸¦åˆ—åº¦ã‚ã‚Š" if has_low_parallelism else "âœ… é©åˆ‡"
    
    report_lines.append(f"- **ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ**: {shuffle_count}å›ž ({shuffle_status})")
    report_lines.append(f"- **ä¸¦åˆ—åº¦**: {parallelism_status}")
    if has_low_parallelism:
        report_lines.append(f"  - ä½Žä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸: {low_parallelism_count}å€‹")
    report_lines.append("")
    
    # ã‚¹ãƒ”ãƒ«åˆ†æž
    report_lines.append("### ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³")
    if has_spill:
        report_lines.append(f"- **ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«**: âŒ ç™ºç”Ÿä¸­ ({spill_gb:.2f}GB)")
        report_lines.append("  - **å¯¾å¿œå¿…è¦**: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šã®è¦‹ç›´ã—ã€ã‚¯ã‚¨ãƒªæœ€é©åŒ–")
    else:
        report_lines.append("- **ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«**: âœ… ãªã—")
    report_lines.append("")
    
    # TOP5 Processing Time Bottlenecks
    report_lines.append("## 3. TOP5 Processing Time Bottlenecks")
    report_lines.append("")
    
    for process in critical_processes:
        severity_icon = "ðŸ”´" if process['severity'] == "CRITICAL" else "ðŸŸ " if process['severity'] == "HIGH" else "ðŸŸ¡"
        report_lines.append(f"### {process['rank']}. {severity_icon} {process['name']}")
        report_lines.append(f"   - **Execution Time**: {process['duration_sec']:.1f}s ({process['percentage']:.1f}% of total)")
        report_lines.append(f"   - **Severity**: {process['severity']}")
        report_lines.append("")
    
    # Liquid Clustering Recommendations
    report_lines.append("## 4. Liquid Clustering Recommendations")
    report_lines.append("")
    
    if identified_tables:
        report_lines.append("### å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«")
        for i, table_name in enumerate(identified_tables, 1):
            # ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã‚’å–å¾—
            table_details = table_info.get(table_name, {})
            current_keys = table_details.get('current_clustering_keys', [])
            current_keys_str = ', '.join(current_keys) if current_keys else 'è¨­å®šãªã—'
            
            report_lines.append(f"{i}. `{table_name}`")
            report_lines.append(f"   - ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: `{current_keys_str}`")
        report_lines.append("")
    
    if filter_columns or join_columns or groupby_columns:
        report_lines.append("### æŽ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼")
        
        if filter_columns:
            report_lines.append("**ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚«ãƒ©ãƒ  (é«˜å„ªå…ˆåº¦)**:")
            for i, col in enumerate(filter_columns[:5], 1):
                expression = col.get('expression', 'Unknown')
                report_lines.append(f"  {i}. `{expression}`")
            report_lines.append("")
        
        if join_columns:
            report_lines.append("**JOINæ¡ä»¶ã‚«ãƒ©ãƒ  (ä¸­å„ªå…ˆåº¦)**:")
            for i, col in enumerate(join_columns[:5], 1):
                expression = col.get('expression', 'Unknown')
                key_type = col.get('key_type', '')
                report_lines.append(f"  {i}. `{expression}` ({key_type})")
            report_lines.append("")
        
        if groupby_columns:
            report_lines.append("**GROUP BYæ¡ä»¶ã‚«ãƒ©ãƒ  (ä¸­å„ªå…ˆåº¦)**:")
            for i, col in enumerate(groupby_columns[:5], 1):
                expression = col.get('expression', 'Unknown')
                report_lines.append(f"  {i}. `{expression}`")
            report_lines.append("")
    
    # å®Ÿè£…SQLä¾‹
    if identified_tables:
        report_lines.append("### å®Ÿè£…SQLä¾‹")
        for table_name in identified_tables[:2]:  # TOP2ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿
            # ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã‚’å–å¾—
            table_details = table_info.get(table_name, {})
            current_keys = table_details.get('current_clustering_keys', [])
            current_keys_str = ', '.join(current_keys) if current_keys else 'è¨­å®šãªã—'
            
            report_lines.append(f"```sql")
            report_lines.append(f"-- {table_name}ãƒ†ãƒ¼ãƒ–ãƒ«ã«Liquid Clusteringã‚’é©ç”¨")
            report_lines.append(f"-- ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: {current_keys_str}")
            report_lines.append(f"ALTER TABLE {table_name}")
            report_lines.append(f"CLUSTER BY (column1, column2, column3, column4);")
            report_lines.append(f"```")
            report_lines.append("")
    
    # Optimization recommendation actions
    report_lines.append("## 5. Recommended Optimization Actions")
    report_lines.append("")
    
    # Priority-based recommendations
    high_priority_actions = []
    medium_priority_actions = []
    low_priority_actions = []
    
    # CRITICAL/HIGH priority actions
    if not photon_enabled:
        high_priority_actions.append("**Enable Photon Engine** - Expected up to 50% performance improvement")
    
    if has_spill:
        high_priority_actions.append(f"**Resolve Memory Spill** - Eliminate {spill_gb:.2f}GB spill")
    
    if has_shuffle_bottleneck:
        high_priority_actions.append("**Shuffle Optimization** - JOIN order and REPARTITION application")
    
    # MEDIUM actions
    if photon_enabled and photon_utilization < 80:
        medium_priority_actions.append("**Improve Photon Utilization** - Configuration optimization")
    
    if has_low_parallelism:
        medium_priority_actions.append("**Improve Parallelism** - Cluster configuration review")
    
    if cache_hit_ratio < 50:
        medium_priority_actions.append("**Improve Cache Efficiency** - Data access pattern optimization")
    
    # Liquid Clustering
    if identified_tables:
        medium_priority_actions.append("**Implement Liquid Clustering** - Clustering of key tables")
    
    # LOW actions
    if data_selectivity < 50:
        low_priority_actions.append("**WHERE Clause Optimization** - Improve filter efficiency")
    
    # Action output
    if high_priority_actions:
        report_lines.append("### ðŸš¨ Urgent Response (HIGH Priority)")
        for i, action in enumerate(high_priority_actions, 1):
            report_lines.append(f"{i}. {action}")
        report_lines.append("")
    
    if medium_priority_actions:
        report_lines.append("### âš ï¸ Important Improvements (MEDIUM Priority)")
        for i, action in enumerate(medium_priority_actions, 1):
            report_lines.append(f"{i}. {action}")
        report_lines.append("")
    
    if low_priority_actions:
        report_lines.append("### ðŸ“ Long-term Optimization (LOW Priority)")
        for i, action in enumerate(low_priority_actions, 1):
            report_lines.append(f"{i}. {action}")
        report_lines.append("")
    
    # Expected effects
    report_lines.append("## 6. Expected Performance Improvements")
    report_lines.append("")
    
    total_improvement_estimate = 0
    improvement_details = []
    
    if not photon_enabled:
        total_improvement_estimate += 40
        improvement_details.append("- **Photon Activation**: 30-50% execution time reduction")
    
    if has_spill:
        total_improvement_estimate += 25
        improvement_details.append(f"- **Spill Resolution**: 20-30% execution time reduction ({spill_gb:.2f}GB spill reduction)")
    
    if has_shuffle_bottleneck:
        total_improvement_estimate += 20
        improvement_details.append("- **Shuffle Optimization**: 15-25% execution time reduction")
    
    if identified_tables:
        total_improvement_estimate += 15
        improvement_details.append("- **Liquid Clustering**: 10-20% execution time reduction")
    
    # Set upper limit for improvement effects
    total_improvement_estimate = min(total_improvement_estimate, 80)
    
    if improvement_details:
        for detail in improvement_details:
            report_lines.append(detail)
        report_lines.append("")
        report_lines.append(f"**Overall Improvement Estimate**: Up to {total_improvement_estimate}% execution time reduction")
    else:
        report_lines.append("Current performance is relatively good. Fine-tuning optimizations can expect 5-10% improvement.")
    
    # === Detailed analysis based on EXPLAIN + EXPLAIN COST results ===
    if explain_enabled.upper() == 'Y' and (physical_plan or cost_statistics):
        report_lines.append("")
        report_lines.append("## 6. EXPLAIN + EXPLAIN COST Detailed Analysis")
        report_lines.append("")
        
        if physical_plan:
            report_lines.append("### ðŸ” Physical Plan Analysis")
            report_lines.append("")
            
            # Extract important information from Physical Plan
            plan_analysis = []
            if "Exchange" in physical_plan:
                plan_analysis.append("- **Shuffle Operation Detected**: Potential data transfer bottleneck")
            if "BroadcastExchange" in physical_plan:
                plan_analysis.append("- **BROADCAST JOIN Applied**: Efficient distribution of small tables")
            if "HashAggregate" in physical_plan:
                plan_analysis.append("- **Hash Aggregation Processing**: Memory efficiency optimization is important")
            if "FileScan" in physical_plan:
                plan_analysis.append("- **File Scan Operation**: Check I/O efficiency and filter pushdown")
            if "SortMergeJoin" in physical_plan:
                plan_analysis.append("- **Sort Merge JOIN**: Large table joins, consider BROADCAST application")
            
            if plan_analysis:
                for analysis in plan_analysis:
                    report_lines.append(analysis)
            else:
                report_lines.append("- Physical Plan detailed information is available")
            report_lines.append("")
        
        if photon_explanation:
            report_lines.append("### ðŸš€ Photon Explanation Analysis")
            report_lines.append("")
            
            photon_analysis = []
            if "photon" in photon_explanation.lower():
                photon_analysis.append("- **Photon Processing Information**: Vectorized processing optimization details")
            if "unsupported" in photon_explanation.lower():
                photon_analysis.append("- **Unsupported Function Detected**: Opportunity to improve Photon utilization")
            if "compiled" in photon_explanation.lower():
                photon_analysis.append("- **Compilation Processing**: Runtime optimization application status")
            
            if photon_analysis:
                for analysis in photon_analysis:
                    report_lines.append(analysis)
            else:
                report_lines.append("- Photon execution detailed information is available")
            report_lines.append("")
        
        if cost_statistics:
            report_lines.append("### ðŸ’° EXPLAIN COST Statistical Analysis")
            report_lines.append("")
            
            # Extract important information from EXPLAIN COST statistics
            cost_analysis = []
            if "ã‚µã‚¤ã‚ºæƒ…å ±" in cost_statistics:
                cost_analysis.append("- **Table Size Statistics**: Improved BROADCAST judgment accuracy with accurate size information")
            if "è¡Œæ•°æƒ…å ±" in cost_statistics:
                cost_analysis.append("- **Row Count Statistics**: Partition number optimization and memory usage prediction")
            if "é¸æŠžçŽ‡æƒ…å ±" in cost_statistics:
                cost_analysis.append("- **Selectivity Statistics**: Filter efficiency optimization and WHERE condition order adjustment")
            if "ã‚³ã‚¹ãƒˆæƒ…å ±" in cost_statistics:
                cost_analysis.append("- **Cost Estimation**: JOIN strategy and access path selection optimization")
            if "ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±" in cost_statistics:
                cost_analysis.append("- **Partition Statistics**: Data distribution optimization and skew countermeasures")
            
            if cost_analysis:
                for analysis in cost_analysis:
                    report_lines.append(analysis)
                report_lines.append("")
                report_lines.append("**Benefits of Statistics-Based Optimization**:")
                report_lines.append("- Optimization based on actual statistics rather than guesswork")
                report_lines.append("- Proactive bottleneck prediction and spill avoidance")
                report_lines.append("- Optimal strategy selection through accurate cost estimation")
            else:
                report_lines.append("- EXPLAIN COST statistical information is available")
            report_lines.append("")
    elif explain_enabled.upper() == 'Y':
        report_lines.append("")
        report_lines.append("## 6. EXPLAIN Analysis")
        report_lines.append("")
        report_lines.append("âš ï¸ EXPLAINãƒ»EXPLAIN COST result files not found")
        report_lines.append("Statistics-based detailed analysis requires prior EXPLAIN execution")
        report_lines.append("")
    
    report_lines.append("")
    report_lines.append("---")
    report_lines.append(f"*Report generated: {timestamp} | Analysis engine: Databricks SQL Profiler + EXPLAIN integration*")
    
    print("âœ… Comprehensive performance analysis report (EXPLAIN+EXPLAIN COST integration) completed")
    
    return "\n".join(report_lines)


def _call_databricks_llm(prompt: str) -> str:
    """Call Databricks Model Serving API"""
    try:
        # Databricksãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—
        try:
            token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
        except Exception:
            token = os.environ.get('DATABRICKS_TOKEN')
            if not token:
                return "âŒ Failed to obtain Databricks token. Please set the environment variable DATABRICKS_TOKEN."
        
        # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹URLã®å–å¾—
        try:
            workspace_url = spark.conf.get("spark.databricks.workspaceUrl")
        except Exception:
            workspace_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get("browserHostName").get()
        
        config = LLM_CONFIG["databricks"]
        endpoint_url = f"https://{workspace_url}/serving-endpoints/{config['endpoint_name']}/invocations"
        
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        # æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ãªå ´åˆã¯è¿½åŠ 
        if config.get("thinking_enabled", False):
            payload["thinking"] = {
                "type": "enabled",
                "budget_tokens": config.get("thinking_budget_tokens", 65536)
            }
        
        # ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ï¼ˆSQLæœ€é©åŒ–ç”¨ã«å¢—å¼·ï¼‰
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    print(f"ðŸ”„ Retrying... (attempt {attempt + 1}/{max_retries})")
                
                response = requests.post(endpoint_url, headers=headers, json=payload, timeout=300)
                
                if response.status_code == 200:
                    result = response.json()
                    analysis_text = result.get('choices', [{}])[0].get('message', {}).get('content', '')
                    print("âœ… Bottleneck analysis completed")
                    return analysis_text
                else:
                    error_msg = f"API Error: Status code {response.status_code}"
                    if response.status_code == 400:
                        # 400ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯è©³ç´°ãªè§£æ±ºç­–ã‚’æä¾›
                        error_detail = response.text
                        if "maximum tokens" in error_detail.lower():
                            if attempt == max_retries - 1:
                                detailed_error = f"""âŒ {error_msg}

ðŸ”§ Token limit error solutions:
1. Reduce LLM_CONFIG["databricks"]["max_tokens"] to 65536 (64K)
2. Retry with simpler query
3. Perform manual SQL optimization
4. Split query and optimize incrementally

ðŸ’¡ Recommended settings:
LLM_CONFIG["databricks"]["max_tokens"] = 65536
LLM_CONFIG["databricks"]["thinking_budget_tokens"] = 32768

Detailed error: {error_detail}"""
                                print(detailed_error)
                                return detailed_error
                            else:
                                print(f"âš ï¸ {error_msg} (Token limit) - Retrying...")
                                continue
                    
                    if attempt == max_retries - 1:
                        print(f"âŒ {error_msg}\nResponse: {response.text}")
                        return f"{error_msg}\nResponse: {response.text}"
                    else:
                        print(f"âš ï¸ {error_msg} - Retrying...")
                        continue
                        
            except requests.exceptions.Timeout:
                if attempt == max_retries - 1:
                    timeout_msg = f"""â° Timeout Error: Databricks endpoint response did not complete within 300 seconds.

ðŸ”§ Solutions:
1. Check LLM endpoint operational status
2. Reduce prompt size
3. Use a higher performance model
4. Execute SQL optimization manually

ðŸ’¡ Recommended Actions:
- Check query complexity
- Scale up Databricks Model Serving endpoint
- Test execution with simpler queries"""
                    print(f"âŒ {timeout_msg}")
                    return timeout_msg
                else:
                    print(f"â° Timeout occurred (300 seconds) - Retrying... (attempt {attempt + 1}/{max_retries})")
                    continue
                    
    except Exception as e:
        return f"Databricks API call error: {str(e)}"

def _call_openai_llm(prompt: str) -> str:
    """Call OpenAI API"""
    try:
        config = LLM_CONFIG["openai"]
        api_key = config["api_key"] or os.environ.get('OPENAI_API_KEY')
        
        if not api_key:
            return "âŒ OpenAI API key is not configured. Please set LLM_CONFIG['openai']['api_key'] or environment variable OPENAI_API_KEY."
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": config["model"],
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        response = requests.post("https://api.openai.com/v1/chat/completions", 
                               headers=headers, json=payload, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result['choices'][0]['message']['content']
            print("âœ… OpenAI analysis completed")
            return analysis_text
        else:
            return f"OpenAI API Error: Status code {response.status_code}\n{response.text}"
            
    except Exception as e:
        return f"OpenAI API call error: {str(e)}"

def _call_azure_openai_llm(prompt: str) -> str:
    """Call Azure OpenAI API"""
    try:
        config = LLM_CONFIG["azure_openai"]
        api_key = config["api_key"] or os.environ.get('AZURE_OPENAI_API_KEY')
        
        if not api_key or not config["endpoint"] or not config["deployment_name"]:
            return "âŒ Azure OpenAI configuration is incomplete. Please set api_key, endpoint, and deployment_name."
        
        endpoint_url = f"{config['endpoint']}/openai/deployments/{config['deployment_name']}/chat/completions?api-version={config['api_version']}"
        
        headers = {
            "api-key": api_key,
            "Content-Type": "application/json"
        }
        
        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        response = requests.post(endpoint_url, headers=headers, json=payload, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result['choices'][0]['message']['content']
            print("âœ… Azure OpenAI analysis completed")
            return analysis_text
        else:
            return f"Azure OpenAI API Error: Status code {response.status_code}\n{response.text}"
            
    except Exception as e:
        return f"Azure OpenAI API call error: {str(e)}"

def _call_anthropic_llm(prompt: str) -> str:
    """Call Anthropic API"""
    try:
        config = LLM_CONFIG["anthropic"]
        api_key = config["api_key"] or os.environ.get('ANTHROPIC_API_KEY')
        
        if not api_key:
            return "âŒ Anthropic APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚LLM_CONFIG['anthropic']['api_key']ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ANTHROPIC_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        
        headers = {
            "x-api-key": api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        payload = {
            "model": config["model"],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"],
            "messages": [{"role": "user", "content": prompt}]
        }
        
        response = requests.post("https://api.anthropic.com/v1/messages", 
                               headers=headers, json=payload, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result['content'][0]['text']
            print("âœ… Anthropic analysis completed")
            return analysis_text
        else:
            return f"Anthropic API Error: Status code {response.status_code}\n{response.text}"
            
    except Exception as e:
        return f"Anthropic API call error: {str(e)}"

print("âœ… Function definition completed: analyze_bottlenecks_with_llm")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸ“‹ LLM Bottleneck Analysis Execution Preparation
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Verification and display of configured LLM provider
# MAGIC - Analysis start preparation and message display
# MAGIC - Stability improvement through prompt optimization

# COMMAND ----------

# LLMãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æžå®Ÿè¡Œã®æº–å‚™
provider = LLM_CONFIG["provider"]

print(f"\nðŸ¤– ã€Starting SQL bottleneck analysis with {provider.upper()} LLMã€‘")
print("=" * 80)

if provider == "databricks":
    endpoint = LLM_CONFIG["databricks"]["endpoint_name"]
    print(f"ðŸ”— Databricks Model Serving endpoint: {endpoint}")
    print("âš ï¸  Model Serving endpoint must be operational")
elif provider == "openai":
    model = LLM_CONFIG["openai"]["model"]
    print(f"ðŸ”— OpenAI model: {model}")
    print("âš ï¸  OpenAI API key is required")
elif provider == "azure_openai":
    deployment = LLM_CONFIG["azure_openai"]["deployment_name"]
    print(f"ðŸ¤– Starting Azure OpenAI ({deployment}) bottleneck analysis...")
    print("âš ï¸  Azure OpenAI API key and endpoint are required")
elif provider == "anthropic":
    model = LLM_CONFIG["anthropic"]["model"]
    print(f"ðŸ¤– Starting Anthropic ({model}) bottleneck analysis...")
    print("âš ï¸  Anthropic API key is required")

print("ðŸ“ Simplifying analysis prompts to reduce timeout risk...")
print()

# Check if extracted_metrics variable is defined
try:
    extracted_metrics
    print("âœ… extracted_metrics variable confirmed")
    analysis_result = analyze_bottlenecks_with_llm(extracted_metrics)
except NameError:
    print("âŒ extracted_metrics variable is not defined")
    print("âš ï¸ Please run Cell 12 (Performance metrics extraction) first")
    print("ðŸ“‹ Correct execution order: Cell 11 â†’ Cell 12 â†’ Cell 15")
    print("ðŸ”„ Setting default analysis results")
    analysis_result = """
ðŸ¤– LLMãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æžçµæžœ

âŒ åˆ†æžã«å¿…è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚

ðŸ“‹ è§£æ±ºæ–¹æ³•:
1. ã‚»ãƒ«11ã§JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
2. ã‚»ãƒ«12ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã™ã‚‹
3. ã“ã®ã‚»ãƒ«ï¼ˆã‚»ãƒ«15ï¼‰ã‚’å†å®Ÿè¡Œã™ã‚‹

âš ï¸ å…ˆã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºã‚’å®Œäº†ã—ã¦ã‹ã‚‰åˆ†æžã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
"""
except Exception as e:
    print(f"âŒ Error occurred during LLM analysis: {str(e)}")
    analysis_result = f"LLM analysis error: {str(e)}"

# COMMAND ----------

# MAGIC %md
# MAGIC # ðŸš€ Query Profile Analysis Section
# MAGIC
# MAGIC **Main analysis processing starts from here**
# MAGIC
# MAGIC ðŸ“‹ **Execution Steps:**
# MAGIC 1. Execute all cells in the ðŸ”§ Configuration & Setup section above
# MAGIC 2. Run the following cells in order to perform analysis
# MAGIC 3. If errors occur, re-execute from the configuration section
# MAGIC
# MAGIC âš ï¸ **Important Notes:**
# MAGIC - Execute in order: ðŸ”§ Configuration & Setup â†’ ðŸš€ Main Processing â†’ ðŸ”§ SQL Optimization sections
# MAGIC - File path configuration must be done in the first cell
# MAGIC - Verify LLM endpoint configuration

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸš€ SQL Profiler JSON File Loading Execution
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - JSON file loading from configured file path
# MAGIC - File size and basic information display
# MAGIC - Error handling and processing stop control

# COMMAND ----------

print("=" * 80)
print("ðŸš€ Databricks SQL Profiler Analysis Tool")
print("=" * 80)
print(f"ðŸ“ Target analysis file: {JSON_FILE_PATH}")
print()

# File existence check
import os
if not os.path.exists(JSON_FILE_PATH):
    print("âŒ File not found:")
    print(f"   Specified path: {JSON_FILE_PATH}")
    print()
    print("ðŸ’¡ File path configuration hints:")
    print("   1. Set the correct path for JSON_FILE_PATH variable in Cell 2")
    print("   2. Available option examples:")
    print("      - /Volumes/main/base/mitsuhiro_vol/pre_tuning_plan_file.json")
    print("      - /Volumes/main/base/mitsuhiro_vol/nophoton.json")
    print("      - /Volumes/main/base/mitsuhiro_vol/POC1.json")
    print("   3. If file is in DBFS FileStore:")
    print("      - /FileStore/shared_uploads/your_username/filename.json")
    print("âš ï¸ Stopping processing.")
    raise RuntimeError(f"Specified file not found: {JSON_FILE_PATH}")

# Load SQL profiler JSON file
profiler_data = load_profiler_json(JSON_FILE_PATH)
if not profiler_data:
    print("âŒ Failed to load JSON file. Please check the file format.")
    print("âš ï¸ Stopping processing.")
    # dbutils.notebook.exit("File loading failed")  # Commented out for safety
    raise RuntimeError("Failed to load JSON file.")

print(f"âœ… Data loading completed")
print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸ“Š Performance Metrics Extraction and Overview Display
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Metrics extraction from profiler data
# MAGIC - Query basic information display
# MAGIC - Overall performance indicator calculation and display
# MAGIC - Liquid Clustering analysis result display

# COMMAND ----------

# ðŸ“Š Performance metrics extraction
extracted_metrics = extract_performance_metrics(profiler_data)
print("âœ… Performance metrics extracted")

# Display extracted metrics overview
print("\n" + "=" * 50)
print("ðŸ“ˆ Extracted Metrics Overview")
print("=" * 50)

query_info = extracted_metrics['query_info']
overall_metrics = extracted_metrics['overall_metrics']
bottleneck_indicators = extracted_metrics['bottleneck_indicators']

print(f"ðŸ†” Query ID: {query_info['query_id']}")
print(f"ðŸ“Š Status: {query_info['status']}")
print(f"ðŸ‘¤ Execution User: {query_info['user']}")
print(f"â±ï¸ Execution Time: {overall_metrics['total_time_ms']:,} ms ({overall_metrics['total_time_ms']/1000:.2f} sec)")
print(f"ðŸ’¾ Data Read: {overall_metrics['read_bytes']/1024/1024/1024:.2f} GB")
print(f"ðŸ“ˆ Output Rows: {overall_metrics['rows_produced_count']:,} rows")
print(f"ðŸ“‰ Read Rows: {overall_metrics['rows_read_count']:,} rows")
print(f"ðŸŽ¯ Filter Rate: {bottleneck_indicators.get('data_selectivity', 0):.4f} ({bottleneck_indicators.get('data_selectivity', 0)*100:.2f}%)")
print(f"ðŸ”§ Stage Count: {len(extracted_metrics['stage_metrics'])}")
print(f"ðŸ—ï¸ Node Count: {len(extracted_metrics['node_metrics'])}")

# Display Liquid Clustering analysis results
liquid_analysis = extracted_metrics['liquid_clustering_analysis']
liquid_summary = liquid_analysis.get('summary', {})
print(f"ðŸ—‚ï¸ Liquid Clustering Target Tables: {liquid_summary.get('tables_identified', 0)}")
print(f"ðŸ“Š High Impact Tables: {liquid_summary.get('high_impact_tables', 0)}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸ” Bottleneck Indicator Details
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Photon engine usage and performance analysis
# MAGIC - Shuffle operations and parallelism issue detection
# MAGIC - Detailed display of various performance indicators

# COMMAND ----------

# ðŸ“‹ Detailed bottleneck indicator display
print("\n" + "=" * 50)
print("ðŸ” Bottleneck Indicator Details")
print("=" * 50)

# Photon-related indicators
photon_enabled = overall_metrics.get('photon_enabled', False)
photon_utilization_ratio = overall_metrics.get('photon_utilization_ratio', 0)
photon_utilization = min(photon_utilization_ratio * 100, 100.0)  # Limit to max 100%
photon_emoji = "âœ…" if photon_enabled and photon_utilization > 80 else "âš ï¸" if photon_enabled else "âŒ"

# Detailed information about utilization rate
if photon_enabled:
    photon_total_ms = overall_metrics.get('photon_total_time_ms', 0)
    task_total_ms = overall_metrics.get('task_total_time_ms', 0)
    print(f"{photon_emoji} Photon Engine: Enabled (Utilization: {photon_utilization:.1f}%)")
    print(f"   ðŸ“Š Photon Execution Time: {photon_total_ms:,} ms | Total Task Time: {task_total_ms:,} ms")
else:
    print(f"{photon_emoji} Photon Engine: Disabled")

# Parallelism and shuffle-related indicators
shuffle_count = bottleneck_indicators.get('shuffle_operations_count', 0)
has_shuffle_bottleneck = bottleneck_indicators.get('has_shuffle_bottleneck', False)
has_low_parallelism = bottleneck_indicators.get('has_low_parallelism', False)
low_parallelism_count = bottleneck_indicators.get('low_parallelism_stages_count', 0)

shuffle_emoji = "ðŸš¨" if has_shuffle_bottleneck else "âš ï¸" if shuffle_count > 5 else "âœ…"
print(f"{shuffle_emoji} Shuffle Operations: {shuffle_count} times ({'Bottleneck detected' if has_shuffle_bottleneck else 'Normal'})")

parallelism_emoji = "ðŸš¨" if has_low_parallelism else "âœ…"
print(f"{parallelism_emoji} Parallelism: {'Issues detected' if has_low_parallelism else 'Appropriate'} (Low parallelism stages: {low_parallelism_count})")

print()
print("ðŸ“Š Other Indicators:")

for key, value in bottleneck_indicators.items():
    # Skip newly added indicators as they are already displayed above
    if key in ['shuffle_operations_count', 'has_shuffle_bottleneck', 'has_low_parallelism', 
               'low_parallelism_stages_count', 'total_shuffle_time_ms', 'shuffle_time_ratio',
               'slowest_shuffle_duration_ms', 'slowest_shuffle_node', 'low_parallelism_details',
               'average_low_parallelism']:
        continue
        
    if 'ratio' in key:
        emoji = "ðŸ“Š" if value < 0.1 else "âš ï¸" if value < 0.3 else "ðŸš¨"
        print(f"{emoji} {key}: {value:.3f} ({value*100:.1f}%)")
    elif 'bytes' in key and key != 'has_spill':
        if value > 0:
            emoji = "ðŸ’¾" if value < 1024*1024*1024 else "âš ï¸"  # Normal if under 1GB, caution if over
            print(f"{emoji} {key}: {value:,} bytes ({value/1024/1024:.2f} MB)")
    elif key == 'has_spill':
        emoji = "âŒ" if not value else "âš ï¸"
        print(f"{emoji} {key}: {'Yes' if value else 'No'}")
    elif 'duration' in key:
        emoji = "â±ï¸"
        print(f"{emoji} {key}: {value:,} ms ({value/1000:.2f} sec)")
    else:
        emoji = "â„¹ï¸"
        print(f"{emoji} {key}: {value}")

print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸ’¾ Metrics Storage and Time Consumption Analysis
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Save extracted metrics in JSON format
# MAGIC - Convert set types to list types
# MAGIC - Detailed analysis of top 10 most time-consuming processes
# MAGIC - Specific metrics-based spill detection and AQE-based skew analysis
# MAGIC
# MAGIC ðŸ’¿ **Spill Detection Logic**:
# MAGIC - Target metric: `"Sink - Num bytes spilled to disk due to memory pressure"`
# MAGIC - Judgment condition: Spill detected when above metric value > 0
# MAGIC - Search targets: detailed_metrics â†’ raw_metrics â†’ key_metrics in order
# MAGIC
# MAGIC ðŸŽ¯ **Skew Detection Logic**:
# MAGIC - `AQEShuffleRead - Number of skewed partitions`: AQE-based skew detection
# MAGIC - Judgment condition: Skew detected when metric value > 0
# MAGIC - Importance: Judgment based on detected value
# MAGIC - Statistics-based judgment is deprecated (AQE-based judgment recommended)
# MAGIC
# MAGIC ðŸ’¡ **Debug Mode**: To display detailed spill/skew judgment basis
# MAGIC ```python
# MAGIC import os
# MAGIC os.environ['DEBUG_SPILL_ANALYSIS'] = 'true'   # Detailed display of specific metrics spill judgment
# MAGIC os.environ['DEBUG_SKEW_ANALYSIS'] = 'true'    # Detailed display of AQE-based skew judgment
# MAGIC ```

# COMMAND ----------

# ðŸ› Debug mode configuration (optional)
# 
# **Execute only when you want to display detailed spill/skew judgment basis**
# 
# ðŸ“‹ Configuration details:
# - DEBUG_SPILL_ANALYSIS=true: Display detailed basis for specific metrics spill judgment
# - DEBUG_SKEW_ANALYSIS=true: Display detailed basis for AQE-based skew judgment
# 
# ðŸ’¿ Spill debug display content:
# - Target metric: "Sink - Num bytes spilled to disk due to memory pressure"
# - Search results in each data source (detailed_metrics, raw_metrics, key_metrics)
# - Values and judgment results when metrics are found
# - List of other spill-related metrics (reference information)
# 
# ðŸŽ¯ Skew debug display content:
# - AQEShuffleRead - Number of skewed partitions metric value
# - Judgment basis for AQE-based skew detection
# - Number of detected skews and importance level
# - Statistics-based judgment is deprecated (AQE-based judgment recommended)

import os

# Uncomment to enable debug display for specific metrics spill analysis
# os.environ['DEBUG_SPILL_ANALYSIS'] = 'true'

# Uncomment to enable debug display for AQE-based skew analysis  
# os.environ['DEBUG_SKEW_ANALYSIS'] = 'true'

print("ðŸ› Debug mode configuration:")
print(f"   Specific metrics spill analysis debug: {os.environ.get('DEBUG_SPILL_ANALYSIS', 'false')}")
print(f"   AQE-based skew analysis debug: {os.environ.get('DEBUG_SKEW_ANALYSIS', 'false')}")
print("   â€» Setting to 'true' displays detailed judgment basis information")
print()
print("ðŸ’¿ Specific metrics spill detection criteria:")
print('   ðŸŽ¯ Target: "Sink - Num bytes spilled to disk due to memory pressure"')
print("   âœ… Judgment condition: Value > 0")
print()
print("ðŸŽ¯ AQE-based skew detection criteria:")
print("   ðŸ“Š AQEShuffleRead - Number of skewed partitions > 0")
print("   ðŸ“Š Judgment condition: Metric value > 0")
print("   ðŸ“Š Importance: Based on detected value")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸŒ Top 10 Most Time-Consuming Processes
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Saving extracted metrics in JSON format
# MAGIC - Converting set types to list types
# MAGIC - Detailed analysis of the top 10 most time-consuming processes
# MAGIC - Spill detection and data skew analysis
# MAGIC - Spark stage execution analysis

# COMMAND ----------

# ðŸ’¾ æŠ½å‡ºã—ãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã¯é™¤å¤–ï¼ˆä¸è¦ï¼‰
def format_thinking_response(response) -> str:
    """
    thinking_enabled: Trueã®å ´åˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’äººé–“ã«èª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›
    æ€è€ƒéŽç¨‹ï¼ˆthinkingï¼‰ã¨ã‚·ã‚°ãƒãƒãƒ£ï¼ˆsignatureï¼‰ç­‰ã®ä¸è¦ãªæƒ…å ±ã¯é™¤å¤–ã—ã€æœ€çµ‚çš„ãªçµè«–ã®ã¿ã‚’è¡¨ç¤º
    JSONæ§‹é€ ã‚„ä¸é©åˆ‡ãªæ–‡å­—åˆ—ã®éœ²å‡ºã‚’é˜²æ­¢
    """
    import re  # reãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
    
    if not isinstance(response, list):
        # ãƒªã‚¹ãƒˆã§ãªã„å ´åˆã¯æ–‡å­—åˆ—ã¨ã—ã¦å‡¦ç†ã—ã€ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        cleaned_text = clean_response_text(str(response))
        return cleaned_text
    
    # é™¤å¤–ã™ã¹ãã‚­ãƒ¼ã®ãƒªã‚¹ãƒˆï¼ˆæ‹¡å¼µï¼‰
    excluded_keys = {
        'thinking', 'signature', 'metadata', 'id', 'request_id', 
        'timestamp', 'uuid', 'reasoning', 'type', 'model'
    }
    
    formatted_parts = []
    
    for item in response:
        if isinstance(item, dict):
            # æœ€ã‚‚é©åˆ‡ãªãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            content = extract_best_content_from_dict(item, excluded_keys)
            if content:
                cleaned_content = clean_response_text(content)
                if is_valid_content(cleaned_content):
                    formatted_parts.append(cleaned_content)
        else:
            # è¾žæ›¸ã§ãªã„å ´åˆã‚‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            cleaned_content = clean_response_text(str(item))
            if is_valid_content(cleaned_content):
                formatted_parts.append(cleaned_content)
    
    final_result = '\n'.join(formatted_parts)
    
    # æœ€çµ‚çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    final_result = final_quality_check(final_result)
    
    return final_result

def extract_best_content_from_dict(item_dict, excluded_keys):
    """Extract optimal content from dictionary"""
    # å„ªå…ˆé †ä½: text > summary_text > content > message > ãã®ä»–
    priority_keys = ['text', 'summary_text', 'content', 'message', 'response']
    
    for key in priority_keys:
        if key in item_dict and item_dict[key]:
            content = str(item_dict[key])
            # JSONæ§‹é€ ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
            if not looks_like_json_structure(content):
                return content
    
    # å„ªå…ˆã‚­ãƒ¼ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ä»–ã®ã‚­ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆé™¤å¤–ã‚­ãƒ¼ä»¥å¤–ï¼‰
    for key, value in item_dict.items():
        if key not in excluded_keys and value and isinstance(value, str):
            if not looks_like_json_structure(value):
                return value
    
    return None

def looks_like_json_structure(text):
    """Check if text contains JSON structure"""
    json_indicators = [
        "{'type':", '[{\'type\':', '{"type":', '[{"type":',
        "'text':", '"text":', "'summary_text':", '"summary_text":',
        'reasoning', 'metadata', 'signature'
    ]
    text_lower = text.lower()
    return any(indicator.lower() in text_lower for indicator in json_indicators)

def clean_response_text(text):
    """Clean up response text"""
    import re
    
    if not text or not isinstance(text, str):
        return ""
    
    # æ”¹è¡Œã‚³ãƒ¼ãƒ‰ã®æ­£è¦åŒ–
    text = text.replace('\\n', '\n').replace('\\t', '\t')
    
    # JSONæ§‹é€ ã®é™¤åŽ»
    
    # å…¸åž‹çš„ãªJSONæ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤åŽ»
    json_patterns = [
        r"'type':\s*'[^']*'",
        r'"type":\s*"[^"]*"',
        r"\[?\{'type':[^}]*\}[,\]]?",
        r'\[?\{"type":[^}]*\}[,\]]?',
        r"'reasoning':\s*\[[^\]]*\]",
        r'"reasoning":\s*\[[^\]]*\]',
        r"'signature':\s*'[A-Za-z0-9+/=]{50,}'",
        r'"signature":\s*"[A-Za-z0-9+/=]{50,}"'
    ]
    
    for pattern in json_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)
    
    # ä¸å®Œå…¨ãªJSONãƒ–ãƒ©ã‚±ãƒƒãƒˆã®é™¤åŽ»
    text = re.sub(r'^\s*[\[\{]', '', text)  # å…ˆé ­ã® [ ã‚„ {
    text = re.sub(r'[\]\}]\s*$', '', text)  # æœ«å°¾ã® ] ã‚„ }
    text = re.sub(r'^\s*[,;]\s*', '', text)  # å…ˆé ­ã®ã‚«ãƒ³ãƒžã‚„ã‚»ãƒŸã‚³ãƒ­ãƒ³
    
    # é€£ç¶šã™ã‚‹ç©ºç™½ãƒ»æ”¹è¡Œã®æ­£è¦åŒ–
    text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)  # 3ã¤ä»¥ä¸Šã®é€£ç¶šæ”¹è¡Œã‚’2ã¤ã«
    text = re.sub(r'[ \t]+', ' ', text)  # é€£ç¶šã™ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ãƒ»ã‚¿ãƒ–ã‚’1ã¤ã«
    
    # å‰å¾Œã®ç©ºç™½ã‚’é™¤åŽ»
    text = text.strip()
    
    return text

def is_valid_content(text):
    """Check if content is valid"""
    import re
    
    if not text or len(text.strip()) < 10:
        return False
    
    # ç„¡åŠ¹ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
    invalid_patterns = [
        r'^[{\[\'"]*$',  # JSONæ§‹é€ ã®ã¿
        r'^[,;:\s]*$',   # åŒºåˆ‡ã‚Šæ–‡å­—ã®ã¿
        r'^\s*reasoning\s*$',  # reasoningã®ã¿
        r'^\s*metadata\s*$',   # metadataã®ã¿
        r'^[A-Za-z0-9+/=]{50,}$',  # Base64ã£ã½ã„é•·ã„æ–‡å­—åˆ—
    ]
    
    for pattern in invalid_patterns:
        if re.match(pattern, text.strip(), re.IGNORECASE):
            return False
    
    return True

def final_quality_check(text):
    """Final quality check and cleanup"""
    import re  # reãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
    
    if not text:
        return "åˆ†æžçµæžœã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
    
    # è¨€èªžã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå®‰å…¨ãªå¤‰æ•°ã‚¢ã‚¯ã‚»ã‚¹ï¼‰
    try:
        language = globals().get('OUTPUT_LANGUAGE', 'ja')  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ—¥æœ¬èªž
    except:
        language = 'ja'
    
    if language == 'ja':
        text = ensure_japanese_consistency(text)
    elif language == 'en':
        text = ensure_english_consistency(text)
    
    # æœ€å°é™ã®é•·ã•ãƒã‚§ãƒƒã‚¯
    if len(text.strip()) < 20:
        if language == 'ja':
            return "åˆ†æžçµæžœãŒä¸å®Œå…¨ã§ã™ã€‚è©³ç´°ãªåˆ†æžã‚’å®Ÿè¡Œä¸­ã§ã™ã€‚"
        else:
            return "Analysis result is incomplete. Detailed analysis in progress."
    
    return text

def ensure_japanese_consistency(text):
    """Ensure Japanese text consistency"""
    import re
    
    # æ˜Žã‚‰ã‹ã«ç ´æã—ã¦ã„ã‚‹éƒ¨åˆ†ã‚’é™¤åŽ»
    # ä¾‹: "æ­£caientify="predicate_liquid_referencet1" ã®ã‚ˆã†ãªç ´ææ–‡å­—åˆ—
    text = re.sub(r'[a-zA-Z0-9_="\']{20,}', '', text)
    
    # ä¸å®Œå…¨ãªãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®ä¿®æ­£
    text = re.sub(r'#\s*[^#\n]*["\'>]+[^#\n]*', '', text)  # ç ´æã—ãŸãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼
    
    # æ„å‘³ä¸æ˜Žãªæ–‡å­—åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é™¤åŽ»ï¼ˆæ‹¡å¼µï¼‰
    nonsense_patterns = [
        r'addressing_sales_column\d*',
        r'predicate_liquid_reference[a-zA-Z0-9]*',
        r'bottlenars\s+effect',
        r'å®Ÿè£…éžä¿å­˜åœ¨',
        r'è£ç¥¨ã®end_by',
        r'riconsistall',
        r'caientify[a-zA-Z0-9="\']*',
        r'iving\s+[a-zA-Z0-9]*',
        r'o\s+Matteré…è³›',
        r'ubsãŒä½Žã„åƒ®æ€§',
        r'åˆ°ç”°ãƒ‡ãƒ¼ã‚¿ã®æ–¹åŠ¹æ€§',
        r'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹.*topic.*é …è¡Œã«è€ƒ',
        r'ï¼»[^ï¼½]*ï¼½">[^<]*',  # ç ´æã—ãŸHTML/XMLè¦ç´ 
        r'\]\s*">\s*$'  # æ–‡æœ«ã®ç ´æã—ãŸã‚¿ã‚°
    ]
    
    for pattern in nonsense_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # é€£ç¶šã™ã‚‹è¨˜å·ã®é™¤åŽ»
    text = re.sub(r'["\'>]{2,}', '', text)
    text = re.sub(r'[=\'"]{3,}', '', text)
    
    # ç ´æã—ãŸæ—¥æœ¬èªžã®ä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³
    broken_japanese_patterns = [
        (r'ã®æ–¹æ³•å‹•çš„ãŒã‚‰', 'å‹•çš„ãªæ–¹æ³•ã§'),
        (r'æ€è€ƒã«æ²¿ã£ã¦é€²ã‚ã¦ã„ãã¾ã™ã€‚$', 'æ€è€ƒã«æ²¿ã£ã¦åˆ†æžã‚’é€²ã‚ã¾ã™ã€‚'),
        (r'ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«æ²¿ã£ãŸæ”¹å–„ã‚’.*ã¾ã§ã—ã¦ã„ã‚‹ã®', 'ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«æ²¿ã£ãŸæ”¹å–„ææ¡ˆ'),
    ]
    
    for broken, fixed in broken_japanese_patterns:
        text = re.sub(broken, fixed, text, flags=re.IGNORECASE)
    
    # ç©ºè¡Œã®æ­£è¦åŒ–
    text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
    
    return text.strip()

def ensure_english_consistency(text):
    """Ensure English text consistency"""
    import re
    
    # åŒæ§˜ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’è‹±èªžç”¨ã«å®Ÿè£…
    text = re.sub(r'[^\x00-\x7F\s]{10,}', '', text)  # éžASCIIæ–‡å­—ã®é•·ã„é€£ç¶šã‚’é™¤åŽ»
    
    return text.strip()

def extract_main_content_from_thinking_response(response) -> str:
    """
    thinkingå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆtextã¾ãŸã¯summary_textï¼‰ã®ã¿ã‚’æŠ½å‡º
    thinkingã€signatureç­‰ã®ä¸è¦ãªæƒ…å ±ã¯é™¤å¤–
    JSONæ§‹é€ ã‚„ç ´æã—ãŸãƒ†ã‚­ã‚¹ãƒˆã®æ··å…¥ã‚’é˜²æ­¢
    """
    if not isinstance(response, list):
        cleaned_text = clean_response_text(str(response))
        return final_quality_check(cleaned_text)
    
    # é™¤å¤–ã™ã¹ãã‚­ãƒ¼
    excluded_keys = {
        'thinking', 'signature', 'metadata', 'id', 'request_id', 
        'timestamp', 'uuid', 'reasoning', 'type', 'model'
    }
    
    for item in response:
        if isinstance(item, dict):
            # æœ€é©ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            content = extract_best_content_from_dict(item, excluded_keys)
            if content:
                cleaned_content = clean_response_text(content)
                if is_valid_content(cleaned_content):
                    return final_quality_check(cleaned_content)
    
    # ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…¨ä½“ã‚’ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ
    return format_thinking_response(response)

def convert_sets_to_lists(obj):
    """Convert set types to list types for JSON serialization"""
    if isinstance(obj, set):
        return list(obj)
    elif isinstance(obj, dict):
        return {key: convert_sets_to_lists(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_sets_to_lists(item) for item in obj]
    else:
        return obj

# output_extracted_metrics ã®ç”Ÿæˆã¯é™¤å¤–ï¼ˆä¸è¦ï¼‰

# ðŸŒ Top 10 Most Time-Consuming Processes
print(f"\nðŸŒ Top 10 Most Time-Consuming Processes")
print("=" * 80)
print("ðŸ“Š Icon explanations: â±ï¸Time ðŸ’¾Memory ðŸ”¥ðŸŒParallelism ðŸ’¿Spill âš–ï¸Skew")
print('ðŸ’¿ Spill judgment: "Sink - Num bytes spilled to disk due to memory pressure" > 0')
print("ðŸŽ¯ Skew judgment: 'AQEShuffleRead - Number of skewed partitions' > 0")

# Sort nodes by execution time
sorted_nodes = sorted(extracted_metrics['node_metrics'], 
                     key=lambda x: x['key_metrics'].get('durationMs', 0), 
                     reverse=True)

# Process maximum 10 nodes
final_sorted_nodes = sorted_nodes[:10]

if final_sorted_nodes:
    # ðŸš¨ Important: Correct total time calculation (regression prevention)
    # 1. Get total execution time from overall_metrics (wall-clock time)
    overall_metrics = extracted_metrics.get('overall_metrics', {})
    total_duration = overall_metrics.get('total_time_ms', 0)
    
    # ðŸš¨ Fix parallel execution issue: Prioritize task_total_time_ms
    task_total_time_ms = overall_metrics.get('task_total_time_ms', 0)
    
    if task_total_time_ms > 0:
        total_duration = task_total_time_ms
        print(f"âœ… Console display: Parallel execution support - using task_total_time_ms: {total_duration:,} ms ({total_duration/3600000:.1f} hours)")
    elif total_duration <= 0:
        # Use execution_time_ms as next priority
        execution_time_ms = overall_metrics.get('execution_time_ms', 0)
        if execution_time_ms > 0:
            total_duration = execution_time_ms
            print(f"âš ï¸ Console display: task_total_time_ms unavailable, using execution_time_ms: {total_duration} ms")
        else:
            # Final fallback
            max_node_time = max([node['key_metrics'].get('durationMs', 0) for node in sorted_nodes], default=1)
            total_duration = int(max_node_time * 1.2)
            print(f"âš ï¸ Console display: Final fallback - using estimated time: {total_duration} ms")
    
    print(f"ðŸ“Š Cumulative task execution time (parallel): {total_duration:,} ms ({total_duration/3600000:.1f} hours)")
    print(f"ðŸ“ˆ TOP10 total time (parallel execution): {sum(node['key_metrics'].get('durationMs', 0) for node in final_sorted_nodes):,} ms")

    print()
    
    for i, node in enumerate(final_sorted_nodes):
        rows_num = node['key_metrics'].get('rowsNum', 0)
        duration_ms = node['key_metrics'].get('durationMs', 0)
        memory_mb = node['key_metrics'].get('peakMemoryBytes', 0) / 1024 / 1024
        
        # ðŸš¨ é‡è¦: æ­£ã—ã„ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
        # wall-clock timeã«å¯¾ã™ã‚‹å„ãƒŽãƒ¼ãƒ‰ã®å®Ÿè¡Œæ™‚é–“ã®å‰²åˆ
        time_percentage = min((duration_ms / max(total_duration, 1)) * 100, 100.0)
        
        # æ™‚é–“ã®é‡è¦åº¦ã«åŸºã¥ã„ã¦ã‚¢ã‚¤ã‚³ãƒ³ã‚’é¸æŠž
        if duration_ms >= 10000:  # 10ç§’ä»¥ä¸Š
            time_icon = "ï¿½"
            severity = "CRITICAL"
        elif duration_ms >= 5000:  # 5ç§’ä»¥ä¸Š
            time_icon = "ðŸŸ "
            severity = "HIGH"
        elif duration_ms >= 1000:  # 1ç§’ä»¥ä¸Š
            time_icon = "ðŸŸ¡"
            severity = "MEDIUM"
        else:
            time_icon = "ï¿½"
            severity = "LOW"
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ã‚¢ã‚¤ã‚³ãƒ³
        memory_icon = "ï¿½" if memory_mb < 100 else "âš ï¸" if memory_mb < 1000 else "ðŸš¨"
        
        # ã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹ãƒŽãƒ¼ãƒ‰åã‚’å–å¾—
        raw_node_name = node['name']
        node_name = get_meaningful_node_name(node, extracted_metrics)
        short_name = node_name[:100] + "..." if len(node_name) > 100 else node_name
        
        # ä¸¦åˆ—åº¦æƒ…å ±ã®å–å¾—ï¼ˆä¿®æ­£ç‰ˆ: è¤‡æ•°ã®Tasks totalãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ï¼‰
        parallelism_data = extract_parallelism_metrics(node)
        
        # å¾“æ¥ã®å˜ä¸€å€¤ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
        num_tasks = parallelism_data.get('tasks_total', 0)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Sink - Tasks totalã¾ãŸã¯Source - Tasks totalãŒã‚ã‚‹å ´åˆ
        if num_tasks == 0:
            if parallelism_data.get('sink_tasks_total', 0) > 0:
                num_tasks = parallelism_data.get('sink_tasks_total', 0)
            elif parallelism_data.get('source_tasks_total', 0) > 0:
                num_tasks = parallelism_data.get('source_tasks_total', 0)
        
        # ãƒ‡ã‚£ã‚¹ã‚¯ã‚¹ãƒ”ãƒ«ã‚¢ã‚¦ãƒˆã®æ¤œå‡ºï¼ˆãƒ¡ãƒ¢ãƒªãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ã«ã‚ˆã‚‹ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹å¯¾å¿œæ”¹å–„ç‰ˆï¼‰
        spill_detected = False
        spill_bytes = 0
        spill_details = []
        
        # ã‚¹ãƒ”ãƒ«æ¤œå‡ºã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹åãƒªã‚¹ãƒˆï¼ˆæ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®ã¿ï¼‰
        exact_spill_metrics = [
            "Num bytes spilled to disk due to memory pressure",
            "Sink - Num bytes spilled to disk due to memory pressure",
            "Sink/Num bytes spilled to disk due to memory pressure"
        ]
        
        # 1. detailed_metricsã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            metric_value = metric_info.get('value', 0)
            metric_label = metric_info.get('label', '')
            
            # æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§ã®ã¿ãƒžãƒƒãƒãƒ³ã‚°
            if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                spill_detected = True
                spill_bytes = max(spill_bytes, metric_value)  # æœ€å¤§å€¤ã‚’ä½¿ç”¨
                spill_details.append({
                    'metric_name': metric_key,
                    'value': metric_value,
                    'label': metric_label,
                    'source': 'detailed_metrics',
                    'matched_field': 'key' if metric_key in exact_spill_metrics else 'label',
                    'matched_pattern': metric_key if metric_key in exact_spill_metrics else metric_label
                })
                break  # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½¿ç”¨
        
        # 2. detailed_metricsã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ç”Ÿãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢
        if not spill_detected:
            raw_metrics = node.get('metrics', [])
            for metric in raw_metrics:
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_value = metric.get('value', 0)
                
                # æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§ã®ã¿ãƒžãƒƒãƒãƒ³ã‚°
                if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                    spill_detected = True
                    spill_bytes = max(spill_bytes, metric_value)  # æœ€å¤§å€¤ã‚’ä½¿ç”¨
                    spill_details.append({
                        'metric_name': metric_key,
                        'value': metric_value,
                        'label': metric_label,
                        'source': 'raw_metrics',
                        'matched_field': 'key' if metric_key in exact_spill_metrics else 'label',
                        'matched_pattern': metric_key if metric_key in exact_spill_metrics else metric_label
                    })
                    break  # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½¿ç”¨
        
        # 3. key_metricsã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢
        if not spill_detected:
            key_metrics = node.get('key_metrics', {})
            for exact_metric in exact_spill_metrics:
                if exact_metric in key_metrics and key_metrics[exact_metric] > 0:
                    spill_detected = True
                    spill_bytes = max(spill_bytes, key_metrics[exact_metric])  # æœ€å¤§å€¤ã‚’ä½¿ç”¨
                    spill_details.append({
                        'metric_name': f"key_metrics.{exact_metric}",
                        'value': key_metrics[exact_metric],
                        'label': f"Key metric: {exact_metric}",
                        'source': 'key_metrics',
                        'matched_field': 'key',
                        'matched_pattern': exact_metric
                    })
                    break
        
        # Data skew detection (AQE-based precise judgment)
        skew_detected = False
        skew_details = []
        skewed_partitions = 0  # Number of skewed partitions
        
        # AQE-based skew detection: "AQEShuffleRead - Number of skewed partitions" > 0
        target_aqe_metrics = [
            "AQEShuffleRead - Number of skewed partitions",
            "AQEShuffleRead - Number of skewed partition splits"
        ]
        
        aqe_skew_value = 0
        aqe_split_value = 0
        aqe_metric_name = ""
        aqe_split_metric_name = ""
        
        # 1. detailed_metricsã§æ¤œç´¢
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            if metric_key == "AQEShuffleRead - Number of skewed partitions":
                aqe_skew_value = metric_info.get('value', 0)
                aqe_metric_name = metric_key
            elif metric_key == "AQEShuffleRead - Number of skewed partition splits":
                aqe_split_value = metric_info.get('value', 0)
                aqe_split_metric_name = metric_key
            elif metric_info.get('label', '') == "AQEShuffleRead - Number of skewed partitions":
                aqe_skew_value = metric_info.get('value', 0)
                aqe_metric_name = metric_info.get('label', '')
            elif metric_info.get('label', '') == "AQEShuffleRead - Number of skewed partition splits":
                aqe_split_value = metric_info.get('value', 0)
                aqe_split_metric_name = metric_info.get('label', '')
        
        # 2. raw_metricsã§æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if aqe_skew_value == 0 or aqe_split_value == 0:
            raw_metrics = node.get('metrics', [])
            if isinstance(raw_metrics, list):
                for raw_metric in raw_metrics:
                    if isinstance(raw_metric, dict):
                        # 'label'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æœ€åˆã«ãƒã‚§ãƒƒã‚¯
                        raw_metric_label = raw_metric.get('label', '')
                        if raw_metric_label == "AQEShuffleRead - Number of skewed partitions" and aqe_skew_value == 0:
                            aqe_skew_value = raw_metric.get('value', 0)
                            aqe_metric_name = raw_metric_label
                        elif raw_metric_label == "AQEShuffleRead - Number of skewed partition splits" and aqe_split_value == 0:
                            aqe_split_value = raw_metric.get('value', 0)
                            aqe_split_metric_name = raw_metric_label
                        
                        # 'key'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚ãƒã‚§ãƒƒã‚¯
                        raw_metric_key = raw_metric.get('key', '')
                        if raw_metric_key == "AQEShuffleRead - Number of skewed partitions" and aqe_skew_value == 0:
                            aqe_skew_value = raw_metric.get('value', 0)
                            aqe_metric_name = raw_metric_key
                        elif raw_metric_key == "AQEShuffleRead - Number of skewed partition splits" and aqe_split_value == 0:
                            aqe_split_value = raw_metric.get('value', 0)
                            aqe_split_metric_name = raw_metric_key
                        
                        # 'metricName'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚ãƒã‚§ãƒƒã‚¯ï¼ˆå¾“æ¥ã®äº’æ›æ€§ï¼‰
                        raw_metric_name = raw_metric.get('metricName', '')
                        if raw_metric_name == "AQEShuffleRead - Number of skewed partitions" and aqe_skew_value == 0:
                            aqe_skew_value = raw_metric.get('value', 0)
                            aqe_metric_name = raw_metric_name
                        elif raw_metric_name == "AQEShuffleRead - Number of skewed partition splits" and aqe_split_value == 0:
                            aqe_split_value = raw_metric.get('value', 0)
                            aqe_split_metric_name = raw_metric_name
        
        # 3. key_metricsã§æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if aqe_skew_value == 0 or aqe_split_value == 0:
            key_metrics = node.get('key_metrics', {})
            for key_metric_name, key_metric_value in key_metrics.items():
                if "AQEShuffleRead - Number of skewed partitions" in key_metric_name and aqe_skew_value == 0:
                    aqe_skew_value = key_metric_value
                    aqe_metric_name = key_metric_name
                elif "AQEShuffleRead - Number of skewed partition splits" in key_metric_name and aqe_split_value == 0:
                    aqe_split_value = key_metric_value
                    aqe_split_metric_name = key_metric_name
        
        # AQE skew judgment
        if aqe_skew_value > 0:
            skew_detected = True
            skewed_partitions = aqe_skew_value  # Set number of skewed partitions
            severity_level = "High" if aqe_skew_value >= 5 else "Medium"
            
            # Basic AQE skew detection information
            description = f'AQE skew detected: {aqe_metric_name} = {aqe_skew_value} > threshold 0 [Importance:{severity_level}]'
            
            # Add detailed information if split value is also available
            if aqe_split_value > 0:
                description += f' | AQE detection details: Spark automatically detected {aqe_skew_value} skewed partitions'
                description += f' | AQE automatic handling: Spark automatically split into {aqe_split_value} partitions'
            
            skew_details.append({
                'type': 'aqe_skew',
                'value': aqe_skew_value,
                'split_value': aqe_split_value,
                'threshold': 0,
                'metric_name': aqe_metric_name,
                'split_metric_name': aqe_split_metric_name,
                'severity': severity_level,
                'description': description
            })
        
        # Use only AQE-based skew detection (spill-based judgment removed)
        # Reason: AQEShuffleRead - Number of skewed partitions is the accurate skew judgment standard
        
        # ä¸¦åˆ—åº¦ã‚¢ã‚¤ã‚³ãƒ³
        parallelism_icon = "ðŸ”¥" if num_tasks >= 10 else "âš ï¸" if num_tasks >= 5 else "ðŸŒ"
        # ã‚¹ãƒ”ãƒ«ã‚¢ã‚¤ã‚³ãƒ³
        spill_icon = "ðŸ’¿" if spill_detected else "âœ…"
        # ã‚¹ã‚­ãƒ¥ãƒ¼ã‚¢ã‚¤ã‚³ãƒ³
        skew_icon = "âš–ï¸" if skew_detected else "âœ…"
        
        print(f"{i+1:2d}. {time_icon}{memory_icon}{parallelism_icon}{spill_icon}{skew_icon} [{severity:8}] {short_name}")
        print(f"    â±ï¸  Execution time: {duration_ms:>8,} ms ({duration_ms/1000:>6.1f} sec) - {time_percentage:>5.1f}% of cumulative time")
        print(f"    ðŸ“Š Rows processed: {rows_num:>8,} rows")
        print(f"    ðŸ’¾ Peak memory: {memory_mb:>6.1f} MB")
        # Display multiple Tasks total metrics
        parallelism_display = []
        for task_metric in parallelism_data.get('all_tasks_metrics', []):
            parallelism_display.append(f"{task_metric['name']}: {task_metric['value']}")
        
        if parallelism_display:
            print(f"    ðŸ”§ Parallelism: {' | '.join(parallelism_display)}")
        else:
            print(f"    ðŸ”§ Parallelism: {num_tasks:>3d} tasks")
        
        # Skew judgment (considering both AQE skew detection and AQEShuffleRead average partition size)
        aqe_shuffle_skew_warning = parallelism_data.get('aqe_shuffle_skew_warning', False)
        
        if skew_detected:
            skew_status = "Detected & handled by AQE"
        elif aqe_shuffle_skew_warning:
            skew_status = "Potential skew possibility"
        else:
            skew_status = "None"
        
        print(f"    ðŸ’¿ Spill: {'Yes' if spill_detected else 'No'} | âš–ï¸ Skew: {skew_status}")
        
        # Display AQEShuffleRead metrics
        aqe_shuffle_metrics = parallelism_data.get('aqe_shuffle_metrics', [])
        if aqe_shuffle_metrics:
            aqe_display = []
            for aqe_metric in aqe_shuffle_metrics:
                if aqe_metric['name'] == "AQEShuffleRead - Number of partitions":
                    aqe_display.append(f"Partitions: {aqe_metric['value']}")
                elif aqe_metric['name'] == "AQEShuffleRead - Partition data size":
                    aqe_display.append(f"Data size: {aqe_metric['value']:,} bytes")
            
            if aqe_display:
                print(f"    ðŸ”„ AQEShuffleRead: {' | '.join(aqe_display)}")
                
                # Average partition size and warning display
                avg_partition_size = parallelism_data.get('aqe_shuffle_avg_partition_size', 0)
                if avg_partition_size > 0:
                    avg_size_mb = avg_partition_size / (1024 * 1024)
                    print(f"    ðŸ“Š Average partition size: {avg_size_mb:.2f} MB")
                    
                    # Warning when 512MB or more
                    if parallelism_data.get('aqe_shuffle_skew_warning', False):
                        print(f"    âš ï¸  ã€WARNINGã€‘ Average partition size exceeds 512MB - Potential skew possibility")
        
        # Calculate efficiency indicator (rows/sec)
        if duration_ms > 0:
            rows_per_sec = (rows_num * 1000) / duration_ms
            print(f"    ðŸš€ Processing efficiency: {rows_per_sec:>8,.0f} rows/sec")
        
# ãƒ•ã‚£ãƒ«ã‚¿çŽ‡è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ä»˜ãï¼‰
        filter_result = calculate_filter_rate(node)
        filter_display = format_filter_rate_display(filter_result)
        if filter_display:
            print(f"    {filter_display}")
        else:
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼šãªãœãƒ•ã‚£ãƒ«ã‚¿çŽ‡ãŒè¡¨ç¤ºã•ã‚Œãªã„ã‹ã‚’ç¢ºèª
            if filter_result["has_filter_metrics"]:
                print(f"    ðŸ“‚ Filter rate: {filter_result['filter_rate']:.1%} (read: {filter_result['files_read_bytes']/(1024*1024*1024):.2f}GB, pruned: {filter_result['files_pruned_bytes']/(1024*1024*1024):.2f}GB)")
            else:
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œç´¢ã®ãƒ‡ãƒãƒƒã‚°
                debug_info = []
                detailed_metrics = node.get('detailed_metrics', {})
                for metric_key, metric_info in detailed_metrics.items():
                    metric_label = metric_info.get('label', '')
                    if 'file' in metric_label.lower() and ('read' in metric_label.lower() or 'prun' in metric_label.lower()):
                        debug_info.append(f"{metric_label}: {metric_info.get('value', 0)}")
                
                if debug_info:
                    print(f"    ðŸ“‚ Filter-related metrics detected: {', '.join(debug_info[:2])}")
                # else:
                #     print(f"    ðŸ“‚ Filter rate: metrics not detected")
        
        # ã‚¹ãƒ”ãƒ«è©³ç´°æƒ…å ±ï¼ˆã‚·ãƒ³ãƒ—ãƒ«è¡¨ç¤ºï¼‰
        spill_display = ""
        if spill_detected and spill_bytes > 0:
            spill_mb = spill_bytes / 1024 / 1024
            if spill_mb >= 1024:  # GBå˜ä½
                spill_display = f"{spill_mb/1024:.2f} GB"
            else:  # MBå˜ä½
                spill_display = f"{spill_mb:.1f} MB"
            print(f"    ðŸ’¿ Spill: {spill_display}")
        
        # ShuffleãƒŽãƒ¼ãƒ‰ã®å ´åˆã¯å¸¸ã«Shuffle attributesã‚’è¡¨ç¤º
        if "shuffle" in short_name.lower():
            shuffle_attributes = extract_shuffle_attributes(node)
            if shuffle_attributes:
                print(f"    ðŸ”„ Shuffle attributes: {', '.join(shuffle_attributes)}")
                
                # REPARTITIONãƒ’ãƒ³ãƒˆã®ææ¡ˆï¼ˆã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã®ã¿ï¼‰
                if spill_detected and spill_bytes > 0 and spill_display:
                    suggested_partitions = max(num_tasks * 2, 200)  # æœ€å°200ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
                    
                    # Shuffleå±žæ€§ã§æ¤œå‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ ã‚’å…¨ã¦ä½¿ç”¨ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
                    repartition_columns = ", ".join(shuffle_attributes)
                    
                    print(f"    ðŸ’¡ Optimization suggestion: REPARTITION({suggested_partitions}, {repartition_columns})")
                    print(f"       Reason: To improve spill ({spill_display})")
                    print(f"       Target: Complete use of all {len(shuffle_attributes)} shuffle attribute columns")
            else:
                print(f"    ðŸ”„ Shuffle attributes: Not configured")
        
        # ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰ã®å ´åˆã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã‚’è¡¨ç¤º
        if "scan" in short_name.lower():
            cluster_attributes = extract_cluster_attributes(node)
            if cluster_attributes:
                print(f"    ðŸ“Š Clustering keys: {', '.join(cluster_attributes)}")
            else:
                print(f"    ðŸ“Š Clustering keys: Not configured")

        
        # Skew details (simplified display)
        if skew_detected and skewed_partitions > 0:
            print(f"    âš–ï¸ Skew details: {skewed_partitions} skewed partitions")
        
        # Also display Node ID
        print(f"    ðŸ†” Node ID: {node.get('node_id', node.get('id', 'N/A'))}")
        print()
        
else:
    print("âš ï¸ Node metrics not found")

print()

# ðŸ”¥ Sparkã‚¹ãƒ†ãƒ¼ã‚¸å®Ÿè¡Œåˆ†æž
if extracted_metrics['stage_metrics']:
    print("\nðŸ”¥ Spark Stage Execution Analysis")
    print("=" * 60)
    
    stage_metrics = extracted_metrics['stage_metrics']
    total_stages = len(stage_metrics)
    completed_stages = len([s for s in stage_metrics if s.get('status') == 'COMPLETE'])
    failed_stages = len([s for s in stage_metrics if s.get('num_failed_tasks', 0) > 0])
    
    print(f"ðŸ“Š Stage overview: Total {total_stages} stages (completed: {completed_stages}, with failed tasks: {failed_stages})")
    print()
    
    # ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
    sorted_stages = sorted(stage_metrics, key=lambda x: x.get('duration_ms', 0), reverse=True)
    
    print("â±ï¸ Stage execution time ranking:")
    print("-" * 60)
    
    for i, stage in enumerate(sorted_stages[:5]):  # TOP5ã‚¹ãƒ†ãƒ¼ã‚¸ã®ã¿è¡¨ç¤º
        stage_id = stage.get('stage_id', 'N/A')
        status = stage.get('status', 'UNKNOWN')
        duration_ms = stage.get('duration_ms', 0)
        num_tasks = stage.get('num_tasks', 0)
        failed_tasks = stage.get('num_failed_tasks', 0)
        complete_tasks = stage.get('num_complete_tasks', 0)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã«å¿œã˜ãŸã‚¢ã‚¤ã‚³ãƒ³
        if status == 'COMPLETE' and failed_tasks == 0:
            status_icon = "âœ…"
        elif failed_tasks > 0:
            status_icon = "âš ï¸"
        else:
            status_icon = "â“"
        
        # ä¸¦åˆ—åº¦ã‚¢ã‚¤ã‚³ãƒ³
        parallelism_icon = "ðŸ”¥" if num_tasks >= 10 else "âš ï¸" if num_tasks >= 5 else "ðŸŒ"
        
        # å®Ÿè¡Œæ™‚é–“ã®é‡è¦åº¦
        if duration_ms >= 10000:
            time_icon = "ðŸ”´"
            severity = "CRITICAL"
        elif duration_ms >= 5000:
            time_icon = "ðŸŸ "
            severity = "HIGH"
        elif duration_ms >= 1000:
            time_icon = "ðŸŸ¡"
            severity = "MEDIUM"
        else:
            time_icon = "ðŸŸ¢"
            severity = "LOW"
        
        print(f"{i+1}. {status_icon}{parallelism_icon}{time_icon} Stage {stage_id} [{severity:8}]")
        print(f"   â±ï¸ Execution time: {duration_ms:,} ms ({duration_ms/1000:.1f} sec)")
        print(f"   ðŸ”§ Tasks: {complete_tasks}/{num_tasks} completed (failed: {failed_tasks})")
        
        # ã‚¿ã‚¹ã‚¯ã‚ãŸã‚Šã®å¹³å‡æ™‚é–“
        if num_tasks > 0:
            avg_task_time = duration_ms / num_tasks
            print(f"   ðŸ“Š Average task time: {avg_task_time:.1f} ms")
        
        # åŠ¹çŽ‡æ€§è©•ä¾¡
        if num_tasks > 0:
            task_efficiency = "é«˜åŠ¹çŽ‡" if num_tasks >= 10 and failed_tasks == 0 else "è¦æ”¹å–„" if failed_tasks > 0 else "æ¨™æº–"
            print(f"   ðŸŽ¯ Efficiency: {task_efficiency}")
        
        print()
    
    if len(sorted_stages) > 5:
        print(f"... {len(sorted_stages) - 5} other stages")
    
    # å•é¡Œã®ã‚ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ
    problematic_stages = [s for s in stage_metrics if s.get('num_failed_tasks', 0) > 0 or s.get('duration_ms', 0) > 30000]
    if problematic_stages:
        print("\nðŸš¨ Stages requiring attention:")
        print("-" * 40)
        for stage in problematic_stages[:3]:
            stage_id = stage.get('stage_id', 'N/A')
            duration_sec = stage.get('duration_ms', 0) / 1000
            failed_tasks = stage.get('num_failed_tasks', 0)
            
            issues = []
            if failed_tasks > 0:
                issues.append(f"å¤±æ•—ã‚¿ã‚¹ã‚¯{failed_tasks}å€‹")
            if duration_sec > 30:
                issues.append(f"é•·æ™‚é–“å®Ÿè¡Œ({duration_sec:.1f}sec)")
            
            print(f"   âš ï¸ Stage {stage_id}: {', '.join(issues)}")
    
    
    print()
else:
    print("\nðŸ”¥ Spark Stage Execution Analysis")
    print("=" * 60)
    print("âš ï¸ Stage metrics not found")
    print()

print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸ—‚ï¸ Detailed Display of Liquid Clustering Analysis Results
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Detailed display of recommended clustering columns by table
# MAGIC - Analysis of expected performance improvements
# MAGIC - Detailed analysis of column usage patterns
# MAGIC - Display of pushdown filter information
# MAGIC - Presentation of SQL implementation examples

# COMMAND ----------

# ðŸ—‚ï¸ LLMã«ã‚ˆã‚‹Liquid Clusteringåˆ†æžçµæžœã®è©³ç´°è¡¨ç¤º
print("\n" + "=" * 50)
print("ðŸ¤– LLM Liquid Clustering Recommendation Analysis")
print("=" * 50)

# LLMãƒ™ãƒ¼ã‚¹ã®Liquid Clusteringåˆ†æžã‚’å®Ÿè¡Œ
liquid_analysis = extracted_metrics['liquid_clustering_analysis']

# LLMåˆ†æžçµæžœã‚’è¡¨ç¤º
print("\nðŸ¤– LLM Analysis Results:")
print("=" * 50)
llm_analysis = liquid_analysis.get('llm_analysis', '')
if llm_analysis:
    print(llm_analysis)
else:
    print("âŒ LLM analysis results not found")

# æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦ã‚’è¡¨ç¤º
extracted_data = liquid_analysis.get('extracted_data', {})
metadata_summary = extracted_data.get('metadata_summary', {})

print(f"\nðŸ“Š Extracted data overview:")
print(f"   ðŸ” Filter conditions: {metadata_summary.get('filter_expressions_count', 0)} items")
print(f"   ðŸ”— JOIN conditions: {metadata_summary.get('join_expressions_count', 0)} items")
print(f"   ðŸ“Š GROUP BY conditions: {metadata_summary.get('groupby_expressions_count', 0)} items")
print(f"   ðŸ“ˆ Aggregate functions: {metadata_summary.get('aggregate_expressions_count', 0)} items")
print(f"   ðŸ·ï¸ Identified tables: {metadata_summary.get('tables_identified', 0)} items")
print(f"   ðŸ“‚ Scan nodes: {metadata_summary.get('scan_nodes_count', 0)} items")

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨ç¤º
performance_context = liquid_analysis.get('performance_context', {})
print(f"\nâš¡ Performance information:")
print(f"   â±ï¸ Execution time: {performance_context.get('total_time_sec', 0):.1f} seconds")
print(f"   ðŸ’¾ Data read: {performance_context.get('read_gb', 0):.2f}GB")
print(f"   ðŸ“Š Output rows: {performance_context.get('rows_produced', 0):,} rows")
print(f"   ðŸŽ¯ Filter rate: {performance_context.get('data_selectivity', 0):.4f}")

# Output analysis results to file
print(f"\nðŸ’¾ Outputting analysis results to file...")
try:
    saved_files = save_liquid_clustering_analysis(liquid_analysis, "/tmp")
    
    if "error" in saved_files:
        print(f"âŒ File output error: {saved_files['error']}")
    else:
        print(f"âœ… File output completed:")
        for file_type, file_path in saved_files.items():
            if file_type == "json":
                print(f"   ðŸ“„ JSON detailed data: {file_path}")
            elif file_type == "markdown":
                print(f"   ðŸ“ Markdown report: {file_path}")
            elif file_type == "sql":
                print(f"   ðŸ”§ SQL implementation example: {file_path}")
                
except Exception as e:
    print(f"âŒ Error occurred during file output: {str(e)}")

# ã‚µãƒžãƒªãƒ¼æƒ…å ±
summary = liquid_analysis.get('summary', {})
print(f"\nðŸ“‹ Analysis summary:")
print(f"   ðŸ”¬ Analysis method: {summary.get('analysis_method', 'Unknown')}")
print(f"   ðŸ¤– LLM provider: {summary.get('llm_provider', 'Unknown')}")
print(f"   ðŸ“Š Target table count: {summary.get('tables_identified', 0)}")
print(f"   ðŸ“ˆ Extracted column count: Filter({summary.get('total_filter_columns', 0)}) + JOIN({summary.get('total_join_columns', 0)}) + GROUP BY({summary.get('total_groupby_columns', 0)})")

print()

# COMMAND ----------

# ðŸ¤– è¨­å®šã•ã‚ŒãŸLLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æž
provider = LLM_CONFIG["provider"]
if provider == "databricks":
    endpoint_name = LLM_CONFIG["databricks"]["endpoint_name"]
    print(f"ðŸ¤– Starting bottleneck analysis with Databricks Model Serving ({endpoint_name})...")
    print(f"âš ï¸  Model Serving endpoint '{endpoint_name}' is required")
elif provider == "openai":
    model = LLM_CONFIG["openai"]["model"]
    print(f"ðŸ¤– Starting bottleneck analysis with OpenAI ({model})...")
    print("âš ï¸  OpenAI API key is required")
elif provider == "azure_openai":
    deployment = LLM_CONFIG["azure_openai"]["deployment_name"]
    print(f"ðŸ¤– Starting bottleneck analysis with Azure OpenAI ({deployment})...")
    print("âš ï¸  Azure OpenAI API key and endpoint are required")
elif provider == "anthropic":
    model = LLM_CONFIG["anthropic"]["model"]
    print(f"ðŸ¤– Starting bottleneck analysis with Anthropic ({model})...")
    print("âš ï¸  Anthropic API key is required")

print("ðŸ“ Simplifying analysis prompt to reduce timeout risk...")
print()

analysis_result = analyze_bottlenecks_with_llm(extracted_metrics)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸŽ¯ Display of LLM Bottleneck Analysis Results
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Display of detailed analysis results by the configured LLM provider
# MAGIC - Visualization of bottleneck identification and improvement recommendations
# MAGIC - Formatting and readable display of analysis results

# COMMAND ----------

# ðŸ“Š åˆ†æžçµæžœã®è¡¨ç¤º
print("\n" + "=" * 80)
print(f"ðŸŽ¯ ã€SQL Bottleneck Analysis Results by {provider.upper()} LLMã€‘")
print("=" * 80)
print()
print(analysis_result)
print()
print("=" * 80)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸ’¾ Saving Analysis Results and Completion Summary
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Saving LLM analysis results to text files
# MAGIC - Recording basic information of analysis targets
# MAGIC - Displaying overall processing completion summary
# MAGIC - Listing output files

# COMMAND ----------

# ðŸ’¾ åˆ†æžçµæžœã®ä¿å­˜ã¨å®Œäº†ã‚µãƒžãƒªãƒ¼
from datetime import datetime
# output_bottleneck_analysis_result_XXX.txtãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›ã¯å»ƒæ­¢ï¼ˆoptimization_reportã«çµ±åˆï¼‰

# æœ€çµ‚çš„ãªã‚µãƒžãƒªãƒ¼
print("\n" + "ðŸŽ‰" * 20)
print("ðŸ ã€Processing Completion Summaryã€‘")
print("ðŸŽ‰" * 20)
print("âœ… SQL profiler JSON file loading completed")
print(f"âœ… Performance metrics extraction completed")

# LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±ã®å‹•çš„è¡¨ç¤º
try:
    current_provider = LLM_CONFIG.get('provider', 'unknown')
    provider_display_names = {
        'databricks': f"Databricks ({LLM_CONFIG.get('databricks', {}).get('endpoint_name', 'Model Serving')})",
        'openai': f"OpenAI ({LLM_CONFIG.get('openai', {}).get('model', 'GPT-4')})",
        'azure_openai': f"Azure OpenAI ({LLM_CONFIG.get('azure_openai', {}).get('deployment_name', 'GPT-4')})",
        'anthropic': f"Anthropic ({LLM_CONFIG.get('anthropic', {}).get('model', 'Claude')})"
    }
    provider_display = provider_display_names.get(current_provider, f"{current_provider}ï¼ˆæœªçŸ¥ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼‰")
    print(f"âœ… Bottleneck analysis completed by {provider_display}")
except Exception as e:
    print("âœ… LLM bottleneck analysis completed")

print("âœ… Analysis results will be integrated into optimization_report later")
print()
print("ðŸš€ Analysis complete! Please check the results and use them for query optimization.")
print("ðŸŽ‰" * 20)

# COMMAND ----------

# MAGIC %md
# MAGIC # ðŸ”§ SQL Optimization Function Section
# MAGIC
# MAGIC **This section performs SQL query optimization**
# MAGIC
# MAGIC ðŸ“‹ **Optimization Process:**
# MAGIC - Extract original query from profiler data
# MAGIC - Execute query optimization using LLM
# MAGIC - Generate optimization result files
# MAGIC - Prepare for test execution
# MAGIC
# MAGIC âš ï¸ **Prerequisites:** Please complete the main processing section before execution

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸ”§ SQL Optimization Related Function Definitions
# MAGIC
# MAGIC This cell defines the following functions:
# MAGIC - `extract_original_query_from_profiler_data`: Extract original query from profiler data
# MAGIC - `generate_optimized_query_with_llm`: Query optimization based on LLM analysis results
# MAGIC - `save_optimized_sql_files`: Save various optimization result files

# COMMAND ----------

def extract_original_query_from_profiler_data(profiler_data: Dict[str, Any]) -> str:
    """
    ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã‚’æŠ½å‡º
    """
    
    # è¤‡æ•°ã®å ´æ‰€ã‹ã‚‰SQLã‚¯ã‚¨ãƒªã‚’æŽ¢ã™
    query_candidates = []
    
    # 1. query.queryText ã‹ã‚‰æŠ½å‡º
    if 'query' in profiler_data and 'queryText' in profiler_data['query']:
        query_text = profiler_data['query']['queryText']
        if query_text and query_text.strip():
            query_candidates.append(query_text.strip())
    
    # 2. metadata ã‹ã‚‰æŠ½å‡º
    if 'metadata' in profiler_data:
        metadata = profiler_data['metadata']
        for key, value in metadata.items():
            if 'sql' in key.lower() or 'query' in key.lower():
                if isinstance(value, str) and value.strip():
                    query_candidates.append(value.strip())
    
    # 3. graphs ã® metadata ã‹ã‚‰æŠ½å‡º
    if 'graphs' in profiler_data:
        for graph in profiler_data['graphs']:
            nodes = graph.get('nodes', [])
            for node in nodes:
                node_metadata = node.get('metadata', [])
                for meta in node_metadata:
                    if meta.get('key', '').upper() in ['SQL', 'QUERY', 'SQL_TEXT']:
                        value = meta.get('value', '')
                        if value and value.strip():
                            query_candidates.append(value.strip())
    
    # æœ€ã‚‚é•·ã„ã‚¯ã‚¨ãƒªã‚’é¸æŠžï¼ˆé€šå¸¸ã€æœ€ã‚‚å®Œå…¨ãªã‚¯ã‚¨ãƒªï¼‰
    if query_candidates:
        original_query = max(query_candidates, key=len)
        return original_query
    
    return ""

def extract_table_size_estimates_from_plan(profiler_data: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã®æŽ¨å®šã‚µã‚¤ã‚ºæƒ…å ±ã‚’æŠ½å‡º
    
    æ³¨æ„: Databricksã‚¯ã‚¨ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ estimatedSizeInBytes ãŒå«ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€
    ã“ã®æ©Ÿèƒ½ã¯ç¾åœ¨ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã®æŽ¨å®šã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
    
    Args:
        profiler_data: ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿
        
    Returns:
        Dict: ç©ºã®è¾žæ›¸ï¼ˆæ©Ÿèƒ½ç„¡åŠ¹åŒ–ï¼‰
    """
    # Databricksã‚¯ã‚¨ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«estimatedSizeInBytesãŒå«ã¾ã‚Œã¦ã„ãªã„ãŸã‚ç„¡åŠ¹åŒ–
    return {}

def extract_table_name_from_scan_node(node: Dict[str, Any]) -> str:
    """
    ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
    
    Args:
        node: å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã®ãƒŽãƒ¼ãƒ‰
        
    Returns:
        str: ãƒ†ãƒ¼ãƒ–ãƒ«å
    """
    try:
        # è¤‡æ•°ã®æ–¹æ³•ã§ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡ºã‚’è©¦è¡Œ
        
        # 1. node outputã‹ã‚‰ã®æŠ½å‡º
        output = node.get("output", "")
        if output:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: [col1#123, col2#456] table_name
            import re
            table_match = re.search(r'\]\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)', output)
            if table_match:
                return table_match.group(1)
        
        # 2. nodeè©³ç´°ã‹ã‚‰ã®æŠ½å‡º
        details = node.get("details", "")
        if details:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: Location: /path/to/table/name
            location_match = re.search(r'Location:.*?([a-zA-Z_][a-zA-Z0-9_]*)', details)
            if location_match:
                return location_match.group(1)
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: Table: database.table_name
            table_match = re.search(r'Table:\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)', details)
            if table_match:
                return table_match.group(1)
        
        # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®æŠ½å‡º
        metadata = node.get("metadata", [])
        for meta in metadata:
            if meta.get("key") == "table" or meta.get("key") == "relation":
                values = meta.get("values", [])
                if values:
                    return str(values[0])
        
        # 4. nodeåã‹ã‚‰ã®æŽ¨æ¸¬ï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
        node_name = node.get("nodeName", "")
        if "delta" in node_name.lower():
            # Delta Scan ã®å ´åˆã€è©³ç´°æƒ…å ±ã‹ã‚‰æŠ½å‡º
            pass
    
    except Exception as e:
        print(f"âš ï¸ Error in table name extraction: {str(e)}")
    
    return None

def extract_broadcast_table_names(profiler_data: Dict[str, Any], broadcast_nodes: list) -> Dict[str, Any]:
    """
    BROADCASTãƒŽãƒ¼ãƒ‰ã‹ã‚‰é–¢é€£ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
    """
    broadcast_table_info = {
        "broadcast_tables": [],
        "broadcast_table_mapping": {},
        "broadcast_nodes_with_tables": []
    }
    
    # å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã®ã‚°ãƒ©ãƒ•æƒ…å ±ã‚’å–å¾—
    graphs = profiler_data.get('graphs', [])
    if not graphs:
        return broadcast_table_info
    
    # å…¨ãƒŽãƒ¼ãƒ‰ã‚’åŽé›†
    all_nodes = []
    for graph in graphs:
        nodes = graph.get('nodes', [])
        all_nodes.extend(nodes)
    
    # ã‚¨ãƒƒã‚¸æƒ…å ±ã‚’åŽé›†ï¼ˆãƒŽãƒ¼ãƒ‰é–“ã®é–¢ä¿‚ï¼‰
    all_edges = []
    for graph in graphs:
        edges = graph.get('edges', [])
        all_edges.extend(edges)
    
    # å„BROADCASTãƒŽãƒ¼ãƒ‰ã«ã¤ã„ã¦é–¢é€£ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç‰¹å®š
    for broadcast_node in broadcast_nodes:
        broadcast_node_id = broadcast_node.get('node_id', '')
        broadcast_node_name = broadcast_node.get('node_name', '')
        
        # BROADCASTãƒŽãƒ¼ãƒ‰ã‹ã‚‰ç›´æŽ¥ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
        table_names = set()
        
        # 1. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
        metadata = broadcast_node.get('metadata', [])
        for meta in metadata:
            key = meta.get('key', '')
            value = meta.get('value', '')
            values = meta.get('values', [])
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ç¤ºã™ã‚­ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
            if key in ['SCAN_IDENTIFIER', 'TABLE_NAME', 'RELATION']:
                if value:
                    table_names.add(value)
                table_names.update(values)
        
        # 2. ãƒŽãƒ¼ãƒ‰åã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŽ¨å®š
        if 'SCAN' in broadcast_node_name:
            # "Broadcast Scan delta orders" â†’ "orders"
            import re
            table_match = re.search(r'SCAN\s+(?:DELTA|PARQUET|JSON|CSV)?\s*([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)', broadcast_node_name, re.IGNORECASE)
            if table_match:
                table_names.add(table_match.group(1))
        
        # 3. ã‚¨ãƒƒã‚¸æƒ…å ±ã‹ã‚‰é–¢é€£ã™ã‚‹ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰ã‚’ç‰¹å®š
        for edge in all_edges:
            source_id = edge.get('source', '')
            target_id = edge.get('target', '')
            
            # BROADCASTãƒŽãƒ¼ãƒ‰ã«å…¥åŠ›ã•ã‚Œã‚‹ãƒŽãƒ¼ãƒ‰ã‚’æ¤œç´¢
            if target_id == broadcast_node_id:
                # å…¥åŠ›ãƒŽãƒ¼ãƒ‰ãŒã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰ã‹ãƒã‚§ãƒƒã‚¯
                for node in all_nodes:
                    if node.get('id', '') == source_id:
                        node_name = node.get('name', '').upper()
                        if any(keyword in node_name for keyword in ['SCAN', 'FILESCAN']):
                            # ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
                            scan_table_name = extract_table_name_from_scan_node(node)
                            if scan_table_name:
                                table_names.add(scan_table_name)
        
        # 4. åŒã˜ã‚°ãƒ©ãƒ•å†…ã®ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰ã¨ã®é–¢é€£ä»˜ã‘
        for node in all_nodes:
            node_name = node.get('name', '').upper()
            if any(keyword in node_name for keyword in ['SCAN', 'FILESCAN']):
                # ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰ã®åå‰ãŒBROADCASTãƒŽãƒ¼ãƒ‰åã«å«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                scan_table_name = extract_table_name_from_scan_node(node)
                if scan_table_name:
                    # ãƒ†ãƒ¼ãƒ–ãƒ«åã®éƒ¨åˆ†ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
                    if any(part in broadcast_node_name for part in scan_table_name.split('.') if len(part) > 2):
                        table_names.add(scan_table_name)
        
        # çµæžœã‚’è¨˜éŒ²
        table_names_list = list(table_names)
        if table_names_list:
            broadcast_table_info["broadcast_tables"].extend(table_names_list)
            broadcast_table_info["broadcast_table_mapping"][broadcast_node_id] = table_names_list
            
            # BROADCASTãƒŽãƒ¼ãƒ‰æƒ…å ±ã‚’æ‹¡å¼µ
            enhanced_broadcast_node = broadcast_node.copy()
            enhanced_broadcast_node["associated_tables"] = table_names_list
            enhanced_broadcast_node["table_count"] = len(table_names_list)
            broadcast_table_info["broadcast_nodes_with_tables"].append(enhanced_broadcast_node)
    
    # é‡è¤‡ã‚’é™¤åŽ»
    broadcast_table_info["broadcast_tables"] = list(set(broadcast_table_info["broadcast_tables"]))
    
    return broadcast_table_info

def extract_execution_plan_info(profiler_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    JSONãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’æŠ½å‡º
    """
    plan_info = {
        "broadcast_nodes": [],
        "join_nodes": [],
        "scan_nodes": [],
        "shuffle_nodes": [],
        "aggregate_nodes": [],
        "plan_summary": {},
        "broadcast_already_applied": False,
        "join_strategies": [],
        "table_scan_details": {},
        "broadcast_table_info": {}
    }
    
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿè¡Œã‚°ãƒ©ãƒ•æƒ…å ±ã‚’å–å¾—
    graphs = profiler_data.get('graphs', [])
    if not graphs:
        return plan_info
    
    # ã™ã¹ã¦ã®ã‚°ãƒ©ãƒ•ã‹ã‚‰ãƒŽãƒ¼ãƒ‰ã‚’åŽé›†
    all_nodes = []
    for graph_index, graph in enumerate(graphs):
        nodes = graph.get('nodes', [])
        for node in nodes:
            node['graph_index'] = graph_index
            all_nodes.append(node)
    
    # ãƒŽãƒ¼ãƒ‰åˆ†æž
    for node in all_nodes:
        node_name = node.get('name', '').upper()
        node_tag = node.get('tag', '').upper()
        node_metadata = node.get('metadata', [])
        
        # BROADCASTãƒŽãƒ¼ãƒ‰ã®æ¤œå‡º
        if 'BROADCAST' in node_name or 'BROADCAST' in node_tag:
            plan_info["broadcast_already_applied"] = True
            broadcast_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "metadata": []
            }
            
            # BROADCASTã«é–¢é€£ã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                value = meta.get('value', '')
                values = meta.get('values', [])
                
                if any(keyword in key.upper() for keyword in ['BROADCAST', 'BUILD', 'PROBE']):
                    broadcast_info["metadata"].append({
                        "key": key,
                        "value": value,
                        "values": values
                    })
            
            plan_info["broadcast_nodes"].append(broadcast_info)
        
        # JOINãƒŽãƒ¼ãƒ‰ã®æ¤œå‡ºã¨æˆ¦ç•¥åˆ†æž
        elif any(keyword in node_name for keyword in ['JOIN', 'HASH']):
            join_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "join_strategy": "unknown",
                "join_keys": [],
                "join_type": "unknown"
            }
            
            # JOINæˆ¦ç•¥ã®ç‰¹å®š
            if 'BROADCAST' in node_name:
                join_info["join_strategy"] = "broadcast_hash_join"
            elif 'SORT' in node_name and 'MERGE' in node_name:
                join_info["join_strategy"] = "sort_merge_join"
            elif 'HASH' in node_name:
                join_info["join_strategy"] = "shuffle_hash_join"
            elif 'NESTED' in node_name:
                join_info["join_strategy"] = "broadcast_nested_loop_join"
            
            # JOINã‚¿ã‚¤ãƒ—ã®ç‰¹å®š
            if 'INNER' in node_name:
                join_info["join_type"] = "inner"
            elif 'LEFT' in node_name:
                join_info["join_type"] = "left"
            elif 'RIGHT' in node_name:
                join_info["join_type"] = "right"
            elif 'OUTER' in node_name:
                join_info["join_type"] = "outer"
            
            # JOINæ¡ä»¶ã®æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                values = meta.get('values', [])
                
                if key in ['LEFT_KEYS', 'RIGHT_KEYS']:
                    join_info["join_keys"].extend(values)
            
            plan_info["join_nodes"].append(join_info)
            plan_info["join_strategies"].append(join_info["join_strategy"])
        
        # ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰ã®è©³ç´°åˆ†æž
        elif any(keyword in node_name for keyword in ['SCAN', 'FILESCAN']):
            scan_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "table_name": "unknown",
                "file_format": "unknown",
                "pushed_filters": [],
                "output_columns": []
            }
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«åã¨ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                value = meta.get('value', '')
                values = meta.get('values', [])
                
                if key == 'SCAN_IDENTIFIER':
                    scan_info["table_name"] = value
                elif key == 'OUTPUT':
                    scan_info["output_columns"] = values
                elif key == 'PUSHED_FILTERS' or key == 'FILTERS':
                    scan_info["pushed_filters"] = values
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®æŽ¨å®š
            if 'DELTA' in node_name:
                scan_info["file_format"] = "delta"
            elif 'PARQUET' in node_name:
                scan_info["file_format"] = "parquet"
            elif 'JSON' in node_name:
                scan_info["file_format"] = "json"
            elif 'CSV' in node_name:
                scan_info["file_format"] = "csv"
            
            plan_info["scan_nodes"].append(scan_info)
            plan_info["table_scan_details"][scan_info["table_name"]] = scan_info
        
        # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒŽãƒ¼ãƒ‰ã®æ¤œå‡º
        elif any(keyword in node_name for keyword in ['SHUFFLE', 'EXCHANGE']):
            shuffle_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "partition_keys": []
            }
            
            # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±ã®æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                values = meta.get('values', [])
                
                if key in ['PARTITION_EXPRESSIONS', 'PARTITION_KEYS']:
                    shuffle_info["partition_keys"] = values
            
            plan_info["shuffle_nodes"].append(shuffle_info)
        
        # é›†ç´„ãƒŽãƒ¼ãƒ‰ã®æ¤œå‡º
        elif any(keyword in node_name for keyword in ['AGGREGATE', 'GROUP']):
            agg_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "group_keys": [],
                "aggregate_expressions": []
            }
            
            # é›†ç´„æƒ…å ±ã®æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                values = meta.get('values', [])
                
                if key == 'GROUPING_EXPRESSIONS':
                    agg_info["group_keys"] = values
                elif key == 'AGGREGATE_EXPRESSIONS':
                    agg_info["aggregate_expressions"] = values
            
            plan_info["aggregate_nodes"].append(agg_info)
    
    # ãƒ—ãƒ©ãƒ³ã‚µãƒžãƒªãƒ¼ã®ç”Ÿæˆ
    plan_info["plan_summary"] = {
        "total_nodes": len(all_nodes),
        "broadcast_nodes_count": len(plan_info["broadcast_nodes"]),
        "join_nodes_count": len(plan_info["join_nodes"]),
        "scan_nodes_count": len(plan_info["scan_nodes"]),
        "shuffle_nodes_count": len(plan_info["shuffle_nodes"]),
        "aggregate_nodes_count": len(plan_info["aggregate_nodes"]),
        "unique_join_strategies": list(set(plan_info["join_strategies"])),
        "has_broadcast_joins": plan_info["broadcast_already_applied"],
        "tables_scanned": len(plan_info["table_scan_details"])
    }
    
    # BROADCASTãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’æŠ½å‡º
    if plan_info["broadcast_nodes"]:
        broadcast_table_info = extract_broadcast_table_names(profiler_data, plan_info["broadcast_nodes"])
        plan_info["broadcast_table_info"] = broadcast_table_info
        
        # ãƒ—ãƒ©ãƒ³ã‚µãƒžãƒªãƒ¼ã«BROADCASTãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’è¿½åŠ 
        plan_info["plan_summary"]["broadcast_tables"] = broadcast_table_info["broadcast_tables"]
        plan_info["plan_summary"]["broadcast_table_count"] = len(broadcast_table_info["broadcast_tables"])
    
    # å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‹ã‚‰ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæŽ¨å®šæƒ…å ±ã‚’è¿½åŠ ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
    plan_info["table_size_estimates"] = {}  # extract_table_size_estimates_from_plan(profiler_data)
    
    return plan_info

def get_spark_broadcast_threshold() -> float:
    """
    Sparkã®å®Ÿéš›ã®broadcasté–¾å€¤è¨­å®šã‚’å–å¾—
    """
    try:
        # Sparkã®è¨­å®šå€¤ã‚’å–å¾—
        threshold_bytes = spark.conf.get("spark.databricks.optimizer.autoBroadcastJoinThreshold", "31457280")  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30MB
        threshold_mb = float(threshold_bytes) / 1024 / 1024
        return threshold_mb
    except:
        # å–å¾—ã§ããªã„å ´åˆã¯æ¨™æº–çš„ãª30MBã‚’è¿”ã™
        return 30.0

def estimate_uncompressed_size(compressed_size_mb: float, file_format: str = "parquet") -> float:
    """
    åœ§ç¸®ã‚µã‚¤ã‚ºã‹ã‚‰éžåœ§ç¸®ã‚µã‚¤ã‚ºã‚’æŽ¨å®šï¼ˆ3.0å€å›ºå®šï¼‰
    
    æ³¨æ„: å®Ÿéš›ã®estimatedSizeInBytesãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€
    ä¿å®ˆçš„ãª3.0å€åœ§ç¸®çŽ‡ã§çµ±ä¸€ã—ã¦æŽ¨å®šã—ã¾ã™ã€‚
    """
    # ä¿å®ˆçš„ãª3.0å€åœ§ç¸®çŽ‡ã§çµ±ä¸€ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ï¼‰
    compression_ratio = 3.0
    
    return compressed_size_mb * compression_ratio

def analyze_broadcast_feasibility(metrics: Dict[str, Any], original_query: str, plan_info: Dict[str, Any] = None) -> Dict[str, Any]:
    """
    BROADCASTãƒ’ãƒ³ãƒˆã®é©ç”¨å¯èƒ½æ€§ã‚’åˆ†æžï¼ˆæ­£ç¢ºãª30MBé–¾å€¤é©ç”¨ï¼‰
    """
    broadcast_analysis = {
        "is_join_query": False,
        "broadcast_candidates": [],
        "recommendations": [],
        "feasibility": "not_applicable",
        "reasoning": [],
        "spark_threshold_mb": get_spark_broadcast_threshold(),
        "compression_analysis": {},
        "detailed_size_analysis": [],
        "execution_plan_analysis": {},
        "existing_broadcast_nodes": [],
        "already_optimized": False,
        "broadcast_applied_tables": []
    }
    
    # ã‚¯ã‚¨ãƒªã«JOINãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    query_upper = original_query.upper()
    join_types = ['JOIN', 'INNER JOIN', 'LEFT JOIN', 'RIGHT JOIN', 'LEFT OUTER JOIN', 'RIGHT OUTER JOIN', 'SEMI JOIN', 'ANTI JOIN']
    has_join = any(join_type in query_upper for join_type in join_types)
    
    if not has_join:
        broadcast_analysis["reasoning"].append("JOINã‚¯ã‚¨ãƒªã§ã¯ãªã„ãŸã‚ã€BROADCASTãƒ’ãƒ³ãƒˆã¯é©ç”¨ä¸å¯")
        return broadcast_analysis
    
    broadcast_analysis["is_join_query"] = True
    broadcast_analysis["reasoning"].append(f"Spark BROADCASTé–¾å€¤: {broadcast_analysis['spark_threshold_mb']:.1f}MBï¼ˆéžåœ§ç¸®ï¼‰")
    
    # å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã®åˆ†æž
    if plan_info:
        plan_summary = plan_info.get("plan_summary", {})
        broadcast_nodes = plan_info.get("broadcast_nodes", [])
        join_nodes = plan_info.get("join_nodes", [])
        table_scan_details = plan_info.get("table_scan_details", {})
        table_size_estimates = plan_info.get("table_size_estimates", {})
        
        # æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã®è¨˜éŒ²
        broadcast_analysis["existing_broadcast_nodes"] = broadcast_nodes
        broadcast_analysis["already_optimized"] = len(broadcast_nodes) > 0
        
        # ãƒ—ãƒ©ãƒ³åˆ†æžçµæžœã®è¨˜éŒ²
        broadcast_analysis["execution_plan_analysis"] = {
            "has_broadcast_joins": plan_summary.get("has_broadcast_joins", False),
            "unique_join_strategies": plan_summary.get("unique_join_strategies", []),
            "broadcast_nodes_count": len(broadcast_nodes),
            "join_nodes_count": len(join_nodes),
            "scan_nodes_count": plan_summary.get("scan_nodes_count", 0),
            "shuffle_nodes_count": plan_summary.get("shuffle_nodes_count", 0),
            "tables_in_plan": list(table_scan_details.keys())
        }
        
        # æ—¢ã«BROADCASTãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹å ´åˆã®è©³ç´°è¨˜éŒ²
        if broadcast_nodes:
            broadcast_analysis["reasoning"].append(f"âœ… å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã§æ—¢ã«BROADCAST JOINãŒé©ç”¨æ¸ˆã¿: {len(broadcast_nodes)}å€‹ã®ãƒŽãƒ¼ãƒ‰")
            
            # BROADCASTãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’å–å¾—
            broadcast_table_info = plan_info.get("broadcast_table_info", {})
            broadcast_tables = broadcast_table_info.get("broadcast_tables", [])
            
            if broadcast_tables:
                broadcast_analysis["reasoning"].append(f"ðŸ“‹ BROADCASTã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(broadcast_tables)}")
                broadcast_analysis["broadcast_applied_tables"] = broadcast_tables
                
                # å„BROADCASTãƒŽãƒ¼ãƒ‰ã®è©³ç´°
                broadcast_nodes_with_tables = broadcast_table_info.get("broadcast_nodes_with_tables", [])
                for i, node in enumerate(broadcast_nodes_with_tables[:3]):  # æœ€å¤§3å€‹ã¾ã§è¡¨ç¤º
                    node_name_short = node['node_name'][:50] + "..." if len(node['node_name']) > 50 else node['node_name']
                    associated_tables = node.get('associated_tables', [])
                    if associated_tables:
                        broadcast_analysis["reasoning"].append(f"  â€¢ BROADCAST Node {i+1}: {node_name_short}")
                        broadcast_analysis["reasoning"].append(f"    â””â”€ ãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(associated_tables)}")
                    else:
                        broadcast_analysis["reasoning"].append(f"  â€¢ BROADCAST Node {i+1}: {node_name_short} (ãƒ†ãƒ¼ãƒ–ãƒ«åæœªç‰¹å®š)")
            else:
                # BROADCASTãƒŽãƒ¼ãƒ‰ã¯å­˜åœ¨ã™ã‚‹ãŒãƒ†ãƒ¼ãƒ–ãƒ«åãŒç‰¹å®šã§ããªã„å ´åˆ
                for i, node in enumerate(broadcast_nodes[:3]):  # æœ€å¤§3å€‹ã¾ã§è¡¨ç¤º
                    broadcast_analysis["reasoning"].append(f"  â€¢ BROADCAST Node {i+1}: {node['node_name'][:50]}... (ãƒ†ãƒ¼ãƒ–ãƒ«åè§£æžä¸­)")
        else:
            # BROADCASTæœªé©ç”¨ã ãŒã€JOINãŒå­˜åœ¨ã™ã‚‹å ´åˆ
            if join_nodes:
                join_strategies = set(node["join_strategy"] for node in join_nodes)
                broadcast_analysis["reasoning"].append(f"ðŸ” ç¾åœ¨ã®JOINæˆ¦ç•¥: {', '.join(join_strategies)}")
                broadcast_analysis["reasoning"].append("ðŸ’¡ BROADCASTæœ€é©åŒ–ã®æ©Ÿä¼šã‚’æ¤œè¨Žä¸­...")
    else:
        broadcast_analysis["reasoning"].append("âš ï¸ å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŽ¨å®šã«åŸºã¥ãåˆ†æžã‚’å®Ÿè¡Œ")
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±ã‚’å–å¾—
    overall_metrics = metrics.get('overall_metrics', {})
    node_metrics = metrics.get('node_metrics', [])
    
    # ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’æŠ½å‡º
    scan_nodes = []
    total_compressed_bytes = 0
    total_rows_all_tables = 0
    
    for node in node_metrics:
        node_name = node.get('name', '').upper()
        if any(keyword in node_name for keyword in ['SCAN', 'FILESCAN', 'PARQUET', 'DELTA']):
            key_metrics = node.get('key_metrics', {})
            rows_num = key_metrics.get('rowsNum', 0)
            duration_ms = key_metrics.get('durationMs', 0)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®æŽ¨å®šï¼ˆãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’å„ªå…ˆï¼‰
            file_format = "parquet"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            table_name_from_plan = "unknown"
            
            # ãƒ—ãƒ©ãƒ³æƒ…å ±ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã¨ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’å–å¾—
            if plan_info and plan_info.get("table_scan_details"):
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©³ç´°ãªãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
                node_metadata = node.get('metadata', [])
                for meta in node_metadata:
                    meta_key = meta.get('key', '')
                    meta_value = meta.get('value', '')
                    if meta_key in ['SCAN_IDENTIFIER', 'SCAN_TABLE', 'TABLE_NAME'] and meta_value:
                        # ãƒ—ãƒ©ãƒ³ã®è©³ç´°ã¨ç…§åˆ
                        for plan_table, scan_detail in plan_info["table_scan_details"].items():
                            if meta_value in plan_table or plan_table in meta_value:
                                table_name_from_plan = plan_table
                                if scan_detail["file_format"] != "unknown":
                                    file_format = scan_detail["file_format"]
                                break
                        break
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒŽãƒ¼ãƒ‰åã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’æŽ¨å®š
            if file_format == "parquet":  # ã¾ã ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å ´åˆ
                if "DELTA" in node_name:
                    file_format = "delta"
                elif "PARQUET" in node_name:
                    file_format = "parquet"
                elif "JSON" in node_name:
                    file_format = "json"
                elif "CSV" in node_name:
                    file_format = "csv"
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æŽ¨å®šã®ã¿ä½¿ç”¨ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ï¼‰
            estimated_compressed_mb = 0
            estimated_uncompressed_mb = 0
            size_source = "metrics_estimation"
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æŽ¨å®š
            total_read_bytes = overall_metrics.get('read_bytes', 0)
            total_rows = overall_metrics.get('rows_read_count', 0)
            
            if total_rows > 0 and total_read_bytes > 0 and rows_num > 0:
                # å…¨ä½“ã®èª­ã¿è¾¼ã¿é‡ã‹ã‚‰ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®å‰²åˆã‚’è¨ˆç®—
                table_ratio = rows_num / total_rows
                estimated_compressed_bytes = total_read_bytes * table_ratio
                estimated_compressed_mb = estimated_compressed_bytes / 1024 / 1024
                 
                # éžåœ§ç¸®ã‚µã‚¤ã‚ºã‚’æŽ¨å®š
                estimated_uncompressed_mb = estimate_uncompressed_size(estimated_compressed_mb, file_format)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è¡Œæ•°ãƒ™ãƒ¼ã‚¹ã®æŽ¨å®šï¼ˆä¿å®ˆçš„ï¼‰
                # å¹³å‡è¡Œã‚µã‚¤ã‚ºã‚’æŽ¨å®šï¼ˆéžåœ§ç¸®ï¼‰
                if total_rows > 0 and total_read_bytes > 0:
                    # å…¨ä½“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åœ§ç¸®å¾Œã®å¹³å‡è¡Œã‚µã‚¤ã‚ºã‚’è¨ˆç®—
                    compressed_avg_row_size = total_read_bytes / total_rows
                    # åœ§ç¸®çŽ‡ã‚’è€ƒæ…®ã—ã¦éžåœ§ç¸®ã‚µã‚¤ã‚ºã‚’æŽ¨å®š
                    uncompressed_avg_row_size = compressed_avg_row_size * estimate_uncompressed_size(1.0, file_format)
                else:
                    # å®Œå…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ä¸€èˆ¬çš„ãªéžåœ§ç¸®è¡Œã‚µã‚¤ã‚ºï¼ˆ1KBï¼‰
                    uncompressed_avg_row_size = 1024
                
                estimated_compressed_mb = (rows_num * compressed_avg_row_size) / 1024 / 1024 if 'compressed_avg_row_size' in locals() else 0
                estimated_uncompressed_mb = (rows_num * uncompressed_avg_row_size) / 1024 / 1024
            
            # æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯
            is_already_broadcasted = False
            if plan_info and plan_info.get("broadcast_nodes"):
                for broadcast_node in plan_info["broadcast_nodes"]:
                    # ãƒ†ãƒ¼ãƒ–ãƒ«åã®éƒ¨åˆ†ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
                    broadcast_node_name = broadcast_node["node_name"]
                    if (table_name_from_plan != "unknown" and 
                        any(part in broadcast_node_name for part in table_name_from_plan.split('.') if len(part) > 3)):
                        is_already_broadcasted = True
                        break
                    # ãƒŽãƒ¼ãƒ‰åã§ã®ç…§åˆ
                    elif any(part in broadcast_node_name for part in node_name.split() if len(part) > 3):
                        is_already_broadcasted = True
                        break

            scan_info = {
                "node_name": node_name,
                "table_name_from_plan": table_name_from_plan,
                "rows": rows_num,
                "duration_ms": duration_ms,
                "estimated_compressed_mb": estimated_compressed_mb,
                "estimated_uncompressed_mb": estimated_uncompressed_mb,
                "file_format": file_format,
                "compression_ratio": 3.0,  # å›ºå®š3.0å€åœ§ç¸®çŽ‡
                "node_id": node.get('node_id', ''),
                "is_already_broadcasted": is_already_broadcasted,
                "size_estimation_source": size_source,
                "size_confidence": "medium"  # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æŽ¨å®šã®ãŸã‚ä¸­ç¨‹åº¦ä¿¡é ¼åº¦
            }
            scan_nodes.append(scan_info)
            
            total_compressed_bytes += estimated_compressed_bytes if 'estimated_compressed_bytes' in locals() else 0
            total_rows_all_tables += rows_num
    
    # BROADCASTå€™è£œã®åˆ¤å®šï¼ˆ30MBé–¾å€¤ä½¿ç”¨ï¼‰
    broadcast_threshold_mb = broadcast_analysis["spark_threshold_mb"]  # å®Ÿéš›ã®Sparkè¨­å®šå€¤
    broadcast_safe_mb = broadcast_threshold_mb * 0.8  # å®‰å…¨ãƒžãƒ¼ã‚¸ãƒ³ï¼ˆ80%ï¼‰
    broadcast_max_mb = broadcast_threshold_mb * 10    # æ˜Žã‚‰ã‹ã«å¤§ãã™ãŽã‚‹é–¾å€¤
    
    small_tables = []
    large_tables = []
    marginal_tables = []
    
    # åœ§ç¸®åˆ†æžã®è¨˜éŒ²
    broadcast_analysis["compression_analysis"] = {
        "total_compressed_gb": total_compressed_bytes / 1024 / 1024 / 1024 if total_compressed_bytes > 0 else 0,
        "total_rows": total_rows_all_tables,
        "avg_compression_ratio": 0
    }
    
    for scan in scan_nodes:
        uncompressed_size_mb = scan["estimated_uncompressed_mb"]
        compressed_size_mb = scan["estimated_compressed_mb"]
        
        # è©³ç´°ã‚µã‚¤ã‚ºåˆ†æžã®è¨˜éŒ²
        table_display_name = scan.get("table_name_from_plan", scan["node_name"])
        is_already_broadcasted = scan.get("is_already_broadcasted", False)
        
        size_analysis = {
            "table": table_display_name,
            "node_name": scan["node_name"],
            "rows": scan["rows"],
            "compressed_mb": compressed_size_mb,
            "uncompressed_mb": uncompressed_size_mb,
            "file_format": scan["file_format"],
            "compression_ratio": scan["compression_ratio"],
            "broadcast_decision": "",
            "decision_reasoning": "",
            "is_already_broadcasted": is_already_broadcasted
        }
        
        # 30MBé–¾å€¤ã§ã®åˆ¤å®šï¼ˆéžåœ§ç¸®ã‚µã‚¤ã‚ºï¼‰- æ—¢å­˜é©ç”¨çŠ¶æ³ã‚’è€ƒæ…®
        if is_already_broadcasted:
            # æ—¢ã«BROADCASTãŒé©ç”¨æ¸ˆã¿
            small_tables.append(scan)  # çµ±è¨ˆç›®çš„ã§è¨˜éŒ²
            size_analysis["broadcast_decision"] = "already_applied"
            size_analysis["decision_reasoning"] = f"æ—¢ã«BROADCASTé©ç”¨æ¸ˆã¿ï¼ˆæŽ¨å®šã‚µã‚¤ã‚º: éžåœ§ç¸®{uncompressed_size_mb:.1f}MBï¼‰"
            broadcast_analysis["broadcast_candidates"].append({
                "table": table_display_name,
                "estimated_uncompressed_mb": uncompressed_size_mb,
                "estimated_compressed_mb": compressed_size_mb,
                "rows": scan["rows"],
                "file_format": scan["file_format"],
                "compression_ratio": scan["compression_ratio"],
                "broadcast_feasible": True,
                "confidence": "confirmed",
                "status": "already_applied",
                "reasoning": f"å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã§æ—¢ã«BROADCASTé©ç”¨ç¢ºèªæ¸ˆã¿ï¼ˆæŽ¨å®šã‚µã‚¤ã‚º: éžåœ§ç¸®{uncompressed_size_mb:.1f}MBã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æŽ¨å®šï¼‰"
            })
        elif uncompressed_size_mb <= broadcast_safe_mb and scan["rows"] > 0:
            # å®‰å…¨ãƒžãƒ¼ã‚¸ãƒ³å†…ï¼ˆ24MBä»¥ä¸‹ï¼‰- å¼·ãæŽ¨å¥¨
            small_tables.append(scan)
            size_analysis["broadcast_decision"] = "strongly_recommended"
            size_analysis["decision_reasoning"] = f"éžåœ§ç¸®{uncompressed_size_mb:.1f}MB â‰¤ å®‰å…¨é–¾å€¤{broadcast_safe_mb:.1f}MB"
            broadcast_analysis["broadcast_candidates"].append({
                "table": table_display_name,
                "estimated_uncompressed_mb": uncompressed_size_mb,
                "estimated_compressed_mb": compressed_size_mb,
                "rows": scan["rows"],
                "file_format": scan["file_format"],
                "compression_ratio": scan["compression_ratio"],
                "broadcast_feasible": True,
                "confidence": "high",
                "status": "new_recommendation",
                "reasoning": f"éžåœ§ç¸®æŽ¨å®šã‚µã‚¤ã‚º {uncompressed_size_mb:.1f}MBï¼ˆå®‰å…¨é–¾å€¤ {broadcast_safe_mb:.1f}MB ä»¥ä¸‹ï¼‰ã§BROADCASTå¼·ãæŽ¨å¥¨ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æŽ¨å®šã€3.0å€åœ§ç¸®çŽ‡ï¼‰"
            })
        elif uncompressed_size_mb <= broadcast_threshold_mb and scan["rows"] > 0:
            # é–¾å€¤å†…ã ãŒå®‰å…¨ãƒžãƒ¼ã‚¸ãƒ³ã¯è¶…éŽï¼ˆ24-30MBï¼‰- æ¡ä»¶ä»˜ãæŽ¨å¥¨
            marginal_tables.append(scan)
            size_analysis["broadcast_decision"] = "conditionally_recommended"
            size_analysis["decision_reasoning"] = f"éžåœ§ç¸®{uncompressed_size_mb:.1f}MB â‰¤ é–¾å€¤{broadcast_threshold_mb:.1f}MBï¼ˆå®‰å…¨ãƒžãƒ¼ã‚¸ãƒ³è¶…éŽï¼‰"
            broadcast_analysis["broadcast_candidates"].append({
                "table": table_display_name,
                "estimated_uncompressed_mb": uncompressed_size_mb,
                "estimated_compressed_mb": compressed_size_mb,
                "rows": scan["rows"],
                "file_format": scan["file_format"],
                "compression_ratio": scan["compression_ratio"],
                "broadcast_feasible": True,
                "confidence": "medium",
                "status": "new_recommendation",
                "reasoning": f"éžåœ§ç¸®æŽ¨å®šã‚µã‚¤ã‚º {uncompressed_size_mb:.1f}MBï¼ˆé–¾å€¤ {broadcast_threshold_mb:.1f}MB ä»¥ä¸‹ã ãŒå®‰å…¨ãƒžãƒ¼ã‚¸ãƒ³ {broadcast_safe_mb:.1f}MB è¶…éŽï¼‰ã§æ¡ä»¶ä»˜ãBROADCASTæŽ¨å¥¨ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æŽ¨å®šã€3.0å€åœ§ç¸®çŽ‡ï¼‰"
            })
        elif uncompressed_size_mb > broadcast_max_mb:
            # æ˜Žã‚‰ã‹ã«å¤§ãã™ãŽã‚‹ï¼ˆ300MBè¶…ï¼‰
            large_tables.append(scan)
            size_analysis["broadcast_decision"] = "not_recommended"
            size_analysis["decision_reasoning"] = f"éžåœ§ç¸®{uncompressed_size_mb:.1f}MB > æœ€å¤§é–¾å€¤{broadcast_max_mb:.1f}MB"
            broadcast_analysis["reasoning"].append(f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_display_name}: éžåœ§ç¸®{uncompressed_size_mb:.1f}MB - BROADCASTä¸å¯ï¼ˆ>{broadcast_max_mb:.1f}MBï¼‰")
        else:
            # ä¸­é–“ã‚µã‚¤ã‚ºã®ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ30-300MBï¼‰
            large_tables.append(scan)
            size_analysis["broadcast_decision"] = "not_recommended"
            size_analysis["decision_reasoning"] = f"éžåœ§ç¸®{uncompressed_size_mb:.1f}MB > é–¾å€¤{broadcast_threshold_mb:.1f}MB"
            broadcast_analysis["reasoning"].append(f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_display_name}: éžåœ§ç¸®{uncompressed_size_mb:.1f}MB - BROADCASTéžæŽ¨å¥¨ï¼ˆ>{broadcast_threshold_mb:.1f}MBé–¾å€¤ï¼‰")
        
        broadcast_analysis["detailed_size_analysis"].append(size_analysis)
    
    # åœ§ç¸®åˆ†æžã‚µãƒžãƒªãƒ¼ã®æ›´æ–°
    if scan_nodes:
        total_uncompressed_mb = sum(scan["estimated_uncompressed_mb"] for scan in scan_nodes)
        total_compressed_mb = sum(scan["estimated_compressed_mb"] for scan in scan_nodes)
        if total_compressed_mb > 0:
            broadcast_analysis["compression_analysis"]["avg_compression_ratio"] = total_uncompressed_mb / total_compressed_mb
        broadcast_analysis["compression_analysis"]["total_uncompressed_mb"] = total_uncompressed_mb
        broadcast_analysis["compression_analysis"]["total_compressed_mb"] = total_compressed_mb
    
    # ç·ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é‡ã¨ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆåœ§ç¸®ãƒ™ãƒ¼ã‚¹ï¼‰
    total_read_gb = overall_metrics.get('read_bytes', 0) / 1024 / 1024 / 1024
    estimated_total_compressed_mb = sum(scan["estimated_compressed_mb"] for scan in scan_nodes)
    
    if estimated_total_compressed_mb > 0:
        size_ratio = (total_read_gb * 1024) / estimated_total_compressed_mb
        if size_ratio > 3 or size_ratio < 0.3:
            broadcast_analysis["reasoning"].append(f"æŽ¨å®šåœ§ç¸®ã‚µã‚¤ã‚º({estimated_total_compressed_mb:.1f}MB)ã¨å®Ÿèª­ã¿è¾¼ã¿é‡({total_read_gb:.1f}GB)ã«ä¹–é›¢ã‚ã‚Š - ã‚µã‚¤ã‚ºæŽ¨å®šã«æ³¨æ„")
        else:
            broadcast_analysis["reasoning"].append(f"ã‚µã‚¤ã‚ºæŽ¨å®šæ•´åˆæ€§: æŽ¨å®šåœ§ç¸®{estimated_total_compressed_mb:.1f}MB vs å®Ÿéš›{total_read_gb:.1f}GBï¼ˆæ¯”çŽ‡:{size_ratio:.2f}ï¼‰")
    
    # BROADCASTæŽ¨å¥¨äº‹é …ã®ç”Ÿæˆï¼ˆ30MBé–¾å€¤å¯¾å¿œã€æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã‚’è€ƒæ…®ï¼‰
    total_broadcast_candidates = len(small_tables) + len(marginal_tables)
    total_tables = len(scan_nodes)
    
    if small_tables or marginal_tables:
        if large_tables:
            # æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã‚’è€ƒæ…®ã—ãŸåˆ¤å®š
            if broadcast_analysis["already_optimized"]:
                broadcast_analysis["feasibility"] = "already_optimized_with_improvements"
                broadcast_analysis["recommendations"] = [
                    f"âœ… æ—¢ã«BROADCAST JOINé©ç”¨æ¸ˆã¿ - è¿½åŠ æ”¹å–„ã®æ¤œè¨Ž",
                    f"ðŸŽ¯ è¿½åŠ æœ€é©åŒ–ãƒ†ãƒ¼ãƒ–ãƒ«: {total_broadcast_candidates}å€‹ï¼ˆå…¨{total_tables}å€‹ä¸­ï¼‰",
                    f"  âœ… å¼·ãæŽ¨å¥¨: {len(small_tables)}å€‹ï¼ˆå®‰å…¨é–¾å€¤{broadcast_safe_mb:.1f}MBä»¥ä¸‹ï¼‰",
                    f"  âš ï¸ æ¡ä»¶ä»˜ãæŽ¨å¥¨: {len(marginal_tables)}å€‹ï¼ˆé–¾å€¤{broadcast_threshold_mb:.1f}MBä»¥ä¸‹ã€è¦æ³¨æ„ï¼‰",
                    f"  âŒ éžæŽ¨å¥¨: {len(large_tables)}å€‹ï¼ˆé–¾å€¤è¶…éŽï¼‰"
                ]
            else:
                broadcast_analysis["feasibility"] = "recommended"
                broadcast_analysis["recommendations"] = [
                    f"ðŸŽ¯ BROADCASTæŽ¨å¥¨ãƒ†ãƒ¼ãƒ–ãƒ«: {total_broadcast_candidates}å€‹ï¼ˆå…¨{total_tables}å€‹ä¸­ï¼‰",
                    f"  âœ… å¼·ãæŽ¨å¥¨: {len(small_tables)}å€‹ï¼ˆå®‰å…¨é–¾å€¤{broadcast_safe_mb:.1f}MBä»¥ä¸‹ï¼‰",
                    f"  âš ï¸ æ¡ä»¶ä»˜ãæŽ¨å¥¨: {len(marginal_tables)}å€‹ï¼ˆé–¾å€¤{broadcast_threshold_mb:.1f}MBä»¥ä¸‹ã€è¦æ³¨æ„ï¼‰",
                    f"  âŒ éžæŽ¨å¥¨: {len(large_tables)}å€‹ï¼ˆé–¾å€¤è¶…éŽï¼‰"
                ]
        else:
            # å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå°ã•ã„å ´åˆ
            if broadcast_analysis["already_optimized"]:
                broadcast_analysis["feasibility"] = "already_optimized_complete"
                broadcast_analysis["recommendations"] = [
                    f"âœ… æ—¢ã«BROADCAST JOINé©ç”¨æ¸ˆã¿ - æœ€é©åŒ–å®Œäº†",
                    f"ðŸŽ¯ å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ{total_tables}å€‹ï¼‰ãŒBROADCASTé–¾å€¤ä»¥ä¸‹ã§é©åˆ‡ã«å‡¦ç†æ¸ˆã¿",
                    f"  âœ… å¼·ãæŽ¨å¥¨: {len(small_tables)}å€‹",
                    f"  âš ï¸ æ¡ä»¶ä»˜ãæŽ¨å¥¨: {len(marginal_tables)}å€‹",
                    "ðŸ“‹ ç¾åœ¨ã®è¨­å®šãŒæœ€é©ã§ã™"
                ]
            else:
                broadcast_analysis["feasibility"] = "all_small"
                broadcast_analysis["recommendations"] = [
                    f"ðŸŽ¯ å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ{total_tables}å€‹ï¼‰ãŒBROADCASTé–¾å€¤ä»¥ä¸‹",
                    f"  âœ… å¼·ãæŽ¨å¥¨: {len(small_tables)}å€‹",
                    f"  âš ï¸ æ¡ä»¶ä»˜ãæŽ¨å¥¨: {len(marginal_tables)}å€‹",
                    "ðŸ“‹ æœ€å°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å„ªå…ˆçš„ã«BROADCASTã™ã‚‹ã“ã¨ã‚’æŽ¨å¥¨"
                ]
        
        # å…·ä½“çš„ãªBROADCASTå€™è£œã®è©³ç´°
        for small_table in small_tables:
            broadcast_analysis["recommendations"].append(
                f"ðŸ”¹ BROADCAST({small_table['node_name']}) - éžåœ§ç¸®{small_table['estimated_uncompressed_mb']:.1f}MBï¼ˆåœ§ç¸®{small_table['estimated_compressed_mb']:.1f}MBã€{small_table['file_format']}ã€åœ§ç¸®çŽ‡{small_table['compression_ratio']:.1f}xï¼‰"
            )
        
        for marginal_table in marginal_tables:
            broadcast_analysis["recommendations"].append(
                f"ðŸ”¸ BROADCAST({marginal_table['node_name']}) - éžåœ§ç¸®{marginal_table['estimated_uncompressed_mb']:.1f}MBï¼ˆæ¡ä»¶ä»˜ãã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¦æ³¨æ„ï¼‰"
            )
            
    elif large_tables:
        broadcast_analysis["feasibility"] = "not_recommended"
        broadcast_analysis["recommendations"] = [
            f"âŒ å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ{len(large_tables)}å€‹ï¼‰ãŒ30MBé–¾å€¤è¶…éŽã®ãŸã‚BROADCASTéžæŽ¨å¥¨",
            f"ðŸ“Š æœ€å°ãƒ†ãƒ¼ãƒ–ãƒ«ã§ã‚‚éžåœ§ç¸®{min(scan['estimated_uncompressed_mb'] for scan in large_tables):.1f}MB",
            "ðŸ”§ ä»£æ›¿æœ€é©åŒ–æ‰‹æ³•ã‚’æŽ¨å¥¨:",
            "  â€¢ Liquid Clusteringå®Ÿè£…",
            "  â€¢ ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°",
            "  â€¢ ã‚¯ã‚¨ãƒªæœ€é©åŒ–ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ç­‰ï¼‰",
            "  â€¢ spark.databricks.optimizer.autoBroadcastJoinThresholdè¨­å®šå€¤ã®èª¿æ•´æ¤œè¨Ž"
        ]
    else:
        broadcast_analysis["feasibility"] = "insufficient_data"
        broadcast_analysis["recommendations"] = [
            "âš ï¸ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€æ‰‹å‹•ã§ã®ã‚µã‚¤ã‚ºç¢ºèªãŒå¿…è¦",
            "ðŸ“‹ ä»¥ä¸‹ã®ã‚³ãƒžãƒ³ãƒ‰ã§ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºã‚’ç¢ºèª:",
            "  â€¢ DESCRIBE DETAIL table_name",
            "  â€¢ SELECT COUNT(*) FROM table_name",
            "  â€¢ SHOW TABLE EXTENDED LIKE 'table_name'"
        ]
    
    # 30MBé–¾å€¤ã«ãƒ’ãƒƒãƒˆã™ã‚‹ç‰¹åˆ¥ãªã‚±ãƒ¼ã‚¹åˆ†æžï¼ˆsmall_tables + marginal_tables ã‚’è€ƒæ…®ï¼‰
    all_30mb_candidates = small_tables + marginal_tables  # 30MBä»¥ä¸‹ã®å…¨å€™è£œ
    
    if all_30mb_candidates:
        broadcast_analysis["30mb_hit_analysis"] = {
            "has_30mb_candidates": True,
            "candidate_count": len(all_30mb_candidates),
            "small_tables_count": len(small_tables),  # 24MBä»¥ä¸‹ï¼ˆå¼·ãæŽ¨å¥¨ï¼‰
            "marginal_tables_count": len(marginal_tables),  # 24-30MBï¼ˆæ¡ä»¶ä»˜ãæŽ¨å¥¨ï¼‰
            "smallest_table_mb": min(scan["estimated_uncompressed_mb"] for scan in all_30mb_candidates),
            "largest_candidate_mb": max(scan["estimated_uncompressed_mb"] for scan in all_30mb_candidates),
            "total_candidate_size_mb": sum(scan["estimated_uncompressed_mb"] for scan in all_30mb_candidates),
            "recommended_broadcast_table": all_30mb_candidates[0]["node_name"] if all_30mb_candidates else None,
            "memory_impact_estimation": f"{sum(scan['estimated_uncompressed_mb'] for scan in all_30mb_candidates):.1f}MB ãŒãƒ¯ãƒ¼ã‚«ãƒ¼ãƒŽãƒ¼ãƒ‰ã«ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ"
        }
        
        # æœ€é©ãªBROADCASTå€™è£œã®ç‰¹å®šï¼ˆå…¨30MBå€™è£œã‹ã‚‰é¸æŠžï¼‰
        if len(all_30mb_candidates) > 1:
            optimal_candidate = min(all_30mb_candidates, key=lambda x: x["estimated_uncompressed_mb"])
            broadcast_analysis["30mb_hit_analysis"]["optimal_candidate"] = {
                "table": optimal_candidate["node_name"],
                "size_mb": optimal_candidate["estimated_uncompressed_mb"],
                "rows": optimal_candidate["rows"],
                "reasoning": f"æœ€å°ã‚µã‚¤ã‚º{optimal_candidate['estimated_uncompressed_mb']:.1f}MBã§æœ€ã‚‚åŠ¹çŽ‡çš„"
            }
        
        # 30MBé–¾å€¤å†…ã®è©³ç´°åˆ†é¡žæƒ…å ±ã‚’è¿½åŠ 
        broadcast_analysis["30mb_hit_analysis"]["size_classification"] = {
            "safe_zone_tables": len(small_tables),  # 0-24MBï¼ˆå®‰å…¨ãƒžãƒ¼ã‚¸ãƒ³å†…ï¼‰
            "caution_zone_tables": len(marginal_tables),  # 24-30MBï¼ˆè¦æ³¨æ„ï¼‰
            "safe_zone_description": "24MBä»¥ä¸‹ï¼ˆå¼·ãæŽ¨å¥¨ã€å®‰å…¨ãƒžãƒ¼ã‚¸ãƒ³å†…ï¼‰",
            "caution_zone_description": "24-30MBï¼ˆæ¡ä»¶ä»˜ãæŽ¨å¥¨ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¦æ³¨æ„ï¼‰"
        }
    else:
        broadcast_analysis["30mb_hit_analysis"] = {
            "has_30mb_candidates": False,
            "reason": f"å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ãŒ30MBé–¾å€¤ã‚’è¶…éŽï¼ˆæœ€å°: {min(scan['estimated_uncompressed_mb'] for scan in scan_nodes):.1f}MBï¼‰" if scan_nodes else "ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ãªã—"
        }
    
    return broadcast_analysis

def extract_structured_physical_plan(physical_plan: str) -> Dict[str, Any]:
    """
    Structured extraction of important information only from Physical Plan (countermeasure for token limits)
    
    Args:
        physical_plan: Complete text of Physical Plan
    
    Returns:
        Dict: Structured important information
    """
    import re
    
    extracted = {
        "joins": [],           # JOINæƒ…å ±ï¼ˆç¨®é¡žã€æ¡ä»¶ã€çµ±è¨ˆï¼‰
        "scans": [],          # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆã‚µã‚¤ã‚ºã€è¡Œæ•°ï¼‰  
        "exchanges": [],      # ãƒ‡ãƒ¼ã‚¿ç§»å‹•ï¼ˆShuffleã€Broadcastï¼‰
        "aggregates": [],     # é›†ç´„å‡¦ç†ï¼ˆGROUP BYã€SUMç­‰ï¼‰
        "filters": [],        # ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã¨é¸æŠžçŽ‡
        "photon_usage": {},   # Photonåˆ©ç”¨çŠ¶æ³
        "bottlenecks": [],    # ç‰¹å®šã•ã‚ŒãŸãƒœãƒˆãƒ«ãƒãƒƒã‚¯
        "statistics": {},     # æ•°å€¤çµ±è¨ˆã‚µãƒžãƒªãƒ¼
        "total_size": len(physical_plan),
        "extraction_summary": ""
    }
    
    try:
        lines = physical_plan.split('\n')
        join_count = scan_count = exchange_count = 0
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # JOINæƒ…å ±ã®æŠ½å‡ºï¼ˆå¾“æ¥å½¢å¼ + Photonå½¢å¼å®Œå…¨å¯¾å¿œï¼‰
            # å¾“æ¥ã®Spark JOINå½¢å¼ï¼ˆStatisticsä»˜ãï¼‰
            join_match = re.search(r'(\w*Join)\s+([^,\n]+).*?Statistics\(([^)]+)\)', line)
            # Photon JOINå½¢å¼ï¼ˆStatisticsç„¡ã—ã€è©³ç´°ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜ãï¼‰
            photon_join_match = re.search(r'(Photon\w*Join)\s+\[([^\]]+)\],\s*\[([^\]]+)\],\s*(\w+),\s*(\w+)', line)
            
            if join_match or photon_join_match:
                if join_match:
                    # å¾“æ¥ã®Spark JOINå½¢å¼
                    join_type = join_match.group(1)
                    condition = join_match.group(2).strip()
                    stats = join_match.group(3)
                    
                    # çµ±è¨ˆæƒ…å ±ã‹ã‚‰æ•°å€¤æŠ½å‡º
                    size_match = re.search(r'sizeInBytes=([0-9.]+)\s*([KMGT]i?B)?', stats)
                    rows_match = re.search(r'rowCount=(\d+)', stats)
                    
                    size_str = f"{size_match.group(1)}{size_match.group(2) or 'B'}" if size_match else "unknown"
                    rows_str = rows_match.group(1) if rows_match else "unknown"
                    
                elif photon_join_match:
                    # Photon JOINå½¢å¼ã®è©³ç´°æŠ½å‡º
                    join_type = photon_join_match.group(1)  # PhotonBroadcastHashJoinç­‰
                    left_keys = photon_join_match.group(2)   # å·¦å´ã®JOINã‚­ãƒ¼
                    right_keys = photon_join_match.group(3)  # å³å´ã®JOINã‚­ãƒ¼
                    join_method = photon_join_match.group(4) # Inner, Leftç­‰
                    build_side = photon_join_match.group(5)  # BuildRight, BuildLeftç­‰
                    
                    # JOINæ¡ä»¶ã®æ§‹æˆ
                    condition = f"{left_keys} = {right_keys} ({join_method}, {build_side})"
                    
                    # Photon JOINã¯çµ±è¨ˆæƒ…å ±ãŒåˆ¥ã®å ´æ‰€ã«ã‚ã‚‹ãŸã‚ã€ã“ã“ã§ã¯åŸºæœ¬æƒ…å ±ã®ã¿
                    size_str = "photon_optimized"
                    rows_str = "photon_optimized"
                
                extracted["joins"].append({
                    "type": join_type,
                    "condition": condition[:100],  # æ¡ä»¶ã‚’100æ–‡å­—ã«åˆ¶é™
                    "size": size_str,
                    "rows": rows_str
                })
                join_count += 1
                
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³æƒ…å ±ã®æŠ½å‡ºï¼ˆå¾“æ¥å½¢å¼ + Photonå½¢å¼å®Œå…¨å¯¾å¿œï¼‰
            elif ('FileScan' in line and 'Statistics(' in line) or ('PhotonScan' in line and 'parquet' in line):
                # å¾“æ¥å½¢å¼ï¼šStatisticsä»˜ãFileScan
                stats_match = re.search(r'Statistics\(([^)]+)\)', line)
                # Photonå½¢å¼ï¼šPhotonScan parquet table_name[columns]
                photon_scan_match = re.search(r'PhotonScan\s+parquet\s+([a-zA-Z_][a-zA-Z0-9_.]*)\[([^\]]+)\]', line)
                # å¾“æ¥å½¢å¼ï¼šFileScan
                file_scan_match = re.search(r'FileScan\s+([^,\s\[]+)', line)
                
                if (stats_match and file_scan_match) or photon_scan_match:
                    if photon_scan_match:
                        # Photonå½¢å¼ã®å ´åˆ
                        table = photon_scan_match.group(1)  # ãƒ†ãƒ¼ãƒ–ãƒ«å
                        columns = photon_scan_match.group(2)  # åˆ—ãƒªã‚¹ãƒˆ
                        stats = None  # PhotonScanã«ã¯çµ±è¨ˆæƒ…å ±ãŒåŒä¸€è¡Œã«ãªã„
                        
                        # ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆã®ä¿å­˜ï¼ˆPhotonç”¨ã®æ§‹é€ ï¼‰
                        extracted["scans"].append({
                            "table": table[:50],
                            "columns": columns[:100],
                            "type": "PhotonScan",
                            "size": "photon_scan",
                            "rows": "photon_scan"
                        })
                        scan_count += 1
                        
                    elif stats_match and file_scan_match:
                        # å¾“æ¥å½¢å¼ã®å ´åˆ
                        stats = stats_match.group(1)
                        table = file_scan_match.group(1)
                        
                        size_match = re.search(r'sizeInBytes=([0-9.]+)\s*([KMGT]i?B)?', stats)
                        rows_match = re.search(r'rowCount=(\d+)', stats)
                        
                        extracted["scans"].append({
                            "table": table[:50],  # ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’50æ–‡å­—ã«åˆ¶é™
                            "type": "FileScan",
                            "size": f"{size_match.group(1)}{size_match.group(2) or 'B'}" if size_match else "unknown",
                            "rows": rows_match.group(1) if rows_match else "unknown"
                        })
                        scan_count += 1
                    
            # ãƒ‡ãƒ¼ã‚¿ç§»å‹•ï¼ˆExchangeï¼‰ã®æŠ½å‡º
            elif 'Exchange' in line:
                if 'BroadcastExchange' in line:
                    extracted["exchanges"].append({"type": "BROADCAST", "detail": line[:100]})
                elif 'ShuffleExchange' in line or 'Exchange' in line:
                    extracted["exchanges"].append({"type": "SHUFFLE", "detail": line[:100]})
                exchange_count += 1
                
            # é›†ç´„å‡¦ç†ã®æŠ½å‡º
            elif 'Aggregate' in line or 'HashAggregate' in line:
                extracted["aggregates"].append({"type": "AGGREGATE", "detail": line[:100]})
                
            # Photonåˆ©ç”¨çŠ¶æ³ã®ç¢ºèª
            elif 'Photon' in line:
                if 'PhotonResultStage' in line:
                    extracted["photon_usage"]["result_stage"] = True
                elif 'PhotonHashJoin' in line:
                    extracted["photon_usage"]["hash_join"] = True
                elif 'PhotonProject' in line:
                    extracted["photon_usage"]["project"] = True
        
        # çµ±è¨ˆã‚µãƒžãƒªãƒ¼ç”Ÿæˆ
        extracted["statistics"] = {
            "total_joins": join_count,
            "total_scans": scan_count,  
            "total_exchanges": exchange_count,
            "photon_operations": len([k for k, v in extracted["photon_usage"].items() if v])
        }
        
        # æŠ½å‡ºã‚µãƒžãƒªãƒ¼ç”Ÿæˆ
        extracted["extraction_summary"] = f"ðŸ“Š Structured extraction completed: JOIN({join_count}) SCAN({scan_count}) EXCHANGE({exchange_count}) PHOTON({len(extracted['photon_usage'])})"
        
        # ðŸš¨ ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾ç­–: æƒ…å ±é‡ãŒå¤šã„å ´åˆã®è‡ªå‹•è¦ç´„
        total_joins_scans = join_count + scan_count
        if total_joins_scans > 30:  # é–¾å€¤ã‚’å¤§å¹…ã«å¼•ãä¸Šã’: JOIN+SCANåˆè¨ˆãŒ30å€‹ä»¥ä¸Š
            # é‡è¦åº¦é †ã«ä¸¦ã³æ›¿ãˆã¦ãƒˆãƒƒãƒ—æƒ…å ±ã®ã¿ä¿æŒ
            extracted = apply_token_limit_optimization(extracted, max_joins=20, max_scans=15)  # åˆ¶é™ã‚’å¤§å¹…ç·©å’Œ
            extracted["extraction_summary"] += f" â†’ JOIN/SCAN information summarized for token limit optimization"
        elif total_joins_scans > 15:  # ä¸­é–“é–¾å€¤: 15-30å€‹ã®å ´åˆ
            # ä¸­ç¨‹åº¦ã®è¦ç´„
            extracted = apply_token_limit_optimization(extracted, max_joins=12, max_scans=10)
            extracted["extraction_summary"] += f" â†’ Moderate JOIN/SCAN information summarization applied"
        
    except Exception as e:
        extracted["extraction_error"] = str(e)
        
    return extracted

def extract_structured_cost_statistics(explain_cost_content: str) -> Dict[str, Any]:
    """
    Structured extraction of numerical statistics only from EXPLAIN COST (countermeasure for token limits)
    
    Args:
        explain_cost_content: Complete EXPLAIN COST results
    
    Returns:
        Dict: Structured statistical information
    """
    import re
    
    extracted = {
        "table_stats": {},      # Table-specific statistics (size, row count)
        "join_costs": {},       # JOIN-specific cost estimates  
        "selectivity": {},      # Filter selectivity
        "partition_info": {},   # Partition statistics
        "memory_estimates": {}, # Memory usage predictions
        "cost_breakdown": {},   # Cost breakdown
        "critical_stats": {},   # Critical statistical values
        "total_size": len(explain_cost_content),
        "extraction_summary": ""
    }
    
    try:
        lines = explain_cost_content.split('\n')
        tables_found = costs_found = memory_found = 0
        
        # é‡è¦çµ±è¨ˆå€¤ã‚’è¿½è·¡
        largest_table = {"name": "", "size": 0, "size_str": ""}
        total_rows = 0
        broadcast_candidates = []
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«åã¨ã‚µã‚¤ã‚ºã®å¯¾å¿œã‚’è¿½è·¡
        table_name_size_map = {}  # {table_name: {"size_bytes": int, "size_str": str, "rows": int}}
        current_table_context = None  # ç¾åœ¨å‡¦ç†ä¸­ã®ãƒ†ãƒ¼ãƒ–ãƒ«å
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ðŸ” ãƒ†ãƒ¼ãƒ–ãƒ«åã®æŠ½å‡ºï¼ˆRelationã‹ã‚‰ï¼‰
            table_name_match = re.search(r'Relation\s+([a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z_][a-zA-Z0-9_]*)', line)
            if table_name_match:
                current_table_context = table_name_match.group(1)
                
            # ðŸ” ãƒ†ãƒ¼ãƒ–ãƒ«åã®æŠ½å‡ºï¼ˆJoinæ¡ä»¶ã‹ã‚‰ï¼‰
            elif 'Join' in line and '=' in line:
                # JOINæ¡ä»¶ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŽ¨å®š (ä¾‹: ty_brand#456 = ly_brand#789)
                join_match = re.search(r'([a-zA-Z_][a-zA-Z0-9_]*)[#.]', line)
                if join_match and not current_table_context:
                    # JOINæ¡ä»¶ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«æŽ¨å®š
                    prefix = join_match.group(1)
                    if len(prefix) > 2:  # æ„å‘³ã®ã‚ã‚‹ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
                        current_table_context = f"{prefix}_table"
                
            # ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆã®æŠ½å‡º
            if 'Statistics(' in line:
                # ã‚µã‚¤ã‚ºæƒ…å ±ã®æŠ½å‡º
                size_match = re.search(r'sizeInBytes=([0-9.]+)\s*([KMGT]i?B)?', line)
                rows_match = re.search(r'rowCount=(\d+)', line)
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«åã®æ±ºå®š
                if current_table_context:
                    table_name = current_table_context
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è¡Œç•ªå·ã‹ã‚‰æŽ¨å®š
                    line_table_match = re.search(r'([a-zA-Z_][a-zA-Z0-9_]*)', line)
                    table_name = line_table_match.group(1) if line_table_match else f"table_{tables_found}"
                
                if size_match:
                    size_val = float(size_match.group(1))
                    size_unit = size_match.group(2) or 'B'
                    size_str = f"{size_val}{size_unit}"
                    
                    # ã‚µã‚¤ã‚ºå¤‰æ›ï¼ˆãƒã‚¤ãƒˆå˜ä½ï¼‰
                    size_bytes = size_val
                    if 'KiB' in size_unit:
                        size_bytes *= 1024
                    elif 'MiB' in size_unit:
                        size_bytes *= 1024 * 1024
                    elif 'GiB' in size_unit:
                        size_bytes *= 1024 * 1024 * 1024
                    elif 'TiB' in size_unit:
                        size_bytes *= 1024 * 1024 * 1024 * 1024
                    
                    # è¡Œæ•°ã®å–å¾—
                    rows = int(rows_match.group(1)) if rows_match else 0
                    
                    # ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆã®ä¿å­˜
                    extracted["table_stats"][table_name] = {
                        "size_bytes": size_bytes,
                        "size_str": size_str,
                        "rows": rows,
                        "is_broadcast_candidate": size_bytes < 30 * 1024 * 1024  # 30MB
                    }
                    
                    # æœ€å¤§ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¿½è·¡
                    if size_bytes > largest_table["size"]:
                        largest_table = {"name": table_name, "size": size_bytes, "size_str": size_str}
                    
                    # ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆå€™è£œï¼ˆ30MBæœªæº€ï¼‰
                    if size_bytes < 30 * 1024 * 1024:  # 30MB
                        broadcast_candidates.append({"table": table_name, "size": size_str})
                    
                    tables_found += 1
                    total_rows += rows
                    
                # ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆæ¬¡ã®ãƒ†ãƒ¼ãƒ–ãƒ«ç”¨ï¼‰
                current_table_context = None
                    
            # ã‚³ã‚¹ãƒˆæƒ…å ±ã®æŠ½å‡º  
            elif 'Cost(' in line:
                cost_match = re.search(r'Cost\(([0-9.]+)\)', line)
                if cost_match:
                    extracted["cost_breakdown"][f"operation_{costs_found}"] = float(cost_match.group(1))
                    costs_found += 1
                    
            # ãƒ¡ãƒ¢ãƒªé–¢é€£æƒ…å ±ã®æŠ½å‡º
            elif any(keyword in line.lower() for keyword in ['memory', 'spill', 'threshold']):
                if 'memory' in line.lower():
                    memory_match = re.search(r'(\d+(?:\.\d+)?)\s*([KMGT]i?B)', line)
                    if memory_match:
                        extracted["memory_estimates"][f"estimate_{memory_found}"] = f"{memory_match.group(1)}{memory_match.group(2)}"
                        memory_found += 1
        
        # é‡è¦çµ±è¨ˆå€¤ã®ã¾ã¨ã‚
        extracted["critical_stats"] = {
            "largest_table": largest_table,
            "total_rows": total_rows,
            "broadcast_candidates": broadcast_candidates[:5],  # ä¸Šä½5å€‹ã¾ã§
            "tables_analyzed": tables_found,
            "cost_operations": costs_found,
            "memory_estimates": memory_found,
            "table_breakdown": {
                "total_tables": len(extracted["table_stats"]),
                "largest_table_name": largest_table.get("name", "unknown"),
                "broadcast_table_names": [bc.get("table", "unknown") for bc in broadcast_candidates[:3]]
            }
        }
        
        # æŠ½å‡ºã‚µãƒžãƒªãƒ¼ç”Ÿæˆ
        extracted["extraction_summary"] = f"ðŸ’° Statistics extraction completed: Tables({tables_found}) Cost({costs_found}) Memory({memory_found}) BROADCAST candidates({len(broadcast_candidates)})"
        
    except Exception as e:
        extracted["extraction_error"] = str(e)
        
    return extracted

def apply_token_limit_optimization(extracted: Dict[str, Any], max_joins: int = 5, max_scans: int = 8) -> Dict[str, Any]:
    """
    Token limit optimization: Priority-based summarization of JOIN/SCAN information
    
    Args:
        extracted: Extracted structured data
        max_joins: Maximum number of JOINs to retain
        max_scans: Maximum number of SCANs to retain
    
    Returns:
        Optimized structured data
    """
    
    # Sort JOIN information by priority
    joins = extracted.get("joins", [])
    if len(joins) > max_joins:
        # Priority order: Broadcast > Hash > Sort > Nested
        join_priority = {
            "PhotonBroadcastHashJoin": 1,
            "BroadcastHashJoin": 2,
            "PhotonHashJoin": 3,
            "HashJoin": 4,
            "SortMergeJoin": 5,
            "NestedLoopJoin": 6
        }
        
        # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
        sorted_joins = sorted(joins, key=lambda j: join_priority.get(j.get("type", ""), 10))
        
        # ä¸Šä½ã®ã¿ä¿æŒã€æ®‹ã‚Šã¯è¦ç´„
        top_joins = sorted_joins[:max_joins]
        remaining_count = len(joins) - max_joins
        
        if remaining_count > 0:
            summary_join = {
                "type": "SUMMARY",
                "condition": f"Other {remaining_count} JOIN operations (details omitted)",
                "size": "multiple",
                "rows": "multiple"
            }
            top_joins.append(summary_join)
        
        extracted["joins"] = top_joins
    
    # SCANæƒ…å ±ã®é‡è¦åº¦åˆ¥ã‚½ãƒ¼ãƒˆ
    scans = extracted.get("scans", [])
    if len(scans) > max_scans:
        # é‡è¦åº¦é †åº: PhotonScan > FileScanã€ãƒ†ãƒ¼ãƒ–ãƒ«åã®é•·ã•ï¼ˆè©³ç´°åº¦ï¼‰
        def scan_priority(scan):
            priority = 1 if scan.get("type") == "PhotonScan" else 2
            table_length = len(scan.get("table", ""))
            return (priority, -table_length)  # ãƒ†ãƒ¼ãƒ–ãƒ«åãŒé•·ã„ï¼ˆè©³ç´°ï¼‰ã»ã©é‡è¦
        
        # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
        sorted_scans = sorted(scans, key=scan_priority)
        
        # ä¸Šä½ã®ã¿ä¿æŒã€æ®‹ã‚Šã¯è¦ç´„
        top_scans = sorted_scans[:max_scans]
        remaining_count = len(scans) - max_scans
        
        if remaining_count > 0:
            # æ®‹ã‚Šã®ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’é›†ç´„
            remaining_tables = [s.get("table", "unknown")[:20] for s in sorted_scans[max_scans:]]
            table_summary = ", ".join(remaining_tables[:3])
            if len(remaining_tables) > 3:
                table_summary += f" ä»–{len(remaining_tables)-3}å€‹"
                
            summary_scan = {
                "table": f"SUMMARY({table_summary})",
                "type": "SUMMARY",
                "size": "multiple",
                "rows": "multiple"
            }
            top_scans.append(summary_scan)
        
        extracted["scans"] = top_scans
    
    # çµ±è¨ˆæƒ…å ±ã®æ›´æ–°
    extracted["statistics"]["optimization_applied"] = True
    extracted["statistics"]["original_joins"] = len(joins)
    extracted["statistics"]["original_scans"] = len(scans)
    extracted["statistics"]["optimized_joins"] = len(extracted["joins"])
    extracted["statistics"]["optimized_scans"] = len(extracted["scans"])
    
    return extracted

def extract_cost_statistics_from_explain_cost(explain_cost_content: str) -> str:
    """
    EXPLAIN COSTçµæžœã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚’æŠ½å‡ºã—ã¦æ§‹é€ åŒ–ï¼ˆæ”¹å–„ç‰ˆ + ã‚µã‚¤ã‚ºåˆ¶é™ï¼‰
    
    Args:
        explain_cost_content: EXPLAIN COSTã®çµæžœæ–‡å­—åˆ—
    
    Returns:
        æ§‹é€ åŒ–ã•ã‚ŒãŸçµ±è¨ˆæƒ…å ±æ–‡å­—åˆ—ï¼ˆãƒ¬ãƒãƒ¼ãƒˆç”¨ã«ç°¡æ½”åŒ–ï¼‰
    """
    if not explain_cost_content:
        return ""
    
    # ðŸš¨ ãƒ¬ãƒãƒ¼ãƒˆè‚¥å¤§åŒ–é˜²æ­¢ï¼šã‚µãƒžãƒªãƒ¼æƒ…å ±ã®ã¿æŠ½å‡º
    statistics_counts = {
        "ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆ": 0,
        "è¡Œæ•°æƒ…å ±": 0, 
        "ã‚µã‚¤ã‚ºæƒ…å ±": 0,
        "ã‚³ã‚¹ãƒˆæƒ…å ±": 0,
        "é¸æŠžçŽ‡æƒ…å ±": 0,
        "ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±": 0,
        "ãƒ¡ãƒ¢ãƒªæƒ…å ±": 0,
        "JOINæƒ…å ±": 0
    }
    
    # é‡è¦ãªçµ±è¨ˆå€¤ã®ã¿æŠ½å‡ºï¼ˆè©³ç´°ã¯é™¤å¤–ï¼‰
    key_statistics = []
    MAX_KEY_STATS = 5  # é‡è¦çµ±è¨ˆæƒ…å ±ã®æœ€å¤§æ•°
    
    try:
        lines = explain_cost_content.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆæƒ…å ±ã®æŠ½å‡ºï¼ˆã‚«ã‚¦ãƒ³ãƒˆã®ã¿ï¼‰
            if 'statistics=' in line.lower() or 'stats=' in line.lower() or 'Statistics(' in line:
                statistics_counts["ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆ"] += 1
                if len(key_statistics) < MAX_KEY_STATS and 'sizeInBytes' in line:
                    # é‡è¦ãªã‚µã‚¤ã‚ºæƒ…å ±ã®ã¿æŠ½å‡º
                    if 'GiB' in line or 'TiB' in line:
                        key_statistics.append(f"ðŸ“Š ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚º: {line[:100]}...")
            
            # è¡Œæ•°æƒ…å ±ã®æŠ½å‡ºï¼ˆã‚«ã‚¦ãƒ³ãƒˆã®ã¿ï¼‰
            elif 'rows=' in line.lower() or 'rowcount=' in line.lower() or 'rows:' in line.lower():
                statistics_counts["è¡Œæ•°æƒ…å ±"] += 1
            
            # ã‚µã‚¤ã‚ºæƒ…å ±ã®æŠ½å‡ºï¼ˆã‚«ã‚¦ãƒ³ãƒˆã®ã¿ï¼‰
            elif ('size=' in line.lower() or 'sizeinbytes=' in line.lower() or 'sizeInBytes=' in line 
                  or 'GB' in line or 'MB' in line or 'size:' in line.lower()):
                statistics_counts["ã‚µã‚¤ã‚ºæƒ…å ±"] += 1
            
            # ãã®ä»–ã®çµ±è¨ˆæƒ…å ±ã®ã‚«ã‚¦ãƒ³ãƒˆ
            elif ('cost=' in line.lower() or 'Cost(' in line or 'cost:' in line.lower()):
                statistics_counts["ã‚³ã‚¹ãƒˆæƒ…å ±"] += 1
            elif ('selectivity=' in line.lower() or 'filter=' in line.lower()):
                statistics_counts["é¸æŠžçŽ‡æƒ…å ±"] += 1
            elif ('partition' in line.lower() and ('count' in line.lower() or 'size' in line.lower())):
                statistics_counts["ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±"] += 1
            elif ('memory' in line.lower() or 'spill' in line.lower()):
                statistics_counts["ãƒ¡ãƒ¢ãƒªæƒ…å ±"] += 1
            elif ('join' in line.lower() and ('cost' in line.lower() or 'selectivity' in line.lower())):
                statistics_counts["JOINæƒ…å ±"] += 1
    
    except Exception as e:
        return f"âš ï¸ çµ±è¨ˆæƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    # ç°¡æ½”ãªã‚µãƒžãƒªãƒ¼ã‚’ç”Ÿæˆ
    summary_lines = ["## ðŸ“Š çµ±è¨ˆæƒ…å ±ã‚µãƒžãƒªãƒ¼ï¼ˆç°¡æ½”ç‰ˆï¼‰"]
    
    total_stats = sum(statistics_counts.values())
    if total_stats > 0:
        summary_lines.append(f"- **ç·çµ±è¨ˆé …ç›®æ•°**: {total_stats}å€‹")
        
        for stat_type, count in statistics_counts.items():
            if count > 0:
                summary_lines.append(f"- **{stat_type}**: {count}å€‹")
        
        if key_statistics:
            summary_lines.append("\n### ðŸŽ¯ ä¸»è¦çµ±è¨ˆ")
            summary_lines.extend(key_statistics)
        
        summary_lines.append(f"\nðŸ’¡ è©³ç´°ãªçµ±è¨ˆæƒ…å ±ã¯ DEBUG_ENABLED='Y' ã§ç¢ºèªã§ãã¾ã™")
    else:
        summary_lines.append("- çµ±è¨ˆæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    return '\n'.join(summary_lines)


def generate_optimized_query_with_llm(original_query: str, analysis_result: str, metrics: Dict[str, Any]) -> str:
    """
    Optimize SQL query based on detailed bottleneck analysis results from Cell 33 (processing speed priority)
    Also leverages statistical information when EXPLAIN + EXPLAIN COST execution flag is Y
    """
    
    # EXPLAIN + EXPLAIN COSTçµæžœãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆEXPLAIN_ENABLEDãŒYã®å ´åˆï¼‰
    explain_content = ""
    explain_cost_content = ""
    physical_plan = ""
    photon_explanation = ""
    cost_statistics = ""
    
    explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
    if explain_enabled.upper() == 'Y':
        import glob
        import os
        
        print("ðŸ” Searching for EXPLAIN + EXPLAIN COST result files...")
        
        # 1. Search for latest EXPLAIN result files (supporting new filename patterns)
        explain_original_files = glob.glob("output_explain_original_*.txt")
        explain_optimized_files = glob.glob("output_explain_optimized_*.txt")
        
        # Prioritize original query EXPLAIN results, use optimized if not available
        explain_files = explain_original_files if explain_original_files else explain_optimized_files
        
        if explain_files:
            latest_explain_file = max(explain_files, key=os.path.getctime)
            try:
                with open(latest_explain_file, 'r', encoding='utf-8') as f:
                    explain_content = f.read()
                    print(f"âœ… Loaded EXPLAIN result file: {latest_explain_file}")
                
                # Extract and process Physical Plan (structured extraction support)
                if "== Physical Plan ==" in explain_content:
                    physical_plan_start = explain_content.find("== Physical Plan ==")
                    physical_plan_end = explain_content.find("== Photon", physical_plan_start)
                    if physical_plan_end == -1:
                        physical_plan_end = len(explain_content)
                    physical_plan_raw = explain_content[physical_plan_start:physical_plan_end].strip()
                    print(f"ðŸ“Š Extracted Physical Plan information: {len(physical_plan_raw)} characters")
                    
                    # ðŸ§  æ§‹é€ åŒ–æŠ½å‡º vs å¾“æ¥ã®åˆ‡ã‚Šè©°ã‚ã®é¸æŠž
                    structured_enabled = globals().get('STRUCTURED_EXTRACTION_ENABLED', 'Y')
                    debug_enabled = globals().get('DEBUG_ENABLED', 'N')
                
                if structured_enabled.upper() == 'Y':
                    # ðŸš€ æ§‹é€ åŒ–æŠ½å‡ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
                    try:
                        structured_plan = extract_structured_physical_plan(physical_plan_raw)
                        
                        # Convert structured results to JSON format string
                        import json
                        physical_plan = json.dumps(structured_plan, ensure_ascii=False, indent=2)
                        
                        print(f"ðŸ§  Structured extraction completed: {len(physical_plan_raw):,} â†’ {len(physical_plan):,} characters")
                        print(f"   {structured_plan.get('extraction_summary', 'ðŸ“Š Structured extraction completed')}")
                        
                        # When DEBUG_ENABLED='Y', save structured results and original data
                        if debug_enabled.upper() == 'Y':
                            try:
                                from datetime import datetime
                                timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                                
                                # Save structured results
                                structured_plan_filename = f"output_physical_plan_structured_{timestamp}.json"
                                with open(structured_plan_filename, 'w', encoding='utf-8') as f:
                                    f.write(physical_plan)
                                
                                print(f"ðŸ“„ Saved structured Physical Plan: {structured_plan_filename}")
                                
                            except Exception as save_error:
                                print(f"âš ï¸ Failed to save Physical Plan: {str(save_error)}")
                                
                    except Exception as extraction_error:
                        print(f"âš ï¸ Structured extraction failed, falling back to traditional method: {str(extraction_error)}")
                        # Fallback: Traditional truncation method
                        MAX_PLAN_SIZE = 30000
                        if len(physical_plan_raw) > MAX_PLAN_SIZE:
                            physical_plan = physical_plan_raw[:MAX_PLAN_SIZE] + "\n\nStructured extraction failed, truncated to limit"
                            print(f"âš ï¸ Fallback: Physical Plan truncated to {MAX_PLAN_SIZE} characters")
                        else:
                            physical_plan = physical_plan_raw
                            print(f"âš ï¸ Physical Plan truncated to {MAX_PLAN_SIZE} characters due to token limit")
                
                # Extract Photon Explanation
                if "== Photon Explanation ==" in explain_content:
                    photon_start = explain_content.find("== Photon Explanation ==")
                    photon_explanation = explain_content[photon_start:].strip()
                    print(f"ðŸš€ Extracted Photon Explanation information: {len(photon_explanation)} characters")
                    
            except Exception as e:
                print(f"âš ï¸ Failed to load EXPLAIN result file: {str(e)}")
                explain_content = ""
        
        # ðŸš€ EXPLAIN COSTçµæžœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
        cached_cost_result = globals().get('cached_original_explain_cost_result')
        explain_cost_content = ""
        
        if cached_cost_result and 'explain_cost_file' in cached_cost_result:
            try:
                with open(cached_cost_result['explain_cost_file'], 'r', encoding='utf-8') as f:
                    explain_cost_content = f.read()
                    print(f"ðŸ’¾ Using cached EXPLAIN COST result file: {cached_cost_result['explain_cost_file']}")
            except Exception as e:
                print(f"âš ï¸ Failed to load cached EXPLAIN COST results: {str(e)}")
                cached_cost_result = None
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯å¾“æ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
        if not cached_cost_result:
            # 2. Search for latest EXPLAIN COST result files
            cost_original_files = glob.glob("output_explain_cost_original_*.txt")
            cost_optimized_files = glob.glob("output_explain_cost_optimized_*.txt")
            
            # Prioritize original query EXPLAIN COST results, use optimized if not available
            cost_files = cost_original_files if cost_original_files else cost_optimized_files
            
            if cost_files:
                latest_cost_file = max(cost_files, key=os.path.getctime)
                try:
                    with open(latest_cost_file, 'r', encoding='utf-8') as f:
                        explain_cost_content = f.read()
                        print(f"ðŸ’° Loaded EXPLAIN COST result file: {latest_cost_file}")
                    
                    # Extract statistical information (structured extraction support)
                    structured_enabled = globals().get('STRUCTURED_EXTRACTION_ENABLED', 'Y')
                    
                    if structured_enabled.upper() == 'Y':
                        # ðŸš€ æ§‹é€ åŒ–æŠ½å‡ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
                        try:
                            structured_cost = extract_structured_cost_statistics(explain_cost_content)
                            
                            # Convert structured results to JSON format string
                            import json
                            cost_statistics = json.dumps(structured_cost, ensure_ascii=False, indent=2)
                            
                            print(f"ðŸ’° EXPLAIN COST structured extraction completed: {len(explain_cost_content):,} â†’ {len(cost_statistics):,} characters (compression ratio: {len(explain_cost_content)//len(cost_statistics) if len(cost_statistics) > 0 else 0}x)")
                            print(f"   {structured_cost.get('extraction_summary', 'ðŸ’° Statistical extraction completed')}")
                            
                        except Exception as extraction_error:
                            print(f"âš ï¸ EXPLAIN COST structured extraction failed, falling back to traditional method: {str(extraction_error)}")
                            # Fallback: Traditional extraction method
                            cost_statistics = extract_cost_statistics_from_explain_cost(explain_cost_content)
                            print(f"ðŸ“Š Extracted EXPLAIN COST statistics (traditional method): {len(cost_statistics)} characters")
                    else:
                        # ðŸ”„ Traditional extraction approach
                        cost_statistics = extract_cost_statistics_from_explain_cost(explain_cost_content)
                        print(f"ðŸ“Š Extracted EXPLAIN COST statistics: {len(cost_statistics)} characters")
                
                    # ðŸš¨ When DEBUG_ENABLED='Y', always save extracted statistical information
                    debug_enabled = globals().get('DEBUG_ENABLED', 'N')
                    if debug_enabled.upper() == 'Y':
                        try:
                            from datetime import datetime
                            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                            extracted_stats_filename = f"output_explain_cost_statistics_extracted_{timestamp}.json"
                            
                            with open(extracted_stats_filename, 'w', encoding='utf-8') as f:
                                f.write(f"# Extracted EXPLAIN COST statistical information (Generated date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\n")
                                f.write(f"# Extraction size: {len(cost_statistics):,} characters\n")
                                f.write(f"# Source file: {latest_cost_file}\n\n")
                                f.write(cost_statistics)
                            
                            print(f"ðŸ“„ Saved extracted statistical information: {extracted_stats_filename}")
                            
                        except Exception as save_error:
                            print(f"âš ï¸ Failed to save extracted statistical information: {str(save_error)}")
                
                    # Size limit for statistical information (countermeasure for LLM token limits)
                    MAX_STATISTICS_SIZE = 50000  # ç´„50KBåˆ¶é™
                    if len(cost_statistics) > MAX_STATISTICS_SIZE:
                        # ðŸš¨ DEBUG_ENABLED='Y'ã®å ´åˆã€å®Œå…¨ãªEXPLAIN COSTçµ±è¨ˆæƒ…å ±ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                        debug_enabled = globals().get('DEBUG_ENABLED', 'N')
                        if debug_enabled.upper() == 'Y':
                            try:
                                from datetime import datetime
                                timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                                full_stats_filename = f"output_explain_cost_statistics_full_{timestamp}.txt"
                                
                                with open(full_stats_filename, 'w', encoding='utf-8') as f:
                                    f.write(f"# Complete EXPLAIN COST statistical information (Generated date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\n")
                                    f.write(f"# Original size: {len(cost_statistics):,} characters\n")
                                    f.write(f"# LLM usage size: {MAX_STATISTICS_SIZE:,} characters\n\n")
                                    f.write(cost_statistics)
                                
                                print(f"ðŸ“„ Saved complete EXPLAIN COST statistical information: {full_stats_filename}")
                                
                            except Exception as save_error:
                                print(f"âš ï¸ Failed to save EXPLAIN COST statistical information: {str(save_error)}")
                        
                        truncated_statistics = cost_statistics[:MAX_STATISTICS_SIZE]
                        truncated_statistics += f"\n\nâš ï¸ Statistical information was too large, truncated to {MAX_STATISTICS_SIZE} characters"
                        cost_statistics = truncated_statistics
                        print(f"âš ï¸ Statistical information truncated to {MAX_STATISTICS_SIZE} characters due to token limit")
                    
                except Exception as e:
                    print(f"âš ï¸ Failed to load EXPLAIN COST result file: {str(e)}")
                    explain_cost_content = ""
        
        if not explain_files and not cost_files:
            print("âš ï¸ EXPLAINãƒ»EXPLAIN COST result files not found")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ãƒã‚§ãƒƒã‚¯
            old_explain_files = glob.glob("output_explain_plan_*.txt")
            if old_explain_files:
                latest_explain_file = max(old_explain_files, key=os.path.getctime)
                try:
                    with open(latest_explain_file, 'r', encoding='utf-8') as f:
                        explain_content = f.read()
                        print(f"âœ… Loaded legacy format EXPLAIN result file: {latest_explain_file}")
                        
                    # Physical PlanæŠ½å‡ºï¼ˆæ—§å½¢å¼å¯¾å¿œï¼‰
                    if "== Physical Plan ==" in explain_content:
                        physical_plan_start = explain_content.find("== Physical Plan ==")
                        physical_plan_end = explain_content.find("== Photon", physical_plan_start)
                        if physical_plan_end == -1:
                            physical_plan_end = len(explain_content)
                        physical_plan = explain_content[physical_plan_start:physical_plan_end].strip()
                        
                    if "== Photon Explanation ==" in explain_content:
                        photon_start = explain_content.find("== Photon Explanation ==")
                        photon_explanation = explain_content[photon_start:].strip()
                except Exception as e:
                    print(f"âš ï¸ Failed to load legacy format EXPLAIN result file: {str(e)}")
            else:
                print("âš ï¸ EXPLAIN result files not found")
    
    # å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã®æŠ½å‡ºï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ï¼‰
    profiler_data = metrics.get('raw_profiler_data', {})
    plan_info = None
    if profiler_data:
        plan_info = extract_execution_plan_info(profiler_data)
    
    # BROADCASTé©ç”¨å¯èƒ½æ€§ã®åˆ†æžï¼ˆãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’å«ã‚€ï¼‰
    # ðŸŽ¯ BROADCASTæœ€é©åŒ–ã¯ç„¡åŠ¹åŒ–ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã«ã‚ˆã‚Šé™¤å¤–ï¼‰
    # ðŸš¨ é‡è¦: ã™ã¹ã¦ã®å¿…è¦ãªã‚­ãƒ¼ã‚’å«ã‚ã‚‹ï¼ˆKeyErroré˜²æ­¢ï¼‰
    broadcast_analysis = {
        "feasibility": "disabled", 
        "broadcast_candidates": [], 
        "recommendations": [],
        "reasoning": ["BROADCASTãƒ’ãƒ³ãƒˆã¯æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®åŽŸå› ã¨ãªã‚‹ãŸã‚ç„¡åŠ¹åŒ–"], 
        "is_join_query": True,
        "already_optimized": False,  # ðŸš¨ ç·Šæ€¥ä¿®æ­£: å¿…é ˆã‚­ãƒ¼è¿½åŠ 
        "spark_threshold_mb": 30.0,
        "compression_analysis": {},
        "detailed_size_analysis": [],
        "execution_plan_analysis": {},
        "existing_broadcast_nodes": [],
        "broadcast_applied_tables": [],
        # ðŸš¨ ç·Šæ€¥ä¿®æ­£: 30mb_hit_analysis ã‚­ãƒ¼è¿½åŠ ï¼ˆKeyErroré˜²æ­¢ï¼‰
        "30mb_hit_analysis": {
            "has_30mb_candidates": False,
            "reason": "BROADCASTãƒ’ãƒ³ãƒˆã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹ãŸã‚åˆ†æžå¯¾è±¡å¤–"
        }
    }
    
    # ãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã§ä½¿ç”¨ï¼‰
    if plan_info:
        metrics['execution_plan_info'] = plan_info
    
    # ðŸš€ ã‚»ãƒ«33ã‚¹ã‚¿ã‚¤ãƒ«ã®è©³ç´°ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æžã‚’å®Ÿè¡Œ
    detailed_bottleneck = extract_detailed_bottleneck_analysis(metrics)
    
    # æœ€é©åŒ–ã®ãŸã‚ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’æº–å‚™ï¼ˆè©³ç´°ç‰ˆï¼‰
    optimization_context = []
    performance_critical_issues = []
    
    # åŸºæœ¬çš„ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯æƒ…å ±ã®æŠ½å‡º
    bottlenecks = metrics.get('bottleneck_indicators', {})
    
    if bottlenecks.get('has_spill', False):
        spill_gb = bottlenecks.get('spill_bytes', 0) / 1024 / 1024 / 1024
        optimization_context.append(f"ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ: {spill_gb:.1f}GB - ãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡ã®æ”¹å–„ãŒå¿…è¦")
    
    if bottlenecks.get('has_shuffle_bottleneck', False):
        optimization_context.append("ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ - JOINã¨GROUP BYã®æœ€é©åŒ–ãŒå¿…è¦")
    
    if bottlenecks.get('cache_hit_ratio', 0) < 0.5:
        optimization_context.append("ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹çŽ‡ä½Žä¸‹ - ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœ€é©åŒ–ãŒå¿…è¦")
    
    # ðŸŽ¯ è©³ç´°ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æžçµæžœã‹ã‚‰ã®è¿½åŠ æƒ…å ±
    if detailed_bottleneck["spill_analysis"]["total_spill_gb"] > 0:
        total_spill = detailed_bottleneck["spill_analysis"]["total_spill_gb"]
        spill_nodes_count = len(detailed_bottleneck["spill_analysis"]["spill_nodes"])
        performance_critical_issues.append(f"ðŸš¨ CRITICAL: åˆè¨ˆ{total_spill:.1f}GBã®ã‚¹ãƒ”ãƒ«ãŒ{spill_nodes_count}å€‹ã®ãƒŽãƒ¼ãƒ‰ã§ç™ºç”Ÿ")
        
        # æœ€ã‚‚é‡è¦ãªã‚¹ãƒ”ãƒ«ãƒŽãƒ¼ãƒ‰ã‚’ç‰¹å®š
        if detailed_bottleneck["spill_analysis"]["spill_nodes"]:
            top_spill_node = max(detailed_bottleneck["spill_analysis"]["spill_nodes"], key=lambda x: x["spill_gb"])
            performance_critical_issues.append(f"   æœ€å¤§ã‚¹ãƒ”ãƒ«ãƒŽãƒ¼ãƒ‰: {top_spill_node['node_name']} ({top_spill_node['spill_gb']:.2f}GB)")
    
    if detailed_bottleneck["skew_analysis"]["total_skewed_partitions"] > 0:
        total_skew = detailed_bottleneck["skew_analysis"]["total_skewed_partitions"]
        skewed_nodes_count = len(detailed_bottleneck["skew_analysis"]["skewed_nodes"])
        performance_critical_issues.append(f"âš–ï¸ ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼: {total_skew}å€‹ã®ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãŒ{skewed_nodes_count}å€‹ã®ãƒŽãƒ¼ãƒ‰ã§æ¤œå‡º")
    
    # TOP3ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãƒŽãƒ¼ãƒ‰ã®è©³ç´°åˆ†æž
    top3_bottlenecks = detailed_bottleneck["top_bottleneck_nodes"][:3]
    performance_critical_issues.append("ðŸ“Š TOP3å‡¦ç†æ™‚é–“ãƒœãƒˆãƒ«ãƒãƒƒã‚¯:")
    for node in top3_bottlenecks:
        severity_icon = "ðŸ”´" if node["severity"] == "CRITICAL" else "ðŸŸ " if node["severity"] == "HIGH" else "ðŸŸ¡"
        performance_critical_issues.append(f"   {severity_icon} #{node['rank']}: {node['node_name'][:60]}...")
        performance_critical_issues.append(f"      å®Ÿè¡Œæ™‚é–“: {node['duration_ms']:,}ms ({node['time_percentage']:.1f}%) | ãƒ¡ãƒ¢ãƒª: {node['memory_mb']:.1f}MB")
        if node["spill_detected"]:
            performance_critical_issues.append(f"      ðŸ’¿ ã‚¹ãƒ”ãƒ«: {node['spill_gb']:.2f}GB - ç·Šæ€¥å¯¾å¿œå¿…è¦")
        if node["skew_detected"]:
            performance_critical_issues.append(f"      âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼: {node['skewed_partitions']}ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ - ãƒ‡ãƒ¼ã‚¿åˆ†æ•£æ”¹å–„å¿…è¦")
    
    # ðŸ”„ REPARTITIONãƒ’ãƒ³ãƒˆã®è©³ç´°ç”Ÿæˆï¼ˆã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿ï¼‰
    repartition_hints = []
    if detailed_bottleneck["shuffle_optimization_hints"]:
        repartition_hints.append("ðŸ”„ REPARTITIONãƒ’ãƒ³ãƒˆï¼ˆã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿ï¼‰:")
        for hint in detailed_bottleneck["shuffle_optimization_hints"]:
            priority_icon = "ðŸš¨" if hint["priority"] == "HIGH" else "ðŸ“ˆ"
            repartition_hints.append(f"   {priority_icon} ãƒŽãƒ¼ãƒ‰ID {hint['node_id']}: {hint['suggested_sql']}")
            repartition_hints.append(f"      å±žæ€§: {', '.join(hint['attributes'])}")
            repartition_hints.append(f"      ç†ç”±: {hint['reason']}")
            repartition_hints.append(f"      åŠ¹æžœ: {hint['estimated_improvement']}")
            
            # ã‚¯ã‚¨ãƒªã¸ã®é©ç”¨æ–¹æ³•ã®å…·ä½“çš„ãªææ¡ˆ
            main_attr = hint['attributes'][0]
            if 'GROUP BY' in original_query.upper():
                repartition_hints.append(f"      é©ç”¨ææ¡ˆ: GROUP BYå‰ã«REPARTITION({hint['suggested_sql'].split('(')[1]}")
            elif 'JOIN' in original_query.upper():
                repartition_hints.append(f"      é©ç”¨ææ¡ˆ: JOINå‰ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’{hint['suggested_sql']}ã§ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³")
    
    # ðŸ“Š å‡¦ç†é€Ÿåº¦é‡è¦–ã®æœ€é©åŒ–æŽ¨å¥¨äº‹é …
    speed_optimization_recommendations = []
    for rec in detailed_bottleneck["performance_recommendations"]:
        priority_icon = "ðŸš¨" if rec["priority"] == "CRITICAL" else "âš ï¸" if rec["priority"] == "HIGH" else "ðŸ“"
        speed_optimization_recommendations.append(f"{priority_icon} {rec['type'].upper()}: {rec['description']}")
    
    # Liquid ClusteringæŽ¨å¥¨æƒ…å ±ï¼ˆLLMãƒ™ãƒ¼ã‚¹å¯¾å¿œï¼‰
    liquid_analysis = metrics.get('liquid_clustering_analysis', {})
    extracted_data = liquid_analysis.get('extracted_data', {})
    table_info = extracted_data.get('table_info', {})
    
    clustering_recommendations = []
    if table_info:
        for table_name in list(table_info.keys())[:3]:  # ä¸Šä½3ãƒ†ãƒ¼ãƒ–ãƒ«
            clustering_recommendations.append(f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_name}: LLMåˆ†æžã«ã‚ˆã‚‹æŽ¨å¥¨ã‚«ãƒ©ãƒ ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æŽ¨å¥¨")
    
    # æœ€é©åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆï¼ˆç°¡æ½”ç‰ˆã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›žé¿ï¼‰
    
    # åˆ†æžçµæžœã‚’ç°¡æ½”åŒ–ï¼ˆ128Kåˆ¶é™å†…ã§æœ€å¤§åŠ¹çŽ‡åŒ–ï¼‰
    analysis_summary = ""
    if isinstance(analysis_result, str) and len(analysis_result) > 2000:
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®¹é‡ã®ç¢ºä¿ã®ãŸã‚ã€åˆ†æžçµæžœã¯è¦ç‚¹ã®ã¿ã«åœ§ç¸®
        analysis_summary = analysis_result[:2000] + "...[è¦ç´„ï¼šä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ã¿ä¿æŒ]"
    else:
        analysis_summary = str(analysis_result)
    
    # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æƒ…å ±ã®ç°¡æ½”åŒ–
    bottleneck_summary = "ã€".join(optimization_context[:3]) if optimization_context else "ç‰¹ã«ãªã—"
    
    # Liquid ClusteringæŽ¨å¥¨ã®ç°¡æ½”åŒ–
    clustering_summary = "ã€".join(clustering_recommendations[:2]) if clustering_recommendations else "ç‰¹ã«ãªã—"
    
    # ðŸš¨ JOINæˆ¦ç•¥åˆ†æžã®ç°¡ç•¥åŒ–ï¼ˆBROADCASTãƒ’ãƒ³ãƒˆç„¡åŠ¹åŒ–ï¼‰
    broadcast_summary = ["ðŸŽ¯ æœ€é©åŒ–æ–¹é‡: JOINé †åºæœ€é©åŒ–ï¼ˆSparkã®è‡ªå‹•æˆ¦ç•¥ã‚’æ´»ç”¨ã€ãƒ’ãƒ³ãƒˆä¸ä½¿ç”¨ï¼‰"]
    
    optimization_prompt = f"""
ã‚ãªãŸã¯Databricksã®SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æœ€é©åŒ–ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®**è©³ç´°ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æžçµæžœ**ã‚’åŸºã«ã€**å‡¦ç†é€Ÿåº¦é‡è¦–**ã§SQLã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªå‡¦ç†æ–¹é‡ã€‘
- ä¸€å›žã®å‡ºåŠ›ã§å®Œå…¨ãªSQLã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„
- æ®µéšŽçš„ãªå‡ºåŠ›ã‚„è¤‡æ•°å›žã«åˆ†ã‘ã¦ã®å‡ºåŠ›ã¯ç¦æ­¢ã§ã™
- thinkingæ©Ÿèƒ½ã§æ§‹é€ ç†è§£â†’ä¸€å›žã§å®Œå…¨ãªSQLå‡ºåŠ›
- **âŒ BROADCASTãƒ’ãƒ³ãƒˆï¼ˆ/*+ BROADCAST */ã€/*+ BROADCAST(table) */ï¼‰ã¯ä¸€åˆ‡ä½¿ç”¨ç¦æ­¢**
- **âœ… JOINæˆ¦ç•¥ã¯Sparkã®è‡ªå‹•æœ€é©åŒ–ã«å§”ã­ã¦ãƒ’ãƒ³ãƒˆä¸ä½¿ç”¨ã§æœ€é©åŒ–**

ã€å…ƒã®SQLã‚¯ã‚¨ãƒªã€‘
```sql
{original_query}
```

ã€ðŸ“Š ã‚»ãƒ«33è©³ç´°ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æžçµæžœã€‘
{chr(10).join(performance_critical_issues) if performance_critical_issues else "ç‰¹åˆ¥ãªé‡è¦èª²é¡Œã¯è¨­å®šãªã—"}

ã€ðŸ”„ REPARTITIONãƒ’ãƒ³ãƒˆï¼ˆã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿ï¼‰ã€‘
{chr(10).join(repartition_hints) if repartition_hints else "ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„ãŸã‚ã€REPARTITIONãƒ’ãƒ³ãƒˆã¯é©ç”¨å¯¾è±¡å¤–ã§ã™"}

ã€ðŸš€ å‡¦ç†é€Ÿåº¦é‡è¦–ã®æœ€é©åŒ–æŽ¨å¥¨äº‹é …ã€‘
{chr(10).join(speed_optimization_recommendations) if speed_optimization_recommendations else "ç‰¹åˆ¥ãªæŽ¨å¥¨äº‹é …ã¯ã‚ã‚Šã¾ã›ã‚“"}

ã€åŸºæœ¬çš„ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯æƒ…å ±ã€‘
{chr(10).join(optimization_context) if optimization_context else "ä¸»è¦ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯è¨­å®šãªã—"}

ã€JOINæˆ¦ç•¥åˆ†æžçµæžœã€‘
Sparkã®è‡ªå‹•JOINæˆ¦ç•¥ã‚’ä½¿ç”¨ï¼ˆã‚¨ãƒ©ãƒ¼å›žé¿ã®ãŸã‚ãƒ’ãƒ³ãƒˆã¯ä½¿ç”¨ã›ãšï¼‰

ã€Liquid ClusteringæŽ¨å¥¨ã€‘
{chr(10).join(clustering_recommendations) if clustering_recommendations else "ç‰¹åˆ¥ãªæŽ¨å¥¨äº‹é …ã¯ã‚ã‚Šã¾ã›ã‚“"}

ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åˆ†æžçµæžœï¼ˆã‚µãƒžãƒªãƒ¼ï¼‰ã€‘
{analysis_summary}

ã€ðŸ” EXPLAINçµæžœåˆ†æžï¼ˆEXPLAIN_ENABLED=Yã®å ´åˆã®ã¿ï¼‰ã€‘
{f'''
**Physical Planåˆ†æž:**
```
{physical_plan}
```

**Photon Explanationåˆ†æž:**
```
{photon_explanation}
```

**Physical Planæœ€é©åŒ–ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆ:**
- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ã®åŠ¹çŽ‡æ€§
- ã‚¸ãƒ§ã‚¤ãƒ³æˆ¦ç•¥ã®å¦¥å½“æ€§
- ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œã®æœ€å°åŒ–
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆåˆ—é¸æŠžï¼‰ã®æœ€é©åŒ–
- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ã®æ´»ç”¨

**Photonæœ€é©åŒ–ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆ:**
- Photonæœªå¯¾å¿œé–¢æ•°ã®æ¤œå‡ºã¨ä»£æ›¿é–¢æ•°ã¸ã®å¤‰æ›´
- ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†ã«é©ã—ãŸé–¢æ•°ã®é¸æŠž
- Photonåˆ©ç”¨çŽ‡å‘ä¸Šã®ãŸã‚ã®æ›¸å¼å¤‰æ›´
- ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚æœ€é©åŒ–ã®æ´»ç”¨
''' if explain_enabled.upper() == 'Y' and (physical_plan or photon_explanation) else '(EXPLAINå®Ÿè¡ŒãŒç„¡åŠ¹ã€ã¾ãŸã¯EXPLAINçµæžœãŒåˆ©ç”¨ã§ãã¾ã›ã‚“)'}

ã€ðŸ’° EXPLAIN COSTçµ±è¨ˆæƒ…å ±åˆ†æžï¼ˆçµ±è¨ˆãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ï¼‰ã€‘
{f'''
**æ§‹é€ åŒ–EXPLAIN COSTçµ±è¨ˆæƒ…å ±:**
```json
{cost_statistics}
```

**ðŸ§  æ§‹é€ åŒ–çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨æŒ‡é‡:**
ä¸Šè¨˜ã¯æ§‹é€ åŒ–æŠ½å‡ºã•ã‚ŒãŸçµ±è¨ˆæƒ…å ±ã§ã™ã€‚ä»¥ä¸‹ã®é …ç›®ã‚’é‡ç‚¹çš„ã«åˆ†æžã—ã¦ãã ã•ã„ï¼š

- **table_stats**: ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥è©³ç´°çµ±è¨ˆï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«åã€ã‚µã‚¤ã‚ºã€è¡Œæ•°ï¼‰
- **critical_stats**: é‡è¦çµ±è¨ˆå€¤ï¼ˆæœ€å¤§ãƒ†ãƒ¼ãƒ–ãƒ«ã€ç·è¡Œæ•°ã€å°ãƒ†ãƒ¼ãƒ–ãƒ«å€™è£œï¼‰
- **largest_table**: æœ€å¤§ãƒ†ãƒ¼ãƒ–ãƒ«ã®åå‰ã¨ã‚µã‚¤ã‚ºï¼ˆJOINé †åºã®åŸºæº–ï¼‰
- **small_table_candidates**: å°ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«åã¨ã‚µã‚¤ã‚ºï¼‰
- **table_breakdown**: ãƒ†ãƒ¼ãƒ–ãƒ«åã®è©³ç´°ï¼ˆæœ€å¤§ãƒ†ãƒ¼ãƒ–ãƒ«åã€å°ãƒ†ãƒ¼ãƒ–ãƒ«åï¼‰

**ðŸŽ¯ ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ä½¿ã£ãŸç²¾å¯†æœ€é©åŒ–:**
1. **JOINé †åºã®æœ€é©åŒ–:**
   - ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºã«åŸºã¥ãåŠ¹çŽ‡çš„ãªJOINé †åºã®æ±ºå®š
   - å°ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å¤§ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®æ®µéšŽçš„çµåˆ

2. **JOINé †åºã®å…·ä½“çš„ææ¡ˆ:**
   - largest_table.nameã‚’æœ€å¾Œã«é…ç½®
   - table_statsã®ã‚µã‚¤ã‚ºé †ã§JOINé †åºã‚’æœ€é©åŒ–
   - å…·ä½“çš„ãªãƒ†ãƒ¼ãƒ–ãƒ«åã§JOINæ–‡ã‚’æ”¹å–„

3. **æ›–æ˜§æ€§è§£æ±ºã®å…·ä½“çš„ææ¡ˆ:**
   - ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ†ãƒ¼ãƒ–ãƒ«åã¨table_statsã‚’ç…§åˆ
   - å…·ä½“çš„ãªã‚¨ã‚¤ãƒªã‚¢ã‚¹ææ¡ˆï¼ˆä¾‹: `store_sales.ss_item_sk`ï¼‰

**ðŸš€ æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿è§£æžã®å®Ÿè¡Œä¾‹:**
1. table_statså†…ã§å°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç‰¹å®šã—ã€åŠ¹çŽ‡çš„ãªJOINé †åºã‚’æ±ºå®š
2. largest_table_nameãŒ1GBä»¥ä¸Š â†’ å¤§ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦æœ€çµ‚JOINã«é…ç½®
3. JOINé †åºã®å…·ä½“çš„ãªæ”¹å–„ææ¡ˆã‚’ç”Ÿæˆ
4. ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æ˜Žç¤ºã—ãŸJOINé †åºææ¡ˆã‚’ç”Ÿæˆ

**ðŸš¨ ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾ç­–ã«ã¤ã„ã¦:**
- JOIN/SCANæƒ…å ±ãŒå¤šæ•°ã®å ´åˆã€é‡è¦åº¦é †ã«è¦ç´„æ¸ˆã¿
- SUMMARYé …ç›®ã¯è¤‡æ•°æ“ä½œã®é›†ç´„ã‚’ç¤ºã—ã¾ã™
- è©³ç´°ã¯ optimization_applied ãƒ•ãƒ©ã‚°ã§ç¢ºèªå¯èƒ½
- Physical PlanãŒ100KBè¶…ã®å ´åˆã¯è‡ªå‹•èª¿æ•´æ¸ˆã¿
''' if explain_enabled.upper() == 'Y' and cost_statistics else '(EXPLAIN COSTå®Ÿè¡ŒãŒç„¡åŠ¹ã€ã¾ãŸã¯çµ±è¨ˆæƒ…å ±ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“)'}

ã€ðŸŽ¯ å‡¦ç†é€Ÿåº¦é‡è¦–ã®æœ€é©åŒ–è¦æ±‚ã€‘
**æœ€é‡è¦**: ä»¥ä¸‹ã®é †åºã§å‡¦ç†é€Ÿåº¦ã®æ”¹å–„ã‚’å„ªå…ˆã—ã¦ãã ã•ã„

1. **ðŸš¨ CRITICALå„ªå…ˆåº¦**: ã‚¹ãƒ”ãƒ«å¯¾ç­–ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡æ”¹å–„ï¼‰
   - å¤§é‡ã‚¹ãƒ”ãƒ«ï¼ˆ5GBä»¥ä¸Šï¼‰ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã¯æœ€å„ªå…ˆã§å¯¾å‡¦
   - ãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡çš„ãªJOINé †åºã®æ¤œè¨Ž
   - ä¸­é–“çµæžœã®ã‚µã‚¤ã‚ºå‰Šæ¸›

2. **ðŸ”„ REPARTITIONãƒ’ãƒ³ãƒˆé©ç”¨**ï¼ˆðŸš¨ **ã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®å ´åˆã®ã¿** - é‡è¦ãªæ¡ä»¶ï¼‰
   - âŒ **ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„å ´åˆ**: REPARTITIONãƒ’ãƒ³ãƒˆã¯ä¸€åˆ‡é©ç”¨ã—ãªã„
   - âœ… **ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã®ã¿**: REPARTITIONãƒ’ãƒ³ãƒˆã‚’é©ç”¨
   - âš ï¸ **è¨˜è¼‰ãƒ«ãƒ¼ãƒ«**: ã‚¹ãƒ”ãƒ«æœªæ¤œå‡ºã®å ´åˆã¯ã€ŒREPARTITIONã®é©ç”¨ã€ã‚’ä¸€åˆ‡è¨˜è¼‰ã—ãªã„
   - æ¤œå‡ºã•ã‚ŒãŸShuffle attributesã‚’åŸºã«å…·ä½“çš„ãªREPARTITIONãƒ’ãƒ³ãƒˆã‚’é©ç”¨ï¼ˆã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿ï¼‰

3. **âš–ï¸ ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼å¯¾ç­–**
   - ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ï¼ˆ10å€‹ä»¥ä¸Šï¼‰æ¤œå‡ºæ™‚ã¯åˆ†æ•£æ”¹å–„ã‚’å„ªå…ˆ
   - é©åˆ‡ãªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚­ãƒ¼ã®é¸æŠž
   - ãƒ‡ãƒ¼ã‚¿åˆ†æ•£ã®å‡ç­‰åŒ–

4. **ðŸ“ˆ ã‚·ãƒ£ãƒƒãƒ•ãƒ«æœ€é©åŒ–**
   - ã‚·ãƒ£ãƒƒãƒ•ãƒ«é‡ã®æœ€å°åŒ–
   - é©åˆ‡ãªJOINæˆ¦ç•¥ã®é¸æŠž
   - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è»¢é€é‡ã®å‰Šæ¸›

5. **ðŸŽ¯ JOINæˆ¦ç•¥æœ€é©åŒ–**
   - å°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å…ˆã«å‡¦ç†ã™ã‚‹åŠ¹çŽ‡çš„ãªJOINé †åº
   - Sparkã®è‡ªå‹•æœ€é©åŒ–ã‚’æ´»ç”¨ã—ãŸJOINæˆ¦ç•¥ï¼ˆãƒ’ãƒ³ãƒˆä¸ä½¿ç”¨ï¼‰
   - ä¸­é–“çµæžœã®ã‚µã‚¤ã‚ºæœ€å°åŒ–

6. **ðŸ’¾ ãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡åŒ–**
   - ä¸è¦ãªã‚«ãƒ©ãƒ ã®é™¤åŽ»
   - é©åˆ‡ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é †åº
   - ä¸­é–“çµæžœã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨

7. **ðŸ”§ å®Ÿè¡Œãƒ—ãƒ©ãƒ³æœ€é©åŒ–**
   - PHOTONã‚¨ãƒ³ã‚¸ãƒ³æœ€é©åŒ–ï¼ˆç›®æ¨™ã¯Photonåˆ©ç”¨çŽ‡90%ä»¥ä¸Š)
   - Liquid Clusteringæ´»ç”¨ (Whereæ¡ä»¶ã®æ›¸ãæ›ãˆå«ã‚€æ¤œè¨Žã‚’å®Ÿæ–½ï¼‰
   - CTEæ´»ç”¨ã«ã‚ˆã‚‹å…±é€šåŒ–

8. **ðŸ“Š EXPLAINçµæžœã«åŸºã¥ãæœ€é©åŒ–**ï¼ˆEXPLAIN_ENABLED=Yã®å ´åˆï¼‰
   - **Physical Planåˆ†æžã«åŸºã¥ãæœ€é©åŒ–**: 
     - éžåŠ¹çŽ‡ãªã‚¹ã‚­ãƒ£ãƒ³æ“ä½œã®æ”¹å–„
     - ã‚¸ãƒ§ã‚¤ãƒ³é †åºã®æœ€é©åŒ–ï¼ˆSparkã®è‡ªå‹•åˆ¤å®šã«ä¾å­˜ï¼‰
     - ä¸è¦ãªã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œã®å‰Šé™¤
     - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ã®é©ç”¨
   - **Photonæœªå¯¾å¿œé–¢æ•°ã®æœ€é©åŒ–**:
     - Photon Explanationã§æ¤œå‡ºã•ã‚ŒãŸæœªå¯¾å¿œé–¢æ•°ã®ä»£æ›¿é–¢æ•°ã¸ã®å¤‰æ›´
     - ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†ã«é©ã—ãŸé–¢æ•°ã¸ã®æ›¸ãæ›ãˆ
     - Photonåˆ©ç”¨çŽ‡å‘ä¸Šã®ãŸã‚ã®é–¢æ•°é¸æŠž
     - ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚æœ€é©åŒ–ã®æ´»ç”¨

9. **ðŸŽ¯ JOINé †åºã¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã®æœ€é©åŒ–**ï¼ˆé‡è¦ãªæ§‹é€ çš„æœ€é©åŒ–ï¼‰
   - **åŠ¹çŽ‡çš„ãªJOINé †åº**: å°ã•ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å¤§ãã„ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®æ®µéšŽçš„çµåˆ
   - **Sparkã®è‡ªå‹•JOINæˆ¦ç•¥**: ã‚¨ãƒ³ã‚¸ãƒ³ã®è‡ªå‹•åˆ¤å®šã«å§”ã­ã‚‹ã“ã¨ã§ã‚¨ãƒ©ãƒ¼å›žé¿
   - **çµåˆå¾Œã®REPARTITION**: çµåˆå¾Œã«GROUP BYã®åŠ¹çŽ‡åŒ–ã®ãŸã‚REPARTITIONãƒ’ãƒ³ãƒˆã‚’é©ç”¨
   - **CTEæ§‹é€ ã®æ´»ç”¨**: å¿…è¦ã«å¿œã˜ã¦CTEã‚’ä½¿ã£ã¦æ®µéšŽçš„ã«å‡¦ç†ã™ã‚‹æ§‹é€ ã§å‡ºåŠ›
   - **ã‚¹ãƒ”ãƒ«å›žé¿ã¨ä¸¦åˆ—åº¦**: ã‚¹ãƒ”ãƒ«ã‚’å›žé¿ã—ã¤ã¤ã€ä¸¦åˆ—åº¦ã®é«˜ã„å‡¦ç†ãŒã§ãã‚‹ã‚ˆã†æœ€é©åŒ–
   
   **ðŸ”„ æŽ¨å¥¨ã™ã‚‹å‡¦ç†ãƒ•ãƒ­ãƒ¼:**
   ```sql
   -- âœ… æŽ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³: åŠ¹çŽ‡çš„JOINé †åº â†’ CTE â†’ REPARTITION â†’ GROUP BY
   WITH efficient_joined AS (
     SELECT 
       large_table.columns...,
       small_table.columns...
     FROM small_table  -- å°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å…ˆã«é…ç½®
       JOIN large_table ON small_table.key = large_table.key
   ),
   repartitioned_for_groupby AS (
     SELECT /*+ REPARTITION(200, group_key) */
       columns...
     FROM efficient_joined
   )
   SELECT 
     group_key,
     COUNT(*),
     SUM(amount)
   FROM repartitioned_for_groupby
   GROUP BY group_key
   ```

ã€ðŸ”„ REPARTITIONãƒ’ãƒ³ãƒˆé©ç”¨ãƒ«ãƒ¼ãƒ« - æ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ã€‘
REPARTITIONãƒ’ãƒ³ãƒˆã‚’ä»˜ä¸Žã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã®æœ€é©åŒ–ãƒ«ãƒ¼ãƒ«ã‚’å®ˆã£ã¦ãã ã•ã„ï¼š

ðŸš¨ **æœ€é‡è¦ãƒ«ãƒ¼ãƒ«**: 
- **âŒ ã‚¹ãƒ”ãƒ«æœªæ¤œå‡ºæ™‚**: REPARTITIONãƒ’ãƒ³ãƒˆã¯çµ¶å¯¾ã«é©ç”¨ãƒ»è¨˜è¼‰ã—ã¦ã¯ã„ã‘ãªã„
- **âœ… ã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿**: REPARTITIONãƒ’ãƒ³ãƒˆã‚’é©ç”¨
- **âš ï¸ è¨˜è¼‰ç¦æ­¢**: ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„å ´åˆã€æŽ¨å¥¨äº‹é …ã‚„ç·Šæ€¥å¯¾å¿œã«ã€ŒREPARTITIONé©ç”¨ã€ã‚’å«ã‚ãªã„

æŠ€è¡“è©³ç´°:
- **REPARTITIONãƒ’ãƒ³ãƒˆã¯ SELECT /*+ REPARTITION(ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°, ã‚«ãƒ©ãƒ å) ã®å½¢å¼ã§æŒ‡å®š**
- **REPARTITIONãƒ’ãƒ³ãƒˆã®é©ç”¨ä½ç½®ã¯ã€å¯¾è±¡ã¨ãªã‚‹JOINã‚„GROUP BYã‚’å«ã‚€SELECTã®ç›´å‰ã§ã‚ã‚‹ãŸã‚ã€å‡ºåŠ›ã•ã‚ŒãŸoutput_explain_plan_*.txtã®Physical Planã‹ã‚‰å®Ÿè¡Œè¨ˆç”»ã‚’ç†è§£ã—ã€é©åˆ‡ãªä½ç½®ã«REPARTITION ãƒ’ãƒ³ãƒˆã‚’ä»˜ä¸Žã™ã‚‹ã“ã¨**

**ðŸš¨ REPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ã®é‡è¦ãªæ§‹æ–‡ãƒ«ãƒ¼ãƒ«:**
1. **JOINã‚„GROUP BYã®å‡¦ç†æ®µéšŽã§åŠ¹æžœã‚’ç™ºæ®ã™ã‚‹ãŸã‚ã€å¿…ãšã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ã™ã‚‹**
2. **ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®SELECTæ–‡ã«é…ç½®ã™ã‚‹ã¨æœ€çµ‚å‡ºåŠ›æ®µéšŽã®ã¿ã«å½±éŸ¿ã—ã€JOIN/GROUP BYå‡¦ç†æ®µéšŽã«ã¯å½±éŸ¿ã—ãªã„**
3. **è¤‡æ•°ã®REPARTITIONãƒ’ãƒ³ãƒˆã¯å„ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«å€‹åˆ¥ã«é…ç½®ã™ã‚‹**
4. **ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã¨ã‚«ãƒ©ãƒ åã¯å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦æŒ‡å®šã™ã‚‹**

ðŸš¨ **REPARTITIONãƒ’ãƒ³ãƒˆé©ç”¨ã®åŽ³æ ¼ãªãƒ«ãƒ¼ãƒ«**ï¼š
- **âŒ ã‚¹ãƒ”ãƒ«æœªæ¤œå‡º**: REPARTITIONãƒ’ãƒ³ãƒˆã¯çµ¶å¯¾ã«é©ç”¨ã—ãªã„ãƒ»è¨˜è¼‰ã—ãªã„
- **âœ… ã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿**: GROUP BYå‰ã«REPARTITION(æŽ¨å¥¨æ•°, group_by_column)
- **âœ… ã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿**: JOINå‰ã«REPARTITION(æŽ¨å¥¨æ•°, join_key)
- **é‡è¦**: ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ŒREPARTITIONã®é©ç”¨ã€ã‚’æŽ¨å¥¨äº‹é …ã«å«ã‚ãªã„
- **è¨˜è¼‰ç¦æ­¢**: ã‚¹ãƒ”ãƒ«æœªæ¤œå‡ºæ™‚ã«ã€Œç·Šæ€¥å¯¾å¿œ: REPARTITIONã®é©ç”¨ã€ç­‰ã‚’è¨˜è¼‰ã—ã¦ã¯ã„ã‘ãªã„

**ðŸš¨ CREATE TABLE AS SELECT (CTAS) ã§ã®REPARTITIONé…ç½®ã®é‡è¦ãªæ³¨æ„äº‹é …:**
- CREATE TABLE AS SELECTæ–‡ã§ã¯ã€ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®SELECTå¥ã«REPARTITIONãƒ’ãƒ³ãƒˆã‚’é…ç½®ã™ã‚‹ã¨ã€**æœ€çµ‚çš„ãªå‡ºåŠ›æ›¸ãè¾¼ã¿æ®µéšŽã®ã¿ã«å½±éŸ¿**ã—ã€JOIN ã‚„é›†è¨ˆãªã©ã®ä¸­é–“å‡¦ç†æ®µéšŽã«ã¯å½±éŸ¿ã—ãªã„
- JOINã®å‰ã«ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã‚’åˆ¶å¾¡ã™ã‚‹ã«ã¯ã€**REPARTITIONãƒ’ãƒ³ãƒˆã‚’ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ã™ã‚‹å¿…è¦ãŒã‚ã‚‹**
- ã“ã‚Œã«ã‚ˆã‚Šã€SparkãŒãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã®é©åˆ‡ãªæ™‚ç‚¹ã§ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ã—ã€æ›¸ãè¾¼ã¿æ®µéšŽã§ã¯ãªãå®Ÿè¡Œæ®µéšŽã§æœ€é©åŒ–ã•ã‚Œã‚‹

**æ­£ã—ã„CTAS REPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ä¾‹:**
```sql
-- âŒ é–“é•ã„: ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®SELECTå¥ï¼ˆæ›¸ãè¾¼ã¿æ®µéšŽã®ã¿ã«å½±éŸ¿ï¼‰
CREATE TABLE optimized_table AS
SELECT /*+ REPARTITION(200, join_key) */
  t1.column1, t2.column2
FROM table1 t1
  JOIN table2 t2 ON t1.join_key = t2.join_key

-- âœ… æ­£ã—ã„: ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ï¼ˆJOINå‡¦ç†æ®µéšŽã§æœ€é©åŒ–ï¼‰
CREATE TABLE optimized_table AS
SELECT 
  t1.column1, t2.column2
FROM (
  SELECT /*+ REPARTITION(200, join_key) */
    column1, join_key
  FROM table1
) t1
  JOIN table2 t2 ON t1.join_key = t2.join_key
```

**ðŸš¨ å…¨èˆ¬çš„ãªREPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ã®é‡è¦ãªæ³¨æ„äº‹é …:**
- **CTASä»¥å¤–ã®ã‚¯ã‚¨ãƒªã§ã‚‚åŒæ§˜**ï¼šãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®ã‚¯ã‚¨ãƒªã«REPARTITIONãƒ’ãƒ³ãƒˆã‚’é…ç½®ã™ã‚‹ã¨ã€**æœ€çµ‚çš„ãªå‡ºåŠ›æ®µéšŽã®ã¿ã«å½±éŸ¿**ã—ã€JOIN ã‚„é›†è¨ˆãªã©ã®ä¸­é–“å¤‰æ›æ®µéšŽã«ã¯å½±éŸ¿ã—ãªã„
- ã“ã®å‹•ä½œã¯ã€çµæžœã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã«æ›¸ãè¾¼ã‚€ã‹ã©ã†ã‹ã«é–¢ä¿‚ãªã**ã™ã¹ã¦ã®Spark SQLã‚¯ã‚¨ãƒªã§ä¸€è²«**ã—ã¦ã„ã‚‹
- JOINã®å…¥åŠ›æ®µéšŽã§ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’ç¢ºå®Ÿã«å®Ÿè¡Œã™ã‚‹ã«ã¯ã€**REPARTITIONãƒ’ãƒ³ãƒˆã‚’ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ã™ã‚‹å¿…è¦ãŒã‚ã‚‹**
- ã“ã‚Œã«ã‚ˆã‚Šã€SparkãŒé©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã®æ™‚ç‚¹ã§ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ã—ã€æœ€çµ‚å‡ºåŠ›æ®µéšŽã§ã¯ãªãå®Ÿè¡Œæ®µéšŽã§æœ€é©åŒ–ã•ã‚Œã‚‹

**ä¸€èˆ¬çš„ãªã‚¯ã‚¨ãƒªã§ã®æ­£ã—ã„REPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ä¾‹:**
```sql
-- âŒ é–“é•ã„: ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®SELECTå¥ï¼ˆæœ€çµ‚å‡ºåŠ›æ®µéšŽã®ã¿ã«å½±éŸ¿ï¼‰
SELECT /*+ REPARTITION(200, join_key) */
  t1.column1, t2.column2
FROM table1 t1
  JOIN table2 t2 ON t1.join_key = t2.join_key

-- âœ… æ­£ã—ã„: ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ï¼ˆJOINå‡¦ç†æ®µéšŽã§æœ€é©åŒ–ï¼‰
SELECT 
  t1.column1, t2.column2
FROM (
  SELECT /*+ REPARTITION(200, join_key) */
    column1, join_key
  FROM table1
) t1
  JOIN table2 t2 ON t1.join_key = t2.join_key

-- âœ… æ­£ã—ã„: ã‚ˆã‚Šè¤‡é›‘ãªã‚±ãƒ¼ã‚¹ï¼ˆè¤‡æ•°ã®ã‚µãƒ–ã‚¯ã‚¨ãƒªã§ã®ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ï¼‰
SELECT 
  t1.column1, t2.column2, t3.column3
FROM (
  SELECT /*+ REPARTITION(200, join_key) */
    column1, join_key
  FROM table1
) t1
  JOIN (
    SELECT /*+ REPARTITION(200, join_key) */
      column2, join_key
    FROM table2
  ) t2 ON t1.join_key = t2.join_key
  JOIN table3 t3 ON t2.join_key = t3.join_key
```

**ðŸš¨ å…¨èˆ¬çš„ãªREPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ã®é‡è¦ãªæ³¨æ„äº‹é …:**
- **CTASä»¥å¤–ã®ã‚¯ã‚¨ãƒªã§ã‚‚åŒæ§˜**ï¼šãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®ã‚¯ã‚¨ãƒªã«REPARTITIONãƒ’ãƒ³ãƒˆã‚’é…ç½®ã™ã‚‹ã¨ã€**æœ€çµ‚çš„ãªå‡ºåŠ›æ®µéšŽã®ã¿ã«å½±éŸ¿**ã—ã€JOIN ã‚„é›†è¨ˆãªã©ã®ä¸­é–“å¤‰æ›æ®µéšŽã«ã¯å½±éŸ¿ã—ãªã„
- ã“ã®å‹•ä½œã¯ã€çµæžœã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã«æ›¸ãè¾¼ã‚€ã‹ã©ã†ã‹ã«é–¢ä¿‚ãªã**ã™ã¹ã¦ã®Spark SQLã‚¯ã‚¨ãƒªã§ä¸€è²«**ã—ã¦ã„ã‚‹
- JOINã®å…¥åŠ›æ®µéšŽã§ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’ç¢ºå®Ÿã«å®Ÿè¡Œã™ã‚‹ã«ã¯ã€**REPARTITIONãƒ’ãƒ³ãƒˆã‚’ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ã™ã‚‹å¿…è¦ãŒã‚ã‚‹**
- ã“ã‚Œã«ã‚ˆã‚Šã€SparkãŒé©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã®æ™‚ç‚¹ã§ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ã—ã€æœ€çµ‚å‡ºåŠ›æ®µéšŽã§ã¯ãªãå®Ÿè¡Œæ®µéšŽã§æœ€é©åŒ–ã•ã‚Œã‚‹

**ä¸€èˆ¬çš„ãªã‚¯ã‚¨ãƒªã§ã®æ­£ã—ã„REPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ä¾‹:**
```sql
-- âŒ é–“é•ã„: ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®SELECTå¥ï¼ˆæœ€çµ‚å‡ºåŠ›æ®µéšŽã®ã¿ã«å½±éŸ¿ï¼‰
SELECT /*+ REPARTITION(200, join_key) */
  t1.column1, t2.column2
FROM table1 t1
  JOIN table2 t2 ON t1.join_key = t2.join_key

-- âœ… æ­£ã—ã„: ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ï¼ˆJOINå‡¦ç†æ®µéšŽã§æœ€é©åŒ–ï¼‰
SELECT 
  t1.column1, t2.column2
FROM (
  SELECT /*+ REPARTITION(200, join_key) */
    column1, join_key
  FROM table1
) t1
  JOIN table2 t2 ON t1.join_key = t2.join_key
```



ã€é‡è¦ãªåˆ¶ç´„ã€‘
- çµ¶å¯¾ã«ä¸å®Œå…¨ãªã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ãªã„ã§ãã ã•ã„
- ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ åã€ãƒ†ãƒ¼ãƒ–ãƒ«åã€CTEåã‚’å®Œå…¨ã«è¨˜è¿°ã—ã¦ãã ã•ã„
- ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆ...ã€[çœç•¥]ã€ç©ºç™½ãªã©ï¼‰ã¯ä¸€åˆ‡ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„
- ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®ã™ã¹ã¦ã®SELECTé …ç›®ã‚’ä¿æŒã—ã¦ãã ã•ã„
- **ðŸš¨ DISTINCTå¥ã®çµ¶å¯¾ä¿æŒ**: å…ƒã®ã‚¯ã‚¨ãƒªã«DISTINCTå¥ãŒã‚ã‚‹å ´åˆã¯ã€**å¿…ãšDISTINCTå¥ã‚’ä¿æŒ**ã—ã¦ãã ã•ã„
- **æœ€é©åŒ–æ™‚ã®DISTINCTä¿æŒ**: REPARTITIONãƒ’ãƒ³ãƒˆã‚’è¿½åŠ ã™ã‚‹éš›ã‚‚ã€DISTINCTå¥ã¯çµ¶å¯¾ã«å‰Šé™¤ã—ãªã„ã§ãã ã•ã„
- å…ƒã®ã‚¯ã‚¨ãƒªãŒé•·ã„å ´åˆã§ã‚‚ã€ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ã‚’çœç•¥ã›ãšã«è¨˜è¿°ã—ã¦ãã ã•ã„
- å®Ÿéš›ã«å®Ÿè¡Œã§ãã‚‹å®Œå…¨ãªSQLã‚¯ã‚¨ãƒªã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„
- å…ƒã®ã‚¯ã‚¨ãƒªã¨åŒã˜ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã«ãªã‚‹ã“ã¨ã‚’åŽ³å®ˆã—ã¦ãã ã•ã„

ã€ðŸš¨ æœ€é©åŒ–ã«ãŠã‘ã‚‹æ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ã€‘
**çµ¶å¯¾ã«å®ˆã‚‹ã¹ãæ–‡æ³•ãƒ«ãƒ¼ãƒ«ï¼ˆæ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ã®ãŸã‚å¿…é ˆï¼‰:**

âœ… **REPARTITIONãƒ’ãƒ³ãƒˆã®æ­£ã—ã„é…ç½®:**
```sql
-- REPARTITIONãƒ’ãƒ³ãƒˆã¯ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTç›´å¾Œã«é…ç½®
SELECT /*+ REPARTITION(200, column_name) */
  column1, column2, ...
FROM table1 t1
  JOIN table2 t2 ON t1.id = t2.id
```

âœ… **DISTINCTå¥ã¨ã®æ­£ã—ã„çµ„ã¿åˆã‚ã›ï¼ˆçµ¶å¯¾å¿…é ˆï¼‰:**
```sql
-- ðŸš¨ é‡è¦: DISTINCTå¥ã¯å¿…ãšãƒ’ãƒ³ãƒˆå¥ã®å¾Œã«é…ç½®
SELECT /*+ REPARTITION(200, column_name) */ DISTINCT
  cs.ID, cs.column1, cs.column2, ...
FROM table1 cs
  JOIN table2 t2 ON cs.id = t2.id
```

**ðŸš¨ æ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ã®ãŸã‚ã®åŸºæœ¬ãƒ«ãƒ¼ãƒ«:**
1. **ãƒ’ãƒ³ãƒˆã¯å¿…ãšãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTæ–‡ã®ç›´å¾Œã«é…ç½®**
2. **FROMå¥ã€JOINå¥ã€WHEREå¥å†…ã«ã¯çµ¶å¯¾ã«é…ç½®ã—ãªã„**
3. **REPARTITIONãƒ’ãƒ³ãƒˆã«ã¯é©åˆ‡ãªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã¨ã‚«ãƒ©ãƒ åã‚’æŒ‡å®š**

ã€å‡ºåŠ›å½¢å¼ã€‘
## ðŸš€ å‡¦ç†é€Ÿåº¦é‡è¦–ã®æœ€é©åŒ–ã•ã‚ŒãŸSQL

**ðŸŽ¯ å®Ÿéš›ã«é©ç”¨ã—ãŸæœ€é©åŒ–æ‰‹æ³•** (å®Ÿæ–½ã—ã¦ã„ãªã„æ‰‹æ³•ã¯è¨˜è¼‰ç¦æ­¢):
- [å…·ä½“çš„ã«å®Ÿè£…ã•ã‚ŒãŸæœ€é©åŒ–æ‰‹æ³•ã®ã¿ã‚’ãƒªã‚¹ãƒˆ]
- âŒ ã‚¹ãƒ”ãƒ«æœªæ¤œå‡ºã®å ´åˆ: REPARTITIONãƒ’ãƒ³ãƒˆé©ç”¨ã¯è¨˜è¼‰ã—ãªã„
- âŒ å®Ÿéš›ã«å¤‰æ›´ã—ã¦ã„ãªã„è¦ç´ : ã€Œæœ€é©åŒ–ã€ã¨ã—ã¦è¨˜è¼‰ã—ãªã„
- âœ… å®Ÿéš›ã®å¤‰æ›´å†…å®¹ã®ã¿: JOINé †åºå¤‰æ›´ã€CTEæ§‹é€ åŒ–ã€ãƒ•ã‚£ãƒ«ã‚¿æ”¹å–„ç­‰

**ðŸ’° EXPLAIN COSTãƒ™ãƒ¼ã‚¹ã®åŠ¹æžœåˆ†æž**:
- ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚³ã‚¹ãƒˆå‰Šæ¸›çŽ‡: [cost_ratio]å€ (EXPLAIN COSTæ¯”è¼ƒçµæžœ)
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›çŽ‡: [memory_ratio]å€ (çµ±è¨ˆæƒ…å ±ãƒ™ãƒ¼ã‚¹æ¯”è¼ƒ)
- æŽ¨å®šãƒ‡ãƒ¼ã‚¿å‡¦ç†åŠ¹çŽ‡: [processing_efficiency]% (ã‚¹ã‚­ãƒ£ãƒ³ãƒ»JOINåŠ¹çŽ‡æ”¹å–„)
- âš ï¸ æ•°å€¤ã¯æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ä¸­ã®ã‚³ã‚¹ãƒˆæ¯”è¼ƒçµæžœã«åŸºã¥ã

**ðŸš¨ æ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ã®æœ€çµ‚ç¢ºèª**:
- âœ… REPARTITIONãƒ’ãƒ³ãƒˆã¯é©åˆ‡ã«ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTç›´å¾Œã«é…ç½®ã•ã‚Œã¦ã„ã‚‹
- âœ… FROMå¥ã€JOINå¥ã€WHEREå¥å†…ã«ãƒ’ãƒ³ãƒˆãŒé…ç½®ã•ã‚Œã¦ã„ãªã„
- âœ… REPARTITIONãƒ’ãƒ³ãƒˆã«ã¯é©åˆ‡ãªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã¨ã‚«ãƒ©ãƒ åãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹
- âœ… **DISTINCTå¥ãŒå…ƒã®ã‚¯ã‚¨ãƒªã«ã‚ã‚‹å ´åˆã¯å¿…ãšä¿æŒã•ã‚Œã¦ã„ã‚‹**
- âœ… **ãƒ’ãƒ³ãƒˆå¥è¿½åŠ æ™‚ã«DISTINCTå¥ãŒå‰Šé™¤ã•ã‚Œã¦ã„ãªã„**
- âœ… **DISTINCTå¥ãŒãƒ’ãƒ³ãƒˆå¥ã®ç›´å¾Œã«æ­£ã—ãé…ç½®ã•ã‚Œã¦ã„ã‚‹**
- âœ… ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆ...ã€[çœç•¥]ç­‰ï¼‰ãŒä¸€åˆ‡ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„
- âœ… å®Œå…¨ãªSQLæ§‹æ–‡ã«ãªã£ã¦ã„ã‚‹ï¼ˆä¸å®Œå…¨ãªã‚¯ã‚¨ãƒªã§ã¯ãªã„ï¼‰
- âœ… NULLãƒªãƒ†ãƒ©ãƒ«ãŒé©åˆ‡ãªåž‹ã§ã‚­ãƒ£ã‚¹ãƒˆã•ã‚Œã¦ã„ã‚‹
- âœ… JOINé †åºãŒåŠ¹çŽ‡çš„ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã‚‹
- âœ… ã‚¹ãƒ”ãƒ«å›žé¿ã¨ä¸¦åˆ—åº¦å‘ä¸Šã®ä¸¡æ–¹ã‚’è€ƒæ…®ã—ãŸæ§‹é€ ã«ãªã£ã¦ã„ã‚‹
- âœ… **BROADCASTãƒ’ãƒ³ãƒˆã¯ä¸€åˆ‡ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ï¼ˆæ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ï¼‰**
- âœ… **Sparkã®è‡ªå‹•JOINæˆ¦ç•¥ã«å§”ã­ã¦ãƒ’ãƒ³ãƒˆä¸ä½¿ç”¨ã§æœ€é©åŒ–ã•ã‚Œã¦ã„ã‚‹**

```sql
-- ðŸš¨ é‡è¦: REPARTITIONãƒ’ãƒ³ãƒˆã¯ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTæ–‡ã®ç›´å¾Œã«é…ç½®
-- ä¾‹: SELECT /*+ REPARTITION(200, column_name) */ column1, column2, ...
-- ðŸš¨ DISTINCTå¥ä¿æŒä¾‹: SELECT /*+ REPARTITION(200, column_name) */ DISTINCT cs.ID, cs.column1, ...
-- ðŸš¨ REPARTITIONãƒ’ãƒ³ãƒˆã®é©åˆ‡ãªé…ç½®: SELECT /*+ REPARTITION(200, join_key) */ column1, column2, ...
-- âŒ ç¦æ­¢: BROADCASTãƒ’ãƒ³ãƒˆï¼ˆ/*+ BROADCAST */ã€/*+ BROADCAST(table) */ï¼‰ã¯ä¸€åˆ‡ä½¿ç”¨ç¦æ­¢
-- âœ… æŽ¨å¥¨: Sparkã®è‡ªå‹•JOINæˆ¦ç•¥ã«å§”ã­ã¦ãƒ’ãƒ³ãƒˆä¸ä½¿ç”¨ã§æœ€é©åŒ–
[å®Œå…¨ãªSQL - ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ãƒ»CTEãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’çœç•¥ãªã—ã§è¨˜è¿°]
```

## æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ
[3ã¤ã®ä¸»è¦æ”¹å–„ç‚¹]

## JOINæœ€é©åŒ–ã®æ ¹æ‹ 
[JOINé †åºæœ€é©åŒ–ã®è©³ç´°æ ¹æ‹ ]
- ðŸ“ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ã®æœ€é©åŒ–: å°ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å¤§ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®åŠ¹çŽ‡çš„çµåˆé †åº
- ðŸŽ¯ æœ€é©åŒ–å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«: [ãƒ†ãƒ¼ãƒ–ãƒ«åãƒªã‚¹ãƒˆ]
- âš–ï¸ JOINæˆ¦ç•¥: Sparkã®è‡ªå‹•æœ€é©åŒ–ã‚’æ´»ç”¨ã—ãŸåŠ¹çŽ‡çš„ãªçµåˆå‡¦ç†
- ðŸš€ æœŸå¾…åŠ¹æžœ: [ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è»¢é€é‡å‰Šæ¸›ãƒ»JOINå‡¦ç†é«˜é€ŸåŒ–ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«å‰Šæ¸›ãªã©]

## æœŸå¾…åŠ¹æžœ  
[å®Ÿè¡Œæ™‚é–“ãƒ»ãƒ¡ãƒ¢ãƒªãƒ»ã‚¹ãƒ”ãƒ«æ”¹å–„ã®è¦‹è¾¼ã¿ï¼ˆJOINæœ€é©åŒ–åŠ¹æžœã‚’å«ã‚€ï¼‰]
"""

    # è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
    provider = LLM_CONFIG["provider"]
    
    try:
        if provider == "databricks":
            optimized_result = _call_databricks_llm(optimization_prompt)
        elif provider == "openai":
            optimized_result = _call_openai_llm(optimization_prompt)
        elif provider == "azure_openai":
            optimized_result = _call_azure_openai_llm(optimization_prompt)
        elif provider == "anthropic":
            optimized_result = _call_anthropic_llm(optimization_prompt)
        else:
            error_msg = "âš ï¸ Configured LLM provider is not recognized"
            print(f"âŒ LLM optimization error: {error_msg}")
            return f"LLM_ERROR: {error_msg}"
        
        # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆæ”¹å–„ç‰ˆï¼šåˆ†æžçµæžœã‚’èª¤ã£ã¦ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦èªè­˜ã—ãªã„ï¼‰
        if isinstance(optimized_result, str):
            # ã‚ˆã‚Šç²¾å¯†ãªã‚¨ãƒ©ãƒ¼åˆ¤å®šï¼ˆçœŸã®ã‚¨ãƒ©ãƒ¼ã®ã¿ã‚’æ¤œå‡ºï¼‰
            def is_actual_error_response(response_text: str) -> bool:
                if not response_text:
                    return False
                
                # çœŸã®ã‚¨ãƒ©ãƒ¼ã®ã¿ã‚’æ¤œå‡ºã™ã‚‹åŽ³å¯†ãªæŒ‡æ¨™
                critical_error_indicators = [
                    '{"error_code":',
                    'HTTPError',
                    'ConnectionError',
                    'TimeoutError', 
                    'APIã‚¨ãƒ©ãƒ¼:',
                    'APIå‘¼ã³å‡ºã—ã«å¤±æ•—',
                    'ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãŒç™ºç”Ÿ',
                    'ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼:',
                    'ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è§£æžã«å¤±æ•—',
                    'Input is too long',
                    'Bad Request'
                ]
                
                # åˆ†æžçµæžœã®å…¸åž‹çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã¯é™¤å¤–
                analysis_patterns = [
                    '## ðŸš€ å‡¦ç†é€Ÿåº¦é‡è¦–ã®æœ€é©åŒ–ã•ã‚ŒãŸSQL',
                    '**ðŸŽ¯ å®Ÿéš›ã«é©ç”¨ã—ãŸæœ€é©åŒ–æ‰‹æ³•**',
                    '**ðŸ’° EXPLAIN COSTãƒ™ãƒ¼ã‚¹ã®åŠ¹æžœåˆ†æž**',
                    'WITH',
                    'SELECT',
                    '```sql',
                    'æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ',
                    'æœŸå¾…åŠ¹æžœ',
                    'ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚³ã‚¹ãƒˆå‰Šæ¸›çŽ‡',
                    'ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›çŽ‡'
                ]
                
                # åˆ†æžçµæžœã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã§ã¯ãªã„
                for pattern in analysis_patterns:
                    if pattern in response_text:
                        return False
                
                # çœŸã®ã‚¨ãƒ©ãƒ¼ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ãŒã‚ã‚‹å ´åˆã®ã¿ã‚¨ãƒ©ãƒ¼ã¨åˆ¤å®š
                for indicator in critical_error_indicators:
                    if indicator in response_text:
                        return True
                
                return False
             
             # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆæ”¹å–„ç‰ˆï¼‰
            is_error_response = is_actual_error_response(optimized_result)
            
            if is_error_response:
                print(f"âŒ Error occurred in LLM API call: {optimized_result[:200]}...")
                return f"LLM_ERROR: {optimized_result}"
        
        # thinking_enabled: Trueã®å ´åˆã«optimized_resultãŒãƒªã‚¹ãƒˆã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚å¯¾å¿œ
        # ã“ã“ã§ã¯å…ƒã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã‚’ä¿æŒã—ã¦è¿”ã™ï¼ˆå¾Œã§ç”¨é€”ã«å¿œã˜ã¦å¤‰æ›ï¼‰
        return optimized_result
        
    except Exception as e:
        error_msg = f"âš ï¸ Error occurred during SQL optimization generation: {str(e)}"
        print(f"âŒ LLM optimization exception error: {error_msg}")
        return f"LLM_ERROR: {error_msg}"



def generate_top10_time_consuming_processes_report(extracted_metrics: Dict[str, Any], limit_nodes: int = 10) -> str:
    """
    æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’æ–‡å­—åˆ—ã¨ã—ã¦ç”Ÿæˆ
    
    ðŸš¨ é‡è¦: ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ãƒ‡ã‚°ãƒ¬é˜²æ­¢
    - ä¸¦åˆ—å®Ÿè¡ŒãƒŽãƒ¼ãƒ‰ã®æ™‚é–“åˆè¨ˆã‚’å…¨ä½“æ™‚é–“ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢
    - overall_metrics.total_time_msï¼ˆwall-clock timeï¼‰ã‚’å„ªå…ˆä½¿ç”¨
    - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã¯æœ€å¤§ãƒŽãƒ¼ãƒ‰æ™‚é–“ã‚’ä½¿ç”¨ï¼ˆåˆè¨ˆã§ã¯ãªã„ï¼‰
    
    Args:
        extracted_metrics: æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹
        limit_nodes: è¡¨ç¤ºã™ã‚‹ãƒŽãƒ¼ãƒ‰æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10ã€ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›æ™‚ã¯5ï¼‰
    
    Returns:
        str: å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆ
    """
    report_lines = []
    
    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãƒŽãƒ¼ãƒ‰æ•°ã«å¿œã˜ã¦èª¿æ•´
    title = f"æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP{limit_nodes}" if limit_nodes <= 10 else "æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10"
    report_lines.append(f"## ðŸŒ {title}")
    report_lines.append("=" * 80)
    report_lines.append("ðŸ“Š ã‚¢ã‚¤ã‚³ãƒ³èª¬æ˜Ž: â±ï¸æ™‚é–“ ðŸ’¾ãƒ¡ãƒ¢ãƒª ðŸ”¥ðŸŒä¸¦åˆ—åº¦ ðŸ’¿ã‚¹ãƒ”ãƒ« âš–ï¸ã‚¹ã‚­ãƒ¥ãƒ¼")
    report_lines.append('ðŸ’¿ ã‚¹ãƒ”ãƒ«åˆ¤å®š: "Num bytes spilled to disk due to memory pressure" ã¾ãŸã¯ "Sink - Num bytes spilled to disk due to memory pressure" > 0')
    report_lines.append("ðŸŽ¯ ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®š: 'AQEShuffleRead - Number of skewed partitions' > 0")
    report_lines.append("")

    # ãƒŽãƒ¼ãƒ‰ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
    sorted_nodes = sorted(extracted_metrics['node_metrics'], 
                         key=lambda x: x['key_metrics'].get('durationMs', 0), 
                         reverse=True)
    
    # æŒ‡å®šã•ã‚ŒãŸãƒŽãƒ¼ãƒ‰æ•°ã¾ã§å‡¦ç†
    final_sorted_nodes = sorted_nodes[:limit_nodes]

    if final_sorted_nodes:
        # ðŸš¨ é‡è¦: æ­£ã—ã„å…¨ä½“æ™‚é–“ã®è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
        # 1. overall_metricsã‹ã‚‰å…¨ä½“å®Ÿè¡Œæ™‚é–“ã‚’å–å¾—ï¼ˆwall-clock timeï¼‰
        overall_metrics = extracted_metrics.get('overall_metrics', {})
        total_duration = overall_metrics.get('total_time_ms', 0)
        
        # ðŸš¨ ä¸¦åˆ—å®Ÿè¡Œå•é¡Œã®ä¿®æ­£: task_total_time_msã‚’å„ªå…ˆä½¿ç”¨
        task_total_time_ms = overall_metrics.get('task_total_time_ms', 0)
        
        if task_total_time_ms > 0:
            total_duration = task_total_time_ms
            print(f"âœ… generate_top10 report: Parallel execution support - using task_total_time_ms: {total_duration:,} ms ({total_duration/3600000:.1f} hours)")
        elif total_duration <= 0:
            # execution_time_msã‚’æ¬¡ã®å„ªå…ˆåº¦ã§ä½¿ç”¨
            execution_time_ms = overall_metrics.get('execution_time_ms', 0)
            if execution_time_ms > 0:
                total_duration = execution_time_ms
                print(f"âš ï¸ generate_top10 report: task_total_time_ms unavailable, using execution_time_ms: {total_duration} ms")
            else:
                # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                max_node_time = max([node['key_metrics'].get('durationMs', 0) for node in sorted_nodes], default=1)
                total_duration = int(max_node_time * 1.2)
                print(f"âš ï¸ generate_top10 report: Final fallback - using estimated time: {total_duration} ms")
        
        report_lines.append(f"ðŸ“Š ç´¯ç©ã‚¿ã‚¹ã‚¯å®Ÿè¡Œæ™‚é–“ï¼ˆä¸¦åˆ—ï¼‰: {total_duration:,} ms ({total_duration/3600000:.1f} æ™‚é–“)")
        report_lines.append(f"ðŸ“ˆ TOP{limit_nodes}åˆè¨ˆæ™‚é–“ï¼ˆä¸¦åˆ—å®Ÿè¡Œï¼‰: {sum(node['key_metrics'].get('durationMs', 0) for node in final_sorted_nodes):,} ms")

        report_lines.append("")
        
        for i, node in enumerate(final_sorted_nodes):
            # ãƒã‚°ä¿®æ­£ï¼šå¤‰æ•°ã‚’æ­£ã—ãå®šç¾©
            duration_ms = node['key_metrics'].get('durationMs', 0)
            rows_num = node['key_metrics'].get('numOutputRows', 0)
            memory_mb = node['key_metrics'].get('peakMemoryBytes', 0) / 1024 / 1024
            
            # ðŸš¨ é‡è¦: æ­£ã—ã„ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
            # wall-clock timeã«å¯¾ã™ã‚‹å„ãƒŽãƒ¼ãƒ‰ã®å®Ÿè¡Œæ™‚é–“ã®å‰²åˆ
            time_percentage = min((duration_ms / max(total_duration, 1)) * 100, 100.0)
            
            # æ™‚é–“ã®é‡è¦åº¦ã«åŸºã¥ã„ã¦ã‚¢ã‚¤ã‚³ãƒ³ã‚’é¸æŠž
            if duration_ms >= 10000:  # 10ç§’ä»¥ä¸Š
                time_icon = "ðŸ”´"
                severity = "CRITICAL"
            elif duration_ms >= 5000:  # 5ç§’ä»¥ä¸Š
                time_icon = "ðŸŸ "
                severity = "HIGH"
            elif duration_ms >= 1000:  # 1ç§’ä»¥ä¸Š
                time_icon = "ðŸŸ¡"
                severity = "MEDIUM"
            else:
                time_icon = "ðŸŸ¢"
                severity = "LOW"
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ã‚¢ã‚¤ã‚³ãƒ³
            memory_icon = "ðŸ’š" if memory_mb < 100 else "âš ï¸" if memory_mb < 1000 else "ðŸš¨"
            
            # ã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹ãƒŽãƒ¼ãƒ‰åã‚’å–å¾—
            raw_node_name = node['name']
            node_name = get_meaningful_node_name(node, extracted_metrics)
            short_name = node_name[:100] + "..." if len(node_name) > 100 else node_name
            
            # ä¸¦åˆ—åº¦æƒ…å ±ã®å–å¾—ï¼ˆä¿®æ­£ç‰ˆ: è¤‡æ•°ã®Tasks totalãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ï¼‰
            parallelism_data = extract_parallelism_metrics(node)
            
            # å¾“æ¥ã®å˜ä¸€å€¤ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
            num_tasks = parallelism_data.get('tasks_total', 0)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Sink - Tasks totalã¾ãŸã¯Source - Tasks totalãŒã‚ã‚‹å ´åˆ
            if num_tasks == 0:
                if parallelism_data.get('sink_tasks_total', 0) > 0:
                    num_tasks = parallelism_data.get('sink_tasks_total', 0)
                elif parallelism_data.get('source_tasks_total', 0) > 0:
                    num_tasks = parallelism_data.get('source_tasks_total', 0)
            
            # ã‚¹ãƒ”ãƒ«æ¤œå‡ºï¼ˆã‚»ãƒ«33ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ - æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®ã¿ï¼‰
            spill_detected = False
            spill_bytes = 0
            exact_spill_metrics = [
                "Num bytes spilled to disk due to memory pressure",
                "Sink - Num bytes spilled to disk due to memory pressure",
                "Sink/Num bytes spilled to disk due to memory pressure"
            ]
            
            # detailed_metricsã‹ã‚‰æ¤œç´¢
            detailed_metrics = node.get('detailed_metrics', {})
            for metric_key, metric_info in detailed_metrics.items():
                metric_value = metric_info.get('value', 0)
                metric_label = metric_info.get('label', '')
                
                if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                    spill_detected = True
                    spill_bytes = max(spill_bytes, metric_value)
                    break
            
            # raw_metricsã‹ã‚‰æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not spill_detected:
                raw_metrics = node.get('metrics', [])
                for metric in raw_metrics:
                    metric_key = metric.get('key', '')
                    metric_label = metric.get('label', '')
                    metric_value = metric.get('value', 0)
                    
                    if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                        spill_detected = True
                        spill_bytes = max(spill_bytes, metric_value)
                        break
            
            # ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º: AQEShuffleRead - Number of skewed partitions ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä½¿ç”¨ï¼ˆæ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®ã¿ï¼‰
            skew_detected = False
            skewed_partitions = 0
            target_skew_metric = "AQEShuffleRead - Number of skewed partitions"
            
            # detailed_metricsã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢
            detailed_metrics = node.get('detailed_metrics', {})
            for metric_key, metric_info in detailed_metrics.items():
                if metric_key == target_skew_metric:
                    try:
                        skewed_partitions = int(metric_info.get('value', 0))
                        if skewed_partitions > 0:
                            skew_detected = True
                        break
                    except (ValueError, TypeError):
                        continue
            
            # key_metricsã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not skew_detected:
                key_metrics = node.get('key_metrics', {})
                if target_skew_metric in key_metrics:
                    try:
                        skewed_partitions = int(key_metrics[target_skew_metric])
                        if skewed_partitions > 0:
                            skew_detected = True
                    except (ValueError, TypeError):
                        pass
            
            # ä¸¦åˆ—åº¦ã‚¢ã‚¤ã‚³ãƒ³
            parallelism_icon = "ðŸ”¥" if num_tasks >= 10 else "âš ï¸" if num_tasks >= 5 else "ðŸŒ"
            # ã‚¹ãƒ”ãƒ«ã‚¢ã‚¤ã‚³ãƒ³
            spill_icon = "ðŸ’¿" if spill_detected else "âœ…"
            # ã‚¹ã‚­ãƒ¥ãƒ¼ã‚¢ã‚¤ã‚³ãƒ³
            skew_icon = "âš–ï¸" if skew_detected else "âœ…"
            
            report_lines.append(f"{i+1:2d}. {time_icon}{memory_icon}{parallelism_icon}{spill_icon}{skew_icon} [{severity:8}] {short_name}")
            report_lines.append(f"    â±ï¸  å®Ÿè¡Œæ™‚é–“: {duration_ms:>8,} ms ({duration_ms/1000:>6.1f} sec) - ç´¯ç©æ™‚é–“ã® {time_percentage:>5.1f}%")
            report_lines.append(f"    ðŸ“Š å‡¦ç†è¡Œæ•°: {rows_num:>8,} è¡Œ")
            report_lines.append(f"    ðŸ’¾ ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: {memory_mb:>6.1f} MB")
            # è¤‡æ•°ã®Tasks totalãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º
            parallelism_display = []
            for task_metric in parallelism_data.get('all_tasks_metrics', []):
                parallelism_display.append(f"{task_metric['name']}: {task_metric['value']}")
            
            if parallelism_display:
                report_lines.append(f"    ðŸ”§ ä¸¦åˆ—åº¦: {' | '.join(parallelism_display)}")
            else:
                report_lines.append(f"    ðŸ”§ ä¸¦åˆ—åº¦: {num_tasks:>3d} ã‚¿ã‚¹ã‚¯")
            
            # ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®šï¼ˆAQEã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã¨AQEShuffleReadå¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã®ä¸¡æ–¹ã‚’è€ƒæ…®ï¼‰
            aqe_shuffle_skew_warning = parallelism_data.get('aqe_shuffle_skew_warning', False)
            
            if skew_detected:
                skew_status = "AQEã§æ¤œå‡ºãƒ»å¯¾å¿œæ¸ˆ"
            elif aqe_shuffle_skew_warning:
                skew_status = "æ½œåœ¨çš„ãªã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§ã‚ã‚Š"
            else:
                skew_status = "ãªã—"
            
            report_lines.append(f"    ðŸ’¿ ã‚¹ãƒ”ãƒ«: {'ã‚ã‚Š' if spill_detected else 'ãªã—'} | âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼: {skew_status}")
            
            # AQEShuffleReadãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¡¨ç¤º
            aqe_shuffle_metrics = parallelism_data.get('aqe_shuffle_metrics', [])
            if aqe_shuffle_metrics:
                aqe_display = []
                for aqe_metric in aqe_shuffle_metrics:
                    if aqe_metric['name'] == "AQEShuffleRead - Number of partitions":
                        aqe_display.append(f"ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {aqe_metric['value']}")
                    elif aqe_metric['name'] == "AQEShuffleRead - Partition data size":
                        aqe_display.append(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {aqe_metric['value']:,} bytes")
                
                if aqe_display:
                    report_lines.append(f"    ðŸ”„ AQEShuffleRead: {' | '.join(aqe_display)}")
                    
                    # å¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã¨è­¦å‘Šè¡¨ç¤º
                    avg_partition_size = parallelism_data.get('aqe_shuffle_avg_partition_size', 0)
                    if avg_partition_size > 0:
                        avg_size_mb = avg_partition_size / (1024 * 1024)
                        report_lines.append(f"    ðŸ“Š å¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚º: {avg_size_mb:.2f} MB")
                        
                        # 512MBä»¥ä¸Šã®å ´åˆã«è­¦å‘Š
                        if parallelism_data.get('aqe_shuffle_skew_warning', False):
                            report_lines.append(f"    âš ï¸  ã€è­¦å‘Šã€‘ å¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºãŒ512MBä»¥ä¸Š - æ½œåœ¨çš„ãªã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§ã‚ã‚Š")
            
            # åŠ¹çŽ‡æ€§æŒ‡æ¨™ï¼ˆè¡Œ/ç§’ï¼‰ã‚’è¨ˆç®—
            if duration_ms > 0:
                rows_per_sec = (rows_num * 1000) / duration_ms
                report_lines.append(f"    ðŸš€ å‡¦ç†åŠ¹çŽ‡: {rows_per_sec:>8,.0f} è¡Œ/ç§’")
            
            # ãƒ•ã‚£ãƒ«ã‚¿çŽ‡è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ä»˜ãï¼‰
            filter_result = calculate_filter_rate(node)
            filter_display = format_filter_rate_display(filter_result)
            if filter_display:
                report_lines.append(f"    {filter_display}")
            else:
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼šãªãœãƒ•ã‚£ãƒ«ã‚¿çŽ‡ãŒè¡¨ç¤ºã•ã‚Œãªã„ã‹ã‚’ç¢ºèª
                if filter_result["has_filter_metrics"]:
                    report_lines.append(f"    ðŸ“‚ ãƒ•ã‚£ãƒ«ã‚¿çŽ‡: {filter_result['filter_rate']:.1%} (èª­ã¿è¾¼ã¿: {filter_result['files_read_bytes']/(1024*1024*1024):.2f}GB, ãƒ—ãƒ«ãƒ¼ãƒ³: {filter_result['files_pruned_bytes']/(1024*1024*1024):.2f}GB)")
                else:
                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œç´¢ã®ãƒ‡ãƒãƒƒã‚°
                    debug_info = []
                    detailed_metrics = node.get('detailed_metrics', {})
                    for metric_key, metric_info in detailed_metrics.items():
                        metric_label = metric_info.get('label', '')
                        if 'file' in metric_label.lower() and ('read' in metric_label.lower() or 'prun' in metric_label.lower()):
                            debug_info.append(f"{metric_label}: {metric_info.get('value', 0)}")
                    
                    if debug_info:
                        report_lines.append(f"    ðŸ“‚ ãƒ•ã‚£ãƒ«ã‚¿é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œå‡º: {', '.join(debug_info[:2])}")
            
            # ã‚¹ãƒ”ãƒ«è©³ç´°æƒ…å ±ï¼ˆã‚·ãƒ³ãƒ—ãƒ«è¡¨ç¤ºï¼‰
            spill_display = ""
            if spill_detected and spill_bytes > 0:
                spill_mb = spill_bytes / 1024 / 1024
                if spill_mb >= 1024:  # GBå˜ä½
                    spill_display = f"{spill_mb/1024:.2f} GB"
                else:  # MBå˜ä½
                    spill_display = f"{spill_mb:.1f} MB"
                report_lines.append(f"    ðŸ’¿ ã‚¹ãƒ”ãƒ«: {spill_display}")
            
            # ShuffleãƒŽãƒ¼ãƒ‰ã®å ´åˆã¯å¸¸ã«Shuffle attributesã‚’è¡¨ç¤º
            if "shuffle" in raw_node_name.lower():
                shuffle_attributes = extract_shuffle_attributes(node)
                if shuffle_attributes:
                    report_lines.append(f"    ðŸ”„ Shuffleå±žæ€§: {', '.join(shuffle_attributes)}")
                    
                    # REPARTITIONãƒ’ãƒ³ãƒˆã®ææ¡ˆï¼ˆã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã®ã¿ï¼‰
                    if spill_detected and spill_bytes > 0 and spill_display:
                        suggested_partitions = max(num_tasks * 2, 200)  # æœ€å°200ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
                        
                        # Shuffleå±žæ€§ã§æ¤œå‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ ã‚’å…¨ã¦ä½¿ç”¨ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
                        repartition_columns = ", ".join(shuffle_attributes)
                        
                        report_lines.append(f"    ðŸ’¡ æœ€é©åŒ–ææ¡ˆ: REPARTITION({suggested_partitions}, {repartition_columns})")
                        report_lines.append(f"       ç†ç”±: ã‚¹ãƒ”ãƒ«({spill_display})ã‚’æ”¹å–„ã™ã‚‹ãŸã‚")
                        report_lines.append(f"       å¯¾è±¡: Shuffleå±žæ€§å…¨{len(shuffle_attributes)}ã‚«ãƒ©ãƒ ã‚’å®Œå…¨ä½¿ç”¨")
                else:
                    report_lines.append(f"    ðŸ”„ Shuffleå±žæ€§: è¨­å®šãªã—")
            
            # ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰ã®å ´åˆã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã‚’è¡¨ç¤º
            if "scan" in raw_node_name.lower():
                cluster_attributes = extract_cluster_attributes(node)
                if cluster_attributes:
                    report_lines.append(f"    ðŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: {', '.join(cluster_attributes)}")
                else:
                    report_lines.append(f"    ðŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: è¨­å®šãªã—")
            
            # ã‚¹ã‚­ãƒ¥ãƒ¼è©³ç´°æƒ…å ±
            if skew_detected and skewed_partitions > 0:
                report_lines.append(f"    âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼è©³ç´°: {skewed_partitions} å€‹ã®ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ï¼ˆAQEShuffleReadæ¤œå‡ºï¼‰")
            
            # ãƒŽãƒ¼ãƒ‰IDã‚‚è¡¨ç¤º
            report_lines.append(f"    ðŸ†” ãƒŽãƒ¼ãƒ‰ID: {node.get('node_id', node.get('id', 'N/A'))}")
            report_lines.append("")
            
    else:
        report_lines.append("âš ï¸ ãƒŽãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    return "\n".join(report_lines)

def save_execution_plan_analysis(plan_info: Dict[str, Any], output_dir: str = "/tmp") -> Dict[str, str]:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æžçµæžœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    
    Args:
        plan_info: extract_execution_plan_info()ã®çµæžœ
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        
    Returns:
        Dict: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«åã®è¾žæ›¸
    """
    from datetime import datetime
    import json
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç”Ÿæˆ
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åå®šç¾©
    plan_json_filename = f"output_execution_plan_analysis_{timestamp}.json"
    plan_report_filename = f"output_execution_plan_report_{timestamp}.md"
    
    # JSONå½¢å¼ã§ãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’ä¿å­˜
    with open(plan_json_filename, 'w', encoding='utf-8') as f:
        json.dump(plan_info, f, ensure_ascii=False, indent=2)
    
    # Markdownå½¢å¼ã§ãƒ—ãƒ©ãƒ³åˆ†æžãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
    with open(plan_report_filename, 'w', encoding='utf-8') as f:
        report_content = generate_execution_plan_markdown_report(plan_info)
        f.write(report_content)
    
    return {
        'plan_json_file': plan_json_filename,
        'plan_report_file': plan_report_filename
    }

def generate_execution_plan_markdown_report(plan_info: Dict[str, Any]) -> str:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æžçµæžœã®Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    
    Args:
        plan_info: extract_execution_plan_info()ã®çµæžœ
        
    Returns:
        str: Markdownãƒ¬ãƒãƒ¼ãƒˆ
    """
    if OUTPUT_LANGUAGE == 'ja':
        return generate_execution_plan_markdown_report_ja(plan_info)
    else:
        return generate_execution_plan_markdown_report_en(plan_info)

def generate_execution_plan_markdown_report_ja(plan_info: Dict[str, Any]) -> str:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æžçµæžœã®Markdownãƒ¬ãƒãƒ¼ãƒˆï¼ˆæ—¥æœ¬èªžç‰ˆï¼‰
    """
    from datetime import datetime
    
    lines = []
    lines.append("# Databricks SQLå®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æžãƒ¬ãƒãƒ¼ãƒˆ")
    lines.append("")
    lines.append(f"**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
    lines.append("")
    
    # ãƒ—ãƒ©ãƒ³ã‚µãƒžãƒªãƒ¼
    plan_summary = plan_info.get("plan_summary", {})
    lines.append("## ðŸ“Š å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‚µãƒžãƒªãƒ¼")
    lines.append("")
    lines.append(f"- **ç·ãƒŽãƒ¼ãƒ‰æ•°**: {plan_summary.get('total_nodes', 0)}")
    lines.append(f"- **BROADCASTãƒŽãƒ¼ãƒ‰æ•°**: {plan_summary.get('broadcast_nodes_count', 0)}")
    lines.append(f"- **JOINãƒŽãƒ¼ãƒ‰æ•°**: {plan_summary.get('join_nodes_count', 0)}")
    lines.append(f"- **ã‚¹ã‚­ãƒ£ãƒ³ãƒŽãƒ¼ãƒ‰æ•°**: {plan_summary.get('scan_nodes_count', 0)}")
    lines.append(f"- **ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒŽãƒ¼ãƒ‰æ•°**: {plan_summary.get('shuffle_nodes_count', 0)}")
    lines.append(f"- **é›†ç´„ãƒŽãƒ¼ãƒ‰æ•°**: {plan_summary.get('aggregate_nodes_count', 0)}")
    lines.append(f"- **BROADCASTãŒä½¿ç”¨ä¸­**: {'ã¯ã„' if plan_summary.get('has_broadcast_joins', False) else 'ã„ã„ãˆ'}")
    lines.append(f"- **ã‚¹ã‚­ãƒ£ãƒ³ã•ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«æ•°**: {plan_summary.get('tables_scanned', 0)}")
    lines.append("")
    
    # JOINæˆ¦ç•¥åˆ†æž
    unique_join_strategies = plan_summary.get('unique_join_strategies', [])
    if unique_join_strategies:
        lines.append("## ðŸ”— JOINæˆ¦ç•¥åˆ†æž")
        lines.append("")
        for strategy in unique_join_strategies:
            strategy_jp = {
                'broadcast_hash_join': 'ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆãƒãƒƒã‚·ãƒ¥JOIN',
                'sort_merge_join': 'ã‚½ãƒ¼ãƒˆãƒžãƒ¼ã‚¸JOIN',
                'shuffle_hash_join': 'ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒƒã‚·ãƒ¥JOIN',
                'broadcast_nested_loop_join': 'ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆãƒã‚¹ãƒˆãƒ«ãƒ¼ãƒ—JOIN'
            }.get(strategy, strategy)
            lines.append(f"- **{strategy_jp}** (`{strategy}`)")
        lines.append("")
    
    # BROADCASTãƒŽãƒ¼ãƒ‰è©³ç´°
    broadcast_nodes = plan_info.get("broadcast_nodes", [])
    if broadcast_nodes:
        lines.append("## ðŸ“¡ BROADCASTãƒŽãƒ¼ãƒ‰è©³ç´°")
        lines.append("")
        for i, node in enumerate(broadcast_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **ãƒŽãƒ¼ãƒ‰ID**: {node['node_id']}")
            lines.append(f"- **ãƒŽãƒ¼ãƒ‰ã‚¿ã‚°**: {node['node_tag']}")
            
            metadata = node.get('metadata', [])
            if metadata:
                lines.append("- **é–¢é€£ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿**:")
                for meta in metadata[:5]:  # æœ€å¤§5å€‹ã¾ã§è¡¨ç¤º
                    key = meta.get('key', '')
                    value = meta.get('value', '')
                    values = meta.get('values', [])
                    if values:
                        lines.append(f"  - **{key}**: {', '.join(map(str, values[:3]))}")
                    elif value:
                        lines.append(f"  - **{key}**: {value}")
            lines.append("")
    
    # JOINãƒŽãƒ¼ãƒ‰è©³ç´°
    join_nodes = plan_info.get("join_nodes", [])
    if join_nodes:
        lines.append("## ðŸ”— JOINãƒŽãƒ¼ãƒ‰è©³ç´°")
        lines.append("")
        for i, node in enumerate(join_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **ãƒŽãƒ¼ãƒ‰ID**: {node['node_id']}")
            lines.append(f"- **JOINæˆ¦ç•¥**: {node['join_strategy']}")
            lines.append(f"- **JOINã‚¿ã‚¤ãƒ—**: {node['join_type']}")
            
            join_keys = node.get('join_keys', [])
            if join_keys:
                lines.append(f"- **JOINã‚­ãƒ¼**: {', '.join(join_keys[:5])}")
            lines.append("")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³è©³ç´°ï¼ˆã‚µã‚¤ã‚ºæŽ¨å®šæƒ…å ±ã‚’å«ã‚€ï¼‰
    table_scan_details = plan_info.get("table_scan_details", {})
    table_size_estimates = plan_info.get("table_size_estimates", {})
    if table_scan_details:
        lines.append("## ðŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³è©³ç´°")
        lines.append("")
        for table_name, scan_detail in table_scan_details.items():
            lines.append(f"### {table_name}")
            lines.append("")
            lines.append(f"- **ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼**: {scan_detail.get('file_format', 'unknown')}")
            lines.append(f"- **ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿æ•°**: {len(scan_detail.get('pushed_filters', []))}")
            lines.append(f"- **å‡ºåŠ›ã‚«ãƒ©ãƒ æ•°**: {len(scan_detail.get('output_columns', []))}")
            
            # å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‹ã‚‰ã®ã‚µã‚¤ã‚ºæŽ¨å®šæƒ…å ±ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
            # size_info = table_size_estimates.get(table_name)
            # if size_info:
            #     lines.append(f"- **æŽ¨å®šã‚µã‚¤ã‚ºï¼ˆå®Ÿè¡Œãƒ—ãƒ©ãƒ³ï¼‰**: {size_info['estimated_size_mb']:.1f}MB")
            #     lines.append(f"- **ã‚µã‚¤ã‚ºæŽ¨å®šä¿¡é ¼åº¦**: {size_info.get('confidence', 'medium')}")
            #     if 'num_files' in size_info:
            #         lines.append(f"- **ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {size_info['num_files']}")
            #     if 'num_partitions' in size_info:
            #         lines.append(f"- **ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°**: {size_info['num_partitions']}")
            
            pushed_filters = scan_detail.get('pushed_filters', [])
            if pushed_filters:
                lines.append("- **ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿**:")
                for filter_expr in pushed_filters[:3]:  # æœ€å¤§3å€‹ã¾ã§è¡¨ç¤º
                    lines.append(f"  - `{filter_expr}`")
            lines.append("")
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒŽãƒ¼ãƒ‰è©³ç´°
    shuffle_nodes = plan_info.get("shuffle_nodes", [])
    if shuffle_nodes:
        lines.append("## ðŸ”„ ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒŽãƒ¼ãƒ‰è©³ç´°")
        lines.append("")
        for i, node in enumerate(shuffle_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **ãƒŽãƒ¼ãƒ‰ID**: {node['node_id']}")
            
            partition_keys = node.get('partition_keys', [])
            if partition_keys:
                lines.append(f"- **ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚­ãƒ¼**: {', '.join(partition_keys)}")
            lines.append("")
    
    # é›†ç´„ãƒŽãƒ¼ãƒ‰è©³ç´°
    aggregate_nodes = plan_info.get("aggregate_nodes", [])
    if aggregate_nodes:
        lines.append("## ðŸ“Š é›†ç´„ãƒŽãƒ¼ãƒ‰è©³ç´°")
        lines.append("")
        for i, node in enumerate(aggregate_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **ãƒŽãƒ¼ãƒ‰ID**: {node['node_id']}")
            
            group_keys = node.get('group_keys', [])
            if group_keys:
                lines.append(f"- **ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã‚­ãƒ¼**: {', '.join(group_keys[:5])}")
            
            agg_expressions = node.get('aggregate_expressions', [])
            if agg_expressions:
                lines.append(f"- **é›†ç´„é–¢æ•°**: {', '.join(agg_expressions[:5])}")
            lines.append("")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæŽ¨å®šæƒ…å ±ã‚µãƒžãƒªãƒ¼ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     lines.append("## ðŸ“ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæŽ¨å®šæƒ…å ±ï¼ˆå®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒ™ãƒ¼ã‚¹ï¼‰")
    #     lines.append("")
    #     total_estimated_size = sum(size_info['estimated_size_mb'] for size_info in table_size_estimates.values())
    #     lines.append(f"- **æŽ¨å®šå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°**: {len(table_size_estimates)}")
    #     lines.append(f"- **ç·æŽ¨å®šã‚µã‚¤ã‚º**: {total_estimated_size:.1f}MB")
    #     lines.append("")
    #     
    #     for table_name, size_info in list(table_size_estimates.items())[:5]:  # æœ€å¤§5ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
    #         lines.append(f"### {table_name}")
    #         lines.append(f"- **æŽ¨å®šã‚µã‚¤ã‚º**: {size_info['estimated_size_mb']:.1f}MB")
    #         lines.append(f"- **ä¿¡é ¼åº¦**: {size_info.get('confidence', 'medium')}")
    #         lines.append(f"- **ãƒŽãƒ¼ãƒ‰**: {size_info.get('node_name', 'unknown')}")
    #         if 'num_files' in size_info:
    #             lines.append(f"- **ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {size_info['num_files']}")
    #         lines.append("")
    #     
    #     if len(table_size_estimates) > 5:
    #         lines.append(f"...ä»– {len(table_size_estimates) - 5} ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆè©³ç´°ã¯ä¸Šè¨˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³å‚ç…§ï¼‰")
    #         lines.append("")
    
    # æœ€é©åŒ–æŽ¨å¥¨äº‹é …
    lines.append("## ðŸ’¡ ãƒ—ãƒ©ãƒ³ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–æŽ¨å¥¨äº‹é …")
    lines.append("")
    
    if plan_summary.get('has_broadcast_joins', False):
        lines.append("âœ… **æ—¢ã«BROADCAST JOINãŒé©ç”¨ã•ã‚Œã¦ã„ã¾ã™**")
        lines.append("- ç¾åœ¨ã®å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã§BROADCASTæœ€é©åŒ–ãŒæœ‰åŠ¹")
        
        # BROADCASTã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
        broadcast_tables = plan_summary.get('broadcast_tables', [])
        if broadcast_tables:
            lines.append(f"- **BROADCASTã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«**: {', '.join(broadcast_tables)}")
        
        lines.append("- è¿½åŠ ã®BROADCASTé©ç”¨æ©Ÿä¼šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    else:
        lines.append("âš ï¸ **BROADCAST JOINãŒæœªé©ç”¨ã§ã™**")
        lines.append("- å°ãƒ†ãƒ¼ãƒ–ãƒ«ã«BROADCASTãƒ’ãƒ³ãƒˆã®é©ç”¨ã‚’æ¤œè¨Ž")
        lines.append("- 30MBé–¾å€¤ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç‰¹å®šã—ã¦ãã ã•ã„")
    lines.append("")
    
    if plan_summary.get('shuffle_nodes_count', 0) > 3:
        lines.append("âš ï¸ **å¤šæ•°ã®ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ**")
        lines.append("- ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ•£ã¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã‚’è¦‹ç›´ã—")
        lines.append("- Liquid Clusteringã®é©ç”¨ã‚’æ¤œè¨Ž")
    lines.append("")
    
    # ã‚µã‚¤ã‚ºæŽ¨å®šãƒ™ãƒ¼ã‚¹ã®æœ€é©åŒ–ææ¡ˆï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     small_tables = [name for name, info in table_size_estimates.items() if info['estimated_size_mb'] <= 30]
    #     if small_tables:
    #         lines.append("ðŸ’¡ **å®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒ™ãƒ¼ã‚¹BROADCASTæŽ¨å¥¨**")
    #         lines.append(f"- 30MBä»¥ä¸‹ã®å°ãƒ†ãƒ¼ãƒ–ãƒ«: {len(small_tables)}å€‹æ¤œå‡º")
    #         for table in small_tables[:3]:  # æœ€å¤§3å€‹è¡¨ç¤º
    #             size_mb = table_size_estimates[table]['estimated_size_mb']
    #             lines.append(f"  â€¢ {table}: {size_mb:.1f}MBï¼ˆBROADCASTå€™è£œï¼‰")
    #         if len(small_tables) > 3:
    #             lines.append(f"  â€¢ ...ä»– {len(small_tables) - 3} ãƒ†ãƒ¼ãƒ–ãƒ«")
    #         lines.append("")
    
    lines.append("---")
    lines.append("")
    lines.append("ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ã€Databricks SQLå®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æžãƒ„ãƒ¼ãƒ«ã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚")
    
    return '\n'.join(lines)

def generate_execution_plan_markdown_report_en(plan_info: Dict[str, Any]) -> str:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æžçµæžœã®Markdownãƒ¬ãƒãƒ¼ãƒˆï¼ˆè‹±èªžç‰ˆï¼‰
    """
    from datetime import datetime
    
    lines = []
    lines.append("# Databricks SQL Execution Plan Analysis Report")
    lines.append("")
    lines.append(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("")
    
    # Plan Summary
    plan_summary = plan_info.get("plan_summary", {})
    lines.append("## ðŸ“Š Execution Plan Summary")
    lines.append("")
    lines.append(f"- **Total Nodes**: {plan_summary.get('total_nodes', 0)}")
    lines.append(f"- **BROADCAST Nodes**: {plan_summary.get('broadcast_nodes_count', 0)}")
    lines.append(f"- **JOIN Nodes**: {plan_summary.get('join_nodes_count', 0)}")
    lines.append(f"- **Scan Nodes**: {plan_summary.get('scan_nodes_count', 0)}")
    lines.append(f"- **Shuffle Nodes**: {plan_summary.get('shuffle_nodes_count', 0)}")
    lines.append(f"- **Aggregate Nodes**: {plan_summary.get('aggregate_nodes_count', 0)}")
    lines.append(f"- **BROADCAST in Use**: {'Yes' if plan_summary.get('has_broadcast_joins', False) else 'No'}")
    lines.append(f"- **Tables Scanned**: {plan_summary.get('tables_scanned', 0)}")
    lines.append("")
    
    # JOIN Strategy Analysis
    unique_join_strategies = plan_summary.get('unique_join_strategies', [])
    if unique_join_strategies:
        lines.append("## ðŸ”— JOIN Strategy Analysis")
        lines.append("")
        for strategy in unique_join_strategies:
            lines.append(f"- **{strategy.replace('_', ' ').title()}** (`{strategy}`)")
        lines.append("")
    
    # BROADCAST Node Details
    broadcast_nodes = plan_info.get("broadcast_nodes", [])
    if broadcast_nodes:
        lines.append("## ðŸ“¡ BROADCAST Node Details")
        lines.append("")
        for i, node in enumerate(broadcast_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            lines.append(f"- **Node Tag**: {node['node_tag']}")
            
            metadata = node.get('metadata', [])
            if metadata:
                lines.append("- **Related Metadata**:")
                for meta in metadata[:5]:  # Show up to 5
                    key = meta.get('key', '')
                    value = meta.get('value', '')
                    values = meta.get('values', [])
                    if values:
                        lines.append(f"  - **{key}**: {', '.join(map(str, values[:3]))}")
                    elif value:
                        lines.append(f"  - **{key}**: {value}")
            lines.append("")
    
    # JOIN Node Details
    join_nodes = plan_info.get("join_nodes", [])
    if join_nodes:
        lines.append("## ðŸ”— JOIN Node Details")
        lines.append("")
        for i, node in enumerate(join_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            lines.append(f"- **JOIN Strategy**: {node['join_strategy']}")
            lines.append(f"- **JOIN Type**: {node['join_type']}")
            
            join_keys = node.get('join_keys', [])
            if join_keys:
                lines.append(f"- **JOIN Keys**: {', '.join(join_keys[:5])}")
            lines.append("")
    
    # Table Scan Details (with size estimation info)
    table_scan_details = plan_info.get("table_scan_details", {})
    table_size_estimates = plan_info.get("table_size_estimates", {})
    if table_scan_details:
        lines.append("## ðŸ“‹ Table Scan Details")
        lines.append("")
        for table_name, scan_detail in table_scan_details.items():
            lines.append(f"### {table_name}")
            lines.append("")
            lines.append(f"- **File Format**: {scan_detail.get('file_format', 'unknown')}")
            lines.append(f"- **Pushed Filters**: {len(scan_detail.get('pushed_filters', []))}")
            lines.append(f"- **Output Columns**: {len(scan_detail.get('output_columns', []))}")
            
            # Add execution plan size estimation info (disabled - estimatedSizeInBytes not available)
            # size_info = table_size_estimates.get(table_name)
            # if size_info:
            #     lines.append(f"- **Estimated Size (Execution Plan)**: {size_info['estimated_size_mb']:.1f}MB")
            #     lines.append(f"- **Size Estimation Confidence**: {size_info.get('confidence', 'medium')}")
            #     if 'num_files' in size_info:
            #         lines.append(f"- **Number of Files**: {size_info['num_files']}")
            #     if 'num_partitions' in size_info:
            #         lines.append(f"- **Number of Partitions**: {size_info['num_partitions']}")
            
            pushed_filters = scan_detail.get('pushed_filters', [])
            if pushed_filters:
                lines.append("- **Pushed Down Filters**:")
                for filter_expr in pushed_filters[:3]:  # Show up to 3
                    lines.append(f"  - `{filter_expr}`")
            lines.append("")
    
    # Shuffle Node Details
    shuffle_nodes = plan_info.get("shuffle_nodes", [])
    if shuffle_nodes:
        lines.append("## ðŸ”„ Shuffle Node Details")
        lines.append("")
        for i, node in enumerate(shuffle_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            
            partition_keys = node.get('partition_keys', [])
            if partition_keys:
                lines.append(f"- **Partition Keys**: {', '.join(partition_keys)}")
            lines.append("")
    
    # Aggregate Node Details
    aggregate_nodes = plan_info.get("aggregate_nodes", [])
    if aggregate_nodes:
        lines.append("## ðŸ“Š Aggregate Node Details")
        lines.append("")
        for i, node in enumerate(aggregate_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            
            group_keys = node.get('group_keys', [])
            if group_keys:
                lines.append(f"- **Group Keys**: {', '.join(group_keys[:5])}")
            
            agg_expressions = node.get('aggregate_expressions', [])
            if agg_expressions:
                lines.append(f"- **Aggregate Functions**: {', '.join(agg_expressions[:5])}")
            lines.append("")
    
    # Table Size Estimation Summary (disabled - estimatedSizeInBytes not available)
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     lines.append("## ðŸ“ Table Size Estimation (Execution Plan Based)")
    #     lines.append("")
    #     total_estimated_size = sum(size_info['estimated_size_mb'] for size_info in table_size_estimates.values())
    #     lines.append(f"- **Estimated Tables Count**: {len(table_size_estimates)}")
    #     lines.append(f"- **Total Estimated Size**: {total_estimated_size:.1f}MB")
    #     lines.append("")
    #     
    #     for table_name, size_info in list(table_size_estimates.items())[:5]:  # Show up to 5 tables
    #         lines.append(f"### {table_name}")
    #         lines.append(f"- **Estimated Size**: {size_info['estimated_size_mb']:.1f}MB")
    #         lines.append(f"- **Confidence**: {size_info.get('confidence', 'medium')}")
    #         lines.append(f"- **Node**: {size_info.get('node_name', 'unknown')}")
    #         if 'num_files' in size_info:
    #             lines.append(f"- **Number of Files**: {size_info['num_files']}")
    #         lines.append("")
    #     
    #     if len(table_size_estimates) > 5:
    #         lines.append(f"...and {len(table_size_estimates) - 5} more tables (see details in sections above)")
    #         lines.append("")
    
    # Plan-based Optimization Recommendations
    lines.append("## ðŸ’¡ Plan-based Optimization Recommendations")
    lines.append("")
    
    if plan_summary.get('has_broadcast_joins', False):
        lines.append("âœ… **BROADCAST JOIN is already applied**")
        lines.append("- Current execution plan has BROADCAST optimization enabled")
        
        # Show list of broadcast tables
        broadcast_tables = plan_summary.get('broadcast_tables', [])
        if broadcast_tables:
            lines.append(f"- **Tables Being Broadcast**: {', '.join(broadcast_tables)}")
        
        lines.append("- Check for additional BROADCAST application opportunities")
    else:
        lines.append("âš ï¸ **BROADCAST JOIN is not applied**")
        lines.append("- Consider applying BROADCAST hints to small tables")
        lines.append("- Identify tables under 30MB threshold")
    lines.append("")
    
    if plan_summary.get('shuffle_nodes_count', 0) > 3:
        lines.append("âš ï¸ **Multiple shuffle operations detected**")
        lines.append("- Review data distribution and Liquid Clustering strategy")
        lines.append("- Consider applying Liquid Clustering for data layout optimization")
    lines.append("")
    
    # Size estimation based optimization suggestions (disabled - estimatedSizeInBytes not available)
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     small_tables = [name for name, info in table_size_estimates.items() if info['estimated_size_mb'] <= 30]
    #     if small_tables:
    #         lines.append("ðŸ’¡ **Execution Plan Based BROADCAST Recommendations**")
    #         lines.append(f"- Small tables â‰¤30MB detected: {len(small_tables)}")
    #         for table in small_tables[:3]:  # Show up to 3 tables
    #             size_mb = table_size_estimates[table]['estimated_size_mb']
    #             lines.append(f"  â€¢ {table}: {size_mb:.1f}MB (BROADCAST candidate)")
    #         if len(small_tables) > 3:
    #             lines.append(f"  â€¢ ...and {len(small_tables) - 3} more tables")
    #         lines.append("")
    
    lines.append("---")
    lines.append("")
    lines.append("This report was automatically generated by the Databricks SQL Execution Plan Analysis Tool.")
    
    return '\n'.join(lines)


def summarize_explain_results_with_llm(explain_content: str, explain_cost_content: str, query_type: str = "original", optimization_success: bool = None) -> Dict[str, str]:
    """
    EXPLAIN + EXPLAIN COSTçµæžœã‚’LLMã§è¦ç´„ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã«å¯¾å¿œ
    
    Args:
        explain_content: EXPLAINçµæžœã®å†…å®¹
        explain_cost_content: EXPLAIN COSTçµæžœã®å†…å®¹  
        query_type: ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—ï¼ˆ"original" ã¾ãŸã¯ "optimized"ï¼‰
        optimization_success: æœ€é©åŒ–ã®æˆåŠŸçŠ¶æ…‹ï¼ˆNone, True, Falseï¼‰
    
    Returns:
        Dict containing summarized results
    """
    
    # ðŸš€ LLMã‚³ã‚¹ãƒˆå‰Šæ¸›: ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã¯æœ€é©åŒ–æˆåŠŸæ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
    if query_type == "original" and optimization_success is True:
        print(f"ðŸ’° Skipping LLM summarization for original query (optimization succeeded - cost reduction)")
        return {
            'explain_summary': explain_content,
            'explain_cost_summary': explain_cost_content,
            'physical_plan_summary': explain_content,
            'cost_statistics_summary': extract_cost_statistics_from_explain_cost(explain_cost_content),
            'summarized': False,
            'skipped_for_cost_optimization': True
        }
    
    # ã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯ï¼ˆåˆè¨ˆ200KBä»¥ä¸Šã§è¦ç´„ã‚’å®Ÿè¡Œï¼‰
    total_size = len(explain_content) + len(explain_cost_content)
    SUMMARIZATION_THRESHOLD = 200000  # 200KB
    
    if total_size < SUMMARIZATION_THRESHOLD:
        print(f"ðŸ“Š EXPLAIN + EXPLAIN COST total size: {total_size:,} characters (no summary needed)")
        return {
            'explain_summary': explain_content,
            'explain_cost_summary': explain_cost_content,
            'physical_plan_summary': explain_content,
            'cost_statistics_summary': extract_cost_statistics_from_explain_cost(explain_cost_content),
            'summarized': False
        }
    
    print(f"ðŸ“Š EXPLAIN + EXPLAIN COST total size: {total_size:,} characters (summary executed)")
    
    # è¦ç´„ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    summarization_prompt = f"""
ã‚ãªãŸã¯Databricksã®SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®EXPLAIN + EXPLAIN COSTçµæžœã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ã€è¦ç´„å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã€‘
- ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—: {query_type}
- EXPLAINçµæžœã‚µã‚¤ã‚º: {len(explain_content):,} æ–‡å­—
- EXPLAIN COSTçµæžœã‚µã‚¤ã‚º: {len(explain_cost_content):,} æ–‡å­—

ã€EXPLAINçµæžœã€‘
```
{explain_content[:20000]}{"..." if len(explain_content) > 20000 else ""}
```

ã€EXPLAIN COSTçµæžœã€‘  
```
{explain_cost_content[:20000]}{"..." if len(explain_cost_content) > 20000 else ""}
```

ã€è¦ç´„æŒ‡ç¤ºã€‘
ä»¥ä¸‹ã®å½¢å¼ã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ï¼ˆåˆè¨ˆ5000æ–‡å­—ä»¥å†…ï¼‰:

## ðŸ“Š Physical Planè¦ç´„
- ä¸»è¦ãªå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆ5-10å€‹ã®é‡è¦ãªæ“ä½œï¼‰
- JOINæ–¹å¼ã¨ãƒ‡ãƒ¼ã‚¿ç§»å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³
- Photonåˆ©ç”¨çŠ¶æ³ã¨ãƒœãƒˆãƒ«ãƒãƒƒã‚¯

## ðŸ’° çµ±è¨ˆæƒ…å ±ã‚µãƒžãƒªãƒ¼
- ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºã¨è¡Œæ•°ã®é‡è¦ãªæƒ…å ±
- JOINé¸æŠžçŽ‡ã¨ãƒ•ã‚£ãƒ«ã‚¿åŠ¹çŽ‡
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨ã‚¹ãƒ”ãƒ«äºˆæ¸¬
- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†æ•£çŠ¶æ³

## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åˆ†æž
- å®Ÿè¡Œã‚³ã‚¹ãƒˆã®å†…è¨³
- ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚Šãã†ãªæ“ä½œ
- æœ€é©åŒ–ã®ä½™åœ°ãŒã‚ã‚‹ç®‡æ‰€

ã€é‡è¦ã€‘: 
- æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã¯æ­£ç¢ºã«è¨˜è¼‰
- SQLæœ€é©åŒ–ã«é‡è¦ãªæƒ…å ±ã‚’å„ªå…ˆ
- 5000æ–‡å­—ä»¥å†…ã§å®Œçµã«ã¾ã¨ã‚ã‚‹
"""

    try:
        # è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
        provider = LLM_CONFIG["provider"]
        
        if provider == "databricks":
            summary_result = _call_databricks_llm(summarization_prompt)
        elif provider == "openai":
            summary_result = _call_openai_llm(summarization_prompt)
        elif provider == "azure_openai":
            summary_result = _call_azure_openai_llm(summarization_prompt)
        elif provider == "anthropic":
            summary_result = _call_anthropic_llm(summarization_prompt)
        else:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯åˆ‡ã‚Šè©°ã‚ç‰ˆã‚’è¿”ã™
            print("âŒ LLM provider error: Using truncated version")
            return {
                'explain_summary': explain_content[:30000] + "\n\nâš ï¸ åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ",
                'explain_cost_summary': explain_cost_content[:30000] + "\n\nâš ï¸ åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ", 
                'physical_plan_summary': explain_content[:20000] + "\n\nâš ï¸ åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ",
                'cost_statistics_summary': extract_cost_statistics_from_explain_cost(explain_cost_content),
                'summarized': True
            }
        
        # LLMã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        if isinstance(summary_result, str) and summary_result.startswith("LLM_ERROR:"):
            print(f"âŒ LLM summary error: Using truncated version - {summary_result[10:200]}...")
            return {
                'explain_summary': explain_content[:30000] + "\n\nâš ï¸ åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ",
                'explain_cost_summary': explain_cost_content[:30000] + "\n\nâš ï¸ åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ",
                'physical_plan_summary': explain_content[:20000] + "\n\nâš ï¸ åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ", 
                'cost_statistics_summary': extract_cost_statistics_from_explain_cost(explain_cost_content),
                'summarized': True
            }
        
        # thinking_enabledå¯¾å¿œ
        if isinstance(summary_result, list):
            summary_text = extract_main_content_from_thinking_response(summary_result)
        else:
            summary_text = str(summary_result)
        
        # è¦ç´„çµæžœã‚’åˆ†å‰²ã—ã¦è¿”ã™
        print(f"âœ… EXPLAIN + EXPLAIN COST summary completed: {len(summary_text):,} characters")
        
        # ðŸš¨ DEBUG_ENABLED='Y'ã®å ´åˆã€è¦ç´„çµæžœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        debug_enabled = globals().get('DEBUG_ENABLED', 'N')
        # ðŸš€ LLMã‚³ã‚¹ãƒˆå‰Šæ¸›: ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã¯æœ€é©åŒ–æˆåŠŸæ™‚ã¯ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã‚‚ã‚¹ã‚­ãƒƒãƒ—
        if debug_enabled.upper() == 'Y' and not (query_type == "original" and optimization_success is True):
            try:
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                summary_filename = f"output_explain_summary_{query_type}_{timestamp}.md"
                
                # è¦ç´„çµæžœã‚’Markdownå½¢å¼ã§ä¿å­˜ï¼ˆOUTPUT_LANGUAGEã«å¿œã˜ã¦è¨€èªžã‚’åˆ‡ã‚Šæ›¿ãˆï¼‰
                output_language = globals().get('OUTPUT_LANGUAGE', 'ja')
                
                if output_language == 'en':
                    summary_content = f"""# EXPLAIN + EXPLAIN COST Summary Results ({query_type})

## ðŸ“Š Basic Information
- Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- Query Type: {query_type}
- Original Size: EXPLAIN({len(explain_content):,} chars) + EXPLAIN COST({len(explain_cost_content):,} chars) = {total_size:,} chars
- Summary Size: {len(summary_text):,} chars
- Compression Ratio: {total_size//len(summary_text) if len(summary_text) > 0 else 0}x

## ðŸ§  LLM Summary Results

{summary_text}

## ðŸ’° Statistical Information Extraction

{extract_cost_statistics_from_explain_cost(explain_cost_content)}
"""
                else:
                    summary_content = f"""# EXPLAIN + EXPLAIN COSTè¦ç´„çµæžœ ({query_type})

## ðŸ“Š åŸºæœ¬æƒ…å ±
- ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—: {query_type}
- å…ƒã‚µã‚¤ã‚º: EXPLAIN({len(explain_content):,}æ–‡å­—) + EXPLAIN COST({len(explain_cost_content):,}æ–‡å­—) = {total_size:,}æ–‡å­—
- è¦ç´„å¾Œã‚µã‚¤ã‚º: {len(summary_text):,}æ–‡å­—
- åœ§ç¸®çŽ‡: {total_size//len(summary_text) if len(summary_text) > 0 else 0}x

## ðŸ§  LLMè¦ç´„çµæžœ

{summary_text}

## ðŸ’° çµ±è¨ˆæƒ…å ±æŠ½å‡º

{extract_cost_statistics_from_explain_cost(explain_cost_content)}
"""
                
                with open(summary_filename, 'w', encoding='utf-8') as f:
                    f.write(summary_content)
                
                print(f"ðŸ“„ Saving summary results: {summary_filename}")
                
            except Exception as save_error:
                print(f"âš ï¸ Failed to save summary results: {str(save_error)}")
        
        return {
            'explain_summary': summary_text,
            'explain_cost_summary': summary_text,  # çµ±åˆè¦ç´„ã¨ã—ã¦åŒã˜å†…å®¹
            'physical_plan_summary': summary_text,
            'cost_statistics_summary': extract_cost_statistics_from_explain_cost(explain_cost_content),
            'summarized': True
        }
        
    except Exception as e:
        print(f"âŒ Error during EXPLAIN summarization: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯åˆ‡ã‚Šè©°ã‚ç‰ˆã‚’è¿”ã™
        return {
            'explain_summary': explain_content[:30000] + f"\n\nâš ï¸ è¦ç´„ã‚¨ãƒ©ãƒ¼ã®ãŸã‚åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ: {str(e)}",
            'explain_cost_summary': explain_cost_content[:30000] + f"\n\nâš ï¸ è¦ç´„ã‚¨ãƒ©ãƒ¼ã®ãŸã‚åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ: {str(e)}",
            'physical_plan_summary': explain_content[:20000] + f"\n\nâš ï¸ è¦ç´„ã‚¨ãƒ©ãƒ¼ã®ãŸã‚åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ: {str(e)}",
            'cost_statistics_summary': extract_cost_statistics_from_explain_cost(explain_cost_content),
            'summarized': True
        }


def generate_optimization_strategy_summary(optimized_result: str, metrics: Dict[str, Any], analysis_result: str = "") -> str:
    """
    Generate optimization strategy summary
    
    Args:
        optimized_result: Optimization result (LLM response)
        metrics: Metrics information
        analysis_result: Bottleneck analysis result
        
    Returns:
        str: Optimization strategy summary
    """
    try:
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã‚’å–å¾—
        bottleneck_indicators = metrics.get('bottleneck_indicators', {})
        overall_metrics = metrics.get('overall_metrics', {})
        
        # æœ€é©åŒ–ã§ä½¿ç”¨ã•ã‚ŒãŸæ‰‹æ³•ã‚’æ¤œå‡º
        optimization_techniques = []
        performance_issues = []
        
        # æœ€é©åŒ–å†…å®¹ã‹ã‚‰æ‰‹æ³•ã‚’æŠ½å‡º
        if optimized_result:
            content_upper = optimized_result.upper()
            
            # JOINæœ€é©åŒ–
            if 'BROADCAST' in content_upper or 'MAPJOIN' in content_upper:
                optimization_techniques.append("**Broadcast Join**: å°ã•ãªãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã—ã¦åˆ†æ•£çµåˆã‚’æœ€é©åŒ–")
            
            if 'REPARTITION' in content_upper or 'REDISTRIBUTE' in content_upper:
                optimization_techniques.append("**ãƒ‡ãƒ¼ã‚¿å†åˆ†æ•£**: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã‚„ã‚­ãƒ¼ã‚’èª¿æ•´ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ã‚’è§£æ¶ˆ")
            
            # Databrickså›ºæœ‰ã®ãƒ‡ãƒ¼ã‚¿æœ€é©åŒ–
            if 'PARTITION' in content_upper and 'BY' in content_upper:
                optimization_techniques.append("**Liquid Clustering**: ã‚¯ã‚¨ãƒªãƒ•ã‚£ãƒ«ã‚¿ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æœ€é©åŒ–")
            
            if 'CLUSTER' in content_upper or 'LIQUID' in content_upper:
                optimization_techniques.append("**Liquid Clustering**: é »ç¹ãªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°")
            
            # Photonæœ€é©åŒ–
            if 'PHOTON' in content_upper or 'VECTORIZED' in content_upper:
                optimization_techniques.append("**Photon Engine**: ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Ÿè¡Œã«ã‚ˆã‚‹é«˜é€ŸåŒ–")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–
            if 'CACHE' in content_upper or 'PERSIST' in content_upper:
                optimization_techniques.append("**ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥**: ä¸­é–“çµæžœã®æ°¸ç¶šåŒ–ã«ã‚ˆã‚‹å†è¨ˆç®—å›žé¿")
            
            # ãƒ•ã‚£ãƒ«ã‚¿æœ€é©åŒ–
            if 'WHERE' in content_upper and ('PUSHDOWN' in content_upper or 'PREDICATE' in content_upper):
                optimization_techniques.append("**è¿°èªžãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³**: ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã®æ—©æœŸé©ç”¨ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿é‡å‰Šæ¸›")
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æžã‹ã‚‰å•é¡Œç‚¹ã‚’æŠ½å‡º
        if bottleneck_indicators.get('has_spill', False):
            performance_issues.append("ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«ç™ºç”Ÿ")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            performance_issues.append("ã‚·ãƒ£ãƒƒãƒ•ãƒ«å‡¦ç†ãƒœãƒˆãƒ«ãƒãƒƒã‚¯")
        
        if bottleneck_indicators.get('low_parallelism', False):
            performance_issues.append("ä¸¦åˆ—åº¦ä¸è¶³")
        
        if bottleneck_indicators.get('cache_hit_ratio', 1.0) < 0.5:
            performance_issues.append("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆçŽ‡ä½Žä¸‹")
        
        if not overall_metrics.get('photon_enabled', True):
            performance_issues.append("Photon Engineæœªæ´»ç”¨")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º
        if bottleneck_indicators.get('has_skew', False):
            performance_issues.append("ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ç™ºç”Ÿ")
        
        # è¦ç´„ç”Ÿæˆ
        summary_parts = []
        
        # æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ
        if performance_issues:
            issues_text = "ã€".join(performance_issues)
            summary_parts.append(f"**ðŸ” æ¤œå‡ºã•ã‚ŒãŸä¸»è¦èª²é¡Œ**: {issues_text}")
        
        # é©ç”¨ã•ã‚ŒãŸæœ€é©åŒ–æ‰‹æ³•
        if optimization_techniques:
            summary_parts.append("**ðŸ› ï¸ é©ç”¨ã•ã‚ŒãŸæœ€é©åŒ–æ‰‹æ³•**:")
            for i, technique in enumerate(optimization_techniques, 1):
                summary_parts.append(f"   {i}. {technique}")
        
        # æœ€é©åŒ–æ–¹é‡
        strategy_focus = []
        
        if bottleneck_indicators.get('has_spill', False):
            strategy_focus.append("ãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡åŒ–")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            strategy_focus.append("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è² è·è»½æ¸›")
        
        if bottleneck_indicators.get('low_parallelism', False):
            strategy_focus.append("ä¸¦åˆ—å‡¦ç†èƒ½åŠ›å‘ä¸Š")
        
        if strategy_focus:
            focus_text = "ã€".join(strategy_focus)
            summary_parts.append(f"**ðŸŽ¯ æœ€é©åŒ–é‡ç‚¹åˆ†é‡Ž**: {focus_text}")
        
        # EXPLAINçµ±è¨ˆæƒ…å ±ã®æ´»ç”¨
        explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
        if explain_enabled.upper() == 'Y':
            summary_parts.append("**ðŸ“Š çµ±è¨ˆæƒ…å ±æ´»ç”¨**: EXPLAIN + EXPLAIN COSTåˆ†æžã«ã‚ˆã‚Šã€çµ±è¨ˆãƒ™ãƒ¼ã‚¹ã®ç²¾å¯†ãªæœ€é©åŒ–ã‚’å®Ÿè¡Œ")
        
        if summary_parts:
            return "\n".join(summary_parts)
        else:
            return "**ðŸ¤– AIåˆ†æžã«ã‚ˆã‚‹åŒ…æ‹¬çš„ãªæœ€é©åŒ–**: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æžã€çµ±è¨ˆæƒ…å ±ã€ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’ç·åˆã—ãŸæœ€é©åŒ–ã‚’å®Ÿè¡Œ"
    
    except Exception as e:
        return f"**ðŸ¤– AIæœ€é©åŒ–**: åŒ…æ‹¬çš„ãªåˆ†æžã«åŸºã¥ãæœ€é©åŒ–ã‚’å®Ÿè¡Œï¼ˆè¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}ï¼‰"

def format_sql_content_for_report(content: str, filename: str = "") -> str:
    """
    SQLãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã¾ãŸã¯LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ¬ãƒãƒ¼ãƒˆç”¨ã«é©åˆ‡ã«ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ
    é•·ã„ã‚¯ã‚¨ãƒªã¯é©åˆ‡ã«çœç•¥ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§ã‚’æ¡ˆå†…
    
    Args:
        content: SQLãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã¾ãŸã¯LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹
        filename: SQLãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆçœç•¥æ™‚ã®å‚ç…§ç”¨ï¼‰
        
    Returns:
        str: ãƒ¬ãƒãƒ¼ãƒˆç”¨ã«ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã•ã‚ŒãŸå†…å®¹
    """
    # çœç•¥åˆ¤å®šã®åŸºæº–
    MAX_LINES_IN_REPORT = 120  # 100è¡Œã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã«å¯¾å¿œ
    MAX_CHARS_IN_REPORT = 10000  # ã‚ˆã‚Šé•·ã„ã‚¯ã‚¨ãƒªã«ã‚‚å¯¾å¿œ
    PREVIEW_LINES = 100  # 100è¡Œã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    
    # SQLãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®å ´åˆï¼ˆ-- ã§å§‹ã¾ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆï¼‰
    if content.startswith('--') and 'USE CATALOG' in content:
        # SQLãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®å ´åˆã¯ã€é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã§è¡¨ç¤º
        lines = content.split('\n')
        sql_lines = []
        in_sql_section = False
        
        for line in lines:
            # USE CATALOG/USE SCHEMAä»¥é™ãŒå®Ÿéš›ã®ã‚¯ã‚¨ãƒªéƒ¨åˆ†
            if line.strip().startswith('USE CATALOG') or line.strip().startswith('USE SCHEMA'):
                in_sql_section = True
                sql_lines.append(line)
            elif in_sql_section and line.strip():
                sql_lines.append(line)
            elif not in_sql_section and line.strip().startswith('--'):
                # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã¯æ®‹ã™ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ï¼‰
                continue
        
        # é•·ã•åˆ¤å®šã¨çœç•¥å‡¦ç†
        if sql_lines:
            full_sql = chr(10).join(sql_lines)
            needs_truncation = (len(sql_lines) > MAX_LINES_IN_REPORT or 
                              len(full_sql) > MAX_CHARS_IN_REPORT)
            
            if needs_truncation:
                # çœç•¥ç‰ˆã‚’ä½œæˆ
                preview_lines = sql_lines[:PREVIEW_LINES]
                omitted_lines = len(sql_lines) - PREVIEW_LINES
                
                return f"""**ðŸš€ å‹•ä½œä¿è¨¼æ¸ˆã¿æœ€é©åŒ–ã‚¯ã‚¨ãƒª (SQLãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒä¸€):**

```sql
{chr(10).join(preview_lines)}

-- ... (çœç•¥: ã‚ã¨{omitted_lines}è¡Œ)
-- å®Œå…¨ç‰ˆã¯ {filename if filename else 'output_optimized_query_*.sql'} ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§
```

ðŸ’¡ ã“ã®ã‚¯ã‚¨ãƒªã¯å®Ÿéš›ã®EXPLAINå®Ÿè¡Œã§å‹•ä½œç¢ºèªæ¸ˆã¿ã§ã™ã€‚  
ðŸ“‚ **å®Œå…¨ç‰ˆ**: `{filename if filename else 'output_optimized_query_*.sql'}` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚

**ðŸ“Š ã‚¯ã‚¨ãƒªæ¦‚è¦:**
- ç·è¡Œæ•°: {len(sql_lines)}è¡Œ
- è¡¨ç¤º: æœ€åˆã®{PREVIEW_LINES}è¡Œã®ã¿
- çœç•¥: {omitted_lines}è¡Œ"""
            else:
                # çŸ­ã„å ´åˆã¯å…¨æ–‡è¡¨ç¤º
                return f"""**ðŸš€ å‹•ä½œä¿è¨¼æ¸ˆã¿æœ€é©åŒ–ã‚¯ã‚¨ãƒª (SQLãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒä¸€):**

```sql
{full_sql}
```

ðŸ’¡ ã“ã®ã‚¯ã‚¨ãƒªã¯å®Ÿéš›ã®EXPLAINå®Ÿè¡Œã§å‹•ä½œç¢ºèªæ¸ˆã¿ã§ã™ã€‚"""
        else:
            return f"""**ðŸš€ SQLãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹:**

```sql
{content}
```"""
    
    # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆ
    else:
        # é•·ã„LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚‚çœç•¥å¯¾è±¡
        if len(content) > MAX_CHARS_IN_REPORT:
            preview_content = content[:MAX_CHARS_IN_REPORT]
            omitted_chars = len(content) - MAX_CHARS_IN_REPORT
            
            if '```sql' in content:
                return f"""**ðŸ’¡ LLMæœ€é©åŒ–åˆ†æž (çœç•¥ç‰ˆ):**

{preview_content}...

**çœç•¥æƒ…å ±:** ã‚ã¨{omitted_chars}æ–‡å­—  
ðŸ“ æ³¨æ„: ä¸Šè¨˜ã¯åˆ†æžçµæžœã®ä¸€éƒ¨ã§ã™ã€‚å®Ÿéš›ã®å®Ÿè¡Œç”¨ã‚¯ã‚¨ãƒªã¯ `{filename if filename else 'output_optimized_query_*.sql'}` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"""
            else:
                return f"""**ðŸ’¡ LLMæœ€é©åŒ–åˆ†æž (çœç•¥ç‰ˆ):**

{preview_content}...

**çœç•¥æƒ…å ±:** ã‚ã¨{omitted_chars}æ–‡å­—  
ðŸ“ æ³¨æ„: ä¸Šè¨˜ã¯åˆ†æžçµæžœã®ä¸€éƒ¨ã§ã™ã€‚å®Ÿéš›ã®å®Ÿè¡Œç”¨ã‚¯ã‚¨ãƒªã¯ `{filename if filename else 'output_optimized_query_*.sql'}` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"""
        else:
            # çŸ­ã„å ´åˆã¯å…¨æ–‡è¡¨ç¤º
            if '```sql' in content:
                return f"""**ðŸ’¡ LLMæœ€é©åŒ–åˆ†æž:**

{content}"""
            else:
                return f"""**ðŸ’¡ LLMæœ€é©åŒ–åˆ†æž:**

{content}

ðŸ“ æ³¨æ„: ä¸Šè¨˜ã¯åˆ†æžçµæžœã§ã™ã€‚å®Ÿéš›ã®å®Ÿè¡Œç”¨ã‚¯ã‚¨ãƒªã¯å¯¾å¿œã™ã‚‹SQLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"""

def generate_performance_comparison_section(performance_comparison: Dict[str, Any] = None, language: str = 'ja') -> str:
    """
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒçµæžœã®è©³ç´°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
    
    Args:
        performance_comparison: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒçµæžœ
        language: è¨€èªžè¨­å®š ('ja' ã¾ãŸã¯ 'en')
        
    Returns:
        str: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
    """
    
    # ðŸš¨ ç·Šæ€¥ä¿®æ­£: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡å¯¾å¿œ
    if not performance_comparison:
        if language == 'ja':
            return """

**ðŸ“‹ å®Ÿè¡ŒçŠ¶æ³**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ

| é …ç›® | çŠ¶æ³ |
|------|------|
| æ¯”è¼ƒå®Ÿè¡Œ | âŒ æœªå®Ÿè¡Œ |
| ç†ç”± | EXPLAINåŠã³EXPLAIN COSTå–å¾—å¤±æ•— |
| å®‰å…¨æ€§ | âœ… æ§‹æ–‡æ¤œè¨¼æ¸ˆã¿ã§å®Ÿè¡Œå¯èƒ½ |
| æŽ¨å¥¨ | ðŸš€ æœ€é©åŒ–ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ |

ðŸ’¡ **Note**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒã¯å®Ÿè¡Œã§ãã¾ã›ã‚“ã§ã—ãŸãŒã€æ§‹æ–‡çš„ã«æ­£å¸¸ãªæœ€é©åŒ–ã‚¯ã‚¨ãƒªãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™ã€‚
"""
        else:
            return """

**ðŸ“‹ Execution Status**: Performance comparison was not executed

| Item | Status |
|------|--------|
| Comparison | âŒ Not executed |
| Reason | EXPLAIN and EXPLAIN COST acquisition failed |
| Safety | âœ… Syntax verified and executable |
| Recommendation | ðŸš€ Use optimized query (default) |

ðŸ’¡ **Note**: Although performance comparison was not executed, a syntactically correct optimized query has been generated.
"""
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡ã®å ´åˆã®ç‰¹åˆ¥å‡¦ç†
    if performance_comparison.get('evaluation_type') == 'fallback_plan_analysis':
        fallback_eval = performance_comparison.get('fallback_evaluation', {})
        return generate_fallback_performance_section(fallback_eval, language)
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒçµæžœã®è©³ç´°è¡¨ç¤º
    recommendation = performance_comparison.get('recommendation', 'unknown')
    total_cost_ratio = performance_comparison.get('total_cost_ratio', 1.0) or 1.0
    memory_usage_ratio = performance_comparison.get('memory_usage_ratio', 1.0) or 1.0
    degradation_detected = performance_comparison.get('performance_degradation_detected', False)
    details = performance_comparison.get('details', [])
    
    if language == 'ja':
        # æ—¥æœ¬èªžç‰ˆã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
        status_text = "ðŸš¨ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ‚ªåŒ–æ¤œå‡º" if degradation_detected else "âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ”¹å–„ç¢ºèª"
        recommendation_text = "å…ƒã‚¯ã‚¨ãƒªä½¿ç”¨" if recommendation == 'use_original' else "æœ€é©åŒ–ã‚¯ã‚¨ãƒªä½¿ç”¨"
        
        # æ”¹å–„/æ‚ªåŒ–ã®åˆ¤å®šã‚¢ã‚¤ã‚³ãƒ³
        cost_icon = "âŒ" if total_cost_ratio > 1.1 else "âœ…" if total_cost_ratio < 0.9 else "âž–"
        memory_icon = "âŒ" if memory_usage_ratio > 1.1 else "âœ…" if memory_usage_ratio < 0.9 else "âž–"
        
        section = f"""

**ðŸ“Š å®Ÿè¡Œçµæžœ**: {status_text}

#### ðŸ” è©³ç´°æ¯”è¼ƒãƒ¡ãƒˆãƒªã‚¯ã‚¹

| é …ç›® | å…ƒã‚¯ã‚¨ãƒª | æœ€é©åŒ–ã‚¯ã‚¨ãƒª | æ¯”çŽ‡ | è©•ä¾¡ |
|------|----------|-------------|------|------|
| å®Ÿè¡Œã‚³ã‚¹ãƒˆ | 1.00 (åŸºæº–) | {total_cost_ratio:.2f} | {total_cost_ratio:.2f}å€ | {cost_icon} |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | 1.00 (åŸºæº–) | {memory_usage_ratio:.2f} | {memory_usage_ratio:.2f}å€ | {memory_icon} |

#### ðŸ“‹ åˆ¤å®šçµæžœ

| é …ç›® | çµæžœ |
|------|------|
| ç·åˆåˆ¤å®š | **{status_text}** |
| æŽ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ | **{recommendation_text}** |
| æ‚ªåŒ–æ¤œå‡º | {'ã¯ã„' if degradation_detected else 'ã„ã„ãˆ'} |

#### ðŸŽ¯ è©³ç´°åˆ†æžçµæžœ

"""
        
        if details:
            for detail in details:
                section += f"- {detail}\n"
        else:
            section += "- è©³ç´°ãªåˆ†æžæƒ…å ±ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“\n"
        
        section += f"""

#### ðŸ›¡ï¸ å®‰å…¨æ€§ä¿è¨¼

- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ‚ªåŒ–é˜²æ­¢**: {'âœ… æ‚ªåŒ–æ¤œå‡ºã«ã‚ˆã‚Šå…ƒã‚¯ã‚¨ãƒªã‚’é¸æŠž' if degradation_detected else 'âœ… æ”¹å–„ç¢ºèªã«ã‚ˆã‚Šæœ€é©åŒ–ã‚¯ã‚¨ãƒªã‚’é¸æŠž'}
- **å®Ÿè¡Œå¯èƒ½æ€§**: âœ… EXPLAINå®Ÿè¡Œã§æ§‹æ–‡æ¤œè¨¼æ¸ˆã¿
- **è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯**: {'âœ… ä½œå‹• - å®‰å…¨æ€§ã‚’å„ªå…ˆ' if degradation_detected else 'âŒ ä¸è¦ - æ”¹å–„åŠ¹æžœã‚ã‚Š'}

ðŸ’¡ **åˆ¤å®šåŸºæº–**: å®Ÿè¡Œã‚³ã‚¹ãƒˆ30%å¢—åŠ  ã¾ãŸã¯ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡50%å¢—åŠ  ã§æ‚ªåŒ–ã¨åˆ¤å®š
"""
    
    else:
        # è‹±èªžç‰ˆã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
        status_text = "ðŸš¨ Performance Degradation Detected" if degradation_detected else "âœ… Performance Improvement Confirmed"
        recommendation_text = "Use Original Query" if recommendation == 'use_original' else "Use Optimized Query"
        
        # æ”¹å–„/æ‚ªåŒ–ã®åˆ¤å®šã‚¢ã‚¤ã‚³ãƒ³
        cost_icon = "âŒ" if total_cost_ratio > 1.1 else "âœ…" if total_cost_ratio < 0.9 else "âž–"
        memory_icon = "âŒ" if memory_usage_ratio > 1.1 else "âœ…" if memory_usage_ratio < 0.9 else "âž–"
        
        section = f"""

**ðŸ“Š Execution Result**: {status_text}

#### ðŸ” Detailed Comparison Metrics

| Item | Original Query | Optimized Query | Ratio | Evaluation |
|------|----------------|-----------------|-------|------------|
| Execution Cost | 1.00 (baseline) | {total_cost_ratio:.2f} | {total_cost_ratio:.2f}x | {cost_icon} |
| Memory Usage | 1.00 (baseline) | {memory_usage_ratio:.2f} | {memory_usage_ratio:.2f}x | {memory_icon} |

#### ðŸ“‹ Judgment Results

| Item | Result |
|------|--------|
| Overall Judgment | **{status_text}** |
| Recommended Action | **{recommendation_text}** |
| Degradation Detected | {'Yes' if degradation_detected else 'No'} |

#### ðŸŽ¯ Detailed Analysis Results

"""
        
        if details:
            for detail in details:
                section += f"- {detail}\n"
        else:
            section += "- Detailed analysis information is not available\n"
        
        section += f"""

#### ðŸ›¡ï¸ Safety Guarantee

- **Performance Degradation Prevention**: {'âœ… Degradation detected, original query selected' if degradation_detected else 'âœ… Improvement confirmed, optimized query selected'}
- **Executability**: âœ… Syntax verified via EXPLAIN execution
- **Automatic Fallback**: {'âœ… Activated - Safety prioritized' if degradation_detected else 'âŒ Not needed - Improvement achieved'}

ðŸ’¡ **Judgment Criteria**: Degradation detected if execution cost increases by 30% OR memory usage increases by 50%
"""
    
    return section

def translate_analysis_to_japanese(english_text: str) -> str:
    """
    LLMã‚’ä½¿ç”¨ã—ã¦è‹±èªžã®åˆ†æžçµæžœã‚’æ—¥æœ¬èªžã«ç¿»è¨³
    """
    try:
        print("ðŸŒ Translating analysis result to Japanese...")
        
        translation_prompt = f"""
ä»¥ä¸‹ã®è‹±èªžã®SQLåˆ†æžçµæžœã‚’ã€æŠ€è¡“çš„ãªæ­£ç¢ºæ€§ã‚’ä¿ã¡ãªãŒã‚‰è‡ªç„¶ãªæ—¥æœ¬èªžã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚
å°‚é–€ç”¨èªžã¯é©åˆ‡ãªæ—¥æœ¬èªžã«ç¿»è¨³ã—ã€æ•°å€¤ã‚„ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã¯ãã®ã¾ã¾ä¿æŒã—ã¦ãã ã•ã„ã€‚

ã€ç¿»è¨³å¯¾è±¡ã€‘
{english_text}

ã€ç¿»è¨³è¦ä»¶ã€‘
- æŠ€è¡“çš„æ­£ç¢ºæ€§ã‚’æœ€å„ªå…ˆ
- è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„æ—¥æœ¬èªž
- SQLç”¨èªžã¯é©åˆ‡ãªæ—¥æœ¬èªžè¡¨ç¾ã‚’ä½¿ç”¨
- æ•°å€¤ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æŒ‡æ¨™ã¯ãã®ã¾ã¾ä¿æŒ
- æŽ¨å¥¨äº‹é …ã¯å®Ÿç”¨çš„ãªæ—¥æœ¬èªžã§è¡¨ç¾

æ—¥æœ¬èªžç¿»è¨³çµæžœã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
"""
        
        provider = LLM_CONFIG.get("provider", "databricks")
        
        if provider == "databricks":
            japanese_result = _call_databricks_llm(translation_prompt)
        elif provider == "openai":
            japanese_result = _call_openai_llm(translation_prompt)
        elif provider == "azure_openai":
            japanese_result = _call_azure_openai_llm(translation_prompt)
        elif provider == "anthropic":
            japanese_result = _call_anthropic_llm(translation_prompt)
        else:
            print(f"âš ï¸ Unknown LLM provider: {provider}, skipping translation")
            return english_text
        
        if japanese_result and japanese_result.strip():
            print("âœ… Translation to Japanese completed")
            return japanese_result.strip()
        else:
            print("âš ï¸ Translation failed, using original English text")
            return english_text
            
    except Exception as e:
        print(f"âš ï¸ Translation error: {str(e)}, using original English text")
        return english_text

def format_trial_history_summary(optimization_attempts: list, language: str = 'ja') -> str:
    """
    æœ€é©åŒ–è©¦è¡Œå±¥æ­´ã‚’ç°¡æ½”ã«ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã™ã‚‹ï¼ˆDEBUG_ENABLEDè¨­å®šã«é–¢ä¿‚ãªãåˆ©ç”¨å¯èƒ½ï¼‰
    
    Args:
        optimization_attempts: è©¦è¡Œå±¥æ­´ãƒªã‚¹ãƒˆ
        language: å‡ºåŠ›è¨€èªž ('ja' or 'en')
    
    Returns:
        str: ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã•ã‚ŒãŸè©¦è¡Œå±¥æ­´
    """
    if not optimization_attempts:
        return ""
    
    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¿»è¨³ãƒžãƒƒãƒ”ãƒ³ã‚°
    status_mapping = {
        'ja': {
            'substantial_success': 'å¤§å¹…æ”¹å–„é”æˆ',
            'partial_improvement': 'éƒ¨åˆ†çš„æ”¹å–„', 
            'insufficient_improvement': 'æ”¹å–„ä¸è¶³',
            'performance_degraded': 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ‚ªåŒ–',
            'llm_error': 'LLMã‚¨ãƒ©ãƒ¼',
            'explain_failed': 'EXPLAINå®Ÿè¡Œå¤±æ•—',
            'comparison_error': 'æ¯”è¼ƒã‚¨ãƒ©ãƒ¼',
            'fallback_improved': 'ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ”¹å–„',
            'fallback_degradation_detected': 'ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‚ªåŒ–æ¤œå‡º',
            'fallback_insufficient_improvement': 'ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ”¹å–„ä¸è¶³'
        },
        'en': {
            'substantial_success': 'Significant improvement achieved',
            'partial_improvement': 'Partial improvement',
            'insufficient_improvement': 'Insufficient improvement', 
            'performance_degraded': 'Performance degraded',
            'llm_error': 'LLM error',
            'explain_failed': 'EXPLAIN execution failed',
            'comparison_error': 'Comparison error',
            'fallback_improved': 'Fallback improved',
            'fallback_degradation_detected': 'Fallback degradation detected',
            'fallback_insufficient_improvement': 'Fallback insufficient improvement'
        }
    }
    
    # åŽŸå› ç¿»è¨³ãƒžãƒƒãƒ”ãƒ³ã‚°
    cause_mapping = {
        'ja': {
            'excessive_joins': 'JOINæ“ä½œå¢—åŠ ',
            'cost_increase': 'ã‚³ã‚¹ãƒˆå¢—åŠ ',
            'memory_increase': 'ãƒ¡ãƒ¢ãƒªå¢—åŠ ', 
            'optimization_backfire': 'æœ€é©åŒ–é€†åŠ¹æžœ',
            'analysis_error': 'åˆ†æžã‚¨ãƒ©ãƒ¼',
            'no_degradation': 'æ‚ªåŒ–ãªã—',
            'unknown': 'åŽŸå› ä¸æ˜Ž'
        },
        'en': {
            'excessive_joins': 'Excessive JOIN operations',
            'cost_increase': 'Cost increase',
            'memory_increase': 'Memory increase',
            'optimization_backfire': 'Optimization backfire', 
            'analysis_error': 'Analysis error',
            'no_degradation': 'No degradation',
            'unknown': 'Unknown cause'
        }
    }
    
    trial_lines = []
    
    for attempt in optimization_attempts:
        attempt_num = attempt.get('attempt', 0)
        status = attempt.get('status', 'unknown')
        cost_ratio = attempt.get('cost_ratio', 1.0)
        memory_ratio = attempt.get('memory_ratio', 1.0)
        
        # Noneå€¤ã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        if cost_ratio is None:
            cost_ratio = 1.0
        if memory_ratio is None:
            memory_ratio = 1.0
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
        status_text = status_mapping.get(language, status_mapping['en']).get(status, status)
        
        # ã‚³ã‚¹ãƒˆæ”¹å–„çŽ‡è¨ˆç®—
        cost_improvement = (1 - cost_ratio) * 100
        cost_display = f"{cost_improvement:+.1f}%" if cost_ratio != 1.0 else "0%"
        
        # åŸºæœ¬æƒ…å ±
        if language == 'ja':
            trial_line = f"- è©¦è¡Œ{attempt_num}: {status_text}"
        else:
            trial_line = f"- Trial {attempt_num}: {status_text}"
        
        # ã‚³ã‚¹ãƒˆæƒ…å ±è¿½åŠ ï¼ˆã‚¨ãƒ©ãƒ¼ä»¥å¤–ã®å ´åˆï¼‰
        if status not in ['llm_error', 'explain_failed', 'comparison_error']:
            if language == 'ja':
                trial_line += f" (ã‚³ã‚¹ãƒˆå¤‰åŒ–: {cost_display})"
            else:
                trial_line += f" (Cost change: {cost_display})"
        
        # æ‚ªåŒ–åŽŸå› è¿½åŠ ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
        if status in ['performance_degraded', 'fallback_degradation_detected']:
            degradation_analysis = attempt.get('degradation_analysis', {})
            primary_cause = degradation_analysis.get('primary_cause', 'unknown')
            cause_text = cause_mapping.get(language, cause_mapping['en']).get(primary_cause, primary_cause)
            
            if language == 'ja':
                trial_line += f", åŽŸå› : {cause_text}"
            else:
                trial_line += f", Cause: {cause_text}"
        
        trial_lines.append(trial_line)
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼è¿½åŠ 
    if language == 'ja':
        header = "**ðŸ“Š è©³ç´°è©¦è¡Œå±¥æ­´:**"
    else:
        header = "**ðŸ“Š Detailed Trial History:**"
    
    return header + "\n" + "\n".join(trial_lines)

def generate_comprehensive_optimization_report(query_id: str, optimized_result: str, metrics: Dict[str, Any], analysis_result: str = "", performance_comparison: Dict[str, Any] = None, best_attempt_number: int = None, optimization_attempts: list = None, optimization_success: bool = None) -> str:
    """
    åŒ…æ‹¬çš„ãªæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    EXPLAIN + EXPLAIN COSTå®Ÿè¡Œãƒ•ãƒ©ã‚°ãŒYã®å ´åˆã¯ã€çµ±è¨ˆæƒ…å ±ã‚‚å«ã‚ã‚‹
    
    Args:
        query_id: ã‚¯ã‚¨ãƒªID
        optimized_result: æœ€é©åŒ–çµæžœ
        metrics: ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±
        analysis_result: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æžçµæžœ
    
    Returns:
        str: èª­ã¿ã‚„ã™ãæ§‹æˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ
    """
    from datetime import datetime
    
    # EXPLAIN + EXPLAIN COSTçµæžœãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆEXPLAIN_ENABLEDãŒYã®å ´åˆï¼‰
    explain_section = ""
    explain_cost_section = ""
    explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
    
    # ðŸ“Š æœ€æ–°ã®SQLãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¤œç´¢ï¼ˆçœç•¥è¡¨ç¤ºæ™‚ã®å‚ç…§ç”¨ - å¸¸ã«å®Ÿè¡Œï¼‰
    import glob
    import os
    
    optimized_sql_files = glob.glob("output_optimized_query_*.sql")
    latest_sql_filename = ""
    if optimized_sql_files:
        # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆï¼‰
        optimized_sql_files.sort(reverse=True)
        latest_sql_filename = optimized_sql_files[0]
    
    if explain_enabled.upper() == 'Y':
        print("ðŸ” For comprehensive report: Searching EXPLAIN + EXPLAIN COST result files...")
        
        # 1. æœ€æ–°ã®EXPLAINçµæžœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆæ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
        explain_original_files = glob.glob("output_explain_original_*.txt")
        explain_optimized_files = glob.glob("output_explain_optimized_*.txt")
        
        # 2. æœ€æ–°ã®EXPLAIN COSTçµæžœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        # ðŸš€ ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
        cached_cost_result = globals().get('cached_original_explain_cost_result')
        cost_original_files = []
        if cached_cost_result and 'explain_cost_file' in cached_cost_result:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å¾©å…ƒ
            cost_original_files = [cached_cost_result['explain_cost_file']]
            print(f"ðŸ’¾ Using cached original EXPLAIN COST file for comprehensive report")
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
            cost_original_files = glob.glob("output_explain_cost_original_*.txt")
        
        cost_optimized_files = glob.glob("output_explain_cost_optimized_*.txt")
        
        # ðŸŽ¯ ãƒ™ã‚¹ãƒˆè©¦è¡Œç•ªå·ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆé¸æŠž
        if best_attempt_number is not None:
            print(f"ðŸŽ¯ Searching for files from best attempt {best_attempt_number}...")
            
            # ãƒ™ã‚¹ãƒˆè©¦è¡Œã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            best_explain_files = [f for f in explain_optimized_files if f"attempt_{best_attempt_number}" in f]
            best_cost_files = [f for f in cost_optimized_files if f"attempt_{best_attempt_number}" in f]
            
            if best_explain_files:
                print(f"âœ… Found EXPLAIN file from best attempt {best_attempt_number}: {best_explain_files[0]}")
                explain_files = best_explain_files
            else:
                print(f"âš ï¸ EXPLAIN file from best attempt {best_attempt_number} not found, using post-optimization")
                explain_files = explain_optimized_files if explain_optimized_files else explain_original_files
            
            if best_cost_files:
                print(f"âœ… Found EXPLAIN COST file from best attempt {best_attempt_number}: {best_cost_files[0]}")
                cost_files = best_cost_files
            else:
                print(f"âš ï¸ EXPLAIN COST file from best attempt {best_attempt_number} not found, using post-optimization")
                cost_files = cost_optimized_files if cost_optimized_files else cost_original_files
        else:
            # å¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯: æœ€é©åŒ–å¾Œã‚’å„ªå…ˆã€ãªã‘ã‚Œã°ã‚ªãƒªã‚¸ãƒŠãƒ«
            explain_files = explain_optimized_files if explain_optimized_files else explain_original_files
            cost_files = cost_optimized_files if cost_optimized_files else cost_original_files
        
        # ðŸ“Š EXPLAIN + EXPLAIN COSTçµæžœã‚’è¦ç´„ã—ã¦ã‹ã‚‰ãƒ¬ãƒãƒ¼ãƒˆã«çµ„ã¿è¾¼ã¿
        explain_content = ""
        explain_cost_content = ""
        query_type = "optimized" if (explain_optimized_files or cost_optimized_files) else "original"
        # EXPLAIN ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        if explain_files:
            latest_explain_file = max(explain_files, key=os.path.getctime)
            try:
                with open(latest_explain_file, 'r', encoding='utf-8') as f:
                    explain_content = f.read()
                print(f"âœ… Loaded EXPLAIN results: {latest_explain_file}")
            except Exception as e:
                print(f"âš ï¸ Failed to load EXPLAIN results: {str(e)}")
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ãƒã‚§ãƒƒã‚¯
            old_explain_files = glob.glob("output_explain_plan_*.txt")
            if old_explain_files:
                latest_explain_file = max(old_explain_files, key=os.path.getctime)
                try:
                    with open(latest_explain_file, 'r', encoding='utf-8') as f:
                        explain_content = f.read()
                        print(f"âœ… Loaded legacy format EXPLAIN results: {latest_explain_file}")
                except Exception as e:
                    print(f"âš ï¸ Failed to load legacy format EXPLAIN results: {str(e)}")
        
        # EXPLAIN COST ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        if cost_files:
            latest_cost_file = max(cost_files, key=os.path.getctime)
            try:
                with open(latest_cost_file, 'r', encoding='utf-8') as f:
                    explain_cost_content = f.read()
                    print(f"ðŸ’° Loaded EXPLAIN COST results for comprehensive report: {latest_cost_file}")
            except Exception as e:
                print(f"âš ï¸ Failed to load EXPLAIN COST results: {str(e)}")
        
        # ðŸ“Š è¦ç´„æ©Ÿèƒ½ã‚’ä½¿ã£ã¦ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã«å¯¾å¿œ
        summary_results = summarize_explain_results_with_llm(explain_content, explain_cost_content, query_type, optimization_success)
        
        # è¦ç´„çµæžœã‚’ä½¿ã£ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰
        if summary_results['summarized']:
            print(f"ðŸ“Š Generating summary report sections (total size reduction)")
        
        if OUTPUT_LANGUAGE == 'ja':
            explain_section = f"""

## ðŸ” 6. EXPLAIN + EXPLAIN COSTçµ±åˆåˆ†æžçµæžœ

### ðŸ“Š è¦ç´„ã•ã‚ŒãŸå®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒ»çµ±è¨ˆæƒ…å ±

**åˆ†æžå¯¾è±¡**: {query_type}ã‚¯ã‚¨ãƒª
**è¦ç´„å®Ÿè¡Œ**: {'ã¯ã„ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾å¿œï¼‰' if summary_results['summarized'] else 'ã„ã„ãˆï¼ˆã‚µã‚¤ã‚ºå°ï¼‰'}

{summary_results['explain_summary']}

### ðŸ’° çµ±è¨ˆãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ã®åŠ¹æžœ

çµ±è¨ˆæƒ…å ±ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ä»¥ä¸‹ã®æ”¹å–„åŠ¹æžœãŒæœŸå¾…ã§ãã¾ã™ï¼š

| é …ç›® | å¾“æ¥ï¼ˆæŽ¨æ¸¬ãƒ™ãƒ¼ã‚¹ï¼‰ | çµ±è¨ˆãƒ™ãƒ¼ã‚¹ | æ”¹å–„åŠ¹æžœ |
|------|-------------------|-----------|----------|
| BROADCASTåˆ¤å®šç²¾åº¦ | ç´„60% | ç´„95% | **+35%** |
| ã‚¹ãƒ”ãƒ«äºˆæ¸¬ç²¾åº¦ | ç´„40% | ç´„85% | **+45%** |
| ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æœ€é©åŒ– | ç´„50% | ç´„90% | **+40%** |
| å…¨ä½“æœ€é©åŒ–åŠ¹æžœ | å¹³å‡30%æ”¹å–„ | å¹³å‡60%æ”¹å–„ | **+30%** |

### ðŸŽ¯ çµ±è¨ˆæƒ…å ±æ¦‚è¦

çµ±è¨ˆæƒ…å ±ã«ã‚ˆã‚‹æœ€é©åŒ–ãŒå®Ÿè¡Œã•ã‚Œã¾ã—ãŸï¼ˆè©³ç´°ã¯DEBUG_ENABLED='Y'ã§ç¢ºèªå¯èƒ½ï¼‰ã€‚

"""
            explain_cost_section = ""  # çµ±åˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ãªã®ã§å€‹åˆ¥ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ä¸è¦
        else:
            explain_section = f"""

## ðŸ” 6. EXPLAIN + EXPLAIN COST Integrated Analysis Results

### ðŸ“Š Summarized Execution Plan & Statistical Information

**Analysis Target**: {query_type} query
**Summarization**: {'Yes (Token Limit Adaptation)' if summary_results['summarized'] else 'No (Small Size)'}

{summary_results['explain_summary']}

### ðŸ’° Effects of Statistics-Based Optimization

The following improvement effects can be expected by leveraging statistical information:

| Item | Traditional (Guess-based) | Statistics-based | Improvement |
|------|---------------------------|------------------|-------------|
| BROADCAST Judgment Accuracy | ~60% | ~95% | **+35%** |
| Spill Prediction Accuracy | ~40% | ~85% | **+45%** |
| Partition Optimization | ~50% | ~90% | **+40%** |
| Overall Optimization Effect | Average 30% improvement | Average 60% improvement | **+30%** |

### ðŸŽ¯ Statistical Information Overview

Statistical optimization has been executed (details available with DEBUG_ENABLED='Y').

"""
            explain_cost_section = ""  # Integrated section, so no separate section needed
    else:
        if OUTPUT_LANGUAGE == 'ja':
            explain_section = "\n\n## ðŸ” 6. EXPLAIN + EXPLAIN COSTçµ±åˆåˆ†æžçµæžœ\n\nâš ï¸ EXPLAIN_ENABLED = 'N' ã®ãŸã‚ã€EXPLAINåˆ†æžã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚\n"
            explain_cost_section = ""
        else:
            explain_section = "\n\n## ðŸ” 6. EXPLAIN + EXPLAIN COST Integrated Analysis Results\n\nâš ï¸ EXPLAIN analysis was skipped because EXPLAIN_ENABLED = 'N'.\n"
            explain_cost_section = ""
    
    # åŸºæœ¬æƒ…å ±ã®å–å¾—
    # åŸºæœ¬æƒ…å ±ã®å–å¾—
    overall_metrics = metrics.get('overall_metrics', {})
    bottleneck_indicators = metrics.get('bottleneck_indicators', {})
    liquid_analysis = metrics.get('liquid_clustering_analysis', {})
    
    # thinking_enabledå¯¾å¿œ: analysis_resultãŒãƒªã‚¹ãƒˆã®å ´åˆã®å‡¦ç†
    if isinstance(analysis_result, list):
        analysis_result_str = format_thinking_response(analysis_result)
    else:
        analysis_result_str = str(analysis_result)
    
    # signatureæƒ…å ±ã®é™¤åŽ»
    import re
    signature_pattern = r"'signature':\s*'[A-Za-z0-9+/=]{100,}'"
    analysis_result_str = re.sub(signature_pattern, "'signature': '[REMOVED]'", analysis_result_str)
    
    # æ—¥æœ¬èªžå‡ºåŠ›ã®å ´åˆã€analysis_result_strã‚’LLMã§æ—¥æœ¬èªžã«ç¿»è¨³
    if OUTPUT_LANGUAGE == 'ja' and analysis_result_str and analysis_result_str.strip():
        analysis_result_str = translate_analysis_to_japanese(analysis_result_str)
    
    # ãƒ¬ãƒãƒ¼ãƒˆã®æ§‹æˆ
    if OUTPUT_LANGUAGE == 'ja':
        report = f"""# ðŸ“Š SQLæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ

**ã‚¯ã‚¨ãƒªID**: {query_id}  
**ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ðŸŽ¯ 1. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æžçµæžœ

### ðŸ¤– AIã«ã‚ˆã‚‹è©³ç´°åˆ†æž

{analysis_result_str}

### ðŸ“Š ä¸»è¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æŒ‡æ¨™

| æŒ‡æ¨™ | å€¤ | è©•ä¾¡ |
|------|-----|------|
| å®Ÿè¡Œæ™‚é–“ | {overall_metrics.get('total_time_ms', 0):,} ms | {'âœ… è‰¯å¥½' if overall_metrics.get('total_time_ms', 0) < 60000 else 'âš ï¸ æ”¹å–„å¿…è¦'} |
| Photonæœ‰åŠ¹ | {'ã¯ã„' if overall_metrics.get('photon_enabled', False) else 'ã„ã„ãˆ'} | {'âœ… è‰¯å¥½' if overall_metrics.get('photon_enabled', False) else 'âŒ æœªæœ‰åŠ¹'} |
| ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹çŽ‡ | {bottleneck_indicators.get('cache_hit_ratio', 0) * 100:.1f}% | {'âœ… è‰¯å¥½' if bottleneck_indicators.get('cache_hit_ratio', 0) > 0.8 else 'âš ï¸ æ”¹å–„å¿…è¦'} |
| ãƒ•ã‚£ãƒ«ã‚¿çŽ‡ | {bottleneck_indicators.get('data_selectivity', 0) * 100:.2f}% | {'âœ… è‰¯å¥½' if bottleneck_indicators.get('data_selectivity', 0) > 0.5 else 'âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã‚’ç¢ºèª'} |
| ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ | {bottleneck_indicators.get('shuffle_operations_count', 0)}å›ž | {'âœ… è‰¯å¥½' if bottleneck_indicators.get('shuffle_operations_count', 0) < 5 else 'âš ï¸ å¤šæ•°'} |
| ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ | {'ã¯ã„' if bottleneck_indicators.get('has_spill', False) else 'ã„ã„ãˆ'} | {'âŒ å•é¡Œã‚ã‚Š' if bottleneck_indicators.get('has_spill', False) else 'âœ… è‰¯å¥½'} |
| ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º | {'AQEã§æ¤œå‡ºãƒ»å¯¾å¿œæ¸ˆ' if bottleneck_indicators.get('has_skew', False) else 'æ½œåœ¨çš„ãªã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§ã‚ã‚Š' if bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False) else 'æœªæ¤œå‡º'} | {'ðŸ”§ AQEå¯¾å¿œæ¸ˆ' if bottleneck_indicators.get('has_skew', False) else 'âš ï¸ æ”¹å–„å¿…è¦' if bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False) else 'âœ… è‰¯å¥½'} |

### ðŸš¨ ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯

"""
        
        # ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®è©³ç´°
        bottlenecks = []
        
        if bottleneck_indicators.get('has_spill', False):
            spill_gb = bottleneck_indicators.get('spill_bytes', 0) / 1024 / 1024 / 1024
            bottlenecks.append(f"**ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«**: {spill_gb:.2f}GB - ãƒ¡ãƒ¢ãƒªä¸è¶³ã«ã‚ˆã‚‹æ€§èƒ½ä½Žä¸‹")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            bottlenecks.append("**ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: JOIN/GROUP BYå‡¦ç†ã§ã®å¤§é‡ãƒ‡ãƒ¼ã‚¿è»¢é€")
        
        if bottleneck_indicators.get('has_skew', False):
            bottlenecks.append("**ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼**: AQEã§æ¤œå‡ºãƒ»å¯¾å¿œæ¸ˆ - SparkãŒè‡ªå‹•çš„ã«æœ€é©åŒ–å®Ÿè¡Œ")
        elif bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False):
            bottlenecks.append("**ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼**: æ½œåœ¨çš„ãªã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§ã‚ã‚Š - ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºãŒ512MBä»¥ä¸Š")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            bottlenecks.append("**ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹çŽ‡ä½Žä¸‹**: ãƒ‡ãƒ¼ã‚¿å†åˆ©ç”¨åŠ¹çŽ‡ãŒä½Žã„")
        
        if not overall_metrics.get('photon_enabled', False):
            bottlenecks.append("**Photonæœªæœ‰åŠ¹**: é«˜é€Ÿå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã•ã‚Œã¦ã„ãªã„")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.2:
            bottlenecks.append("**ãƒ•ã‚£ãƒ«ã‚¿åŠ¹çŽ‡ä½Žä¸‹**: å¿…è¦ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã‚‹")
        
        if bottlenecks:
            for i, bottleneck in enumerate(bottlenecks, 1):
                report += f"{i}. {bottleneck}\n"
        else:
            report += "ä¸»è¦ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯è¨­å®šãªã—ã€‚\n"
        
        report += "\n"
        
        # Add Liquid Clustering analysis results
        if liquid_analysis:
            performance_context = liquid_analysis.get('performance_context', {})
            llm_analysis = liquid_analysis.get('llm_analysis', '')
            
            report += f"""

## ðŸ—‚ï¸ 3. Liquid Clusteringåˆ†æžçµæžœ

### ðŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¦‚è¦

| é …ç›® | å€¤ |
|------|-----|
| å®Ÿè¡Œæ™‚é–“ | {performance_context.get('total_time_sec', 0):.1f}ç§’ |
| ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ | {performance_context.get('read_gb', 0):.2f}GB |
| å‡ºåŠ›è¡Œæ•° | {performance_context.get('rows_produced', 0):,}è¡Œ |
| èª­ã¿è¾¼ã¿è¡Œæ•° | {performance_context.get('rows_read', 0):,}è¡Œ |
| ãƒ•ã‚£ãƒ«ã‚¿çŽ‡ | {performance_context.get('data_selectivity', 0):.4f} |

### ðŸ¤– AIåˆ†æžçµæžœ

{llm_analysis}

"""
        
        # æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10ã‚’çµ±åˆ
        report += f"""
## ðŸŒ 2. æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10

### ðŸ“Š è©³ç´°ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æž

ä»¥ä¸‹ã®ãƒˆãƒ”ãƒƒã‚¯ã«åŸºã¥ã„ã¦å‡¦ç†ã‚’åˆ†æžã—ã¾ã™ï¼š

#### ðŸ” åˆ†æžå¯¾è±¡ãƒˆãƒ”ãƒƒã‚¯
- **â±ï¸ å®Ÿè¡Œæ™‚é–“**: å…¨ä½“ã«å ã‚ã‚‹å‡¦ç†æ™‚é–“ã®å‰²åˆ
- **ðŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼
- **ðŸ”§ ä¸¦åˆ—åº¦**: ã‚¿ã‚¹ã‚¯æ•°ã¨ä¸¦åˆ—å®Ÿè¡ŒåŠ¹çŽ‡
- **ðŸ’¿ ã‚¹ãƒ”ãƒ«æ¤œå‡º**: ãƒ¡ãƒ¢ãƒªä¸è¶³ã«ã‚ˆã‚‹ãƒ‡ã‚£ã‚¹ã‚¯ã‚¹ãƒ”ãƒ«
- **âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º**: AQEãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿åˆ†æ•£ä¸å‡ç­‰æ¤œå‡º
- **ðŸ”„ Shuffleå±žæ€§**: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å†åˆ†æ•£ã®æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆ
- **ðŸš€ å‡¦ç†åŠ¹çŽ‡**: è¡Œ/ç§’ã§ã®å‡¦ç†åŠ¹çŽ‡æŒ‡æ¨™

"""
        
        # TOP10ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã¨çµ±åˆ
        try:
            top10_report = generate_top10_time_consuming_processes_report(metrics, 10)
            # ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤åŽ»ã—ã¦çµ±åˆ
            top10_lines = top10_report.split('\n')
            # "## ðŸŒ æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10"ã®è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            filtered_lines = []
            skip_header = True
            for line in top10_lines:
                if skip_header and line.startswith("## ðŸŒ"):
                    skip_header = False
                    continue
                if not skip_header:
                    filtered_lines.append(line)
            
            report += '\n'.join(filtered_lines)
            
        except Exception as e:
            report += f"âš ï¸ TOP10å‡¦ç†æ™‚é–“åˆ†æžã®ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\n"
        
        # SQLæœ€é©åŒ–åˆ†æžçµæžœã®è¿½åŠ 
        # ðŸš€ SQLãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®å ´åˆã¯é©åˆ‡ã«ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆï¼ˆçœç•¥æ©Ÿèƒ½ä»˜ãï¼‰
        formatted_sql_content = format_sql_content_for_report(optimized_result, latest_sql_filename)
        
        # ðŸŽ¯ æœ€é©åŒ–æ–¹é‡è¦ç´„ã‚’ç”Ÿæˆ
        optimization_strategy = generate_optimization_strategy_summary(optimized_result, metrics, analysis_result_str)
        
        # ðŸ“Š æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹è©³ç´°ã®ç”Ÿæˆ
        optimization_process_details = ""
        if optimization_attempts is not None and best_attempt_number is not None:
            total_attempts = len(optimization_attempts)
            cost_improvement = "N/A"
            memory_improvement = "N/A"
            
            if performance_comparison:
                cost_ratio = performance_comparison.get('total_cost_ratio', 1.0)
                memory_ratio = performance_comparison.get('memory_usage_ratio', 1.0)
                
                # Noneå€¤ã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
                if cost_ratio is None:
                    cost_ratio = 1.0
                if memory_ratio is None:
                    memory_ratio = 1.0
                    
                cost_improvement = f"{(1-cost_ratio)*100:.1f}"
                memory_improvement = f"{(1-memory_ratio)*100:.1f}"
            
            # æœ€çµ‚é¸æŠžã®è¡¨ç¤ºã‚’åˆ†ã‹ã‚Šã‚„ã™ãã™ã‚‹
            if best_attempt_number == 0:
                final_selection = "å…ƒã®ã‚¯ã‚¨ãƒªï¼ˆæœ€é©åŒ–ã«ã‚ˆã‚Šæ”¹å–„ã•ã‚Œãªã‹ã£ãŸãŸã‚ï¼‰"
                selection_reason = "æœ€é©åŒ–è©¦è¡Œã§æœ‰åŠ¹ãªæ”¹å–„ãŒå¾—ã‚‰ã‚Œãªã‹ã£ãŸãŸã‚ã€å…ƒã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨"
                # ðŸ“„ å…ƒã®ã‚¯ã‚¨ãƒªãƒ•ã‚¡ã‚¤ãƒ«åæƒ…å ±ã‚’è¿½åŠ 
                if latest_sql_filename:
                    selection_reason += f"\n- ðŸ“„ å‚è€ƒãƒ•ã‚¡ã‚¤ãƒ«: {latest_sql_filename}ï¼ˆæœ€é©åŒ–è©¦è¡Œçµæžœï¼‰"
                else:
                    selection_reason += "\n- ðŸ“„ å…ƒã®ã‚¯ã‚¨ãƒª: ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æŠ½å‡º"
            else:
                final_selection = f"è©¦è¡Œ{best_attempt_number}ç•ª"
                selection_reason = "ã‚³ã‚¹ãƒˆåŠ¹çŽ‡ãŒæœ€ã‚‚è‰¯ã„è©¦è¡Œã‚’é¸æŠž"
            
            # è©³ç´°è©¦è¡Œå±¥æ­´ã‚’ç”Ÿæˆ
            detailed_trial_history = format_trial_history_summary(optimization_attempts, 'ja')
            
            optimization_process_details = f"""### ðŸŽ¯ æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹è©³ç´°
æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œã•ã‚ŒãŸè©¦è¡Œã¨ãã®é¸æŠžç†ç”±ã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ï¼š

**ðŸ“Š æœ€é©åŒ–è©¦è¡Œå±¥æ­´:**
- è©¦è¡Œå›žæ•°: {total_attempts}å›žå®Ÿè¡Œ
- æœ€çµ‚é¸æŠž: {final_selection}
- é¸æŠžç†ç”±: {selection_reason}

{detailed_trial_history}

**ðŸ† é¸æŠžã•ã‚ŒãŸæœ€é©åŒ–ã®åŠ¹æžœ:**
- ã‚³ã‚¹ãƒˆå‰Šæ¸›çŽ‡: {cost_improvement}% (EXPLAIN COSTæ¯”è¼ƒ)
- ãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡æ”¹å–„: {memory_improvement}% (çµ±è¨ˆæ¯”è¼ƒ)

"""
        
        report += f"""

## ðŸš€ 4. SQLæœ€é©åŒ–åˆ†æžçµæžœ

{optimization_process_details}### ðŸŽ¯ æœ€é©åŒ–å®Ÿè¡Œæ–¹é‡

{optimization_strategy}

### ðŸ’¡ æœ€é©åŒ–ææ¡ˆ

{formatted_sql_content}

### ðŸ” 5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¤œè¨¼çµæžœ

{generate_performance_comparison_section(performance_comparison)}

### ðŸ“ˆ 6. æœŸå¾…ã•ã‚Œã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ”¹å–„åŠ¹æžœ

#### ðŸŽ¯ äºˆæƒ³ã•ã‚Œã‚‹æ”¹å–„ç‚¹

"""
        
        # æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„åŠ¹æžœã‚’è¨ˆç®—
        expected_improvements = []
        
        if bottleneck_indicators.get('has_spill', False):
            expected_improvements.append("**ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«è§£æ¶ˆ**: æœ€å¤§50-80%ã®æ€§èƒ½æ”¹å–„ãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            expected_improvements.append("**ã‚·ãƒ£ãƒƒãƒ•ãƒ«æœ€é©åŒ–**: 20-60%ã®å®Ÿè¡Œæ™‚é–“çŸ­ç¸®ãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            expected_improvements.append("**ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹çŽ‡å‘ä¸Š**: 30-70%ã®èª­ã¿è¾¼ã¿æ™‚é–“çŸ­ç¸®ãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if not overall_metrics.get('photon_enabled', False):
            expected_improvements.append("**Photonæœ‰åŠ¹åŒ–**: 2-10å€ã®å‡¦ç†é€Ÿåº¦å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.2:
            expected_improvements.append("**ãƒ•ã‚£ãƒ«ã‚¿åŠ¹çŽ‡æ”¹å–„**: 40-90%ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é‡å‰Šæ¸›ãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if expected_improvements:
            for i, improvement in enumerate(expected_improvements, 1):
                report += f"{i}. {improvement}\n"
            
            # ç·åˆçš„ãªæ”¹å–„åŠ¹æžœ
            total_time_ms = overall_metrics.get('total_time_ms', 0)
            if total_time_ms > 0:
                improvement_ratio = min(0.8, len(expected_improvements) * 0.15)  # æœ€å¤§80%æ”¹å–„
                expected_time = total_time_ms * (1 - improvement_ratio)
                report += f"\n**ç·åˆæ”¹å–„åŠ¹æžœ**: å®Ÿè¡Œæ™‚é–“ {total_time_ms:,}ms â†’ {expected_time:,.0f}msï¼ˆç´„{improvement_ratio*100:.0f}%æ”¹å–„ï¼‰\n"
        else:
            report += "ç¾åœ¨ã®ã‚¯ã‚¨ãƒªã¯æ—¢ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚å¤§å¹…ãªæ”¹å–„ã¯æœŸå¾…ã•ã‚Œã¾ã›ã‚“ã€‚\n"
        
        report += f"""

#### ðŸ”§ å®Ÿè£…å„ªå…ˆåº¦

1. **é«˜å„ªå…ˆåº¦**: Photonæœ‰åŠ¹åŒ–ã€ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«è§£æ¶ˆ
2. **ä¸­å„ªå…ˆåº¦**: Liquid Clusteringã€ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæœ€é©åŒ–
3. **ä½Žå„ªå…ˆåº¦**: çµ±è¨ˆæƒ…å ±æ›´æ–°ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥

{explain_section}

{explain_cost_section}

---

*ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
    else:
        # è‹±èªžç‰ˆï¼ˆåŒæ§˜ã®æ§‹æˆï¼‰
        report = f"""# ðŸ“Š SQL Optimization Report

**Query ID**: {query_id}  
**Report Generation Time**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ðŸŽ¯ 1. Bottleneck Analysis Results

### ðŸ¤– AI-Powered Analysis

{analysis_result_str}

### ðŸ“Š Key Performance Indicators

| Metric | Value | Status |
|--------|-------|--------|
| Execution Time | {overall_metrics.get('total_time_ms', 0):,} ms | {'âœ… Good' if overall_metrics.get('total_time_ms', 0) < 60000 else 'âš ï¸ Needs Improvement'} |
| Photon Enabled | {'Yes' if overall_metrics.get('photon_enabled', False) else 'No'} | {'âœ… Good' if overall_metrics.get('photon_enabled', False) else 'âŒ Not Enabled'} |
| Cache Efficiency | {bottleneck_indicators.get('cache_hit_ratio', 0) * 100:.1f}% | {'âœ… Good' if bottleneck_indicators.get('cache_hit_ratio', 0) > 0.8 else 'âš ï¸ Needs Improvement'} |
| Filter Rate | {bottleneck_indicators.get('data_selectivity', 0) * 100:.2f}% | {'âœ… Good' if bottleneck_indicators.get('data_selectivity', 0) > 0.5 else 'âš ï¸ Check Filter Conditions'} |
| Shuffle Operations | {bottleneck_indicators.get('shuffle_operations_count', 0)} times | {'âœ… Good' if bottleneck_indicators.get('shuffle_operations_count', 0) < 5 else 'âš ï¸ High'} |
| Spill Occurrence | {'Yes' if bottleneck_indicators.get('has_spill', False) else 'No'} | {'âŒ Issues' if bottleneck_indicators.get('has_spill', False) else 'âœ… Good'} |
| Skew Detection | {'AQE Detected & Handled' if bottleneck_indicators.get('has_skew', False) else 'Not Detected'} | {'ðŸ”§ AQE Handled' if bottleneck_indicators.get('has_skew', False) else 'âœ… Good'} |

### ðŸš¨ Key Bottlenecks

"""
        
        # ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®è©³ç´°ï¼ˆè‹±èªžç‰ˆï¼‰
        bottlenecks = []
        
        if bottleneck_indicators.get('has_spill', False):
            spill_gb = bottleneck_indicators.get('spill_bytes', 0) / 1024 / 1024 / 1024
            bottlenecks.append(f"**Memory Spill**: {spill_gb:.2f}GB - Performance degradation due to memory shortage")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            bottlenecks.append("**Shuffle Bottleneck**: Large data transfer in JOIN/GROUP BY operations")
        
        if bottleneck_indicators.get('has_skew', False):
            bottlenecks.append("**Data Skew**: AQE Detected & Handled - Spark automatically optimized execution")
        elif bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False):
            bottlenecks.append("**Data Skew**: Potential skew possibility - Partition size â‰¥ 512MB")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            bottlenecks.append("**Cache Inefficiency**: Low data reuse efficiency")
        
        if not overall_metrics.get('photon_enabled', False):
            bottlenecks.append("**Photon Not Enabled**: High-speed processing engine not utilized")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.2:
            bottlenecks.append("**Poor Filter Efficiency**: Reading more data than necessary")
        
        if bottlenecks:
            for i, bottleneck in enumerate(bottlenecks, 1):
                report += f"{i}. {bottleneck}\n"
        else:
            report += "No major bottlenecks detected.\n"
        
        report += "\n"
        
        # æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10ã‚’çµ±åˆï¼ˆè‹±èªžç‰ˆï¼‰
        report += f"""
## ðŸŒ 2. Top 10 Most Time-Consuming Processes

### ðŸ“Š Detailed Bottleneck Analysis

The following topics are analyzed for process evaluation:

#### ðŸ” Analysis Topics
- **â±ï¸ Execution Time**: Percentage of total processing time
- **ðŸ’¾ Memory Usage**: Peak memory usage and memory pressure
- **ðŸ”§ Parallelism**: Number of tasks and parallel execution efficiency
- **ðŸ’¿ Spill Detection**: Disk spill due to memory shortage
- **âš–ï¸ Skew Detection**: AQE-based data distribution imbalance detection
- **ðŸ”„ Shuffle Attributes**: Optimization points for partition redistribution
- **ðŸš€ Processing Efficiency**: Processing efficiency metrics in rows/second

"""
        
        # TOP10ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã¨çµ±åˆï¼ˆè‹±èªžç‰ˆï¼‰
        try:
            top10_report = generate_top10_time_consuming_processes_report(metrics, 10)
            # ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤åŽ»ã—ã¦çµ±åˆ
            top10_lines = top10_report.split('\n')
            # "## ðŸŒ æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10"ã®è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            filtered_lines = []
            skip_header = True
            for line in top10_lines:
                if skip_header and line.startswith("## ðŸŒ"):
                    skip_header = False
                    continue
                if not skip_header:
                    filtered_lines.append(line)
            
            report += '\n'.join(filtered_lines)
            
        except Exception as e:
            report += f"âš ï¸ Error generating TOP10 analysis: {str(e)}\n"
        
        # Add Liquid Clustering analysis results (English version)
        if liquid_analysis:
            performance_context = liquid_analysis.get('performance_context', {})
            llm_analysis = liquid_analysis.get('llm_analysis', '')
            
            report += f"""

## ðŸ—‚ï¸ 3. Liquid Clustering Analysis Results

### ðŸ“Š Performance Overview

| Item | Value |
|------|-------|
| Execution Time | {performance_context.get('total_time_sec', 0):.1f}s |
| Data Read | {performance_context.get('read_gb', 0):.2f}GB |
| Output Rows | {performance_context.get('rows_produced', 0):,} |
| Read Rows | {performance_context.get('rows_read', 0):,} |
| Filter Rate | {performance_context.get('data_selectivity', 0):.4f} |

### ðŸ¤– AI Analysis Results

{llm_analysis}

"""
        
        # SQLæœ€é©åŒ–åˆ†æžçµæžœã®è¿½åŠ ï¼ˆè‹±èªžç‰ˆï¼‰
        # ðŸš€ SQLãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®å ´åˆã¯é©åˆ‡ã«ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆï¼ˆçœç•¥æ©Ÿèƒ½ä»˜ãï¼‰
        formatted_sql_content = format_sql_content_for_report(optimized_result, latest_sql_filename)
        
        # ðŸŽ¯ æœ€é©åŒ–æ–¹é‡è¦ç´„ã‚’ç”Ÿæˆï¼ˆè‹±èªžç‰ˆï¼‰
        optimization_strategy = generate_optimization_strategy_summary(optimized_result, metrics, analysis_result_str)
        
        # ðŸ“Š æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹è©³ç´°ã®ç”Ÿæˆï¼ˆè‹±èªžç‰ˆï¼‰
        optimization_process_details_en = ""
        if optimization_attempts is not None and best_attempt_number is not None:
            total_attempts = len(optimization_attempts)
            cost_improvement = "N/A"
            memory_improvement = "N/A"
            
            if performance_comparison:
                cost_ratio = performance_comparison.get('total_cost_ratio', 1.0)
                memory_ratio = performance_comparison.get('memory_usage_ratio', 1.0)
                
                # Noneå€¤ã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
                if cost_ratio is None:
                    cost_ratio = 1.0
                if memory_ratio is None:
                    memory_ratio = 1.0
                    
                cost_improvement = f"{(1-cost_ratio)*100:.1f}"
                memory_improvement = f"{(1-memory_ratio)*100:.1f}"
            
            # Make final selection display clearer
            if best_attempt_number == 0:
                final_selection_en = "Original Query (no improvement achieved through optimization)"
                selection_reason_en = "Using original query as optimization trials did not yield effective improvements"
                # ðŸ“„ Add original query file name information
                if latest_sql_filename:
                    selection_reason_en += f"\n- ðŸ“„ Reference file: {latest_sql_filename} (optimization trial result)"
                else:
                    selection_reason_en += "\n- ðŸ“„ Original query: Extracted from profiler data"
            else:
                final_selection_en = f"Trial {best_attempt_number}"
                selection_reason_en = "Selected the trial with the best cost efficiency"
            
            # è©³ç´°è©¦è¡Œå±¥æ­´ã‚’ç”Ÿæˆï¼ˆè‹±èªžç‰ˆï¼‰
            detailed_trial_history_en = format_trial_history_summary(optimization_attempts, 'en')
            
            optimization_process_details_en = f"""### ðŸŽ¯ Optimization Process Details
The following shows the trials executed during the optimization process and the selection rationale:

**ðŸ“Š Optimization Trial History:**
- Trial count: {total_attempts} attempts executed
- Final selection: {final_selection_en}
- Selection reason: {selection_reason_en}

{detailed_trial_history_en}

**ðŸ† Selected Optimization Effects:**
- Cost reduction rate: {cost_improvement}% (EXPLAIN COST comparison)
- Memory efficiency improvement: {memory_improvement}% (statistics comparison)

"""
        
        # æ—¥æœ¬èªžã‹ã‚‰è‹±èªžã¸ã®ç¿»è¨³ãƒžãƒƒãƒ”ãƒ³ã‚°
        translation_map = {
            "ðŸ” æ¤œå‡ºã•ã‚ŒãŸä¸»è¦èª²é¡Œ": "ðŸ” Key Issues Identified",
            "ðŸ› ï¸ é©ç”¨ã•ã‚ŒãŸæœ€é©åŒ–æ‰‹æ³•": "ðŸ› ï¸ Applied Optimization Techniques",
            "ðŸŽ¯ æœ€é©åŒ–é‡ç‚¹åˆ†é‡Ž": "ðŸŽ¯ Optimization Focus Areas",
            "ðŸ“Š çµ±è¨ˆæƒ…å ±æ´»ç”¨": "ðŸ“Š Statistical Analysis Utilization",
            "EXPLAIN + EXPLAIN COSTåˆ†æžã«ã‚ˆã‚Šã€çµ±è¨ˆãƒ™ãƒ¼ã‚¹ã®ç²¾å¯†ãªæœ€é©åŒ–ã‚’å®Ÿè¡Œ": "Statistical-based precise optimization through EXPLAIN + EXPLAIN COST analysis",
            "ðŸ¤– AIåˆ†æžã«ã‚ˆã‚‹åŒ…æ‹¬çš„ãªæœ€é©åŒ–": "ðŸ¤– Comprehensive AI-driven Optimization",
            "ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æžã€çµ±è¨ˆæƒ…å ±ã€ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’ç·åˆã—ãŸæœ€é©åŒ–ã‚’å®Ÿè¡Œ": "Comprehensive optimization integrating bottleneck analysis, statistical data, and best practices",
            "ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«ç™ºç”Ÿ": "Memory Spill Occurrence",
            "ã‚·ãƒ£ãƒƒãƒ•ãƒ«å‡¦ç†ãƒœãƒˆãƒ«ãƒãƒƒã‚¯": "Shuffle Processing Bottleneck",
            "ä¸¦åˆ—åº¦ä¸è¶³": "Insufficient Parallelism",
            "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆçŽ‡ä½Žä¸‹": "Low Cache Hit Rate",
            "Photon Engineæœªæ´»ç”¨": "Photon Engine Not Utilized",
            "ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ç™ºç”Ÿ": "Data Skew Occurrence",
            "ãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡åŒ–": "Memory Efficiency",
            "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è² è·è»½æ¸›": "Network Load Reduction",
            "ä¸¦åˆ—å‡¦ç†èƒ½åŠ›å‘ä¸Š": "Parallel Processing Enhancement"
        }
        
        optimization_strategy_en = optimization_strategy
        for jp_text, en_text in translation_map.items():
            optimization_strategy_en = optimization_strategy_en.replace(jp_text, en_text)
        
        # EXPLAINè¦ç´„ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨è¿½åŠ ï¼ˆå‹•çš„ã«æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼‰
        explain_summary_section = ""
        try:
            # ðŸš€ æœ€é©åŒ–æˆåŠŸæ™‚ã¯ã‚ªãƒªã‚¸ãƒŠãƒ«è¦ç´„ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚¨ãƒ©ãƒ¼ãƒªã‚¹ã‚¯æŽ’é™¤ï¼‰
            optimized_files = glob.glob("output_explain_summary_optimized_*.md")
            
            if optimization_success is True:
                # æœ€é©åŒ–æˆåŠŸæ™‚ã¯ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œãªã„ãŸã‚ã€æœ€é©åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿æ¤œç´¢
                all_explain_files = optimized_files
                print("ðŸ’° Skipping original summary file search (optimization succeeded - cost reduction)")
            else:
                # é€šå¸¸ã¯ä¸¡æ–¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
                original_files = glob.glob("output_explain_summary_original_*.md")
                all_explain_files = optimized_files + original_files
            
            if all_explain_files:
                # ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæ™‚åˆ»ã§æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠžï¼ˆã‚ˆã‚Šç¢ºå®Ÿï¼‰
                import os
                latest_explain_summary = max(all_explain_files, key=os.path.getctime)
                file_age = os.path.getctime(latest_explain_summary)
                
                print(f"ðŸ” Found {len(all_explain_files)} EXPLAIN summary files:")
                for f in sorted(all_explain_files, key=os.path.getctime, reverse=True):
                    age = os.path.getctime(f)
                    status = "ðŸ“ SELECTED" if f == latest_explain_summary else "  "
                    print(f"   {status} {f} (created: {os.path.getctime(f)})")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿è¾¼ã¿
                with open(latest_explain_summary, 'r', encoding='utf-8') as f:
                    explain_content = f.read()
                
                # è‹±èªžç‰ˆã«ç¿»è¨³
                explain_content_en = translate_explain_summary_to_english(explain_content)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®šï¼ˆoptimized/originalï¼‰
                file_type = "Optimized" if "optimized" in latest_explain_summary else "Original"
                
                explain_summary_section = f"""
### ðŸ“‹ Current Query Explain Output ({file_type} Query)

> **Source File**: `{latest_explain_summary}`  
> **Analysis Type**: {file_type} query execution plan analysis

{explain_content_en}

"""
                print(f"âœ… EXPLAIN summary integrated: {latest_explain_summary} ({file_type})")
            else:
                print("âš ï¸ No EXPLAIN summary files found (searched: output_explain_summary_*.md)")
                # EXPLAINå®Ÿè¡ŒãŒç„¡åŠ¹ãªå ´åˆã®èª¬æ˜Žã‚’è¿½åŠ 
                explain_summary_section = f"""
### ðŸ“‹ Current Query Explain Output

âš ï¸ **EXPLAIN analysis not available**

No EXPLAIN summary files were found. This could be due to:
- EXPLAIN_ENABLED setting is 'N' (disabled)
- EXPLAIN execution failed or was skipped
- Files haven't been generated yet for this query

To enable EXPLAIN analysis, set `EXPLAIN_ENABLED = 'Y'` before running the analysis.

"""
        except Exception as e:
            print(f"âš ï¸ Error loading EXPLAIN summary: {str(e)}")
            explain_summary_section = f"""
### ðŸ“‹ Current Query Explain Output

âŒ **Error loading EXPLAIN analysis**

An error occurred while loading EXPLAIN summary files: `{str(e)}`

Please check:
- File permissions and accessibility
- EXPLAIN_ENABLED setting
- Query execution status

"""

        report += f"""
## ðŸš€ 4. SQL Optimization Analysis Results

{optimization_process_details_en}### ðŸŽ¯ Optimization Strategy

{optimization_strategy_en}

### ðŸ’¡ Optimization Recommendations

{formatted_sql_content}

{explain_summary_section}### ðŸ” 5. Performance Verification Results

{generate_performance_comparison_section(performance_comparison, language='en')}

### ðŸ“ˆ 6. Expected Performance Improvement

#### ðŸŽ¯ Anticipated Improvements

"""
        
        # æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„åŠ¹æžœã‚’è¨ˆç®—ï¼ˆè‹±èªžç‰ˆï¼‰
        expected_improvements = []
        
        if bottleneck_indicators.get('has_spill', False):
            expected_improvements.append("**Memory Spill Resolution**: Up to 50-80% performance improvement expected")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            expected_improvements.append("**Shuffle Optimization**: 20-60% execution time reduction expected")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            expected_improvements.append("**Cache Efficiency**: 30-70% read time reduction expected")
        
        if not overall_metrics.get('photon_enabled', False):
            expected_improvements.append("**Photon Enablement**: 2-10x processing speed improvement expected")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.2:
            expected_improvements.append("**Filter Efficiency**: 40-90% data read volume reduction expected")
        
        if expected_improvements:
            for i, improvement in enumerate(expected_improvements, 1):
                report += f"{i}. {improvement}\n"
            
            # ç·åˆçš„ãªæ”¹å–„åŠ¹æžœ
            total_time_ms = overall_metrics.get('total_time_ms', 0)
            if total_time_ms > 0:
                improvement_ratio = min(0.8, len(expected_improvements) * 0.15)  # æœ€å¤§80%æ”¹å–„
                expected_time = total_time_ms * (1 - improvement_ratio)
                report += f"\n**Overall Improvement**: Execution time {total_time_ms:,}ms â†’ {expected_time:,.0f}ms (approx. {improvement_ratio*100:.0f}% improvement)\n"
        else:
            report += "Current query is already optimized. No significant improvements expected.\n"
        
        report += f"""

#### ðŸ”§ Implementation Priority

1. **High Priority**: Photon enablement, Memory spill resolution
2. **Medium Priority**: Liquid Clustering, Data layout optimization
3. **Low Priority**: Statistics update, Cache strategy

{explain_section}

{explain_cost_section}

---

*Report generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""

    # ðŸŽ¯ æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆè¦ç´„ã‚’æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã«è¿½åŠ 
    optimization_points_summary = load_optimization_points_summary()
    if optimization_points_summary:
        report += "\n" + optimization_points_summary
    
    return report

def refine_report_with_llm(raw_report: str, query_id: str) -> str:
    """
    LLMã‚’ä½¿ã£ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’æŽ¨æ•²ã—ã€èª­ã¿ã‚„ã™ã„æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    
    Args:
        raw_report: åˆæœŸç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ
        query_id: ã‚¯ã‚¨ãƒªID
        
    Returns:
        str: LLMã§æŽ¨æ•²ã•ã‚ŒãŸèª­ã¿ã‚„ã™ã„ãƒ¬ãƒãƒ¼ãƒˆ
    """
    
    print("ðŸ¤– Executing LLM-based report refinement...")
    
    # ðŸš¨ ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾ç­–: ãƒ¬ãƒãƒ¼ãƒˆã‚µã‚¤ã‚ºåˆ¶é™
    MAX_REPORT_SIZE = 50000  # 50KBåˆ¶é™
    original_size = len(raw_report)
    
    if original_size > MAX_REPORT_SIZE:
        print(f"âš ï¸ Report size too large: {original_size:,} characters â†’ truncated to {MAX_REPORT_SIZE:,} characters")
        # é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å„ªå…ˆçš„ã«ä¿æŒ
        truncated_report = raw_report[:MAX_REPORT_SIZE]
        truncated_report += f"\n\nâš ï¸ ãƒ¬ãƒãƒ¼ãƒˆãŒå¤§ãã™ãŽã‚‹ãŸã‚ã€{MAX_REPORT_SIZE:,} æ–‡å­—ã«åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸï¼ˆå…ƒã‚µã‚¤ã‚º: {original_size:,} æ–‡å­—ï¼‰"
        raw_report = truncated_report
    else:
        print(f"ðŸ“Š Report size: {original_size:,} characters (executing refinement)")
    
    # è¨€èªžã«å¿œã˜ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ‡ã‚Šæ›¿ãˆ
    if OUTPUT_LANGUAGE == 'ja':
        refinement_prompt = f"""
æŠ€è¡“æ–‡æ›¸ã®ç·¨é›†è€…ã¨ã—ã¦ã€Databricks SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åˆ†æžãƒ¬ãƒãƒ¼ãƒˆã‚’ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦æŽ¨æ•²ã—ã¦ãã ã•ã„ã€‚

ã€çµ¶å¯¾ã«å®ˆã‚‹ã¹ãè¦‹å‡ºã—æ§‹é€ ã€‘
```
# ðŸ“Š SQLæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ

## ðŸ” 1. åˆ†æžã‚µãƒžãƒªãƒ¼

### çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åˆ†æžè¡¨
ä¸»è¦èª²é¡Œã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æŒ‡æ¨™ã‚’ä»¥ä¸‹ã®çµ±åˆè¡¨å½¢å¼ã§ã¾ã¨ã‚ã¦ãã ã•ã„ï¼š

ðŸ” åˆ†æžã‚µãƒžãƒªãƒ¼
ã‚¯ã‚¨ãƒªå®Ÿè¡Œæ™‚é–“ã¯[X.X]ç§’ã¨[è©•ä¾¡]ã§ã™ãŒã€ä»¥ä¸‹ã®æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆãŒç‰¹å®šã•ã‚Œã¾ã—ãŸï¼š

| é …ç›® | ç¾åœ¨ã®çŠ¶æ³ | è©•ä¾¡ | å„ªå…ˆåº¦ |
|------|-----------|------|--------|
| å®Ÿè¡Œæ™‚é–“ | [X.X]ç§’ | âœ… è‰¯å¥½ / âš ï¸ æ”¹å–„å¿…è¦ | - |
| ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Šé‡ | [X.XX]GB | âœ… è‰¯å¥½ / âš ï¸ å¤§å®¹é‡ | - |
| Photonæœ‰åŠ¹åŒ– | ã¯ã„/ã„ã„ãˆ | âœ… è‰¯å¥½ / âŒ æœªæœ‰åŠ¹ | - |
| ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ | [N]å›ž | âœ… è‰¯å¥½ / âš ï¸ å¤šã„ | ðŸš¨ é«˜ / âš ï¸ ä¸­ |
| ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ | ãªã—/ã‚ã‚Š | âœ… è‰¯å¥½ / âŒ å•é¡Œ | ðŸš¨ é«˜ / - |
| ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹çŽ‡ | [X.X]% | âœ… è‰¯å¥½ / âš ï¸ ä½ŽåŠ¹çŽ‡ | âš ï¸ ä¸­ |
| ãƒ•ã‚£ãƒ«ã‚¿åŠ¹çŽ‡ | [X.X]% | âœ… è‰¯å¥½ / âš ï¸ ä½ŽåŠ¹çŽ‡ | âš ï¸ ä¸­ |
| ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ | AQEå¯¾å¿œæ¸ˆ / æœªæ¤œå‡º | âœ… å¯¾å¿œæ¸ˆ / âœ… è‰¯å¥½ | - |

## ðŸ“Š 2. TOP10æ™‚é–“æ¶ˆè²»ãƒ—ãƒ­ã‚»ã‚¹åˆ†æž

### â±ï¸ å®Ÿè¡Œæ™‚é–“ãƒ©ãƒ³ã‚­ãƒ³ã‚°

## ðŸ—‚ï¸ 3. Liquid Clusteringåˆ†æžçµæžœ

### ðŸ“‹ æŽ¨å¥¨ãƒ†ãƒ¼ãƒ–ãƒ«åˆ†æž

## ðŸš€ 4. æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒª

### ðŸŽ¯ æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹è©³ç´°
æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œã•ã‚ŒãŸè©¦è¡Œã¨ãã®é¸æŠžç†ç”±ã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ï¼š

**ðŸ“Š æœ€é©åŒ–è©¦è¡Œå±¥æ­´:**
- è©¦è¡Œå›žæ•°: [total_attempts]å›žå®Ÿè¡Œ
- æœ€çµ‚é¸æŠž: è©¦è¡Œ[selected_attempt_num]ç•ªãŒæœ€é©è§£ã¨ã—ã¦é¸æŠž
- é¸æŠžç†ç”±: [selection_reason]

**ðŸ† é¸æŠžã•ã‚ŒãŸæœ€é©åŒ–ã®åŠ¹æžœ:**
- ã‚³ã‚¹ãƒˆå‰Šæ¸›çŽ‡: [cost_improvement]% (EXPLAIN COSTæ¯”è¼ƒ)
- ãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡æ”¹å–„: [memory_improvement]% (çµ±è¨ˆæ¯”è¼ƒ)

### ðŸ’¡ å…·ä½“çš„ãªæœ€é©åŒ–å†…å®¹ã¨ã‚³ã‚¹ãƒˆåŠ¹æžœ
æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒªã®å‰ã«ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’å¿…ãšå«ã‚ã¦ãã ã•ã„ï¼š

**ðŸŽ¯ é©ç”¨ã•ã‚ŒãŸæœ€é©åŒ–æ‰‹æ³•:**
ã€é‡è¦ã€‘æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹è©³ç´°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã€Œå…ƒã®ã‚¯ã‚¨ãƒªï¼ˆæœ€é©åŒ–ã«ã‚ˆã‚Šæ”¹å–„ã•ã‚Œãªã‹ã£ãŸãŸã‚ï¼‰ã€ãŒé¸æŠžã•ã‚Œã¦ã„ã‚‹å ´åˆï¼š
- âš ï¸ æœ€é©åŒ–æ‰‹æ³•ã¯é©ç”¨ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼ˆå…ƒã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ï¼‰
- ðŸ“„ ä½¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ«: ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸå…ƒã®ã‚¯ã‚¨ãƒª
- ðŸ’¡ ç†ç”±: æœ€é©åŒ–è©¦è¡Œã§æœ‰åŠ¹ãªæ”¹å–„ãŒå¾—ã‚‰ã‚Œãªã‹ã£ãŸãŸã‚

ãã‚Œä»¥å¤–ã®å ´åˆã®ã¿ä»¥ä¸‹ã‚’è¨˜è¼‰ï¼š
- [å®Ÿéš›ã®ã‚¯ã‚¨ãƒªæ›¸ãæ›ãˆå†…å®¹ã‚’å…·ä½“çš„ã«è¦ç´„]
- ä¾‹: "JOINé †åºã®æœ€é©åŒ–ï¼ˆå°ãƒ†ãƒ¼ãƒ–ãƒ«å„ªå…ˆï¼‰", "ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã®æ—©æœŸé©ç”¨", "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ’ãƒ³ãƒˆã®è¿½åŠ "
- âŒ å®Ÿæ–½ã•ã‚Œã¦ã„ãªã„æ‰‹æ³•ã¯è¨˜è¼‰ã—ãªã„ï¼ˆä¾‹: ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„å ´åˆã¯REPARTITIONé©ç”¨ã‚’è¨˜è¼‰ã—ãªã„ï¼‰
- âŒ "Liquid Clustering implementation"ç­‰ã®æœªå®Ÿæ–½ã®å¤‰æ›´ã¯è¨˜è¼‰ã—ãªã„

**ðŸ’° EXPLAIN COSTãƒ™ãƒ¼ã‚¹ã®åŠ¹æžœåˆ†æž:**
ã€é‡è¦ã€‘å…ƒã®ã‚¯ã‚¨ãƒªãŒé¸æŠžã•ã‚Œã¦ã„ã‚‹å ´åˆï¼š
- âš ï¸ æœ€é©åŒ–ã«ã‚ˆã‚‹æ”¹å–„ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ
- ðŸ“Š å…ƒã®ã‚¯ã‚¨ãƒªã‚’ãã®ã¾ã¾ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æŽ¨å¥¨

ãã‚Œä»¥å¤–ã®å ´åˆã®ã¿ä»¥ä¸‹ã‚’è¨˜è¼‰ï¼š
- ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚³ã‚¹ãƒˆå‰Šæ¸›çŽ‡: [cost_ratio]å€ (EXPLAIN COSTæ¯”è¼ƒçµæžœ)
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›çŽ‡: [memory_ratio]å€ (çµ±è¨ˆæƒ…å ±ãƒ™ãƒ¼ã‚¹æ¯”è¼ƒ)
- æŽ¨å®šãƒ‡ãƒ¼ã‚¿å‡¦ç†åŠ¹çŽ‡: [processing_efficiency]% (ã‚¹ã‚­ãƒ£ãƒ³ãƒ»JOINåŠ¹çŽ‡æ”¹å–„)
```

ã€ðŸš¨ REPARTITIONã«é–¢ã™ã‚‹é‡è¦ãªä¿®æ­£æŒ‡ç¤ºã€‘
- **ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„å ´åˆ**: ã€ŒREPARTITIONã®é©ç”¨ã€ã‚’æŽ¨å¥¨æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«å«ã‚ãªã„
- **å®Ÿéš›ã«é©ç”¨ã•ã‚Œã¦ã„ãªã„æœ€é©åŒ–æ‰‹æ³•**: ã€Œç·Šæ€¥å¯¾å¿œã€ã‚„ã€ŒæŽ¨å¥¨æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€ã«è¨˜è¼‰ã—ãªã„
- **äº‹å®Ÿãƒ™ãƒ¼ã‚¹ã®è¨˜è¼‰**: å®Ÿéš›ã«æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã¨é©ç”¨ã•ã‚ŒãŸå¯¾ç­–ã®ã¿ã‚’è¨˜è¼‰

ã€ðŸ’° ã‚³ã‚¹ãƒˆåŠ¹æžœåˆ†æžã§ã®å¿…é ˆä½¿ç”¨ãƒ‡ãƒ¼ã‚¿ã€‘
- **performance_comparisonçµæžœã‚’å¿…ãšä½¿ç”¨**: cost_ratioã€memory_ratioç­‰ã®å®Ÿéš›ã®æ¯”è¼ƒå€¤
- **å®Ÿè¡Œæ™‚é–“äºˆæ¸¬ã¯ä½¿ç”¨ç¦æ­¢**: ä¸æ­£ç¢ºãªãŸã‚è¨˜è¼‰ã—ãªã„
- **EXPLAIN COSTãƒ™ãƒ¼ã‚¹ã®æ•°å€¤ã®ã¿**: æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ä¸­ã®å®Ÿéš›ã®è¨ˆç®—çµæžœã‚’ä½¿ç”¨

ã€åŽ³æ ¼ãªç¦æ­¢äº‹é …ã€‘
- TOP10ã‚’çµ¶å¯¾ã«TOP5ã«å¤‰æ›´ã—ãªã„
- "=========="ç­‰ã®åŒºåˆ‡ã‚Šæ–‡å­—ã‚’å‰Šé™¤ï¼ˆãŸã ã—çµµæ–‡å­—ã«ã‚ˆã‚‹è¦–è¦šçš„è¡¨ç¤ºã¯ä¿æŒï¼‰
- ç•ªå·ä»˜ããƒªã‚¹ãƒˆã§åŒã˜ç•ªå·ã‚’é‡è¤‡ã•ã›ãªã„
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤ã‚„æŠ€è¡“æƒ…å ±ã‚’å‰Šé™¤ã—ãªã„
- å®Ÿæ–½ã•ã‚Œã¦ã„ãªã„æœ€é©åŒ–æ‰‹æ³•ã‚’ã€Œå®Ÿæ–½æ¸ˆã¿ã€ã¨ã—ã¦è¨˜è¼‰ã—ãªã„
- åŒã˜ã‚³ã‚¹ãƒˆæ¯”ã‚„åŠ¹æžœæ•°å€¤ã‚’è¤‡æ•°å€‹æ‰€ã§é‡è¤‡è¨˜è¼‰ã—ãªã„ï¼ˆæœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹è©³ç´°ã§ä¸€åº¦è¨˜è¼‰ã™ã‚Œã°ååˆ†ï¼‰

ã€ðŸš¨ é‡è¦ãªæƒ…å ±ä¿æŒã®å¿…é ˆè¦ä»¶ã€‘
- **ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±**: å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã€Œç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: XXã€æƒ…å ±ã¯å¿…ãšä¿æŒ
- **ãƒ•ã‚£ãƒ«ã‚¿çŽ‡æƒ…å ±**: ã€Œãƒ•ã‚£ãƒ«ã‚¿çŽ‡: X.X% (èª­ã¿è¾¼ã¿: XX.XXGB, ãƒ—ãƒ«ãƒ¼ãƒ³: XX.XXGB)ã€å½¢å¼ã®æƒ…å ±ã¯å¿…ãšä¿æŒ
- **ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—**: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æžã®ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ï¼ˆå…¨ä½“ã®â—‹â—‹%ï¼‰ã¯æ­£ç¢ºãªå€¤ã‚’ä¿æŒ
- **æŽ¨å¥¨vsç¾åœ¨ã®æ¯”è¼ƒ**: æŽ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã¨ç¾åœ¨ã®ã‚­ãƒ¼ã®æ¯”è¼ƒæƒ…å ±ã¯å‰Šé™¤ç¦æ­¢
- **æ•°å€¤ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: å®Ÿè¡Œæ™‚é–“ã€ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é‡ã€ã‚¹ãƒ”ãƒ«é‡ç­‰ã®æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã¯å‰Šé™¤ç¦æ­¢
- **SQLå®Ÿè£…ä¾‹**: ALTER TABLEæ–‡ã‚„CLUSTER BYæ§‹æ–‡ã®å…·ä½“ä¾‹ã¯å‰Šé™¤ç¦æ­¢

ã€å‡¦ç†è¦ä»¶ã€‘
1. ä¸Šè¨˜ã®è¦‹å‡ºã—æ§‹é€ ã‚’å¿…ãšä½¿ç”¨
2. ä¸»è¦èª²é¡Œã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æŒ‡æ¨™ã‚’çµ±åˆè¡¨å½¢å¼ã§ã¾ã¨ã‚ã‚‹
3. å®Ÿéš›ã«é©ç”¨ã•ã‚ŒãŸæœ€é©åŒ–æ‰‹æ³•ã®ã¿ã‚’è¨˜è¼‰ï¼ˆå®Ÿæ–½ã•ã‚Œã¦ã„ãªã„æ‰‹æ³•ã¯è¨˜è¼‰ã—ãªã„ï¼‰
4. å…·ä½“çš„ãªã‚³ã‚¹ãƒˆåŠ¹æžœã‚’æ•°å€¤ã§ç¤ºã™
5. æŠ€è¡“æƒ…å ±ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å®Œå…¨ä¿æŒï¼ˆç‰¹ã«ä¸Šè¨˜ã®é‡è¦æƒ…å ±ï¼‰
6. TOP10è¡¨ç¤ºã‚’ç¶­æŒ
7. çµµæ–‡å­—ã«ã‚ˆã‚‹è¦–è¦šçš„è¡¨ç¤ºã‚’ä¿æŒï¼ˆðŸš¨ CRITICALã€âš ï¸ HIGHã€âœ…è‰¯å¥½ç­‰ï¼‰
8. ä¸è¦ãªåŒºåˆ‡ã‚Šæ–‡å­—ï¼ˆ========ç­‰ï¼‰ã®ã¿å‰Šé™¤
9. ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã¨ãƒ•ã‚£ãƒ«ã‚¿çŽ‡æƒ…å ±ã¯çµ¶å¯¾ã«ä¿æŒ

ã€ç¾åœ¨ã®ãƒ¬ãƒãƒ¼ãƒˆã€‘
```
{raw_report}
```

ä¸Šè¨˜ã®è¦‹å‡ºã—æ§‹é€ ã«å¾“ã£ã¦æŽ¨æ•²ã—ã€æŠ€è¡“æƒ…å ±ã‚’å®Œå…¨ã«ä¿æŒã—ãŸãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
    else:
        refinement_prompt = f"""
As a technical document editor, please refine the following Databricks SQL performance analysis report according to these rules.

ã€Required Heading Structureã€‘
```
# ðŸ“Š SQL Optimization Report

## ðŸ” 1. Analysis Summary

### Integrated Performance Analysis Table
Merge major issues and performance indicators into the following integrated table format:

ðŸ” Analysis Summary
Query execution time is [X.X] seconds, which is [evaluation], but the following optimization points were identified:

| Item | Current Status | Evaluation | Priority |
|------|---------------|------------|----------|
| Execution Time | [X.X]s | âœ… Good / âš ï¸ Needs Improvement | - |
| Data Read Volume | [X.XX]GB | âœ… Good / âš ï¸ Large Volume | - |
| Photon Enabled | Yes/No | âœ… Good / âŒ Not Enabled | - |
| Shuffle Operations | [N] times | âœ… Good / âš ï¸ High | ðŸš¨ High / âš ï¸ Medium |
| Spill Occurrence | None/Present | âœ… Good / âŒ Issues | ðŸš¨ High / - |
| Cache Efficiency | [X.X]% | âœ… Good / âš ï¸ Low Efficiency | âš ï¸ Medium |
| Filter Efficiency | [X.X]% | âœ… Good / âš ï¸ Low Efficiency | âš ï¸ Medium |
| Data Skew | AQE Handled / Not Detected | âœ… Handled / âœ… Good | - |

## ðŸ“Š 2. TOP10 Time-Consuming Processes Analysis

### â±ï¸ Execution Time Ranking

## ðŸ—‚ï¸ 3. Liquid Clustering Analysis Results

### ðŸ“‹ Recommended Table Analysis

## ðŸš€ 4. Optimized SQL Query

### ðŸŽ¯ Optimization Process Details
The following shows the trials executed during the optimization process and the selection rationale:

**ðŸ“Š Optimization Trial History:**
- Trial count: [total_attempts] attempts executed
- Final selection: Trial [selected_attempt_num] was chosen as the optimal solution
- Selection reason: [selection_reason]

**ðŸ† Selected Optimization Effects:**
- Cost reduction rate: [cost_improvement]% (EXPLAIN COST comparison)
- Memory efficiency improvement: [memory_improvement]% (statistics comparison)

### ðŸ’¡ Specific Optimization Details and Cost Effects
Before the optimized SQL query, must include the following information:

**ðŸŽ¯ Applied Optimization Techniques:**
ã€Importantã€‘If "Original Query (no improvement achieved through optimization)" is selected in the Optimization Process Details section:
- âš ï¸ No optimization techniques were applied (using original query)
- ðŸ“„ Used file: Original query extracted from profiler data
- ðŸ’¡ Reason: Optimization trials did not yield effective improvements

Only for other cases, list the following:
- [Summarize actual query rewriting content specifically]
- Examples: "JOIN order optimization (small table first)", "Early filter condition application", "Index hint addition"
- âŒ Do not list techniques that were not implemented (e.g., do not mention REPARTITION application if no spill was detected)
- âŒ Do not mention unimplemented changes like "Liquid Clustering implementation"

**ðŸ’° EXPLAIN COST-Based Effect Analysis:**
ã€Importantã€‘If original query is selected:
- âš ï¸ No improvement was achieved through optimization
- ðŸ“Š Recommend using the original query as-is

Only for other cases, list the following:
- Query execution cost reduction: [cost_ratio]x (EXPLAIN COST comparison result)
- Memory usage reduction: [memory_ratio]x (statistics-based comparison)
- Estimated data processing efficiency: [processing_efficiency]% (scan/JOIN efficiency improvement)
```

ã€ðŸš¨ Critical REPARTITION Correction Instructionsã€‘
- **When no spill is detected**: Do not include "REPARTITION application" in recommended improvement actions
- **Actually non-applied optimization techniques**: Do not list in "Emergency Response" or "Recommended Improvement Actions"
- **Fact-based reporting**: Only list actually detected problems and applied countermeasures

ã€ðŸ’° Required Data for Cost Effect Analysisã€‘
- **Must use performance_comparison results**: cost_ratio, memory_ratio and other actual comparison values
- **Execution time prediction is prohibited**: Do not include due to inaccuracy
- **EXPLAIN COST-based numbers only**: Use actual calculation results from optimization process

ã€Strict Prohibitionsã€‘
- Never change TOP10 to TOP5
- Remove separator characters like "==========" (but keep emoji visual displays)
- Do not duplicate numbered list items
- Do not delete metric values or technical information
- Do not report non-implemented optimization techniques as "implemented"
- Do not duplicate the same cost ratios or effect numbers in multiple sections (once in optimization process details is sufficient)

ã€ðŸš¨ Critical Information Preservation Requirementsã€‘
- **Current clustering key information**: Must preserve each table's "Current clustering key: XX" information
- **Filter rate information**: Must preserve "Filter rate: X.X% (read: XX.XXGB, pruned: XX.XXGB)" format
- **Percentage calculations**: Preserve accurate percentage values in bottleneck analysis (XX% of total)
- **Recommended vs current comparison**: Do not delete comparison information between recommended and current clustering keys
- **Numerical metrics**: Do not delete execution time, data read volume, spill volume, etc.
- **SQL implementation examples**: Do not delete specific examples of ALTER TABLE and CLUSTER BY syntax

ã€Processing Requirementsã€‘
1. Must use the above heading structure
2. Merge major issues and performance indicators into integrated table format
3. List only actually applied optimization techniques (do not list non-implemented techniques)
4. Show specific cost effects with numerical values
5. Completely preserve technical information and metrics (especially the important information above)
6. Maintain TOP10 display
7. Keep emoji visual displays (ðŸš¨ CRITICAL, âš ï¸ HIGH, âœ… Good, etc.)
8. Remove only unnecessary separator characters (======== etc.)
9. Absolutely preserve current clustering key information and filter rate information

ã€Current Reportã€‘
```
{raw_report}
```

Please refine according to the above heading structure and output a report that completely preserves technical information.
"""
    
    try:
        provider = LLM_CONFIG.get("provider", "databricks")
        
        if provider == "databricks":
            refined_report = _call_databricks_llm(refinement_prompt)
        elif provider == "openai":
            refined_report = _call_openai_llm(refinement_prompt)
        elif provider == "azure_openai":
            refined_report = _call_azure_openai_llm(refinement_prompt)
        elif provider == "anthropic":
            refined_report = _call_anthropic_llm(refinement_prompt)
        else:
            raise ValueError(f"Unsupported LLM provider: {provider}")
        
        # ðŸš¨ LLMã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ¤œå‡ºï¼ˆç²¾å¯†åŒ–ï¼‰
        if isinstance(refined_report, str):
            # ã‚ˆã‚Šç²¾å¯†ãªã‚¨ãƒ©ãƒ¼æ¤œå‡ºï¼ˆãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã®çµµæ–‡å­—ã¨åŒºåˆ¥ï¼‰
            actual_error_indicators = [
                "APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰",
                "Input is too long for requested model",
                "Bad Request",
                "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼:",
                "APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼:",
                'ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {"error_code":',
                "âŒ APIã‚¨ãƒ©ãƒ¼:",
                "âš ï¸ APIã‚¨ãƒ©ãƒ¼:"
            ]
            
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®é–‹å§‹éƒ¨åˆ†ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚ŠåŽ³å¯†ï¼‰
            is_error_response = any(
                refined_report.strip().startswith(indicator) or 
                f"\n{indicator}" in refined_report[:500]  # å…ˆé ­500æ–‡å­—ä»¥å†…ã§ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                for indicator in actual_error_indicators
            )
            
            if is_error_response:
                print(f"âŒ Error detected in LLM report refinement: {refined_report[:200]}...")
                print("ðŸ“„ Returning original report")
                return raw_report
        
        # thinking_enabledå¯¾å¿œ
        if isinstance(refined_report, list):
            refined_report = format_thinking_response(refined_report)
        
        # signatureæƒ…å ±ã®é™¤åŽ»
        import re
        signature_pattern = r"'signature':\s*'[A-Za-z0-9+/=]{100,}'"
        refined_report = re.sub(signature_pattern, "'signature': '[REMOVED]'", refined_report)
        
        print(f"âœ… LLM-based report refinement completed (Query ID: {query_id})")
        return refined_report
        
    except Exception as e:
        print(f"âš ï¸ Error occurred during LLM-based report refinement: {str(e)}")
        print("ðŸ“„ Returning original report")
        return raw_report

def validate_and_fix_sql_syntax(sql_query: str) -> str:
    """
    SQLæ§‹æ–‡ã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯ã¨ä¿®æ­£ã‚’è¡Œã†ï¼ˆæ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ï¼‰
    
    ä¸»è¦ãƒã‚§ãƒƒã‚¯é …ç›®ï¼š
    1. BROADCASTãƒ’ãƒ³ãƒˆã®é…ç½®ä½ç½®æ¤œè¨¼
    2. å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆSELECTã€FROMã€WHEREç­‰ã®åŸºæœ¬æ§‹æ–‡ï¼‰
    3. åŸºæœ¬çš„ãªæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£
    4. ã‚³ãƒ¡ãƒ³ãƒˆã‚„ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã®é™¤åŽ»
    
    Args:
        sql_query: ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®SQLã‚¯ã‚¨ãƒª
        
    Returns:
        str: ä¿®æ­£ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒª
    """
    import re
    
    if not sql_query or not sql_query.strip():
        return ""
    
    # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    sql_query = sql_query.strip()
    
    # 1. BROADCASTãƒ’ãƒ³ãƒˆã®é…ç½®ä½ç½®ãƒã‚§ãƒƒã‚¯
    sql_query = fix_broadcast_hint_placement(sql_query)
    
    # 2. ä¸å®Œå…¨ãªSQLæ§‹æ–‡ã®æ¤œå‡ºã¨ä¿®æ­£
    sql_query = fix_incomplete_sql_syntax(sql_query)
    
    # 3. ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚„çœç•¥è¨˜å·ã®é™¤åŽ»
    sql_query = remove_sql_placeholders(sql_query)
    
    # 4. åŸºæœ¬çš„ãªæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£
    sql_query = fix_basic_syntax_errors(sql_query)
    
    return sql_query

def fix_broadcast_hint_placement(sql_query: str) -> str:
    """
    BROADCASTãƒ’ãƒ³ãƒˆã®é…ç½®ä½ç½®ã‚’ä¿®æ­£ï¼ˆã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨é…ç½®ã‚’ç¦æ­¢ï¼‰
    
    ä¿®æ­£å†…å®¹ï¼š
    - ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã®BROADCASTãƒ’ãƒ³ãƒˆã‚’ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã«ç§»å‹•
    - FROMå¥ã€JOINå¥ã€WHEREå¥å†…ã®ãƒ’ãƒ³ãƒˆã‚’å‰Šé™¤
    - è¤‡æ•°ã®BROADCASTãƒ’ãƒ³ãƒˆã‚’çµ±åˆ
    - DISTINCTå¥ã®ä¿æŒã‚’ç¢ºä¿
    """
    import re
    
    # ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã®BROADCASTãƒ’ãƒ³ãƒˆã‚’æ¤œå‡ºã¨å‰Šé™¤
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: LEFT JOIN (SELECT /*+ BROADCAST(...) */ ... ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    subquery_broadcast_pattern = r'JOIN\s*\(\s*SELECT\s*/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
    sql_query = re.sub(subquery_broadcast_pattern, 'JOIN (\n  SELECT', sql_query, flags=re.IGNORECASE)
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: WITHå¥ã‚„ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã®BROADCASTãƒ’ãƒ³ãƒˆ
    cte_broadcast_pattern = r'(WITH\s+\w+\s+AS\s*\(\s*SELECT\s*)/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
    sql_query = re.sub(cte_broadcast_pattern, r'\1', sql_query, flags=re.IGNORECASE)
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: FROMå¥å†…ã®BROADCASTãƒ’ãƒ³ãƒˆ
    from_broadcast_pattern = r'FROM\s+\w+\s*/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
    sql_query = re.sub(from_broadcast_pattern, 'FROM', sql_query, flags=re.IGNORECASE)
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³4: WHEREå¥å†…ã®BROADCASTãƒ’ãƒ³ãƒˆ
    where_broadcast_pattern = r'WHERE\s*/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
    sql_query = re.sub(where_broadcast_pattern, 'WHERE', sql_query, flags=re.IGNORECASE)
    
    # DISTINCTå¥ã®å­˜åœ¨ç¢ºèªï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ï¼‰
    distinct_pattern = r'^\s*SELECT\s*(/\*\+[^*]*\*/)?\s*DISTINCT\b'
    has_distinct = bool(re.search(distinct_pattern, sql_query, re.IGNORECASE))
    
    # BROADCASTãƒ’ãƒ³ãƒˆãŒãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTç›´å¾Œã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    main_select_pattern = r'^\s*SELECT\s*(/\*\+[^*]*\*/)?\s*(DISTINCT\s*)?'
    if not re.search(main_select_pattern, sql_query, re.IGNORECASE):
        # ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTç›´å¾Œã«BROADCASTãƒ’ãƒ³ãƒˆãŒãªã„å ´åˆã®å‡¦ç†
        # å‰Šé™¤ã•ã‚ŒãŸBROADCASTãƒ’ãƒ³ãƒˆã‚’å¾©å…ƒã—ã¦ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã«é…ç½®
        broadcast_tables = extract_broadcast_tables_from_sql(sql_query)
        if broadcast_tables:
            broadcast_hint = f"/*+ BROADCAST({', '.join(broadcast_tables)}) */"
            if has_distinct:
                # DISTINCTå¥ãŒã‚ã‚‹å ´åˆï¼šSELECT /*+ BROADCAST(...) */ DISTINCT ã®å½¢å¼ã«ã™ã‚‹
                sql_query = re.sub(r'^\s*SELECT\s*', f'SELECT {broadcast_hint} ', sql_query, flags=re.IGNORECASE)
            else:
                # DISTINCTå¥ãŒãªã„å ´åˆï¼šå¾“æ¥ã®å½¢å¼
                sql_query = re.sub(r'^\s*SELECT\s*', f'SELECT {broadcast_hint}\n  ', sql_query, flags=re.IGNORECASE)
    else:
        # æ—¢ã«ãƒ’ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆã€DISTINCTå¥ãŒæ­£ã—ã„ä½ç½®ã«ã‚ã‚‹ã‹ç¢ºèª
        # é–“é•ã£ãŸé †åºï¼ˆSELECT DISTINCT /*+ BROADCAST(...) */ ï¼‰ã‚’ä¿®æ­£
        wrong_order_pattern = r'^\s*SELECT\s*DISTINCT\s*(/\*\+[^*]*\*/)'
        if re.search(wrong_order_pattern, sql_query, re.IGNORECASE):
            # é–“é•ã£ãŸé †åºã‚’ä¿®æ­£ï¼šSELECT DISTINCT /*+ HINT */ â†’ SELECT /*+ HINT */ DISTINCT
            sql_query = re.sub(wrong_order_pattern, lambda m: f'SELECT {m.group(1)} DISTINCT', sql_query, flags=re.IGNORECASE)
    
    return sql_query


def fix_join_broadcast_hint_placement(sql_query: str) -> str:
    """
    JOINå¥å†…ã®BROADCASTãƒ’ãƒ³ãƒˆé…ç½®ã‚¨ãƒ©ãƒ¼ã‚’å¼·åˆ¶ä¿®æ­£ï¼ˆPARSE_SYNTAX_ERRORå¯¾ç­–ï¼‰
    ãƒ¦ãƒ¼ã‚¶ãƒ¼å ±å‘Šã®ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ï¼š join /*+ BROADCAST(i) */ item i ON ...
    """
    import re
    
    try:
        # JOINå¥å†…ã®BROADCASTãƒ’ãƒ³ãƒˆã‚’æ¤œå‡ºãƒ»æŠ½å‡º
        join_broadcast_pattern = r'JOIN\s+/\*\+\s*BROADCAST\(([^)]+)\)\s*\*/\s*(\w+)'
        join_broadcast_matches = re.findall(join_broadcast_pattern, sql_query, re.IGNORECASE | re.MULTILINE)
        
        if not join_broadcast_matches:
            # JOINå¥å†…ã®BROADCASTãƒ’ãƒ³ãƒˆãŒãªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™
            return sql_query
        
        print(f"ðŸ”§ Detected BROADCAST hints in JOIN clauses: {len(join_broadcast_matches)} instances")
        
        # æŠ½å‡ºã•ã‚ŒãŸBROADCASTå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«å/ã‚¨ã‚¤ãƒªã‚¢ã‚¹åã‚’åŽé›†
        broadcast_tables = []
        for table_name, table_alias in join_broadcast_matches:
            # ã‚«ãƒ³ãƒžåŒºåˆ‡ã‚Šã®å ´åˆã‚‚è€ƒæ…®
            tables = [t.strip() for t in table_name.split(',')]
            broadcast_tables.extend(tables)
            # ã‚¨ã‚¤ãƒªã‚¢ã‚¹åã‚‚è¿½åŠ ï¼ˆé‡è¤‡å‰Šé™¤ã¯å¾Œã§è¡Œã†ï¼‰
            if table_alias.strip():
                broadcast_tables.append(table_alias.strip())
        
        # é‡è¤‡å‰Šé™¤
        broadcast_tables = list(set(broadcast_tables))
        print(f"ðŸ“‹ BROADCAST targets: {', '.join(broadcast_tables)}")
        
        # JOINå¥å†…ã®BROADCASTãƒ’ãƒ³ãƒˆã‚’å‰Šé™¤
        fixed_query = re.sub(
            r'JOIN\s+/\*\+\s*BROADCAST\([^)]+\)\s*\*/\s*',
            'JOIN ',
            sql_query,
            flags=re.IGNORECASE | re.MULTILINE
        )
        
        # ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®æœ€åˆã®SELECTæ–‡ã‚’æ¤œå‡º
        select_pattern = r'^(\s*SELECT)\s+'
        select_match = re.search(select_pattern, fixed_query, re.IGNORECASE | re.MULTILINE)
        
        if select_match:
            # æ—¢å­˜ã®ãƒ’ãƒ³ãƒˆå¥ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            existing_hint_pattern = r'^(\s*SELECT)\s+(/\*\+[^*]*\*/)\s+'
            existing_hint_match = re.search(existing_hint_pattern, fixed_query, re.IGNORECASE | re.MULTILINE)
            
            if existing_hint_match:
                # æ—¢å­˜ã®ãƒ’ãƒ³ãƒˆå¥ã«BROADCASTã‚’è¿½åŠ 
                existing_hint = existing_hint_match.group(2)
                
                # æ—¢å­˜ã®BROADCASTæŒ‡å®šã‚’ç¢ºèª
                existing_broadcast_pattern = r'BROADCAST\(([^)]+)\)'
                existing_broadcast_match = re.search(existing_broadcast_pattern, existing_hint, re.IGNORECASE)
                
                if existing_broadcast_match:
                    # æ—¢å­˜ã®BROADCASTæŒ‡å®šã«è¿½åŠ 
                    existing_broadcast_tables = [t.strip() for t in existing_broadcast_match.group(1).split(',')]
                    all_broadcast_tables = list(set(existing_broadcast_tables + broadcast_tables))
                    new_broadcast = f"BROADCAST({', '.join(all_broadcast_tables)})"
                    new_hint = re.sub(
                        r'BROADCAST\([^)]+\)',
                        new_broadcast,
                        existing_hint,
                        flags=re.IGNORECASE
                    )
                else:
                    # æ—¢å­˜ã®ãƒ’ãƒ³ãƒˆå¥ã«BROADCASTã‚’è¿½åŠ 
                    broadcast_hint = f"BROADCAST({', '.join(broadcast_tables)})"
                    # ãƒ’ãƒ³ãƒˆå¥ã®æœ«å°¾ã® */ ã®å‰ã«è¿½åŠ 
                    new_hint = existing_hint.replace('*/', f', {broadcast_hint} */')
                
                # ãƒ’ãƒ³ãƒˆå¥ã‚’ç½®æ›
                fixed_query = re.sub(
                    r'^(\s*SELECT)\s+(/\*\+[^*]*\*/)\s+',
                    f'{select_match.group(1)} {new_hint} ',
                    fixed_query,
                    flags=re.IGNORECASE | re.MULTILINE
                )
            else:
                # æ–°ã—ããƒ’ãƒ³ãƒˆå¥ã‚’è¿½åŠ 
                broadcast_hint = f"/*+ BROADCAST({', '.join(broadcast_tables)}) */"
                fixed_query = re.sub(
                    r'^(\s*SELECT)\s+',
                    f'{select_match.group(1)} {broadcast_hint} ',
                    fixed_query,
                    flags=re.IGNORECASE | re.MULTILINE
                )
            
            print(f"âœ… Completed moving BROADCAST hints to correct positions")
            return fixed_query
        else:
            print("âš ï¸ Main query SELECT statement not found, returning original query")
            return sql_query
            
    except Exception as e:
        print(f"âš ï¸ Error in JOIN BROADCAST placement correction: {str(e)}")
        print("ðŸ”„ Returning original query")
        return sql_query


def enhance_error_correction_with_syntax_validation(corrected_query: str, original_query: str, error_info: str) -> str:
    """
    ã‚¨ãƒ©ãƒ¼ä¿®æ­£å¾Œã®ã‚¯ã‚¨ãƒªã‚’æ¤œè¨¼ã—ã€PARSE_SYNTAX_ERRORãŒè§£æ±ºã•ã‚Œã¦ã„ãªã„å ´åˆã¯å…ƒã‚¯ã‚¨ãƒªã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    """
    
    try:
        # ä¿®æ­£ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®å¾Œå‡¦ç†
        print("ðŸ”§ Executing post-processing of corrected query...")
        
        # JOINå¥å†…ã®BROADCASTé…ç½®ã®å¼·åˆ¶ä¿®æ­£
        final_query = fix_join_broadcast_hint_placement(corrected_query)
        
        # åŸºæœ¬çš„ãªæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
        if "/*+" in error_info and "PARSE_SYNTAX_ERROR" in error_info:
            # PARSE_SYNTAX_ERRORã®å ´åˆã¯ç‰¹ã«åŽ³æ ¼ã«ãƒã‚§ãƒƒã‚¯
            
            # JOINå¥å†…ã®BROADCASTãƒ’ãƒ³ãƒˆãŒæ®‹ã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            import re
            join_broadcast_pattern = r'JOIN\s+/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
            if re.search(join_broadcast_pattern, final_query, re.IGNORECASE | re.MULTILINE):
                print("ðŸš¨ BROADCAST hints still remain in JOIN clauses after correction, using original query")
                return f"""-- âŒ PARSE_SYNTAX_ERRORä¿®æ­£å¤±æ•—ã®ãŸã‚ã€å…ƒã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨
-- ðŸ“‹ ã‚¨ãƒ©ãƒ¼å†…å®¹: {error_info[:200]}
-- ðŸ’¡ æŽ¨å¥¨: æ‰‹å‹•ã§BROADCASTãƒ’ãƒ³ãƒˆã®é…ç½®ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„

{original_query}"""
        
        print("âœ… Corrected query validation completed")
        return final_query
        
    except Exception as e:
        print(f"âš ï¸ Error in post-correction validation: {str(e)}")
        print("ðŸ”„ Using original query for safety")
        return f"""-- âŒ ã‚¨ãƒ©ãƒ¼ä¿®æ­£æ¤œè¨¼ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã€å…ƒã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨
-- ðŸ“‹ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}
-- ðŸ“‹ å…ƒã®ã‚¨ãƒ©ãƒ¼: {error_info[:200]}

{original_query}"""


def fallback_performance_evaluation(original_explain: str, optimized_explain: str) -> Dict[str, Any]:
    """
    EXPLAIN COSTå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹è©•ä¾¡
    EXPLAINçµæžœã®ãƒ—ãƒ©ãƒ³è¤‡é›‘åº¦ã¨Photonåˆ©ç”¨åº¦ã§ç°¡æ˜“æ¯”è¼ƒ
    """
    
    try:
        import re
        
        # ãƒ—ãƒ©ãƒ³è¤‡é›‘åº¦ã®è©•ä¾¡
        def analyze_plan_complexity(explain_text):
            metrics = {
                'join_count': 0,
                'scan_count': 0,
                'exchange_count': 0,
                'photon_ops': 0,
                'plan_depth': 0,
                'total_operations': 0
            }
            
            # JOINæ“ä½œã‚«ã‚¦ãƒ³ãƒˆ
            metrics['join_count'] = len(re.findall(r'\bJoin\b|\bBroadcastHashJoin\b|\bSortMergeJoin\b', explain_text, re.IGNORECASE))
            
            # SCANæ“ä½œã‚«ã‚¦ãƒ³ãƒˆ
            metrics['scan_count'] = len(re.findall(r'\bScan\b|\bFileScan\b|\bTableScan\b', explain_text, re.IGNORECASE))
            
            # Exchangeæ“ä½œã‚«ã‚¦ãƒ³ãƒˆï¼ˆShuffleï¼‰
            metrics['exchange_count'] = len(re.findall(r'\bExchange\b|\bShuffle\b', explain_text, re.IGNORECASE))
            
            # Photonæ“ä½œã‚«ã‚¦ãƒ³ãƒˆ
            metrics['photon_ops'] = len(re.findall(r'\bPhoton\w*\b', explain_text, re.IGNORECASE))
            
            # ãƒ—ãƒ©ãƒ³æ·±åº¦ã®æŽ¨å®šï¼ˆã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆæ•°ã®æœ€å¤§å€¤ï¼‰
            lines = explain_text.split('\n')
            max_indent = 0
            for line in lines:
                if line.strip():
                    indent_level = (len(line) - len(line.lstrip(' +'))) // 2
                    max_indent = max(max_indent, indent_level)
            metrics['plan_depth'] = max_indent
            
            # ç·æ“ä½œæ•°
            metrics['total_operations'] = metrics['join_count'] + metrics['scan_count'] + metrics['exchange_count']
            
            return metrics
        
        original_metrics = analyze_plan_complexity(original_explain)
        optimized_metrics = analyze_plan_complexity(optimized_explain)
        
        # æ”¹å–„ãƒã‚¤ãƒ³ãƒˆã®è©•ä¾¡
        improvements = []
        concerns = []
        
        # JOINåŠ¹çŽ‡åŒ–
        if optimized_metrics['join_count'] < original_metrics['join_count']:
            improvements.append(f"JOINåŠ¹çŽ‡åŒ–: {original_metrics['join_count']} â†’ {optimized_metrics['join_count']}æ“ä½œ")
        elif optimized_metrics['join_count'] > original_metrics['join_count']:
            concerns.append(f"JOINæ“ä½œå¢—åŠ : {original_metrics['join_count']} â†’ {optimized_metrics['join_count']}æ“ä½œ")
        
        # Photonæ´»ç”¨åº¦
        if optimized_metrics['photon_ops'] > original_metrics['photon_ops']:
            improvements.append(f"Photonæ´»ç”¨æ‹¡å¤§: {original_metrics['photon_ops']} â†’ {optimized_metrics['photon_ops']}æ“ä½œ")
        elif optimized_metrics['photon_ops'] < original_metrics['photon_ops']:
            concerns.append(f"Photonæ´»ç”¨æ¸›å°‘: {original_metrics['photon_ops']} â†’ {optimized_metrics['photon_ops']}æ“ä½œ")
        
        # Exchange/ShuffleåŠ¹çŽ‡åŒ–
        if optimized_metrics['exchange_count'] < original_metrics['exchange_count']:
            improvements.append(f"Shuffleå‰Šæ¸›: {original_metrics['exchange_count']} â†’ {optimized_metrics['exchange_count']}æ“ä½œ")
        elif optimized_metrics['exchange_count'] > original_metrics['exchange_count']:
            concerns.append(f"Shuffleå¢—åŠ : {original_metrics['exchange_count']} â†’ {optimized_metrics['exchange_count']}æ“ä½œ")
        
        # ãƒ—ãƒ©ãƒ³è¤‡é›‘åº¦
        if optimized_metrics['plan_depth'] < original_metrics['plan_depth']:
            improvements.append(f"ãƒ—ãƒ©ãƒ³ç°¡ç´ åŒ–: æ·±åº¦{original_metrics['plan_depth']} â†’ {optimized_metrics['plan_depth']}")
        elif optimized_metrics['plan_depth'] > original_metrics['plan_depth']:
            concerns.append(f"ãƒ—ãƒ©ãƒ³è¤‡é›‘åŒ–: æ·±åº¦{original_metrics['plan_depth']} â†’ {optimized_metrics['plan_depth']}")
        
        # ç·åˆè©•ä¾¡
        improvement_score = len(improvements)
        concern_score = len(concerns)
        
        if improvement_score > concern_score:
            overall_status = "improvement_likely"
            recommendation = "use_optimized"
            summary = "âœ… å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æžã«ã‚ˆã‚Šãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ”¹å–„ã®å¯èƒ½æ€§ãŒé«˜ã„"
        elif concern_score > improvement_score:
            overall_status = "degradation_possible"
            recommendation = "use_original"
            summary = "âš ï¸ å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æžã«ã‚ˆã‚Šãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ‚ªåŒ–ã®å¯èƒ½æ€§ã‚ã‚Š"
        else:
            overall_status = "neutral"
            recommendation = "use_optimized"
            summary = "âž– å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æžã§ã¯å¤§ããªå¤‰åŒ–ãªã—ï¼ˆæœ€é©åŒ–ã‚¯ã‚¨ãƒªã‚’æŽ¨å¥¨ï¼‰"
        
        return {
            'evaluation_type': 'fallback_plan_analysis',
            'original_metrics': original_metrics,
            'optimized_metrics': optimized_metrics,
            'improvements': improvements,
            'concerns': concerns,
            'overall_status': overall_status,
            'recommendation': recommendation,
            'summary': summary,
            'confidence': 'medium',
            'details': improvements + concerns if improvements or concerns else ["å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã«å¤§ããªå¤‰åŒ–ãªã—"],
            'original_estimated_spill_gb': original_metrics.get('estimated_spill_gb', 0),
            'optimized_estimated_spill_gb': optimized_metrics.get('estimated_spill_gb', 0)
        }
        
    except Exception as e:
        return {
            'evaluation_type': 'fallback_error',
            'error': str(e),
            'overall_status': 'unknown',
            'recommendation': 'use_optimized',
            'summary': f"âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼: {str(e)}ï¼ˆæœ€é©åŒ–ã‚¯ã‚¨ãƒªã‚’æŽ¨å¥¨ï¼‰",
            'confidence': 'low',
            'details': [f"è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}", "ä¿å®ˆçš„ã«æœ€é©åŒ–ã‚¯ã‚¨ãƒªã‚’æŽ¨å¥¨"]
        }


def generate_fallback_performance_section(fallback_evaluation: Dict[str, Any], language: str = 'ja') -> str:
    """
    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹è©•ä¾¡ã®ãƒ¬ãƒãƒ¼ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ
    """
    
    if not fallback_evaluation:
        return ""
    
    if language == 'ja':
        section = f"""

### ðŸ” 5. ç°¡æ˜“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹è©•ä¾¡çµæžœï¼ˆEXPLAIN COSTä»£æ›¿ï¼‰

**ðŸ“Š è©•ä¾¡çµæžœ**: {fallback_evaluation['summary']}

#### ðŸŽ¯ å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æžã«ã‚ˆã‚‹è©•ä¾¡

**ä¿¡é ¼åº¦**: {fallback_evaluation['confidence'].upper()}ï¼ˆEXPLAINçµæžœãƒ™ãƒ¼ã‚¹ï¼‰

**æŽ¨å¥¨**: {'**æœ€é©åŒ–ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨**' if fallback_evaluation['recommendation'] == 'use_optimized' else '**å…ƒã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨**'}

#### ðŸ“‹ æ¤œå‡ºã•ã‚ŒãŸå¤‰åŒ–

"""
        
        if fallback_evaluation.get('details'):
            for detail in fallback_evaluation['details']:
                section += f"- {detail}\n"
        else:
            section += "- å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã«å¤§ããªå¤‰åŒ–ãªã—\n"
        
        if fallback_evaluation.get('original_metrics') and fallback_evaluation.get('optimized_metrics'):
            orig = fallback_evaluation['original_metrics']
            opt = fallback_evaluation['optimized_metrics']
            
            section += f"""

#### ðŸ“Š ãƒ—ãƒ©ãƒ³è¤‡é›‘åº¦æ¯”è¼ƒ

| é …ç›® | å…ƒã®ã‚¯ã‚¨ãƒª | æœ€é©åŒ–ã‚¯ã‚¨ãƒª | å¤‰åŒ– |
|------|------------|-------------|------|
| JOINæ“ä½œæ•° | {orig['join_count']} | {opt['join_count']} | {'âœ…æ”¹å–„' if opt['join_count'] < orig['join_count'] else 'âŒå¢—åŠ ' if opt['join_count'] > orig['join_count'] else 'âž–åŒç­‰'} |
| Photonæ“ä½œæ•° | {orig['photon_ops']} | {opt['photon_ops']} | {'âœ…æ”¹å–„' if opt['photon_ops'] > orig['photon_ops'] else 'âŒæ¸›å°‘' if opt['photon_ops'] < orig['photon_ops'] else 'âž–åŒç­‰'} |
| Shuffleæ“ä½œæ•° | {orig['exchange_count']} | {opt['exchange_count']} | {'âœ…æ”¹å–„' if opt['exchange_count'] < orig['exchange_count'] else 'âŒå¢—åŠ ' if opt['exchange_count'] > orig['exchange_count'] else 'âž–åŒç­‰'} |
| ãƒ—ãƒ©ãƒ³æ·±åº¦ | {orig['plan_depth']} | {opt['plan_depth']} | {'âœ…æ”¹å–„' if opt['plan_depth'] < orig['plan_depth'] else 'âŒå¢—åŠ ' if opt['plan_depth'] > orig['plan_depth'] else 'âž–åŒç­‰'} |"""
            
            # ã‚¹ãƒ”ãƒ«æŽ¨å®šå€¤ãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
            if orig.get('estimated_spill_gb', 0) > 0 or opt.get('estimated_spill_gb', 0) > 0:
                orig_spill = orig.get('estimated_spill_gb', 0)
                opt_spill = opt.get('estimated_spill_gb', 0)
                spill_status = 'âœ…æ”¹å–„' if opt_spill < orig_spill else 'âŒå¢—åŠ ' if opt_spill > orig_spill else 'âž–åŒç­‰'
                section += f"""| æŽ¨å®šã‚¹ãƒ”ãƒ«é‡ | {orig_spill:.2f}GB | {opt_spill:.2f}GB | {spill_status} |"""
            
            section += f"""

"""
        
        section += f"""

#### âš ï¸ è©•ä¾¡ã®åˆ¶é™äº‹é …

- **EXPLAIN COSTæœªå–å¾—**: æ­£ç¢ºãªã‚³ã‚¹ãƒˆãƒ»ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¯”è¼ƒä¸å¯
- **å®Ÿè¡Œçµ±è¨ˆä¸æ˜Ž**: å®Ÿéš›ã®å®Ÿè¡Œæ™‚é–“ã‚„ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã¯ä¸æ˜Ž
- **æŽ¨å®šãƒ™ãƒ¼ã‚¹**: å®Ÿè¡Œãƒ—ãƒ©ãƒ³æ§‹é€ ã‹ã‚‰ã®æŽ¨å®šè©•ä¾¡ã®ã¿
- **æŽ¨å¥¨**: å¯èƒ½ã§ã‚ã‚Œã°å®Ÿéš›ã®å®Ÿè¡Œãƒ†ã‚¹ãƒˆã§ç¢ºèªã™ã‚‹ã“ã¨ã‚’æŽ¨å¥¨

ðŸ’¡ **ã‚ˆã‚Šæ­£ç¢ºãªè©•ä¾¡ã®ãŸã‚**: AMBIGUOUS_REFERENCEç­‰ã®ã‚¨ãƒ©ãƒ¼ã‚’è§£æ±ºã—ã¦EXPLAIN COSTã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’æŽ¨å¥¨
"""
        
    return section


def fix_common_ambiguous_references(sql_query: str) -> str:
    """
    ã€å»ƒæ­¢ã€‘æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹ä¿®æ­£ã¯å»ƒæ­¢ - LLMã«ã‚ˆã‚‹é«˜åº¦ãªä¿®æ­£ã«å®Œå…¨ä¾å­˜
    """
    print("ðŸš« Regex-based pre-correction discontinued: Relying on advanced LLM-based correction")
    return sql_query


def fix_incomplete_sql_syntax(sql_query: str) -> str:
    """
    ä¸å®Œå…¨ãªSQLæ§‹æ–‡ã®æ¤œå‡ºã¨ä¿®æ­£
    """
    import re
    
    # åŸºæœ¬çš„ãªSQLã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    has_select = bool(re.search(r'\bSELECT\b', sql_query, re.IGNORECASE))
    has_from = bool(re.search(r'\bFROM\b', sql_query, re.IGNORECASE))
    
    # SELECTãŒãªã„å ´åˆã¯åŸºæœ¬çš„ãªSQLã§ã¯ãªã„å¯èƒ½æ€§ãŒé«˜ã„
    if not has_select:
        return sql_query
    
    # FROMãŒãªã„å ´åˆã¯ä¸å®Œå…¨ãªSQLã®å¯èƒ½æ€§
    if not has_from:
        # ä¸å®Œå…¨ãªSQLã®å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã§è­¦å‘Šã‚’è¿½åŠ 
        sql_query = f"-- âš ï¸ ä¸å®Œå…¨ãªSQLæ§‹æ–‡ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æ‰‹å‹•ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n{sql_query}"
    
    return sql_query

def remove_sql_placeholders(sql_query: str) -> str:
    """
    ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚„çœç•¥è¨˜å·ã®é™¤åŽ»ï¼ˆSQLãƒ’ãƒ³ãƒˆã¯ä¿æŒï¼‰
    """
    import re
    
    # ä¸€èˆ¬çš„ãªãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆSQLãƒ’ãƒ³ãƒˆã¯é™¤å¤–ï¼‰
    placeholders = [
        r'\.\.\.',  # çœç•¥è¨˜å·
        r'\[çœç•¥\]',  # çœç•¥è¡¨è¨˜
        r'\[ã‚«ãƒ©ãƒ å\]',  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
        r'\[ãƒ†ãƒ¼ãƒ–ãƒ«å\]',  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
        r'column1, column2, \.\.\.',  # ã‚«ãƒ©ãƒ çœç•¥
        r'-- \.\.\.',  # ã‚³ãƒ¡ãƒ³ãƒˆå†…ã®çœç•¥
        r'column1, column2, \.\.\.',  # ã‚«ãƒ©ãƒ çœç•¥ãƒ‘ã‚¿ãƒ¼ãƒ³
        r', \.\.\.',  # æœ«å°¾ã®çœç•¥è¨˜å·
        r'å®Œå…¨ãªSQL - ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ .*?ã‚’çœç•¥ãªã—ã§è¨˜è¿°',  # æŒ‡ç¤ºæ–‡ã®é™¤åŽ»
        r'\[å®Œå…¨ãªSQL.*?\]',  # å®Œå…¨ãªSQLæŒ‡ç¤ºã®é™¤åŽ»
    ]
    
    for pattern in placeholders:
        sql_query = re.sub(pattern, '', sql_query, flags=re.IGNORECASE)
    
    # SQLãƒ’ãƒ³ãƒˆä»¥å¤–ã®è¤‡æ•°è¡Œã‚³ãƒ¡ãƒ³ãƒˆã‚’é™¤åŽ»ï¼ˆãƒ’ãƒ³ãƒˆã¯ä¿æŒï¼‰
    # /*+ ... */ å½¢å¼ã®ãƒ’ãƒ³ãƒˆã¯ä¿æŒã—ã€ãã®ä»–ã® /* ... */ ã‚³ãƒ¡ãƒ³ãƒˆã®ã¿å‰Šé™¤
    sql_query = re.sub(r'/\*(?!\+).*?\*/', '', sql_query, flags=re.DOTALL)
    
    # ä¸å®Œå…¨ãªSQLæŒ‡ç¤ºã‚³ãƒ¡ãƒ³ãƒˆã‚’é™¤åŽ»
    instruction_comments = [
        r'-- ðŸš¨ é‡è¦:.*',
        r'-- ä¾‹:.*',
        r'-- è¤‡æ•°ãƒ’ãƒ³ãƒˆä¾‹.*',
        r'-- ç„¡åŠ¹ãªä¾‹:.*',
        r'-- ðŸš¨ REPARTITIONãƒ’ãƒ³ãƒˆ.*',
    ]
    
    for pattern in instruction_comments:
        sql_query = re.sub(pattern, '', sql_query, flags=re.IGNORECASE)
    
    # ç©ºè¡Œã‚’æ­£è¦åŒ–
    sql_query = re.sub(r'\n\s*\n\s*\n+', '\n\n', sql_query)
    
    return sql_query.strip()

def fix_basic_syntax_errors(sql_query: str) -> str:
    """
    åŸºæœ¬çš„ãªæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£
    """
    import re
    
    # 1. NULLãƒªãƒ†ãƒ©ãƒ«ã®åž‹ã‚­ãƒ£ã‚¹ãƒˆä¿®æ­£ - ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼ˆå†—é•·CASTç”Ÿæˆã®åŽŸå› ï¼‰
    # SELECT null as col01 â†’ SELECT cast(null as String) as col01
    # null_literal_pattern = r'\bnull\s+as\s+(\w+)'
    # sql_query = re.sub(null_literal_pattern, r'cast(null as String) as \1', sql_query, flags=re.IGNORECASE)
    
    # 2. é€£ç¶šã™ã‚‹ã‚«ãƒ³ãƒžã®ä¿®æ­£
    sql_query = re.sub(r',\s*,', ',', sql_query)
    
    # 3. ä¸æ­£ãªç©ºç™½ã®ä¿®æ­£ï¼ˆè¡Œå†…ã®é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«ï¼‰
    sql_query = re.sub(r'[ \t]+', ' ', sql_query)
    
    # 4. è¡Œæœ«ã®ä¸è¦ãªæ–‡å­—å‰Šé™¤
    sql_query = re.sub(r'[,;]\s*$', '', sql_query.strip())
    
    # 5. ä¸å®Œå…¨ãªSELECTæ–‡ã®ä¿®æ­£
    # SELECTã®å¾Œã«ç›´æŽ¥FROMãŒæ¥ã‚‹å ´åˆã‚’ä¿®æ­£
    sql_query = re.sub(r'SELECT\s+FROM', 'SELECT *\nFROM', sql_query, flags=re.IGNORECASE)
    
    # 6. ä¸å®Œå…¨ãªJOINå¥ã®ä¿®æ­£
    # JOINã®å¾Œã«ONãŒæ¥ãªã„å ´åˆã®åŸºæœ¬çš„ãªä¿®æ­£
    lines = sql_query.split('\n')
    fixed_lines = []
    
    for line in lines:
        line = line.strip()
        if line:
            # JOINã®å¾Œã«ONãŒãªã„å ´åˆã®è­¦å‘Šã‚³ãƒ¡ãƒ³ãƒˆè¿½åŠ 
            if re.search(r'\bJOIN\s+\w+\s*$', line, re.IGNORECASE):
                fixed_lines.append(line)
                fixed_lines.append('  -- âš ï¸ JOINæ¡ä»¶ï¼ˆONå¥ï¼‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„')
            else:
                fixed_lines.append(line)
    
    sql_query = '\n'.join(fixed_lines)
    
    # 7. åŸºæœ¬çš„ãªæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
    sql_query = add_syntax_warnings(sql_query)
    
    return sql_query

def add_syntax_warnings(sql_query: str) -> str:
    """
    åŸºæœ¬çš„ãªæ§‹æ–‡ãƒã‚§ãƒƒã‚¯ã¨è­¦å‘Šã®è¿½åŠ 
    """
    import re
    
    warnings = []
    
    # åŸºæœ¬çš„ãªSQLã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    has_select = bool(re.search(r'\bSELECT\b', sql_query, re.IGNORECASE))
    has_from = bool(re.search(r'\bFROM\b', sql_query, re.IGNORECASE))
    
    # JOINãŒã‚ã‚‹ãŒONãŒãªã„å ´åˆ
    joins = re.findall(r'\b(LEFT|RIGHT|INNER|OUTER)?\s*JOIN\s+\w+', sql_query, re.IGNORECASE)
    ons = re.findall(r'\bON\b', sql_query, re.IGNORECASE)
    
    if len(joins) > len(ons):
        warnings.append('-- âš ï¸ JOINå¥ã®æ•°ã«å¯¾ã—ã¦ONå¥ãŒä¸è¶³ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™')
    
    # WITHå¥ãŒã‚ã‚‹å ´åˆã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯
    if re.search(r'\bWITH\s+\w+\s+AS\s*\(', sql_query, re.IGNORECASE):
        if not re.search(r'\)\s*SELECT\b', sql_query, re.IGNORECASE):
            warnings.append('-- âš ï¸ WITHå¥ã®å¾Œã®ãƒ¡ã‚¤ãƒ³SELECTæ–‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„')
    
    # è­¦å‘ŠãŒã‚ã‚‹å ´åˆã¯å…ˆé ­ã«è¿½åŠ 
    if warnings:
        sql_query = '\n'.join(warnings) + '\n\n' + sql_query
    
    return sql_query

def extract_broadcast_tables_from_sql(sql_query: str) -> list:
    """
    SQLã‚¯ã‚¨ãƒªã‹ã‚‰BROADCASTã•ã‚Œã‚‹ã¹ããƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
    """
    import re
    
    # å‰Šé™¤ã•ã‚ŒãŸBROADCASTãƒ’ãƒ³ãƒˆã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
    broadcast_pattern = r'BROADCAST\(([^)]+)\)'
    matches = re.findall(broadcast_pattern, sql_query, re.IGNORECASE)
    
    tables = []
    for match in matches:
        # ã‚«ãƒ³ãƒžã§åŒºåˆ‡ã‚‰ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«åã‚’åˆ†å‰²
        table_names = [name.strip() for name in match.split(',')]
        tables.extend(table_names)
    
    return list(set(tables))  # é‡è¤‡ã‚’é™¤åŽ»

def validate_final_sql_syntax(sql_query: str) -> bool:
    """
    æœ€çµ‚çš„ãªSQLæ§‹æ–‡ãƒã‚§ãƒƒã‚¯ï¼ˆä¿å­˜å‰ã®ç¢ºèªï¼‰
    
    Returns:
        bool: æ§‹æ–‡ãŒæ­£ã—ã„ã¨åˆ¤å®šã•ã‚ŒãŸå ´åˆTrueã€å•é¡ŒãŒã‚ã‚‹å ´åˆFalse
    """
    import re
    
    if not sql_query or not sql_query.strip():
        return False
    
    # åŸºæœ¬çš„ãªSQLã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    has_select = bool(re.search(r'\bSELECT\b', sql_query, re.IGNORECASE))
    
    # SELECTãŒãªã„å ´åˆã¯ä¸æ­£
    if not has_select:
        return False
    
    # æ˜Žã‚‰ã‹ã«ä¸å®Œå…¨ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒƒã‚¯
    incomplete_patterns = [
        r'\.\.\.',  # çœç•¥è¨˜å·
        r'\[çœç•¥\]',  # çœç•¥è¡¨è¨˜
        r'\[ã‚«ãƒ©ãƒ å\]',  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
        r'\[ãƒ†ãƒ¼ãƒ–ãƒ«å\]',  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
        r'column1, column2, \.\.\.',  # ã‚«ãƒ©ãƒ çœç•¥
        r'å®Œå…¨ãªSQL.*?ã‚’.*?è¨˜è¿°',  # æŒ‡ç¤ºæ–‡
    ]
    
    for pattern in incomplete_patterns:
        if re.search(pattern, sql_query, re.IGNORECASE):
            return False
    
    # BROADCASTãƒ’ãƒ³ãƒˆé…ç½®ã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯
    broadcast_hints = re.findall(r'/\*\+\s*BROADCAST\([^)]+\)\s*\*/', sql_query, re.IGNORECASE)
    if broadcast_hints:
        # BROADCASTãƒ’ãƒ³ãƒˆãŒã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        subquery_broadcast = re.search(r'JOIN\s*\(\s*SELECT\s*/\*\+\s*BROADCAST', sql_query, re.IGNORECASE)
        if subquery_broadcast:
            return False
    
    # åŸºæœ¬çš„ãªæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
    # é€£ç¶šã™ã‚‹ã‚«ãƒ³ãƒž
    if re.search(r',\s*,', sql_query):
        return False
    
    # ä¸æ­£ãªç©ºç™½ãƒ‘ã‚¿ãƒ¼ãƒ³
    if re.search(r'\s{5,}', sql_query):  # 5å€‹ä»¥ä¸Šã®é€£ç¶šã™ã‚‹ç©ºç™½
        return False
    
    return True

def save_optimized_sql_files(original_query: str, optimized_result: str, metrics: Dict[str, Any], analysis_result: str = "", llm_response: str = "", performance_comparison: Dict[str, Any] = None, best_attempt_number: int = None, optimization_attempts: list = None, optimization_success: bool = None) -> Dict[str, str]:
    """
    æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œå¯èƒ½ãªå½¢ã§ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    
    ç‰¹å¾´:
    - SQLãƒ•ã‚¡ã‚¤ãƒ«ã®æœ«å°¾ã«è‡ªå‹•ã§ã‚»ãƒŸã‚³ãƒ­ãƒ³(;)ã‚’ä»˜ä¸Ž
    - ãã®ã¾ã¾Databricks Notebookã§å®Ÿè¡Œå¯èƒ½
    - %sql ãƒžã‚¸ãƒƒã‚¯ã‚³ãƒžãƒ³ãƒ‰ã§ã‚‚ç›´æŽ¥å®Ÿè¡Œå¯èƒ½
    - LLMã«ã‚ˆã‚‹ãƒ¬ãƒãƒ¼ãƒˆæŽ¨æ•²ã§èª­ã¿ã‚„ã™ã„æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    """
    
    import re
    from datetime import datetime
    
    # thinking_enabled: Trueã®å ´åˆã«optimized_resultãŒãƒªã‚¹ãƒˆã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚å¯¾å¿œ
    optimized_result_for_file = optimized_result
    optimized_result_main_content = optimized_result
    
    if isinstance(optimized_result, list):
        # Convert to human-readable format for file saving
        optimized_result_for_file = format_thinking_response(optimized_result)
        # SQLæŠ½å‡ºç”¨ã¯ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã¿ã‚’ä½¿ç”¨
        optimized_result_main_content = extract_main_content_from_thinking_response(optimized_result)
    
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    query_id = metrics.get('query_info', {}).get('query_id', 'unknown')
    
    # ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã¯é™¤å¤–ï¼ˆä¸è¦ï¼‰
    original_filename = None
    
    # æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®æŠ½å‡ºã¨ä¿å­˜ï¼ˆæ”¹å–„ç‰ˆï¼šå¼·åŒ–ã•ã‚ŒãŸSQLæŠ½å‡ºã‚’ä½¿ç”¨ï¼‰
    optimized_filename = f"output_optimized_query_{timestamp}.sql"
    
    # æ”¹å–„ã•ã‚ŒãŸSQLæŠ½å‡ºé–¢æ•°ã‚’ä½¿ç”¨
    optimized_sql = extract_sql_from_llm_response(optimized_result_main_content)
    
    # åˆ†æžçµæžœã®æŠ½å‡ºï¼ˆSQLã¨åˆ†é›¢ã—ã¦ä¿å­˜ã™ã‚‹ãŸã‚ï¼‰
    analysis_content = extract_analysis_content_from_llm_response(optimized_result_main_content)
    
    # SQLæ§‹æ–‡ã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯ï¼ˆå®Œå…¨æ€§ç¢ºèªï¼‰
    if optimized_sql:
        optimized_sql = validate_and_fix_sql_syntax(optimized_sql)
    
    # æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰
    try:
        with open(optimized_filename, 'w', encoding='utf-8') as f:
            f.write(f"-- æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒª\n")
            f.write(f"-- å…ƒã‚¯ã‚¨ãƒªID: {query_id}\n")
            f.write(f"-- æœ€é©åŒ–æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"-- ãƒ•ã‚¡ã‚¤ãƒ«: {optimized_filename}\n\n")
            
            
            # ðŸŽ¯ CATALOG/DATABASEè¨­å®šã®è‡ªå‹•è¿½åŠ 
            catalog_name = globals().get("CATALOG", "tpcds")
            database_name = globals().get("DATABASE", "tpcds_sf1000_delta_lc")
            
            f.write(f"-- ðŸ—‚ï¸ ã‚«ã‚¿ãƒ­ã‚°ãƒ»ã‚¹ã‚­ãƒ¼ãƒžè¨­å®šï¼ˆè‡ªå‹•è¿½åŠ ï¼‰\n")
            f.write(f"USE CATALOG {catalog_name};\n")
            f.write(f"USE SCHEMA {database_name};\n\n")
                
            if optimized_sql:
                # SQLã®æœ«å°¾ã«ã‚»ãƒŸã‚³ãƒ­ãƒ³ã‚’ç¢ºå®Ÿã«è¿½åŠ 
                optimized_sql_clean = optimized_sql.strip()
                if optimized_sql_clean and not optimized_sql_clean.endswith(';'):
                    optimized_sql_clean += ';'
                
                # æœ€çµ‚çš„ãªæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
                if validate_final_sql_syntax(optimized_sql_clean):
                    f.write(optimized_sql_clean)
                else:
                    f.write("-- âš ï¸ æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æ‰‹å‹•ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n")
                    f.write(f"-- å…ƒã®SQL:\n{optimized_sql_clean}\n")
                    f.write("-- ä»¥ä¸‹ã¯æœ€é©åŒ–åˆ†æžã®å…¨çµæžœã§ã™:\n\n")
                    f.write(f"/*\n{optimized_result_main_content}\n*/")
            else:
                f.write("-- âš ï¸ SQLã‚³ãƒ¼ãƒ‰ã®è‡ªå‹•æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ\n")
                f.write("-- ä»¥ä¸‹ã¯æœ€é©åŒ–åˆ†æžã®å…¨çµæžœã§ã™:\n\n")
                f.write(f"/*\n{optimized_result_main_content}\n*/")
    except Exception as e:
        print(f"âš ï¸ Error occurred during SQL file saving: {str(e)}")
        # Generate basic file on error
        with open(optimized_filename, 'w', encoding='utf-8') as f:
            f.write(f"-- âš ï¸ Error occurred during SQL file saving: {str(e)}\n")
            f.write(f"-- Optimization result:\n{optimized_result_main_content}\n")
    
    # Save analysis report file (readable report refined by LLM)
    # Generate filename based on OUTPUT_LANGUAGE setting
    language_suffix = 'en' if OUTPUT_LANGUAGE == 'en' else 'jp'
    report_filename = f"output_optimization_report_{language_suffix}_{timestamp}.md"
    
    print("ðŸ¤– Executing LLM report refinement...")
    
    # ðŸš€ Load content of actually saved SQL file and use for report
    try:
        with open(optimized_filename, 'r', encoding='utf-8') as f:
            actual_sql_content = f.read()
        
        # Use actual SQL file content for report (guaranteed to work)
        print(f"âœ… Loaded SQL file content for report generation: {optimized_filename}")
        report_data = actual_sql_content
        
    except Exception as e:
        print(f"âš ï¸ SQL file loading failed, using initial response: {str(e)}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åˆå›žãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½¿ç”¨
        report_data = llm_response if llm_response else optimized_result
    
    initial_report = generate_comprehensive_optimization_report(
        query_id, report_data, metrics, analysis_result, performance_comparison, best_attempt_number, optimization_attempts, optimization_success
    )
    
    # LLMã§ãƒ¬ãƒãƒ¼ãƒˆã‚’æŽ¨æ•²ï¼ˆè©³ç´°ãªæŠ€è¡“æƒ…å ±ã‚’ä¿æŒï¼‰
    refined_report = refine_report_with_llm(initial_report, query_id)
    
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(refined_report)
    
    print(f"âœ… Report file saving completed: {report_filename}")
    
    # åˆ†æžçµæžœã‚’åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆæ–°æ©Ÿèƒ½ï¼šDEBUG_ENABLED='Y'ã®å ´åˆã®ã¿ï¼‰
    analysis_filename = None
    debug_enabled = globals().get('DEBUG_ENABLED', 'N')
    
    if analysis_content and len(analysis_content.strip()) > 100 and debug_enabled.upper() == 'Y':
        analysis_filename = f"output_optimization_analysis_{timestamp}.md"
        try:
            with open(analysis_filename, 'w', encoding='utf-8') as f:
                f.write(f"# SQLæœ€é©åŒ–åˆ†æžçµæžœ\n")
                f.write(f"## ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±\n")
                f.write(f"- ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"- å…ƒã‚¯ã‚¨ãƒªID: {query_id}\n")
                f.write(f"- æœ€é©åŒ–SQLãƒ•ã‚¡ã‚¤ãƒ«: {optimized_filename}\n")
                f.write(f"- è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_filename}\n\n")
                f.write("---\n\n")
                f.write(analysis_content)
            
            print(f"âœ… Analysis file saving completed: {analysis_filename}")
        except Exception as e:
            print(f"âš ï¸ Analysis file saving failed: {str(e)}")
            analysis_filename = None
    elif debug_enabled.upper() != 'Y':
        print(f"ðŸ› Analysis file saving skipped (DEBUG_ENABLED={debug_enabled})")
    
    # Output file results (analysis file added to results)
    result = {
        'optimized_file': optimized_filename,
        'report_file': report_filename
    }
    
    if analysis_filename:
        result['analysis_file'] = analysis_filename
    
    return result

def demonstrate_execution_plan_size_extraction():
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‹ã‚‰ã®ã‚µã‚¤ã‚ºæŽ¨å®šæ©Ÿèƒ½ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    print("ðŸ§ª Demo of table size estimation feature from execution plan")
    print("-" * 50)
    
    # ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    sample_profiler_data = {
        "executionPlan": {
            "physicalPlan": {
                "nodes": [
                    {
                        "nodeName": "Scan Delta orders",
                        "id": "1",
                        "metrics": {
                            "estimatedSizeInBytes": 10485760,  # 10MB
                            "numFiles": 5,
                            "numPartitions": 2
                        },
                        "output": "[order_id#123, customer_id#124, amount#125] orders",
                        "details": "Table: catalog.database.orders"
                    },
                    {
                        "nodeName": "Scan Delta customers",
                        "id": "2", 
                        "metrics": {
                            "estimatedSizeInBytes": 52428800,  # 50MB
                            "numFiles": 10,
                            "numPartitions": 4
                        },
                        "output": "[customer_id#126, name#127, region#128] customers"
                    }
                ]
            }
        }
    }
    
    print("ðŸ“Š Sample execution plan:")
    print("  â€¢ orders table: estimatedSizeInBytes = 10,485,760 (10MB)")
    print("  â€¢ customers table: estimatedSizeInBytes = 52,428,800 (50MB)")
    print("")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæŽ¨å®šã®å®Ÿè¡Œ
    table_size_estimates = extract_table_size_estimates_from_plan(sample_profiler_data)
    
    print("ðŸ” Extracted table size estimations:")
    if table_size_estimates:
        for table_name, size_info in table_size_estimates.items():
            print(f"  ðŸ“‹ {table_name}:")
            print(f"    - Size: {size_info['estimated_size_mb']:.1f}MB")
            print(f"    - Confidence: {size_info['confidence']}")
            print(f"    - Source: {size_info['source']}")
            if 'num_files' in size_info:
                print(f"    - File count: {size_info['num_files']}")
            if 'num_partitions' in size_info:
                print(f"    - Partition count: {size_info['num_partitions']}")
            print("")
    else:
        print("  âš ï¸ Table size estimation information could not be extracted")
    
    print("ðŸ’¡ Impact on BROADCAST analysis:")
    if table_size_estimates:
        for table_name, size_info in table_size_estimates.items():
            size_mb = size_info['estimated_size_mb']
            if size_mb <= 30:
                print(f"  âœ… {table_name}: {size_mb:.1f}MB â‰¤ 30MB â†’ BROADCAST recommended")
            else:
                print(f"  âŒ {table_name}: {size_mb:.1f}MB > 30MB â†’ BROADCAST not recommended")
    
    print("")
    print("ðŸŽ¯ Comparison with conventional estimation methods:")
    print("  ðŸ“ˆ Conventional: Metrics-based indirect estimation (estimation accuracy: medium)")
    print("  âŒ New feature: Utilizing estimatedSizeInBytes from execution plan (disabled due to unavailability)")
    print("  â„¹ï¸ Current: Adopting conservative estimation with 3.0x compression ratio")
    
    return {}

print("âœ… Function definition completed: SQL optimization related functions (execution plan size estimation support)")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸš€ Original Query Extraction
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Extraction of original query from profiler data
# MAGIC - Detailed display of extracted query (up to 64KB)
# MAGIC - Fallback processing (sample query configuration)

# COMMAND ----------

# ðŸš€ SQLã‚¯ã‚¨ãƒªæœ€é©åŒ–ã®å®Ÿè¡Œ
print("\n" + "ðŸš€" * 20)
print("ðŸ”§ ã€SQL Query Optimization Executionã€‘")
print("ðŸš€" * 20)

# 1. ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®æŠ½å‡º
print("\nðŸ“‹ Step 1: Extract Original Query")
print("-" * 40)

original_query = extract_original_query_from_profiler_data(profiler_data)

if original_query:
    print(f"âœ… Original query extracted ({len(original_query)} characters)")
    print(f"ðŸ” Query preview:")
    # 64KB (65536æ–‡å­—) ã¾ã§è¡¨ç¤º
    max_display_chars = 65536
    if len(original_query) > max_display_chars:
        preview = original_query[:max_display_chars] + f"\n... (æ®‹ã‚Š {len(original_query) - max_display_chars} æ–‡å­—ã¯çœç•¥)"
    else:
        preview = original_query
    print(f"   {preview}")
else:
    print("âš ï¸ Original query not found")
    print("   Please set the query manually")
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã‚’è¨­å®š
    original_query = """
    -- ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªï¼ˆå®Ÿéš›ã®ã‚¯ã‚¨ãƒªã«ç½®ãæ›ãˆã¦ãã ã•ã„ï¼‰
    SELECT 
        customer_id,
        SUM(order_amount) as total_amount,
        COUNT(*) as order_count
    FROM orders 
    WHERE order_date >= '2023-01-01'
    GROUP BY customer_id
    ORDER BY total_amount DESC
    LIMIT 100
    """
    print(f"ðŸ“ Sample query has been set")

# ðŸ“ ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
print("\nðŸ“ Saving original query to file")
print("-" * 40)

from datetime import datetime

# ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
original_query_filename = f"output_original_query_{timestamp}.sql"

try:
    # ã‚«ã‚¿ãƒ­ã‚°ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®šã®å–å¾—
    catalog_name = globals().get('CATALOG', 'tpcds')
    database_name = globals().get('DATABASE', 'tpcds_sf1000_delta_lc')
    
    with open(original_query_filename, 'w', encoding='utf-8') as f:
        f.write(f"-- ðŸ“‹ ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æŠ½å‡ºï¼‰\n")
        f.write(f"-- æŠ½å‡ºæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"-- ãƒ•ã‚¡ã‚¤ãƒ«: {original_query_filename}\n")
        f.write(f"-- ã‚¯ã‚¨ãƒªæ–‡å­—æ•°: {len(original_query):,}\n\n")
        
        # ã‚«ã‚¿ãƒ­ã‚°ãƒ»ã‚¹ã‚­ãƒ¼ãƒžè¨­å®šã®è¿½åŠ 
        f.write(f"-- ðŸ—‚ï¸ ã‚«ã‚¿ãƒ­ã‚°ãƒ»ã‚¹ã‚­ãƒ¼ãƒžè¨­å®šï¼ˆè‡ªå‹•è¿½åŠ ï¼‰\n")
        f.write(f"USE CATALOG {catalog_name};\n")
        f.write(f"USE SCHEMA {database_name};\n\n")
        
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®æ›¸ãè¾¼ã¿
        f.write(f"-- ðŸ” ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒª\n")
        f.write(original_query)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æœ«å°¾ã«æ”¹è¡Œã‚’è¿½åŠ 
        if not original_query.endswith('\n'):
            f.write('\n')
    
    print(f"âœ… Original query saved: {original_query_filename}")
    print(f"ðŸ“Š Saved query character count: {len(original_query):,}")
    print(f"ðŸ’¾ File path: ./{original_query_filename}")
    print("ðŸ“Œ This file is retained as final output regardless of DEBUG_ENABLED setting")
    
except Exception as e:
    print(f"âŒ Failed to save original query file: {str(e)}")
    print("âš ï¸ Processing continues, but original query file was not created")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸ” SQL Optimization Execution
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Retrieve original query extracted in Cell 43
# MAGIC - Generate and execute EXPLAIN statements in Databricks
# MAGIC - Output execution plan details to files
# MAGIC - Error handling and result verification

# COMMAND ----------

def extract_select_from_ctas(query: str) -> str:
    """
    CREATE TABLE AS SELECT (CTAS) ã‚¯ã‚¨ãƒªã‹ã‚‰ASä»¥é™ã®éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
    
    å¯¾å¿œãƒ‘ã‚¿ãƒ¼ãƒ³:
    - CREATE TABLE ... AS SELECT ...
    - CREATE OR REPLACE TABLE ... AS SELECT ...
    - CREATE TABLE ... AS WITH ... SELECT ...
    - AS ã®å¾Œã‚ã«æ‹¬å¼§ãŒãªã„å ´åˆ
    - è¤‡æ•°è¡Œã«ã¾ãŸãŒã‚‹å ´åˆ
    - ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ã®è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆUSINGã€PARTITIONED BYã€TBLPROPERTIESç­‰ï¼‰
    
    Args:
        query: å…ƒã®ã‚¯ã‚¨ãƒª
    
    Returns:
        str: ASä»¥é™ã®éƒ¨åˆ†ã®ã¿ã®ã‚¯ã‚¨ãƒªã€ã¾ãŸã¯CTASã§ãªã„å ´åˆã¯å…ƒã®ã‚¯ã‚¨ãƒª
    """
    import re
    
    # ã‚¯ã‚¨ãƒªã‚’æ­£è¦åŒ–ï¼ˆæ”¹è¡Œãƒ»ç©ºç™½ã‚’çµ±ä¸€ï¼‰
    normalized_query = re.sub(r'\s+', ' ', query.strip())
    
    # CTAS ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºï¼ˆåŒ…æ‹¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    # CREATE [OR REPLACE] TABLE ... AS ... ã®å½¢å¼ã‚’æ¤œå‡º
    # ASã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä½ç½®ã‚’æ­£ç¢ºã«ç‰¹å®šã™ã‚‹
    
    # CREATE [OR REPLACE] TABLEéƒ¨åˆ†ã®æ¤œå‡º
    create_patterns = [
        r'CREATE\s+OR\s+REPLACE\s+TABLE',
        r'CREATE\s+TABLE'
    ]
    
    for create_pattern in create_patterns:
        # CREATE TABLEéƒ¨åˆ†ã‚’æ¤œå‡º
        create_match = re.search(create_pattern, normalized_query, re.IGNORECASE)
        if create_match:
            # CREATE TABLEä»¥é™ã®éƒ¨åˆ†ã‚’å–å¾—
            after_create = normalized_query[create_match.end():].strip()
            
            # AS ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä½ç½®ã‚’æ¤œç´¢ï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ï¼‰
            # AS ã¯å˜èªžå¢ƒç•Œã§åŒºåˆ‡ã‚‰ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚‹
            as_pattern = r'\bAS\b'
            as_match = re.search(as_pattern, after_create, re.IGNORECASE)
            
            if as_match:
                # ASä»¥é™ã®éƒ¨åˆ†ã‚’å–å¾—
                as_part = after_create[as_match.end():].strip()
                
                if as_part:
                    print(f"âœ… CTAS detected: Using part after AS for EXPLAIN statement")
                    print(f"ðŸ“Š Original query length: {len(query):,} characters")
                    print(f"ðŸ“Š Part after AS length: {len(as_part):,} characters")
                    
                    # WITHå¥ã§å§‹ã¾ã‚‹å ´åˆã‚„SELECTå¥ã§å§‹ã¾ã‚‹å ´åˆã‚’åˆ¤å®š
                    if as_part.upper().startswith('WITH'):
                        print("ðŸ“‹ Detected query starting with WITH clause")
                    elif as_part.upper().startswith('SELECT'):
                        print("ðŸ“‹ Detected query starting with SELECT clause")
                    else:
                        print("ðŸ“‹ Detected other query format")
                    
                    return as_part
    
    print("ðŸ“‹ Regular query: Use as is for EXPLAIN statement")
    return query

def generate_improved_query_for_performance_degradation(original_query: str, analysis_result: str, metrics: Dict[str, Any], degradation_analysis: Dict[str, Any], previous_optimized_query: str = "") -> str:
    """
    LLM optimization function specifically for performance degradation
    Apply specific improvement measures based on degradation cause analysis
    
    Args:
        original_query: Original query
        analysis_result: Bottleneck analysis result
        metrics: Metrics information
        degradation_analysis: Degradation cause analysis result
        previous_optimized_query: Previous optimized query
    """
    
    # æ‚ªåŒ–åˆ†æžã®è©³ç´°æƒ…å ±ã‚’æŠ½å‡º
    primary_cause = degradation_analysis.get('primary_cause', 'unknown')
    cost_ratio = degradation_analysis.get('analysis_details', {}).get('cost_ratio', 1.0) or 1.0
    specific_issues = degradation_analysis.get('specific_issues', [])
    fix_instructions = degradation_analysis.get('fix_instructions', [])
    confidence_level = degradation_analysis.get('confidence_level', 'low')
    
    # å‰å›žã‚¯ã‚¨ãƒªã®åˆ†æžã‚»ã‚¯ã‚·ãƒ§ãƒ³
    previous_query_section = ""
    if previous_optimized_query:
        previous_query_section = f"""

ã€ðŸš¨ å‰å›žã®æœ€é©åŒ–ã‚¯ã‚¨ãƒªï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ‚ªåŒ–ï¼‰ã€‘
```sql
{previous_optimized_query}
```

**âŒ æ¤œå‡ºã•ã‚ŒãŸå•é¡Œç‚¹:**
- å®Ÿè¡Œã‚³ã‚¹ãƒˆæ¯”: {cost_ratio:.2f}å€ã®æ‚ªåŒ–
- ä¸»è¦åŽŸå› : {primary_cause}
- å…·ä½“çš„å•é¡Œ: {', '.join(specific_issues)}
"""

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ‚ªåŒ–ä¿®æ­£ã«ç‰¹åŒ–ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    performance_improvement_prompt = f"""
ã‚ãªãŸã¯Databricksã®SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æœ€é©åŒ–ã®å°‚é–€å®¶ã§ã™ã€‚

å‰å›žã®æœ€é©åŒ–ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ‚ªåŒ–ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚æ‚ªåŒ–åŽŸå› åˆ†æžã«åŸºã¥ã„ã¦ **æ ¹æœ¬çš„ãªæ”¹å–„** ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

ã€ðŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ‚ªåŒ–ã®è©³ç´°åˆ†æžã€‘
- **æ‚ªåŒ–çŽ‡**: {cost_ratio:.2f}å€ï¼ˆ{(cost_ratio-1)*100:.1f}%å¢—åŠ ï¼‰
- **ä¸»è¦åŽŸå› **: {primary_cause}
- **ä¿¡é ¼åº¦**: {confidence_level}
- **å…·ä½“çš„å•é¡Œ**: {', '.join(specific_issues)}

ã€å…ƒã®åˆ†æžå¯¾è±¡ã‚¯ã‚¨ãƒªã€‘
```sql
{original_query}
```
{previous_query_section}

ã€ðŸ”§ æ‚ªåŒ–åŽŸå› åˆ¥ã®å…·ä½“çš„ä¿®æ­£æŒ‡ç¤ºã€‘
{chr(10).join(f"- {instruction}" for instruction in fix_instructions)}

ã€ðŸŽ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ”¹å–„ã®é‡è¦ãªæ–¹é‡ã€‘

1. **ðŸš¨ éŽå‰°æœ€é©åŒ–ã®æ˜¯æ­£**:
           - JOINé †åºã®åŠ¹çŽ‡åŒ–
           - åŠ¹çŽ‡çš„ã§ãªã„JOINé †åºã®è¦‹ç›´ã—
   - åŠ¹æžœçš„ã§ãªã„ãƒ’ãƒ³ãƒˆã¯ç©æ¥µçš„ã«å‰Šé™¤

2. **âš¡ JOINåŠ¹çŽ‡åŒ–**:
   - JOINæ“ä½œæ•°ã®å¤§å¹…ãªå¢—åŠ ã‚’é¿ã‘ã‚‹
   - å…ƒã®JOINé †åºã‚’å°Šé‡
   - ä¸è¦ãªã‚µãƒ–ã‚¯ã‚¨ãƒªåŒ–ã«ã‚ˆã‚‹JOINé‡è¤‡ã‚’é˜²ã

3. **ðŸŽ¯ ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºæœ€é©åŒ–**:
   - ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ã‚’æœ€å¤§åŒ–
   - æ—©æœŸã®è¡Œæ•°å‰Šæ¸›ã‚’é‡è¦–
   - ä¸­é–“çµæžœã®ã‚µã‚¤ã‚ºã‚’æœ€å°åŒ–

4. **ðŸ“Š çµ±è¨ˆæƒ…å ±ã«åŸºã¥ãåˆ¤æ–­**:
   - å°ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ<30MBï¼‰ã®ã¿BROADCASTé©ç”¨
   - ãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡ã‚’é‡è¦–ã—ãŸJOINæˆ¦ç•¥
   - ã‚¹ãƒ”ãƒ«ç™ºç”Ÿã®æœ€å°åŒ–

ã€ðŸ”„ æ”¹å–„ã‚¯ã‚¨ãƒªç”Ÿæˆã®æŒ‡é‡ã€‘

**A. ä¿å®ˆçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆæŽ¨å¥¨ï¼‰:**
- å…ƒã‚¯ã‚¨ãƒªã®æ§‹é€ ã‚’æœ€å¤§é™ä¿æŒ
- ç¢ºå®Ÿã«åŠ¹æžœçš„ãªæœ€é©åŒ–ã®ã¿é©ç”¨
- ãƒªã‚¹ã‚¯ã®é«˜ã„å¤‰æ›´ã¯é¿ã‘ã‚‹

**B. æ®µéšŽçš„æ”¹å–„:**
- æœ€ã‚‚å•é¡Œã¨ãªã£ã¦ã„ã‚‹ç®‡æ‰€ã®ã¿ä¿®æ­£
- ä¸€åº¦ã«å¤šãã®å¤‰æ›´ã‚’åŠ ãˆãªã„
- æ¸¬å®šå¯èƒ½ãªæ”¹å–„ã‚’é‡è¦–

**C. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥:**
- ä¸ç¢ºå®Ÿãªæœ€é©åŒ–ã¯å‰Šé™¤
- å…ƒã®ã‚¯ã‚¨ãƒªã«è¿‘ã„å½¢ã§ã®è»½å¾®ãªæ”¹å–„

ã€é‡è¦ãªåˆ¶ç´„ã€‘
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ‚ªåŒ–ã®ä¸»è¦åŽŸå› ã‚’ç¢ºå®Ÿã«è§£æ±º
- å…ƒã‚¯ã‚¨ãƒªã‚ˆã‚Šç¢ºå®Ÿã«é«˜é€Ÿãªã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
- æ©Ÿèƒ½æ€§ã‚’ä¸€åˆ‡æãªã‚ãªã„
- å®Œå…¨ã§å®Ÿè¡Œå¯èƒ½ãªSQLã®ã¿å‡ºåŠ›

ã€å‡ºåŠ›å½¢å¼ã€‘
## ðŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ”¹å–„SQL

**æ”¹å–„ã—ãŸå†…å®¹**:
- [å…·ä½“çš„ãªæ‚ªåŒ–åŽŸå› ã®ä¿®æ­£]
- [å‰Šé™¤/å¤‰æ›´ã—ãŸæœ€é©åŒ–è¦ç´ ]
- [æ–°ãŸã«é©ç”¨ã—ãŸæ”¹å–„ç­–]

```sql
[å®Œå…¨ãªSQL - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ”¹å–„æ¸ˆã¿]
```

## æ”¹å–„è©³ç´°
[æ‚ªåŒ–åŽŸå› ã®è§£æ±ºæ–¹æ³•ã¨æœŸå¾…ã•ã‚Œã‚‹æ€§èƒ½æ”¹å–„ã®èª¬æ˜Ž]
"""

    # è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
    provider = LLM_CONFIG["provider"]
    
    try:
        if provider == "databricks":
            improved_result = _call_databricks_llm(performance_improvement_prompt)
        elif provider == "openai":
            improved_result = _call_openai_llm(performance_improvement_prompt)
        elif provider == "azure_openai":
            improved_result = _call_azure_openai_llm(performance_improvement_prompt)
        elif provider == "anthropic":
            improved_result = _call_anthropic_llm(performance_improvement_prompt)
        else:
            error_msg = "âš ï¸ Configured LLM provider is not recognized"
            print(f"âŒ LLM performance improvement error: {error_msg}")
            return f"LLM_ERROR: {error_msg}"
        
        # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        if isinstance(improved_result, str):
            error_indicators = [
                "APIã‚¨ãƒ©ãƒ¼:",
                "Input is too long", 
                "Bad Request",
                "âŒ",
                "âš ï¸",
                "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼:",
                "APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼:",
            ]
            
            for indicator in error_indicators:
                if indicator in improved_result:
                    print(f"âŒ Error detected in LLM performance improvement: {indicator}")
                    return f"LLM_ERROR: {improved_result}"
        
        return improved_result
        
    except Exception as e:
        error_msg = f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ”¹å–„å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {str(e)}"
        print(f"âŒ {error_msg}")
        return f"LLM_ERROR: {error_msg}"


def generate_optimized_query_with_error_feedback(original_query: str, analysis_result: str, metrics: Dict[str, Any], error_info: str = "", previous_optimized_query: str = "") -> str:
    """
    Execute SQL optimization by LLM including error information
    Use prompts specialized for error correction
    
    Args:
        original_query: Original query
        analysis_result: Bottleneck analysis result
        metrics: Metrics information
        error_info: Error information
        previous_optimized_query: Initial optimized query (for hint retention)
    """
    
    # åˆå›žæœ€é©åŒ–ã‚¯ã‚¨ãƒªã®æƒ…å ±ã‚’å«ã‚ã‚‹
    previous_query_section = ""
    if previous_optimized_query:
        previous_query_section = f"""

ã€ðŸš€ åˆå›žç”Ÿæˆã•ã‚ŒãŸæœ€é©åŒ–ã‚¯ã‚¨ãƒªï¼ˆã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼‰ã€‘
```sql
{previous_optimized_query}
```

**âš ï¸ é‡è¦**: ä¸Šè¨˜ã®æœ€é©åŒ–ã‚¯ã‚¨ãƒªã«å«ã¾ã‚Œã‚‹ä»¥ä¸‹ã®è¦ç´ ã¯å¿…ãšä¿æŒã—ã¦ãã ã•ã„ï¼š
- **REPARTITIONãƒ’ãƒ³ãƒˆ**: `/*+ REPARTITION(æ•°å€¤, ã‚«ãƒ©ãƒ å) */`
- **ãã®ä»–ã®æœ€é©åŒ–ãƒ’ãƒ³ãƒˆ**: COALESCEã€CACHEç­‰
- **æœ€é©åŒ–æ‰‹æ³•**: CTEæ§‹é€ ã€çµåˆé †åºã€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ç­‰
- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ”¹å–„ç­–**: ã‚¹ãƒ”ãƒ«å¯¾ç­–ã€ä¸¦åˆ—åº¦æ”¹å–„ç­‰

**ðŸŽ¯ ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã®æ–¹é‡**: 
- ã‚¨ãƒ©ãƒ¼ç®‡æ‰€ã®ã¿ã‚’ä¿®æ­£ã—ã€æœ€é©åŒ–è¦ç´ ã¯å…¨ã¦ä¿æŒ
- ãƒ’ãƒ³ãƒˆå¥ã®é…ç½®ãƒ«ãƒ¼ãƒ«ã¯åŽ³å®ˆï¼ˆREPARTITIONã¯ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªSELECTç›´å¾Œç­‰ï¼‰
"""

    # ðŸš¨ NEW: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è§£æžã«ã‚ˆã‚‹è©³ç´°ä¿®æ­£æŒ‡ç¤ºç”Ÿæˆ
    def generate_specific_error_guidance(error_message: str) -> str:
        """Generate detailed correction instructions based on specific error messages"""
        guidance = ""
        
        if "AMBIGUOUS_REFERENCE" in error_message.upper():
            # AMBIGUOUS_REFERENCEã‚¨ãƒ©ãƒ¼ã®å…·ä½“çš„å¯¾å‡¦
            import re
            ambiguous_column_match = re.search(r'Reference `([^`]+)` is ambiguous', error_message)
            if ambiguous_column_match:
                ambiguous_column = ambiguous_column_match.group(1)
                guidance += f"""
ðŸŽ¯ **AMBIGUOUS_REFERENCE å°‚ç”¨ä¿®æ­£æŒ‡ç¤º**: 
- **å•é¡Œ**: ã‚«ãƒ©ãƒ  `{ambiguous_column}` ãŒè¤‡æ•°ãƒ†ãƒ¼ãƒ–ãƒ«ã«å­˜åœ¨
- **å¿…é ˆä¿®æ­£**: å…¨ã¦ã® `{ambiguous_column}` å‚ç…§ã«ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’æ˜Žç¤º
- **ä¿®æ­£ä¾‹**: `{ambiguous_column}` â†’ `table_alias.{ambiguous_column}`
- **é‡è¦**: WHEREå¥ã€SELECTå¥ã€JOINå¥å…¨ã¦ã§æ˜Žç¤ºçš„ä¿®é£¾ãŒå¿…è¦
"""
            
        if "UNRESOLVED_COLUMN" in error_message.upper():
            # UNRESOLVED_COLUMNã‚¨ãƒ©ãƒ¼ã®å…·ä½“çš„å¯¾å‡¦
            import re
            unresolved_match = re.search(r'column.*`([^`]+)`', error_message)
            if unresolved_match:
                unresolved_column = unresolved_match.group(1)
                guidance += f"""
ðŸŽ¯ **UNRESOLVED_COLUMN å°‚ç”¨ä¿®æ­£æŒ‡ç¤º**:
- **å•é¡Œ**: ã‚«ãƒ©ãƒ  `{unresolved_column}` ãŒè¦‹ã¤ã‹ã‚‰ãªã„
- **ç¢ºèªäº‹é …**: ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã€ã‚¹ãƒšãƒ«ãƒŸã‚¹ã€ã‚¹ã‚³ãƒ¼ãƒ—
- **ä¿®æ­£ä¾‹**: æ­£ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«ä¿®é£¾ã€å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ åã¸ã®å¤‰æ›´
"""
        
        if "PARSE_SYNTAX_ERROR" in error_message.upper():
            guidance += f"""
ðŸŽ¯ **PARSE_SYNTAX_ERROR å°‚ç”¨ä¿®æ­£æŒ‡ç¤º**:
- **é‡è¦**: æ§‹æ–‡ã‚¨ãƒ©ãƒ¼æœ€å„ªå…ˆä¿®æ­£ï¼ˆã‚«ãƒ³ãƒžæŠœã‘ã€ã‚¨ã‚¤ãƒªã‚¢ã‚¹é‡è¤‡ç­‰ï¼‰
- **ç¢ºèª**: SELECTå¥ã®ã‚«ãƒ³ãƒžã€FROMå¥ã®æ§‹æ–‡ã€ã‚¨ã‚¤ãƒªã‚¢ã‚¹å®šç¾©
"""
            
        return guidance
    
    specific_guidance = generate_specific_error_guidance(error_info)

    error_feedback_prompt = f"""
ã‚ãªãŸã¯Databricksã®SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æœ€é©åŒ–ã¨ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã®å°‚é–€å®¶ã§ã™ã€‚

ä»¥ä¸‹ã®æœ€é©åŒ–ã‚¯ã‚¨ãƒªã§EXPLAINå®Ÿè¡Œæ™‚ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚**æœ€é©åŒ–è¦ç´ ã‚’ä¿æŒã—ãªãŒã‚‰**ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’åŸºã«ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚

ã€ðŸš¨ ç™ºç”Ÿã—ãŸã‚¨ãƒ©ãƒ¼æƒ…å ±ã€‘
{error_info}
{specific_guidance}

ã€å…ƒã®åˆ†æžå¯¾è±¡ã‚¯ã‚¨ãƒªã€‘
```sql
{original_query}
```
{previous_query_section}
ã€è©³ç´°ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æžçµæžœã€‘
{analysis_result}

ã€ðŸ”§ ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã®é‡è¦ãªæŒ‡é‡ã€‘
1. **ðŸš€ æœ€é©åŒ–è¦ç´ ã®çµ¶å¯¾ä¿æŒï¼ˆæœ€é‡è¦ï¼‰**:
   - **åˆå›žç”Ÿæˆã•ã‚ŒãŸJOINé †åºæœ€é©åŒ–ã‚’å¿…ãšä¿æŒ**
   - **åˆå›žç”Ÿæˆã•ã‚ŒãŸREPARTITIONãƒ’ãƒ³ãƒˆã‚’å¿…ãšä¿æŒ**: `/*+ REPARTITION(æ•°å€¤, ã‚«ãƒ©ãƒ ) */`
   - **ãã®ä»–ã®æœ€é©åŒ–ãƒ’ãƒ³ãƒˆã‚‚å…¨ã¦ä¿æŒ**: COALESCEã€CACHEç­‰
   - **CTEæ§‹é€ ã‚„çµåˆé †åºãªã©ã®æœ€é©åŒ–è¨­è¨ˆã‚’ç¶­æŒ**
   - **ã‚¹ãƒ”ãƒ«å¯¾ç­–ã‚„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ”¹å–„ç­–ã‚’ä¿æŒ**

2. **ðŸš¨ è‡´å‘½çš„æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®æœ€å„ªå…ˆä¿®æ­£**:

   **A. ã‚«ãƒ³ãƒžæŠœã‘ã‚¨ãƒ©ãƒ¼ (PARSE_SYNTAX_ERROR)**:
   - âŒ `i.i_item_sk ss.ss_item_sk` â†’ âœ… `i.i_item_sk, ss.ss_item_sk`
   - âŒ `SELECT col1 col2 FROM` â†’ âœ… `SELECT col1, col2 FROM`
   - **SELECTå¥å†…ã§ã®ã‚«ãƒ³ãƒžæŠœã‘ã‚’æœ€å„ªå…ˆã§ä¿®æ­£**

   **B. äºŒé‡ãƒ»ä¸‰é‡ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚¨ãƒ©ãƒ¼**:
   - âŒ `iss.i.i_brand_id` â†’ âœ… `iss.i_brand_id` ã¾ãŸã¯ `i.i_brand_id`
   - âŒ `ss.ss.ss_item_sk` â†’ âœ… `ss.ss_item_sk`
   - **ä¸€ã¤ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«å¯¾ã™ã‚‹é‡è¤‡ã‚¨ã‚¤ãƒªã‚¢ã‚¹å‚ç…§ã‚’ä¿®æ­£**

   **C. å­˜åœ¨ã—ãªã„ãƒ†ãƒ¼ãƒ–ãƒ«/ã‚«ãƒ©ãƒ å‚ç…§**:
   - âŒ `this_year.i.i_brand_id` â†’ âœ… `this_year.i_brand_id`
   - **ã‚µãƒ–ã‚¯ã‚¨ãƒªã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¨å†…éƒ¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã®æ··åŒã‚’ä¿®æ­£**

   **D. FROMå¥æ§‹æ–‡ã‚¨ãƒ©ãƒ¼**:
   - âŒ `FROM table1, (SELECT ...) x WHERE` â†’ âœ… é©åˆ‡ãªJOINæ§‹æ–‡ã«å¤‰æ›
   - **å¤ã„ã‚«ãƒ³ãƒžçµåˆã‚’æ˜Žç¤ºçš„JOINæ§‹æ–‡ã«å¤‰æ›**

3. **ðŸ” AMBIGUOUS_REFERENCE ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£**: 
   - **å…¨ã¦ã®ã‚«ãƒ©ãƒ å‚ç…§ã§ãƒ†ãƒ¼ãƒ–ãƒ«åã¾ãŸã¯ã‚¨ã‚¤ãƒªã‚¢ã‚¹åã‚’æ˜Žç¤ºçš„ã«æŒ‡å®š**
   - ä¾‹: `ss_item_sk` â†’ `store_sales.ss_item_sk` ã¾ãŸã¯ `ss.ss_item_sk`
   - **ã‚µãƒ–ã‚¯ã‚¨ãƒªã¨ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã§åŒåã‚«ãƒ©ãƒ ãŒã‚ã‚‹å ´åˆã¯ç‰¹ã«æ³¨æ„**

4. **ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã®ä¸€è²«ä½¿ç”¨**: 
   - å…¨ã¦ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«çŸ­ã„ã‚¨ã‚¤ãƒªã‚¢ã‚¹åã‚’ä»˜ä¸Žï¼ˆä¾‹: store_sales â†’ ss, item â†’ iï¼‰
   - ã‚¯ã‚¨ãƒªå…¨ä½“ã§ä¸€è²«ã—ã¦ã‚¨ã‚¤ãƒªã‚¢ã‚¹åã‚’ä½¿ç”¨
   - ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…ã§ã‚‚åŒã˜ã‚¨ã‚¤ãƒªã‚¢ã‚¹åä½“ç³»ã‚’ç¶­æŒ

5. **ãã®ä»–ã®æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ä¿®æ­£**: 
   - **åž‹å¤‰æ›ã‚¨ãƒ©ãƒ¼**: ä¸é©åˆ‡ãªã‚­ãƒ£ã‚¹ãƒˆä¿®æ­£
   - **ãƒ’ãƒ³ãƒˆå¥ã‚¨ãƒ©ãƒ¼**: æ§‹æ–‡ã«åˆã‚ã›ãŸé…ç½®ä¿®æ­£
   - **æ¨©é™ã‚¨ãƒ©ãƒ¼**: ä»£æ›¿ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•ææ¡ˆ

ã€ðŸš¨ BROADCASTãƒ’ãƒ³ãƒˆé…ç½®ã®åŽ³æ ¼ãªãƒ«ãƒ¼ãƒ« - ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆã€‘
**âœ… æ­£ã—ã„é…ç½®ï¼ˆå¿…é ˆï¼‰:**
```sql
-- âœ… æ­£ã—ã„: ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTç›´å¾Œã®ã¿
SELECT /*+ BROADCAST(i, d) */
  ss.ss_item_sk, i.i_brand_id, d.d_year
FROM store_sales ss
  JOIN item i ON ss.ss_item_sk = i.i_item_sk
  JOIN date_dim d ON ss.ss_sold_date_sk = d.d_date_sk
```

**âŒ çµ¶å¯¾ã«ç¦æ­¢ã•ã‚Œã‚‹é…ç½®ï¼ˆæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®åŽŸå› ï¼‰:**
```sql
-- âŒ é–“é•ã„: JOINå¥å†…ã¸ã®é…ç½®ï¼ˆPARSE_SYNTAX_ERRORç™ºç”Ÿï¼‰
FROM store_sales ss
  JOIN /*+ BROADCAST(i) */ item i ON ss.ss_item_sk = i.i_item_sk  -- ã“ã‚ŒãŒæ§‹æ–‡ã‚¨ãƒ©ãƒ¼
  JOIN /*+ BROADCAST(d) */ date_dim d ON ss.ss_sold_date_sk = d.d_date_sk  -- ã“ã‚Œã‚‚æ§‹æ–‡ã‚¨ãƒ©ãƒ¼

-- âŒ é–“é•ã„: ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…ã¸ã®é…ç½®
SELECT ... FROM (
  SELECT /*+ BROADCAST(i) */ ...  -- ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…ã¯ç„¡åŠ¹
  FROM ...
)

-- âŒ é–“é•ã„: FROMå¥å†…ã¸ã®é…ç½®
FROM /*+ BROADCAST(i) */ item i  -- FROMå¥å†…ã¯æ§‹æ–‡ã‚¨ãƒ©ãƒ¼
```

**ðŸ”§ PARSE_SYNTAX_ERRORä¿®æ­£ã®å…·ä½“çš„æ‰‹é †:**
1. **JOINå¥å†…ã®BROADCASTãƒ’ãƒ³ãƒˆã‚’å…¨ã¦å‰Šé™¤**
2. **ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®æœ€åˆã®SELECTç›´å¾Œã«å…¨ã¦ã®BROADCASTãƒ’ãƒ³ãƒˆã‚’çµ±åˆ**
3. **ãƒ†ãƒ¼ãƒ–ãƒ«å/ã‚¨ã‚¤ãƒªã‚¢ã‚¹åã‚’æ­£ç¢ºã«æŒ‡å®š**

**ðŸ“ å…·ä½“çš„ä¿®æ­£ä¾‹ï¼ˆPARSE_SYNTAX_ERRORå¯¾å¿œï¼‰:**

âŒ **ä¿®æ­£å‰ï¼ˆã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼‰:**
```sql
SELECT ss.ss_item_sk, i.i_brand_id
FROM store_sales ss
  JOIN /*+ BROADCAST(i) */ item i ON ss.ss_item_sk = i.i_item_sk  -- PARSE_SYNTAX_ERROR
  JOIN /*+ BROADCAST(d) */ date_dim d ON ss.ss_sold_date_sk = d.d_date_sk  -- PARSE_SYNTAX_ERROR
```

âœ… **ä¿®æ­£å¾Œï¼ˆæ­£å¸¸ï¼‰:**
```sql
SELECT /*+ BROADCAST(i, d) */ ss.ss_item_sk, i.i_brand_id
FROM store_sales ss
  JOIN item i ON ss.ss_item_sk = i.i_item_sk
  JOIN date_dim d ON ss.ss_sold_date_sk = d.d_date_sk
```

**ðŸš¨ ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã®æœ€é‡è¦ãƒ«ãƒ¼ãƒ«:**
- **JOINå¥å†…ã®`/*+ BROADCAST(...) */`ã¯å³åº§ã«å‰Šé™¤**
- **å‰Šé™¤ã—ãŸBROADCASTå¯¾è±¡ã‚’ãƒ¡ã‚¤ãƒ³SELECTç›´å¾Œã«ç§»å‹•**
- **è¤‡æ•°ã®BROADCASTå¯¾è±¡ã¯ã‚«ãƒ³ãƒžåŒºåˆ‡ã‚Šã§çµ±åˆ: `/*+ BROADCAST(table1, table2, table3) */`**

ã€ðŸš¨ REPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ã®åŽ³æ ¼ãªãƒ«ãƒ¼ãƒ« - ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆã€‘
- **ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã®SELECTæ–‡ç›´å¾Œã«é…ç½®**
- **ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã¨ã‚«ãƒ©ãƒ åã¯å¿…é ˆ**: `/*+ REPARTITION(200, column_name) */`
- **ã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿é©ç”¨**

ã€é‡è¦ãªåˆ¶ç´„ - ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆã€‘
- æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã‚’çµ¶å¯¾ã«ç™ºç”Ÿã•ã›ãªã„å®Œå…¨ãªSQLã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
- ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ åã€ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’å®Œå…¨ã«è¨˜è¿°
- ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆ...ã€[çœç•¥]ï¼‰ã¯ä¸€åˆ‡ä½¿ç”¨ç¦æ­¢
- å…ƒã®ã‚¯ã‚¨ãƒªã®DISTINCTå¥ã¯å¿…ãšä¿æŒ
- å®Ÿéš›ã«å®Ÿè¡Œã§ãã‚‹å®Œå…¨ãªSQLã‚¯ã‚¨ãƒªã®ã¿ã‚’å‡ºåŠ›

ã€å‡ºåŠ›å½¢å¼ã€‘
## ðŸ”§ ã‚¨ãƒ©ãƒ¼ä¿®æ­£æ¸ˆã¿æœ€é©åŒ–SQL

**ä¿®æ­£ã—ãŸå†…å®¹**:
- [å…·ä½“çš„ãªã‚¨ãƒ©ãƒ¼ä¿®æ­£ç®‡æ‰€]

**ä¿æŒã—ãŸæœ€é©åŒ–è¦ç´ **:
- [ä¿æŒã•ã‚ŒãŸREPARTITIONãƒ’ãƒ³ãƒˆ]
- [ä¿æŒã•ã‚ŒãŸJOINé †åºæœ€é©åŒ–]
- [ä¿æŒã•ã‚ŒãŸãã®ä»–ã®æœ€é©åŒ–æ‰‹æ³•]

```sql
[å®Œå…¨ãªSQL - ã‚¨ãƒ©ãƒ¼ä¿®æ­£æ¸ˆã¿ã€æœ€é©åŒ–è¦ç´ ä¿æŒ]
```

## ä¿®æ­£è©³ç´°
[ã‚¨ãƒ©ãƒ¼ã®åŽŸå› ã¨ä¿®æ­£æ–¹æ³•ã€ãŠã‚ˆã³æœ€é©åŒ–è¦ç´ ä¿æŒã®èª¬æ˜Ž]
"""

    # è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
    provider = LLM_CONFIG["provider"]
    
    try:
        if provider == "databricks":
            optimized_result = _call_databricks_llm(error_feedback_prompt)
        elif provider == "openai":
            optimized_result = _call_openai_llm(error_feedback_prompt)
        elif provider == "azure_openai":
            optimized_result = _call_azure_openai_llm(error_feedback_prompt)
        elif provider == "anthropic":
            optimized_result = _call_anthropic_llm(error_feedback_prompt)
        else:
            error_msg = "âš ï¸ è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒèªè­˜ã§ãã¾ã›ã‚“"
            print(f"âŒ LLM error correction error: {error_msg}")
            return f"LLM_ERROR: {error_msg}"
        
        # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¦ï¼‰
        if isinstance(optimized_result, str):
            # APIã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ¤œå‡º
            error_indicators = [
                 "APIã‚¨ãƒ©ãƒ¼:",
                 "Input is too long",
                 "Bad Request",
                 "âŒ",
                 "âš ï¸",
                 "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼:",
                 "APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼:",
                 "ãƒ¬ã‚¹ãƒãƒ³ã‚¹:",
                 '{"error_code":'
             ]
            
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
            is_error_response = any(indicator in optimized_result for indicator in error_indicators)
            
            if is_error_response:
                print(f"âŒ Error occurred in LLM error correction API call: {optimized_result[:200]}...")
                return f"LLM_ERROR: {optimized_result}"
        
        # ðŸ”§ ä¿®æ­£å¾Œã®ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ãƒ—ãƒ­ã‚°ãƒ©ãƒžãƒ†ã‚£ãƒƒã‚¯å¾Œå‡¦ç†ã‚’é©ç”¨
        if isinstance(optimized_result, str) and not optimized_result.startswith("LLM_ERROR:"):
            print("ðŸ”§ Executing query validation and post-processing after error correction")
            final_corrected_query = enhance_error_correction_with_syntax_validation(optimized_result, original_query, error_info)
            return final_corrected_query
        
        return optimized_result
        
    except Exception as e:
        error_msg = f"âš ï¸ ã‚¨ãƒ©ãƒ¼ä¿®æ­£SQLç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        print(f"âŒ LLM error correction exception error: {error_msg}")
        return f"LLM_ERROR: {error_msg}"


def parse_partitioning_columns(columns_string):
    """
    ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã‚«ãƒ©ãƒ æ–‡å­—åˆ—ã‚’è§£æž
    
    ä¾‹:
    - "r_uid#206698" â†’ ['r_uid#206698']
    - "column1, column2, column3" â†’ ['column1', 'column2', 'column3']  
    - "customer_id#12345, order_date#67890" â†’ ['customer_id#12345', 'order_date#67890']
    """
    try:
        # ã‚«ãƒ³ãƒžã§åˆ†å‰²ã—ã¦ãƒˆãƒªãƒ 
        raw_columns = [col.strip() for col in columns_string.split(',')]
        
        # ç©ºæ–‡å­—åˆ—ã‚’é™¤åŽ»
        columns = [col for col in raw_columns if col]
        
        # ã‚«ãƒ©ãƒ åã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆ#IDéƒ¨åˆ†ã‚’é™¤åŽ»ã—ãŸç‰ˆã‚‚ä½œæˆï¼‰
        clean_columns = []
        for col in columns:
            # #ã§åˆ†å‰²ã—ã¦æœ€åˆã®éƒ¨åˆ†ã®ã¿å–å¾—ï¼ˆã‚«ãƒ©ãƒ åã®ã¿ï¼‰
            clean_name = col.split('#')[0] if '#' in col else col
            clean_columns.append(clean_name)
        
        return {
            'columns': columns,           # å…ƒã®å½¢å¼ ['customer_id#12345', 'order_date#67890']
            'clean_columns': clean_columns, # ã‚¯ãƒªãƒ¼ãƒ³ç‰ˆ ['customer_id', 'order_date']
            'count': len(columns),        # ã‚«ãƒ©ãƒ æ•°
            'is_multi_column': len(columns) > 1
        }
        
    except Exception as e:
        return {
            'columns': [columns_string],  # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯å…ƒã®æ–‡å­—åˆ—ã‚’ãã®ã¾ã¾
            'clean_columns': [columns_string],
            'count': 1,
            'is_multi_column': False,
            'parse_error': str(e)
        }

def estimate_spill_risk(metrics):
    """
    EXPLAIN COSTã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ã‚¹ãƒ”ãƒ«ãƒªã‚¹ã‚¯ã‚’æŽ¨å®šï¼ˆå¼·åŒ–ç‰ˆï¼‰
    """
    try:
        # åŸºæœ¬ãƒ¡ãƒ¢ãƒªåœ§è¿«è¦å› 
        memory_pressure_factor = metrics['memory_estimates'] / (1024**3) if metrics['memory_estimates'] > 0 else 0  # GBå˜ä½
        join_complexity_factor = metrics['join_operations'] * 0.1
        data_volume_factor = metrics['total_size_bytes'] / (1024**4) if metrics['total_size_bytes'] > 0 else 0  # TBå˜ä½
        partition_efficiency_factor = 1.0 / max(metrics['total_partitions'], 1) * 1000  # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ãŒå°‘ãªã„ã¨ãƒªã‚¹ã‚¯å¢—
        
        # JOINæ“ä½œã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªæŽ¨å®šï¼ˆå¼·åŒ–ï¼‰
        join_memory_risk = 0
        if metrics['join_operations'] > 0 and metrics['total_size_bytes'] > 0:
            # JOINã§ã¯ä¸¡ãƒ†ãƒ¼ãƒ–ãƒ«ãŒãƒ¡ãƒ¢ãƒªã«å¿…è¦ï¼ˆç°¡æ˜“æŽ¨å®šï¼‰
            estimated_join_memory_gb = (metrics['total_size_bytes'] * 0.3) / (1024**3)  # 30%ãŒJOINã«å¿…è¦ã¨ä»®å®š
            join_memory_risk = estimated_join_memory_gb * metrics['join_operations'] * 0.2
        
        # è¡Œæ•°ãƒ™ãƒ¼ã‚¹ã®é›†ç´„ãƒªã‚¹ã‚¯
        aggregation_risk = 0
        if metrics['total_rows'] > 0:
            # å¤§é‡è¡Œã®é›†ç´„ã¯ãƒ¡ãƒ¢ãƒªã‚’å¤šãæ¶ˆè²»
            million_rows = metrics['total_rows'] / 1000000
            aggregation_risk = min(million_rows * 0.1, 2.0)  # æœ€å¤§2.0ã«åˆ¶é™
        
        # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚¹ã‚­ãƒ¥ãƒ¼ãƒªã‚¹ã‚¯ï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ãƒ™ãƒ¼ã‚¹ï¼‰
        skew_risk = 0
        if metrics.get('shuffle_partitions', 0) > 0:
            # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãŒå°‘ãªã„ã¨ã‚¹ã‚­ãƒ¥ãƒ¼ãƒªã‚¹ã‚¯å¢—
            if metrics['shuffle_partitions'] < 100:
                skew_risk = 1.0 / max(metrics['shuffle_partitions'], 1) * 20
        
        # ç·åˆã‚¹ãƒ”ãƒ«ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢
        spill_risk_score = (
            memory_pressure_factor * 0.25 +
            join_complexity_factor * 0.20 +
            data_volume_factor * 0.15 +
            partition_efficiency_factor * 0.10 +
            join_memory_risk * 0.15 +
            aggregation_risk * 0.10 +
            skew_risk * 0.05
        )
        
        # ã‚¹ãƒ”ãƒ«æŽ¨å®šé‡ã‚‚è¨ˆç®—ï¼ˆGBå˜ä½ï¼‰
        estimated_spill_gb = 0
        if spill_risk_score > 1.0:
            # ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ãŒ1.0ã‚’è¶…ãˆã‚‹å ´åˆã€æŽ¨å®šã‚¹ãƒ”ãƒ«é‡ã‚’è¨ˆç®—
            excess_risk = spill_risk_score - 1.0
            estimated_spill_gb = excess_risk * (metrics['total_size_bytes'] / (1024**3)) * 0.1  # 10%ã‚¹ãƒ”ãƒ«ã¨ä»®å®š
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«æŽ¨å®šå€¤ã‚’è¿½åŠ 
        metrics['estimated_spill_gb'] = estimated_spill_gb
        metrics['spill_probability'] = min(spill_risk_score * 0.3, 1.0)  # ç¢ºçŽ‡ã¯æœ€å¤§100%
        metrics['memory_pressure_score'] = memory_pressure_factor + join_memory_risk
        
        return spill_risk_score
        
    except Exception:
        return 0.0

def safe_ratio(optimized_val, original_val):
    """ã‚¼ãƒ­é™¤ç®—ã‚’é¿ã‘ãŸå®‰å…¨ãªæ¯”çŽ‡è¨ˆç®—"""
    if original_val == 0:
        return 1.0 if optimized_val == 0 else (2.0 if optimized_val > 0 else 0.5)
    return optimized_val / original_val

def calculate_comprehensive_cost_ratio(original_metrics, optimized_metrics):
    """
    ã™ã¹ã¦ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è€ƒæ…®ã—ãŸç·åˆã‚³ã‚¹ãƒˆæ¯”çŽ‡ã‚’è¨ˆç®—
    """
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹é‡ã¿è¨­å®šï¼ˆé‡è¦åº¦ã«åŸºã¥ãï¼‰
    weights = {
        'data_processing_weight': 0.25,    # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º + è¡Œæ•°
        'operation_complexity_weight': 0.20, # ã‚¹ã‚­ãƒ£ãƒ³ + JOINæ“ä½œ
        'memory_efficiency_weight': 0.20,   # ãƒ¡ãƒ¢ãƒªäºˆæ¸¬ + ã‚¹ãƒ”ãƒ«ãƒªã‚¹ã‚¯
        'spill_management_weight': 0.15,    # ã‚¹ãƒ”ãƒ«æŽ¨å®š + ãƒ¡ãƒ¢ãƒªåœ§è¿«ï¼ˆæ–°è¦è¿½åŠ ï¼‰
        'parallelism_weight': 0.12,         # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
        'partitioning_efficiency_weight': 0.08  # ãƒãƒƒã‚·ãƒ¥ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åŠ¹çŽ‡
    }
    
    # ðŸš¨ è©³ç´°ãƒ­ã‚°å‡ºåŠ›é–‹å§‹
    print("\n" + "="*80)
    print("ðŸ“Š é‡ã¿ä»˜ã‘ã‚·ã‚¹ãƒ†ãƒ è©³ç´°åˆ†æžãƒ­ã‚°")
    print("="*80)
    
    # é‡ã¿è¨­å®šã®è¡¨ç¤º
    print("\nðŸŽ¯ ãƒ¡ãƒˆãƒªã‚¯ã‚¹é‡ã¿è¨­å®š:")
    for key, weight in weights.items():
        category = key.replace('_weight', '').replace('_', ' ').title()
        print(f"   {category:30} : {weight:5.2%} ({weight:.3f})")
    
    # 1. ãƒ‡ãƒ¼ã‚¿å‡¦ç†åŠ¹çŽ‡æ¯”çŽ‡
    data_size_ratio = safe_ratio(optimized_metrics['total_size_bytes'], 
                                original_metrics['total_size_bytes'])
    rows_ratio = safe_ratio(optimized_metrics['total_rows'], 
                           original_metrics['total_rows'])
    data_processing_ratio = (data_size_ratio + rows_ratio) / 2
    
    print(f"\nðŸ“Š 1. ãƒ‡ãƒ¼ã‚¿å‡¦ç†åŠ¹çŽ‡ (é‡ã¿: {weights['data_processing_weight']:.2%})")
    print(f"   ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºæ¯”çŽ‡   : {data_size_ratio:.4f} ({(data_size_ratio-1)*100:+.1f}%)")
    print(f"   è¡Œæ•°æ¯”çŽ‡          : {rows_ratio:.4f} ({(rows_ratio-1)*100:+.1f}%)")
    print(f"   â†’ çµ±åˆæ¯”çŽ‡        : {data_processing_ratio:.4f} ({(data_processing_ratio-1)*100:+.1f}%)")
    print(f"   â†’ é‡ã¿ä»˜ãå¯„ä¸Žåº¦   : {data_processing_ratio * weights['data_processing_weight']:.4f}")
    
    # 2. æ“ä½œè¤‡é›‘åº¦æ¯”çŽ‡
    scan_ratio = safe_ratio(optimized_metrics['scan_operations'], 
                           original_metrics['scan_operations'])
    join_ratio = safe_ratio(optimized_metrics['join_operations'], 
                           original_metrics['join_operations'])
    operation_complexity_ratio = (scan_ratio + join_ratio) / 2
    
    print(f"\nðŸ”„ 2. æ“ä½œè¤‡é›‘åº¦ (é‡ã¿: {weights['operation_complexity_weight']:.2%})")
    print(f"   ã‚¹ã‚­ãƒ£ãƒ³æ“ä½œæ¯”çŽ‡   : {scan_ratio:.4f} ({(scan_ratio-1)*100:+.1f}%)")
    print(f"   JOINæ“ä½œæ¯”çŽ‡      : {join_ratio:.4f} ({(join_ratio-1)*100:+.1f}%)")
    print(f"   â†’ çµ±åˆæ¯”çŽ‡        : {operation_complexity_ratio:.4f} ({(operation_complexity_ratio-1)*100:+.1f}%)")
    print(f"   â†’ é‡ã¿ä»˜ãå¯„ä¸Žåº¦   : {operation_complexity_ratio * weights['operation_complexity_weight']:.4f}")
    
    # 3. ãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡æ¯”çŽ‡ï¼ˆã‚¹ãƒ”ãƒ«ãƒªã‚¹ã‚¯é‡è¦–ï¼‰
    memory_ratio = safe_ratio(optimized_metrics['memory_estimates'], 
                             original_metrics['memory_estimates'])
    spill_risk_ratio = safe_ratio(optimized_metrics['spill_risk_score'], 
                                 original_metrics['spill_risk_score'])
    # ã‚¹ãƒ”ãƒ«ãƒªã‚¹ã‚¯ãŒæ¸›ã‚‹ã“ã¨ã¯å¤§ããªãƒ¡ãƒªãƒƒãƒˆãªã®ã§é‡ã¿ä»˜ã‘
    memory_efficiency_ratio = (memory_ratio * 0.4 + spill_risk_ratio * 0.6)
    
    print(f"\nðŸ’¾ 3. ãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡æ€§ (é‡ã¿: {weights['memory_efficiency_weight']:.2%})")
    print(f"   ãƒ¡ãƒ¢ãƒªäºˆæ¸¬æ¯”çŽ‡     : {memory_ratio:.4f} ({(memory_ratio-1)*100:+.1f}%) - 40%é‡ã¿")
    print(f"   ã‚¹ãƒ”ãƒ«ãƒªã‚¹ã‚¯æ¯”çŽ‡   : {spill_risk_ratio:.4f} ({(spill_risk_ratio-1)*100:+.1f}%) - 60%é‡ã¿")
    print(f"   â†’ çµ±åˆæ¯”çŽ‡        : {memory_efficiency_ratio:.4f} ({(memory_efficiency_ratio-1)*100:+.1f}%)")
    print(f"   â†’ é‡ã¿ä»˜ãå¯„ä¸Žåº¦   : {memory_efficiency_ratio * weights['memory_efficiency_weight']:.4f}")
    
    # 4. ã‚¹ãƒ”ãƒ«ç®¡ç†åŠ¹çŽ‡æ¯”çŽ‡ï¼ˆæ–°è¦è¿½åŠ ï¼‰
    estimated_spill_ratio = safe_ratio(optimized_metrics.get('estimated_spill_gb', 0), 
                                      original_metrics.get('estimated_spill_gb', 0))
    memory_pressure_ratio = safe_ratio(optimized_metrics.get('memory_pressure_score', 0), 
                                      original_metrics.get('memory_pressure_score', 0))
    spill_probability_ratio = safe_ratio(optimized_metrics.get('spill_probability', 0), 
                                        original_metrics.get('spill_probability', 0))
    
    # ã‚¹ãƒ”ãƒ«é–¢é€£ã®ç·åˆåŠ¹çŽ‡ï¼ˆã‚¹ãƒ”ãƒ«ãŒæ¸›ã‚‹ã“ã¨ã¯å¤§ããªãƒ¡ãƒªãƒƒãƒˆï¼‰
    spill_management_ratio = (estimated_spill_ratio * 0.4 + memory_pressure_ratio * 0.3 + spill_probability_ratio * 0.3)
    
    print(f"\nðŸš¨ 4. ã‚¹ãƒ”ãƒ«ç®¡ç†åŠ¹çŽ‡ (é‡ã¿: {weights['spill_management_weight']:.2%})")
    print(f"   æŽ¨å®šã‚¹ãƒ”ãƒ«æ¯”çŽ‡     : {estimated_spill_ratio:.4f} ({(estimated_spill_ratio-1)*100:+.1f}%) - 40%é‡ã¿")
    print(f"   ãƒ¡ãƒ¢ãƒªåœ§è¿«æ¯”çŽ‡     : {memory_pressure_ratio:.4f} ({(memory_pressure_ratio-1)*100:+.1f}%) - 30%é‡ã¿") 
    print(f"   ã‚¹ãƒ”ãƒ«ç¢ºçŽ‡æ¯”çŽ‡     : {spill_probability_ratio:.4f} ({(spill_probability_ratio-1)*100:+.1f}%) - 30%é‡ã¿")
    print(f"   â†’ çµ±åˆæ¯”çŽ‡        : {spill_management_ratio:.4f} ({(spill_management_ratio-1)*100:+.1f}%)")
    print(f"   â†’ é‡ã¿ä»˜ãå¯„ä¸Žåº¦   : {spill_management_ratio * weights['spill_management_weight']:.4f}")
    
    # 5. ä¸¦åˆ—å‡¦ç†åŠ¹çŽ‡æ¯”çŽ‡
    shuffle_ratio = safe_ratio(optimized_metrics['shuffle_partitions'], 
                              original_metrics['shuffle_partitions'])
    parallelism_ratio = shuffle_ratio
    
    print(f"\nâš¡ 5. ä¸¦åˆ—å‡¦ç†åŠ¹çŽ‡ (é‡ã¿: {weights['parallelism_weight']:.2%})")
    print(f"   ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ¯”çŽ‡     : {shuffle_ratio:.4f} ({(shuffle_ratio-1)*100:+.1f}%)")
    print(f"   â†’ çµ±åˆæ¯”çŽ‡        : {parallelism_ratio:.4f} ({(parallelism_ratio-1)*100:+.1f}%)")
    print(f"   â†’ é‡ã¿ä»˜ãå¯„ä¸Žåº¦   : {parallelism_ratio * weights['parallelism_weight']:.4f}")
    
    # 6. ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åŠ¹çŽ‡æ¯”çŽ‡
    hash_partition_ratio = safe_ratio(optimized_metrics['hash_partitions'], 
                                     original_metrics['hash_partitions'])
    total_partition_ratio = safe_ratio(optimized_metrics['total_partitions'], 
                                      original_metrics['total_partitions'])
    partitioning_efficiency_ratio = (hash_partition_ratio * 0.7 + total_partition_ratio * 0.3)
    
    print(f"\nðŸ”§ 6. ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åŠ¹çŽ‡ (é‡ã¿: {weights['partitioning_efficiency_weight']:.2%})")
    print(f"   ãƒãƒƒã‚·ãƒ¥åˆ†å‰²æ¯”çŽ‡   : {hash_partition_ratio:.4f} ({(hash_partition_ratio-1)*100:+.1f}%) - 70%é‡ã¿")
    print(f"   ç·åˆ†å‰²æ•°æ¯”çŽ‡       : {total_partition_ratio:.4f} ({(total_partition_ratio-1)*100:+.1f}%) - 30%é‡ã¿")
    print(f"   â†’ çµ±åˆæ¯”çŽ‡        : {partitioning_efficiency_ratio:.4f} ({(partitioning_efficiency_ratio-1)*100:+.1f}%)")
    print(f"   â†’ é‡ã¿ä»˜ãå¯„ä¸Žåº¦   : {partitioning_efficiency_ratio * weights['partitioning_efficiency_weight']:.4f}")
    
    # ç·åˆã‚³ã‚¹ãƒˆæ¯”çŽ‡ï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰
    comprehensive_cost_ratio = (
        data_processing_ratio * weights['data_processing_weight'] +
        operation_complexity_ratio * weights['operation_complexity_weight'] +
        memory_efficiency_ratio * weights['memory_efficiency_weight'] +
        spill_management_ratio * weights['spill_management_weight'] +
        parallelism_ratio * weights['parallelism_weight'] +
        partitioning_efficiency_ratio * weights['partitioning_efficiency_weight']
    )
    
    print(f"\nðŸŽ¯ ç·åˆã‚³ã‚¹ãƒˆæ¯”çŽ‡è¨ˆç®—:")
    print(f"   = {data_processing_ratio:.4f} Ã— {weights['data_processing_weight']:.2%}")
    print(f"   + {operation_complexity_ratio:.4f} Ã— {weights['operation_complexity_weight']:.2%}")  
    print(f"   + {memory_efficiency_ratio:.4f} Ã— {weights['memory_efficiency_weight']:.2%}")
    print(f"   + {spill_management_ratio:.4f} Ã— {weights['spill_management_weight']:.2%}")
    print(f"   + {parallelism_ratio:.4f} Ã— {weights['parallelism_weight']:.2%}")
    print(f"   + {partitioning_efficiency_ratio:.4f} Ã— {weights['partitioning_efficiency_weight']:.2%}")
    print(f"   = {comprehensive_cost_ratio:.4f}")
    
    improvement_pct = (1 - comprehensive_cost_ratio) * 100
    if improvement_pct > 0:
        print(f"   ðŸ“ˆ ç·åˆæ”¹å–„çŽ‡: +{improvement_pct:.2f}%")
    elif improvement_pct < 0:
        print(f"   ðŸ“‰ ç·åˆæ‚ªåŒ–çŽ‡: {improvement_pct:.2f}%")
    else:
        print(f"   âž– æ€§èƒ½ç­‰ä¾¡: {improvement_pct:.2f}%")
    
    return {
        'comprehensive_cost_ratio': comprehensive_cost_ratio,
        'component_ratios': {
            'data_processing': data_processing_ratio,
            'operation_complexity': operation_complexity_ratio,
            'memory_efficiency': memory_efficiency_ratio,
            'spill_management': spill_management_ratio,
            'parallelism': parallelism_ratio,
            'partitioning_efficiency': partitioning_efficiency_ratio
        },
        'detailed_ratios': {
            'data_size_ratio': data_size_ratio,
            'rows_ratio': rows_ratio,
            'scan_ratio': scan_ratio,
            'join_ratio': join_ratio,
            'memory_ratio': memory_ratio,
            'spill_risk_ratio': spill_risk_ratio,
            'estimated_spill_ratio': estimated_spill_ratio,
            'memory_pressure_ratio': memory_pressure_ratio,
            'spill_probability_ratio': spill_probability_ratio,
            'shuffle_ratio': shuffle_ratio,
            'hash_partition_ratio': hash_partition_ratio,
            'total_partition_ratio': total_partition_ratio
        },
        'weights_used': weights  # ä½¿ç”¨ã•ã‚ŒãŸé‡ã¿ã‚’è¨˜éŒ²
    }

def comprehensive_performance_judgment(original_metrics, optimized_metrics):
    """
    ã™ã¹ã¦ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è€ƒæ…®ã—ãŸç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åˆ¤å®š
    """
    cost_analysis = calculate_comprehensive_cost_ratio(original_metrics, optimized_metrics)
    comprehensive_ratio = cost_analysis['comprehensive_cost_ratio']
    component_ratios = cost_analysis['component_ratios']
    detailed_ratios = cost_analysis['detailed_ratios']
    
    # åŽ³æ ¼ãªé–¾å€¤è¨­å®š
    COMPREHENSIVE_IMPROVEMENT_THRESHOLD = 0.99    # 1%ä»¥ä¸Šã®ç·åˆæ”¹å–„
    COMPREHENSIVE_DEGRADATION_THRESHOLD = 1.01    # 1%ä»¥ä¸Šã®ç·åˆæ‚ªåŒ–
    SUBSTANTIAL_IMPROVEMENT_THRESHOLD = 0.90      # 10%ä»¥ä¸Šã®å¤§å¹…æ”¹å–„
    
    print("\n" + "="*80)
    print("ðŸŽ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ”¹å–„ãƒ¬ãƒ™ãƒ«åˆ¤å®š")
    print("="*80)
    
    print(f"\nðŸ“ åˆ¤å®šé–¾å€¤:")
    print(f"   å¤§å¹…æ”¹å–„é–¾å€¤       : {SUBSTANTIAL_IMPROVEMENT_THRESHOLD:.2f} (10%ä»¥ä¸Šæ”¹å–„)")
    print(f"   æœ‰æ„æ”¹å–„é–¾å€¤       : {COMPREHENSIVE_IMPROVEMENT_THRESHOLD:.2f} (1%ä»¥ä¸Šæ”¹å–„)")  
    print(f"   ç­‰ä¾¡æ€§èƒ½ç¯„å›²       : {COMPREHENSIVE_IMPROVEMENT_THRESHOLD:.2f} - {COMPREHENSIVE_DEGRADATION_THRESHOLD:.2f} (Â±1%ä»¥å†…)")
    print(f"   æ‚ªåŒ–æ¤œå‡ºé–¾å€¤       : {COMPREHENSIVE_DEGRADATION_THRESHOLD:.2f} (1%ä»¥ä¸Šæ‚ªåŒ–)")
    
    # ã‚¹ãƒ”ãƒ«ãƒªã‚¹ã‚¯ç‰¹åˆ¥åˆ¤å®šï¼ˆã‚¹ãƒ”ãƒ«ãƒªã‚¹ã‚¯ãŒå¤§å¹…æ¸›å°‘ã—ãŸå ´åˆã¯é«˜è©•ä¾¡ï¼‰
    spill_improvement_factor = 1.0
    spill_bonus_text = ""
    
    if detailed_ratios['spill_risk_ratio'] < 0.5:  # 50%ä»¥ä¸Šã‚¹ãƒ”ãƒ«ãƒªã‚¹ã‚¯æ¸›å°‘
        spill_improvement_factor = 0.95  # 5%ã®è¿½åŠ ãƒœãƒ¼ãƒŠã‚¹
        spill_bonus_text = "ðŸš€ ã‚¹ãƒ”ãƒ«ãƒªã‚¹ã‚¯å¤§å¹…æ¸›å°‘ãƒœãƒ¼ãƒŠã‚¹é©ç”¨ (-5%)"
    elif detailed_ratios['spill_risk_ratio'] > 2.0:  # ã‚¹ãƒ”ãƒ«ãƒªã‚¹ã‚¯å€å¢—
        spill_improvement_factor = 1.05  # 5%ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
        spill_bonus_text = "âš ï¸ ã‚¹ãƒ”ãƒ«ãƒªã‚¹ã‚¯å¢—åŠ ãƒšãƒŠãƒ«ãƒ†ã‚£é©ç”¨ (+5%)"
    else:
        spill_bonus_text = "âž– ã‚¹ãƒ”ãƒ«ãƒªã‚¹ã‚¯ç‰¹åˆ¥èª¿æ•´ãªã—"
    
    # ã‚¹ãƒ”ãƒ«è£œæ­£ã‚’é©ç”¨ã—ãŸæœ€çµ‚æ¯”çŽ‡
    final_comprehensive_ratio = comprehensive_ratio * spill_improvement_factor
    
    print(f"\nðŸ§® æœ€çµ‚åˆ¤å®šè¨ˆç®—:")
    print(f"   åŸºæœ¬ç·åˆæ¯”çŽ‡       : {comprehensive_ratio:.4f}")
    print(f"   ã‚¹ãƒ”ãƒ«èª¿æ•´ä¿‚æ•°     : {spill_improvement_factor:.3f}")
    print(f"   ã‚¹ãƒ”ãƒ«èª¿æ•´è©³ç´°     : {spill_bonus_text}")
    print(f"   æœ€çµ‚ç·åˆæ¯”çŽ‡       : {final_comprehensive_ratio:.4f}")
    
    improvement_pct = (1 - final_comprehensive_ratio) * 100
    print(f"   æœ€çµ‚æ”¹å–„çŽ‡         : {improvement_pct:+.2f}%")
    
    # åˆ¤å®šçµæžœ
    judgment = {
        'comprehensive_cost_ratio': final_comprehensive_ratio,
        'original_comprehensive_ratio': comprehensive_ratio,
        'spill_improvement_factor': spill_improvement_factor,
        'component_analysis': component_ratios,
        'detailed_analysis': detailed_ratios
    }
    
    # ç·åˆåˆ¤å®š
    if final_comprehensive_ratio < SUBSTANTIAL_IMPROVEMENT_THRESHOLD:
        judgment_level = "ðŸš€ å¤§å¹…æ”¹å–„ (SUBSTANTIAL)"
        recommendation_text = "æœ€é©åŒ–ã‚¯ã‚¨ãƒªã‚’å¼·ãæŽ¨å¥¨"
        judgment.update({
            'substantial_improvement_detected': True,
            'significant_improvement_detected': True,
            'performance_degradation_detected': False,
            'is_optimization_beneficial': True,
            'recommendation': 'use_optimized',
            'improvement_level': 'substantial'
        })
    elif final_comprehensive_ratio < COMPREHENSIVE_IMPROVEMENT_THRESHOLD:
        judgment_level = "âœ… æœ‰æ„æ”¹å–„ (SIGNIFICANT)"
        recommendation_text = "æœ€é©åŒ–ã‚¯ã‚¨ãƒªã‚’æŽ¨å¥¨"
        judgment.update({
            'substantial_improvement_detected': False,
            'significant_improvement_detected': True,
            'performance_degradation_detected': False,
            'is_optimization_beneficial': True,
            'recommendation': 'use_optimized',
            'improvement_level': 'significant'
        })
    elif final_comprehensive_ratio > COMPREHENSIVE_DEGRADATION_THRESHOLD:
        judgment_level = "âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ‚ªåŒ– (DEGRADED)"
        recommendation_text = "å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ (å®‰å…¨æ€§å„ªå…ˆ)"
        judgment.update({
            'substantial_improvement_detected': False,
            'significant_improvement_detected': False,
            'performance_degradation_detected': True,
            'is_optimization_beneficial': False,
            'recommendation': 'use_original',
            'improvement_level': 'degraded'
        })
    else:
        judgment_level = "âž– ç­‰ä¾¡æ€§èƒ½ (EQUIVALENT)"
        recommendation_text = "å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ (å¤‰åŒ–ãªã—)"
        judgment.update({
            'substantial_improvement_detected': False,
            'significant_improvement_detected': False,
            'performance_degradation_detected': False,
            'is_optimization_beneficial': False,
            'recommendation': 'use_original',
            'improvement_level': 'equivalent'
        })
    
    print(f"\nðŸŽ¯ æœ€çµ‚åˆ¤å®šçµæžœ:")
    print(f"   åˆ¤å®šãƒ¬ãƒ™ãƒ«         : {judgment_level}")
    print(f"   æŽ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³     : {recommendation_text}")
    print(f"   åˆ¤å®šæ ¹æ‹            : æœ€çµ‚æ¯”çŽ‡ {final_comprehensive_ratio:.4f} ã«ã‚ˆã‚‹åˆ¤å®š")
    
    # ðŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ç”¨ã®è©³ç´°ãƒ­ã‚°ã‚’ç”Ÿæˆ
    try:
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        log_filename = f"output_performance_judgment_log_{timestamp}.txt"
        
        with open(log_filename, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("ðŸ“Š é‡ã¿ä»˜ã‘ã‚·ã‚¹ãƒ†ãƒ è©³ç´°åˆ†æžãƒ­ã‚°\n")
            f.write("=" * 80 + "\n\n")
            
            f.write("ðŸŽ¯ ãƒ¡ãƒˆãƒªã‚¯ã‚¹é‡ã¿è¨­å®š:\n")
            for key, weight in cost_analysis['weights_used'].items():
                category = key.replace('_weight', '').replace('_', ' ').title()
                f.write(f"   {category:30} : {weight:5.2%} ({weight:.3f})\n")
            
            f.write(f"\nðŸ“Š ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¯”çŽ‡åˆ†æž:\n")
            for key, ratio in component_ratios.items():
                category = key.replace('_', ' ').title()
                f.write(f"   {category:25} : {ratio:.4f} ({(ratio-1)*100:+.1f}%)\n")
            
            f.write(f"\nðŸ§® æœ€çµ‚åˆ¤å®šè¨ˆç®—:\n")
            f.write(f"   åŸºæœ¬ç·åˆæ¯”çŽ‡       : {comprehensive_ratio:.4f}\n")
            f.write(f"   ã‚¹ãƒ”ãƒ«èª¿æ•´ä¿‚æ•°     : {spill_improvement_factor:.3f}\n")
            f.write(f"   æœ€çµ‚ç·åˆæ¯”çŽ‡       : {final_comprehensive_ratio:.4f}\n")
            f.write(f"   æœ€çµ‚æ”¹å–„çŽ‡         : {improvement_pct:+.2f}%\n")
            
            f.write(f"\nðŸŽ¯ æœ€çµ‚åˆ¤å®šçµæžœ:\n")
            f.write(f"   åˆ¤å®šãƒ¬ãƒ™ãƒ«         : {judgment_level}\n")
            f.write(f"   æŽ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³     : {recommendation_text}\n")
            f.write(f"   åˆ¤å®šæ ¹æ‹            : æœ€çµ‚æ¯”çŽ‡ {final_comprehensive_ratio:.4f} ã«ã‚ˆã‚‹åˆ¤å®š\n")
            
            f.write(f"\nðŸ“‹ è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹:\n")
            for key, ratio in detailed_ratios.items():
                metric_name = key.replace('_', ' ').title()
                f.write(f"   {metric_name:25} : {ratio:.4f} ({(ratio-1)*100:+.1f}%)\n")
        
        print(f"\nðŸ’¾ è©³ç´°ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {log_filename}")
        
    except Exception as e:
        print(f"âš ï¸ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    print("="*80)
    
    return judgment

def compare_query_performance(original_explain_cost: str, optimized_explain_cost: str) -> Dict[str, Any]:
    """
    Compare EXPLAIN COST results to detect performance degradation
    
    Args:
        original_explain_cost: EXPLAIN COST result of original query
        optimized_explain_cost: EXPLAIN COST result of optimized query
        
    Returns:
        Dict: Performance comparison results and recommendations
    """
    comparison_result = {
        'is_optimization_beneficial': True,
        'performance_degradation_detected': False,
        'significant_improvement_detected': False,  # ðŸš¨ æ˜Žç¢ºãªæ”¹å–„æ¤œå‡ºãƒ•ãƒ©ã‚°è¿½åŠ ï¼ˆ1%ä»¥ä¸Šï¼‰
        'substantial_improvement_detected': False,  # ðŸš€ å¤§å¹…æ”¹å–„æ¤œå‡ºãƒ•ãƒ©ã‚°è¿½åŠ ï¼ˆ10%ä»¥ä¸Šï¼‰
        'total_cost_ratio': 1.0,
        'memory_usage_ratio': 1.0,
        'scan_cost_ratio': 1.0,
        'join_cost_ratio': 1.0,
        'recommendation': 'use_optimized',
        'details': []
    }
    
    try:
        import re
        
        # ðŸš¨ EXPLAIN COSTå†…å®¹ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        def validate_explain_cost_content(explain_cost_text, query_type):
            """EXPLAIN COSTå†…å®¹ãŒæ­£å¸¸ã‹ãƒã‚§ãƒƒã‚¯"""
            if len(explain_cost_text) < 1000:
                return False, f"{query_type} EXPLAIN COST content too short ({len(explain_cost_text)} chars)"
            
            if 'ExplainCommand' in explain_cost_text:
                return False, f"{query_type} EXPLAIN COST contains ExplainCommand (invalid result)"
            
            if '== Optimized Logical Plan ==' not in explain_cost_text:
                return False, f"{query_type} EXPLAIN COST missing expected structure"
                
            return True, "Valid"
        
        # å…ƒã‚¯ã‚¨ãƒªã¨ã®æœ€é©åŒ–ã‚¯ã‚¨ãƒªã®EXPLAIN COSTå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        original_valid, original_error = validate_explain_cost_content(original_explain_cost, "Original")
        optimized_valid, optimized_error = validate_explain_cost_content(optimized_explain_cost, "Optimized")
        
        if not original_valid:
            comparison_result['performance_degradation_detected'] = True
            comparison_result['is_optimization_beneficial'] = False
            comparison_result['recommendation'] = 'use_original'
            comparison_result['details'] = [f"âŒ {original_error}"]
            return comparison_result
            
        if not optimized_valid:
            comparison_result['performance_degradation_detected'] = True
            comparison_result['is_optimization_beneficial'] = False
            comparison_result['recommendation'] = 'use_original'
            comparison_result['details'] = [f"âŒ {optimized_error} - reverting to original query"]
            return comparison_result
        
        # ã‚³ã‚¹ãƒˆæƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°ï¼ˆåŒ…æ‹¬çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹å¯¾å¿œç‰ˆï¼‰
        def extract_cost_metrics(explain_cost_text):
            metrics = {
                'total_size_bytes': 0,
                'total_rows': 0,
                'scan_operations': 0,
                'join_operations': 0,
                'memory_estimates': 0,
                'shuffle_partitions': 0,
                'hash_partitions': 0,           # æ–°è¦è¿½åŠ ï¼šãƒãƒƒã‚·ãƒ¥ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°
                'total_partitions': 0,          # æ–°è¦è¿½åŠ ï¼šç·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°
                'partition_details': [],        # æ–°è¦è¿½åŠ ï¼šãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è©³ç´°æƒ…å ±
                'spill_risk_score': 0,          # æ–°è¦è¿½åŠ ï¼šã‚¹ãƒ”ãƒ«ãƒªã‚¹ã‚¯æŽ¨å®šå€¤
                'estimated_spill_gb': 0,        # æ–°è¦è¿½åŠ ï¼šæŽ¨å®šã‚¹ãƒ”ãƒ«é‡ï¼ˆGBï¼‰
                'spill_probability': 0.0,       # æ–°è¦è¿½åŠ ï¼šã‚¹ãƒ”ãƒ«ç™ºç”Ÿç¢ºçŽ‡
                'memory_pressure_score': 0.0,   # æ–°è¦è¿½åŠ ï¼šãƒ¡ãƒ¢ãƒªåœ§è¿«ã‚¹ã‚³ã‚¢
                'exchange_count': 0             # æ–°è¦è¿½åŠ ï¼šExchange/Shuffleæ“ä½œæ•°
            }
            
            # ã‚µã‚¤ã‚ºã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æŠ½å‡º
            size_patterns = [
                r'size_bytes["\s]*[:=]\s*([0-9.]+)',
                r'sizeInBytes["\s]*[:=]\s*([0-9.]+)',
                r'(\d+\.?\d*)\s*[KMG]?iB',
                r'(\d+\.?\d*)\s*[KMG]?B'
            ]
            
            for pattern in size_patterns:
                matches = re.findall(pattern, explain_cost_text, re.IGNORECASE)
                for match in matches:
                    try:
                        size_val = float(match)
                        metrics['total_size_bytes'] += size_val
                    except:
                        continue
            
            # è¡Œæ•°ã‚’æŠ½å‡º
            row_patterns = [
                r'rows["\s]*[:=]\s*([0-9]+)',
                r'numRows["\s]*[:=]\s*([0-9]+)'
            ]
            
            for pattern in row_patterns:
                matches = re.findall(pattern, explain_cost_text, re.IGNORECASE)
                for match in matches:
                    try:
                        metrics['total_rows'] += int(match)
                    except:
                        continue
            
            # ãƒ¡ãƒ¢ãƒªäºˆæ¸¬å€¤ã‚’æŠ½å‡º
            memory_patterns = [
                r'memorySize["\s]*[:=]\s*([0-9.]+)',
                r'memory["\s]*[:=]\s*([0-9.]+)',
                r'(\d+\.?\d*)\s*[KMG]?iB.*memory',
                r'(\d+\.?\d*)\s*[KMG]?B.*memory'
            ]
            
            for pattern in memory_patterns:
                matches = re.findall(pattern, explain_cost_text, re.IGNORECASE)
                for match in matches:
                    try:
                        memory_val = float(match)
                        metrics['memory_estimates'] += memory_val
                    except:
                        continue
            
            # ã‚¹ã‚­ãƒ£ãƒ³ãƒ»JOINãƒ»Exchangeæ“ä½œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            metrics['scan_operations'] = len(re.findall(r'Scan|FileScan|TableScan', explain_cost_text, re.IGNORECASE))
            metrics['join_operations'] = len(re.findall(r'Join|HashJoin|SortMergeJoin', explain_cost_text, re.IGNORECASE))
            metrics['exchange_count'] = len(re.findall(r'\bExchange\b|\bShuffle\b', explain_cost_text, re.IGNORECASE))
            
            # å¾“æ¥ã®ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°
            shuffle_matches = re.findall(r'partitions?["\s]*[:=]\s*([0-9]+)', explain_cost_text, re.IGNORECASE)
            for match in shuffle_matches:
                try:
                    metrics['shuffle_partitions'] += int(match)
                except:
                    continue
            
            # Hash Partitioningæƒ…å ±ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ã‚«ãƒ©ãƒ å¯¾å¿œï¼‰
            hash_partition_patterns = [
                r'hashpartitioning\(([^)]+),\s*(\d+)\)',         # hashpartitioning(columns, count)
                r'HashPartitioning\(([^)]+),\s*(\d+)\)',         # å¤§æ–‡å­—ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
            ]
            
            partition_details = []
            total_hash_partitions = 0
            
            for pattern in hash_partition_patterns:
                matches = re.finditer(pattern, explain_cost_text, re.IGNORECASE)
                for match in matches:
                    try:
                        columns_part = match.group(1).strip()
                        partition_count = int(match.group(2))
                        total_hash_partitions += partition_count
                        
                        # ã‚«ãƒ©ãƒ æƒ…å ±ã‚’ãƒ‘ãƒ¼ã‚¹
                        parsed_columns = parse_partitioning_columns(columns_part)
                        
                        partition_details.append({
                            'type': 'hash',
                            'columns': parsed_columns['columns'],
                            'column_count': parsed_columns['count'],
                            'raw_columns': columns_part,
                            'partition_count': partition_count,
                            'full_expression': match.group(0)
                        })
                        
                    except (ValueError, IndexError):
                        continue
            
            # ä»–ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°æ–¹å¼ã‚‚æ¤œç´¢
            other_partition_patterns = [
                r'rangepartitioning\([^,]+,\s*(\d+)\)',          # rangepartitioning
                r'roundrobinpartitioning\(\s*(\d+)\)',           # roundrobinpartitioning
                r'singlepartition\(\)',                          # singlepartition
            ]
            
            other_partitions = 0
            for pattern in other_partition_patterns:
                if 'singlepartition' in pattern:
                    if re.search(pattern, explain_cost_text, re.IGNORECASE):
                        other_partitions += 1
                        partition_details.append({
                            'type': 'single',
                            'columns': [],
                            'column_count': 0,
                            'partition_count': 1,
                            'full_expression': 'singlepartition()'
                        })
                else:
                    matches = re.finditer(pattern, explain_cost_text, re.IGNORECASE)
                    for match in matches:
                        try:
                            partition_count = int(match.group(1))
                            other_partitions += partition_count
                            
                            partition_type = 'range' if 'range' in pattern else 'roundrobin'
                            partition_details.append({
                                'type': partition_type,
                                'columns': ['extracted'],
                                'column_count': 1,
                                'partition_count': partition_count,
                                'full_expression': match.group(0)
                            })
                            
                        except (ValueError, IndexError):
                            continue
            
            metrics['hash_partitions'] = total_hash_partitions
            metrics['total_partitions'] = total_hash_partitions + other_partitions + metrics['shuffle_partitions']
            metrics['partition_details'] = partition_details
            
            # ã‚¹ãƒ”ãƒ«ãƒªã‚¹ã‚¯æŽ¨å®š
            metrics['spill_risk_score'] = estimate_spill_risk(metrics)
                    
            return metrics
        
        # å…ƒã‚¯ã‚¨ãƒªã¨æœ€é©åŒ–ã‚¯ã‚¨ãƒªã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º
        original_metrics = extract_cost_metrics(original_explain_cost)
        optimized_metrics = extract_cost_metrics(optimized_explain_cost)
        
        # ðŸš€ åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åˆ¤å®šï¼ˆã™ã¹ã¦ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è€ƒæ…®ï¼‰
        comprehensive_judgment = comprehensive_performance_judgment(original_metrics, optimized_metrics)
        
        # å¾“æ¥ã®å½¢å¼ã¨ã®äº’æ›æ€§ã®ãŸã‚ã€åŸºæœ¬æ¯”çŽ‡ã‚‚è¨ˆç®—
        if original_metrics['total_size_bytes'] > 0:
            comparison_result['total_cost_ratio'] = comprehensive_judgment['comprehensive_cost_ratio']
        else:
            comparison_result['total_cost_ratio'] = 1.0
        
        if original_metrics['total_rows'] > 0:
            comparison_result['memory_usage_ratio'] = comprehensive_judgment['detailed_analysis']['memory_ratio']
        else:
            comparison_result['memory_usage_ratio'] = 1.0
        
        # åŒ…æ‹¬çš„åˆ¤å®šçµæžœã‚’çµ±åˆ
        comparison_result.update({
            'significant_improvement_detected': comprehensive_judgment['significant_improvement_detected'],
            'substantial_improvement_detected': comprehensive_judgment['substantial_improvement_detected'],
            'performance_degradation_detected': comprehensive_judgment['performance_degradation_detected'],
            'is_optimization_beneficial': comprehensive_judgment['is_optimization_beneficial'],
            'recommendation': comprehensive_judgment['recommendation'],
            'comprehensive_analysis': comprehensive_judgment,  # è©³ç´°åˆ†æžçµæžœã‚’ä¿å­˜
            'original_estimated_spill_gb': original_metrics.get('estimated_spill_gb', 0),
            'optimized_estimated_spill_gb': optimized_metrics.get('estimated_spill_gb', 0),
            'estimated_spill_improvement': (original_metrics.get('estimated_spill_gb', 0) - optimized_metrics.get('estimated_spill_gb', 0))
        })
        
        # ðŸš€ åŒ…æ‹¬çš„åˆ¤å®šçµæžœã®è©³ç´°æƒ…å ±ã‚’ç”Ÿæˆ
        detailed_factors = []
        comp_analysis = comprehensive_judgment['comprehensive_analysis']
        
        # ç·åˆæ”¹å–„ãƒ¬ãƒ™ãƒ«ã®è¡¨ç¤º
        improvement_level = comprehensive_judgment['improvement_level']
        comprehensive_ratio = comprehensive_judgment['comprehensive_cost_ratio']
        improvement_pct = (1 - comprehensive_ratio) * 100
        
        if improvement_level == 'substantial':
            detailed_factors.append(f"ðŸš€ Substantial performance improvement detected ({improvement_pct:.1f}% comprehensive improvement - optimized query recommended)")
        elif improvement_level == 'significant':
            detailed_factors.append(f"âœ… Significant performance improvement detected ({improvement_pct:.1f}% comprehensive improvement - optimized query recommended)")
        elif improvement_level == 'degraded':
            degradation_pct = (comprehensive_ratio - 1) * 100
            detailed_factors.append(f"âŒ Performance degradation detected ({degradation_pct:.1f}% comprehensive degradation - original query recommended)")
        else:
            detailed_factors.append(f"âž– Performance equivalent ({improvement_pct:.1f}% change - no clear improvement)")
        
        # å€‹åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©³ç´°ã®è¿½åŠ 
        detailed_ratios = comp_analysis['detailed_analysis']
        
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†åŠ¹çŽ‡
        data_size_improvement = (1 - detailed_ratios['data_size_ratio']) * 100
        if abs(data_size_improvement) > 1:
            detailed_factors.append(f"ðŸ“Š Data processing: {data_size_improvement:+.1f}% (size: {detailed_ratios['data_size_ratio']:.3f}x)")
        
        # JOINæ“ä½œåŠ¹çŽ‡
        join_improvement = (1 - detailed_ratios['join_ratio']) * 100  
        if abs(join_improvement) > 1:
            detailed_factors.append(f"ðŸ”— JOIN operations: {join_improvement:+.1f}% (ratio: {detailed_ratios['join_ratio']:.3f}x)")
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡
        memory_improvement = (1 - detailed_ratios['memory_ratio']) * 100
        if abs(memory_improvement) > 1:
            detailed_factors.append(f"ðŸ’¾ Memory efficiency: {memory_improvement:+.1f}% (ratio: {detailed_ratios['memory_ratio']:.3f}x)")
        
        # ã‚¹ãƒ”ãƒ«ãƒªã‚¹ã‚¯
        spill_risk_improvement = (1 - detailed_ratios['spill_risk_ratio']) * 100
        if abs(spill_risk_improvement) > 5:
            detailed_factors.append(f"âš¡ Spill risk: {spill_risk_improvement:+.1f}% (ratio: {detailed_ratios['spill_risk_ratio']:.3f}x)")
        
        # ã‚¹ãƒ”ãƒ«æŽ¨å®šé‡ï¼ˆæ–°è¦è¿½åŠ ï¼‰
        if 'estimated_spill_ratio' in detailed_ratios:
            estimated_spill_improvement = (1 - detailed_ratios['estimated_spill_ratio']) * 100
            if abs(estimated_spill_improvement) > 1:
                detailed_factors.append(f"ðŸ’§ Estimated spill: {estimated_spill_improvement:+.1f}% (ratio: {detailed_ratios['estimated_spill_ratio']:.3f}x)")
        
        # ãƒ¡ãƒ¢ãƒªåœ§è¿«ã‚¹ã‚³ã‚¢ï¼ˆæ–°è¦è¿½åŠ ï¼‰
        if 'memory_pressure_ratio' in detailed_ratios:
            memory_pressure_improvement = (1 - detailed_ratios['memory_pressure_ratio']) * 100
            if abs(memory_pressure_improvement) > 5:
                detailed_factors.append(f"ðŸ§  Memory pressure: {memory_pressure_improvement:+.1f}% (ratio: {detailed_ratios['memory_pressure_ratio']:.3f}x)")
        
        # ã‚¹ãƒ”ãƒ«ç¢ºçŽ‡ï¼ˆæ–°è¦è¿½åŠ ï¼‰
        if 'spill_probability_ratio' in detailed_ratios:
            spill_prob_improvement = (1 - detailed_ratios['spill_probability_ratio']) * 100
            if abs(spill_prob_improvement) > 10:
                detailed_factors.append(f"ðŸŽ² Spill probability: {spill_prob_improvement:+.1f}% (ratio: {detailed_ratios['spill_probability_ratio']:.3f}x)")
        
        # ã‚·ãƒ£ãƒƒãƒ•ãƒ«åŠ¹çŽ‡ï¼ˆæ–°è¦è¿½åŠ ï¼‰
        if 'shuffle_ratio' in detailed_ratios:
            shuffle_improvement = (1 - detailed_ratios['shuffle_ratio']) * 100
            if abs(shuffle_improvement) > 1:
                detailed_factors.append(f"ðŸ”€ Shuffle efficiency: {shuffle_improvement:+.1f}% (ratio: {detailed_ratios['shuffle_ratio']:.3f}x)")
        
        # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åŠ¹çŽ‡
        if 'hash_partition_ratio' in detailed_ratios:
            hash_partition_improvement = (1 - detailed_ratios['hash_partition_ratio']) * 100
            if abs(hash_partition_improvement) > 1:
                detailed_factors.append(f"ðŸŽ¯ Hash partitioning: {hash_partition_improvement:+.1f}% (ratio: {detailed_ratios['hash_partition_ratio']:.3f}x)")
        
        # ã‚¹ãƒ”ãƒ«æ”¹å–„ãƒœãƒ¼ãƒŠã‚¹/ãƒšãƒŠãƒ«ãƒ†ã‚£ã®è¡¨ç¤º
        spill_factor = comprehensive_judgment.get('spill_improvement_factor', 1.0)
        if spill_factor != 1.0:
            bonus_pct = (1 - spill_factor) * 100
            if spill_factor < 1.0:
                detailed_factors.append(f"ðŸŽ Spill risk reduction bonus: {bonus_pct:.1f}% additional improvement")
            else:
                detailed_factors.append(f"âš ï¸ Spill risk increase penalty: {-bonus_pct:.1f}% performance impact")
        
        comparison_result['details'] = detailed_factors
        
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨å´ã«å€’ã—ã¦å…ƒã‚¯ã‚¨ãƒªã‚’æŽ¨å¥¨
        comparison_result['performance_degradation_detected'] = True
        comparison_result['is_optimization_beneficial'] = False
        comparison_result['recommendation'] = 'use_original'
        comparison_result['details'] = [f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒã‚¨ãƒ©ãƒ¼ã®ãŸã‚å…ƒã‚¯ã‚¨ãƒªä½¿ç”¨: {str(e)}"]
    
    return comparison_result


def analyze_degradation_causes(performance_comparison: Dict[str, Any], original_explain_cost: str = "", optimized_explain_cost: str = "") -> Dict[str, str]:
    """
    Analyze causes of performance degradation and generate correction instructions
    """
    degradation_analysis = {
        'primary_cause': 'unknown',
        'specific_issues': [],
        'fix_instructions': [],
        'confidence_level': 'low',
        'analysis_details': {}
    }
    
    try:
        if not performance_comparison or not performance_comparison.get('performance_degradation_detected'):
            degradation_analysis['primary_cause'] = 'no_degradation'
            return degradation_analysis
        
        details = performance_comparison.get('details', [])
        cost_ratio = performance_comparison.get('total_cost_ratio', 1.0)
        memory_ratio = performance_comparison.get('memory_usage_ratio', 1.0)
        
        # ðŸ” ã‚³ã‚¹ãƒˆæ‚ªåŒ–ã®æ·±åˆ»åº¦åˆ†æž
        if cost_ratio > 1.5:  # 50%ä»¥ä¸Šã®æ‚ªåŒ–
            degradation_analysis['confidence_level'] = 'high'
            severity = 'critical'
        elif cost_ratio > 1.3:  # 30%ä»¥ä¸Šã®æ‚ªåŒ–
            degradation_analysis['confidence_level'] = 'medium'
            severity = 'significant'
        else:
            degradation_analysis['confidence_level'] = 'low'
            severity = 'minor'
        
        degradation_analysis['analysis_details']['cost_degradation_severity'] = severity
        degradation_analysis['analysis_details']['cost_ratio'] = cost_ratio
        degradation_analysis['analysis_details']['memory_ratio'] = memory_ratio
        
        # ðŸŽ¯ ä¸»è¦åŽŸå› ã®ç‰¹å®šã¨JOINæ“ä½œæ•°åˆ†æž
        for detail in details:
            detail_str = str(detail).lower()
            
            # Detect significant JOIN operations count increase
            if 'join operations count increase' in detail_str or 'join' in detail_str:
                degradation_analysis['primary_cause'] = 'excessive_joins'
                degradation_analysis['specific_issues'].append('Significant JOIN operations count increase')
                
                # JOINæ•°ã®å…·ä½“çš„ãªå¢—åŠ ã‚’è§£æž
                import re
                join_match = re.search(r'(\d+)\s*â†’\s*(\d+)', detail_str)
                if join_match:
                    original_joins = int(join_match.group(1))
                    optimized_joins = int(join_match.group(2))
                    join_increase_ratio = optimized_joins / original_joins if original_joins > 0 else float('inf')
                    
                    degradation_analysis['analysis_details']['original_joins'] = original_joins
                    degradation_analysis['analysis_details']['optimized_joins'] = optimized_joins
                    degradation_analysis['analysis_details']['join_increase_ratio'] = join_increase_ratio
                    
                    if join_increase_ratio > 1.5:  # 50%ä»¥ä¸Šã®JOINå¢—åŠ 
                        degradation_analysis['fix_instructions'].extend([
                            "JOINé †åºã®åŠ¹çŽ‡åŒ–ã‚’æ¤œè¨Žã—ã¦ãã ã•ã„",
                            "å…ƒã®JOINé †åºã‚’å°Šé‡ã—ã€å¤§å¹…ãªæ§‹é€ å¤‰æ›´ã‚’é¿ã‘ã¦ãã ã•ã„",
                            "ä¸è¦ãªã‚µãƒ–ã‚¯ã‚¨ãƒªåŒ–ã«ã‚ˆã‚‹JOINé‡è¤‡ã‚’é˜²ã„ã§ãã ã•ã„",
                            "CTEå±•é–‹ã«ã‚ˆã‚‹JOINå¢—åŠ ã‚’é¿ã‘ã€å…ƒã®æ§‹é€ ã‚’ä¿æŒã—ã¦ãã ã•ã„"
                        ])
                
            # Total execution cost degradation
            elif 'total execution cost degradation' in detail_str or 'cost' in detail_str:
                if degradation_analysis['primary_cause'] == 'unknown':
                    degradation_analysis['primary_cause'] = 'cost_increase'
                degradation_analysis['specific_issues'].append('Total execution cost degradation')
                degradation_analysis['fix_instructions'].extend([
                    "å°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åŠ¹çŽ‡çš„ã«JOINã§å‡¦ç†ã—ã¦ãã ã•ã„",
                                         "å¤§ããªãƒ†ãƒ¼ãƒ–ãƒ«ã®JOINé †åºã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„",
                    "REPARTITIONãƒ’ãƒ³ãƒˆã®é…ç½®ä½ç½®ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„"
                ])
            
            # Memory usage degradation
            elif 'memory usage degradation' in detail_str or 'memory' in detail_str:
                if degradation_analysis['primary_cause'] == 'unknown':
                    degradation_analysis['primary_cause'] = 'memory_increase'
                degradation_analysis['specific_issues'].append('Memory usage degradation')
                degradation_analysis['fix_instructions'].extend([
                    "å¤§ããªãƒ†ãƒ¼ãƒ–ãƒ«ã®BROADCASTé©ç”¨ã‚’å‰Šé™¤ã—ã¦ãã ã•ã„",
                    "ãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡çš„ãªJOINæˆ¦ç•¥ã‚’é¸æŠžã—ã¦ãã ã•ã„",
                    "ä¸­é–“çµæžœã®ã‚µã‚¤ã‚ºã‚’å‰Šæ¸›ã—ã¦ãã ã•ã„"
                ])
        
        # ðŸ” EXPLAIN COSTåˆ†æžã«ã‚ˆã‚‹è©³ç´°åŽŸå› ç‰¹å®šï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if original_explain_cost and optimized_explain_cost:
            cost_analysis = analyze_explain_cost_differences(original_explain_cost, optimized_explain_cost)
            degradation_analysis['analysis_details']['explain_cost_analysis'] = cost_analysis
            
            # BROADCASTé–¢é€£ã®å•é¡Œæ¤œå‡º
            if cost_analysis.get('broadcast_issues'):
                degradation_analysis['fix_instructions'].extend([
                    "æ¤œå‡ºã•ã‚ŒãŸBROADCASTå•é¡Œã‚’ä¿®æ­£ã—ã¦ãã ã•ã„",
                    "é©åˆ‡ãªã‚µã‚¤ã‚ºã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿BROADCASTå¯¾è±¡ã¨ã—ã¦ãã ã•ã„"
                ])
        
        # åŽŸå› ãŒç‰¹å®šã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if degradation_analysis['primary_cause'] == 'unknown':
            degradation_analysis['primary_cause'] = 'optimization_backfire'
            degradation_analysis['fix_instructions'].extend([
                "æœ€é©åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä¿å®ˆçš„ã«å¤‰æ›´ã—ã¦ãã ã•ã„",
                "å…ƒã®ã‚¯ã‚¨ãƒªæ§‹é€ ã‚’ã‚ˆã‚Šå¤šãä¿æŒã—ã¦ãã ã•ã„",
                "ãƒ’ãƒ³ãƒˆå¥ã®é©ç”¨ã‚’æœ€å°é™ã«æŠ‘ãˆã¦ãã ã•ã„"
            ])
        
        # é‡è¤‡ã™ã‚‹ä¿®æ­£æŒ‡ç¤ºã‚’å‰Šé™¤
        degradation_analysis['fix_instructions'] = list(set(degradation_analysis['fix_instructions']))
        
    except Exception as e:
        degradation_analysis['primary_cause'] = 'analysis_error'
        degradation_analysis['specific_issues'] = [f"åˆ†æžã‚¨ãƒ©ãƒ¼: {str(e)}"]
        degradation_analysis['fix_instructions'] = [
            "ä¿å®ˆçš„ãªæœ€é©åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„",
            "å…ƒã®ã‚¯ã‚¨ãƒªæ§‹é€ ã‚’æœ€å¤§é™ä¿æŒã—ã¦ãã ã•ã„"
        ]
    
    return degradation_analysis


def analyze_explain_cost_differences(original_cost: str, optimized_cost: str) -> Dict[str, Any]:
    """
    Identify degradation causes through differential analysis of EXPLAIN COST results
    """
    analysis = {
        'broadcast_issues': False,
        'join_strategy_changes': [],
        'size_estimation_problems': [],
        'plan_structure_changes': []
    }
    
    try:
        import re
        
        # BROADCASTé–¢é€£ã®å•é¡Œæ¤œå‡º
        original_broadcasts = len(re.findall(r'broadcast', original_cost.lower()))
        optimized_broadcasts = len(re.findall(r'broadcast', optimized_cost.lower()))
        
        if optimized_broadcasts > original_broadcasts * 2:  # BROADCASTä½¿ç”¨é‡ãŒ2å€ä»¥ä¸Šå¢—åŠ 
            analysis['broadcast_issues'] = True
            analysis['join_strategy_changes'].append(f"BROADCASTä½¿ç”¨ãŒå¤§å¹…å¢—åŠ : {original_broadcasts} â†’ {optimized_broadcasts}")
        
        # JOINæˆ¦ç•¥ã®å¤‰åŒ–æ¤œå‡º
        original_join_types = set(re.findall(r'(\w+)Join', original_cost))
        optimized_join_types = set(re.findall(r'(\w+)Join', optimized_cost))
        
        if optimized_join_types != original_join_types:
            analysis['join_strategy_changes'].append(f"JOINæˆ¦ç•¥å¤‰åŒ–: {original_join_types} â†’ {optimized_join_types}")
        
        # ãƒ—ãƒ©ãƒ³æ§‹é€ ã®è¤‡é›‘åŒ–æ¤œå‡º
        original_plan_depth = original_cost.count('+-')
        optimized_plan_depth = optimized_cost.count('+-')
        
        if optimized_plan_depth > original_plan_depth * 1.3:  # ãƒ—ãƒ©ãƒ³æ·±åº¦ãŒ30%ä»¥ä¸Šå¢—åŠ 
            analysis['plan_structure_changes'].append(f"å®Ÿè¡Œãƒ—ãƒ©ãƒ³è¤‡é›‘åŒ–: æ·±åº¦ {original_plan_depth} â†’ {optimized_plan_depth}")
        
    except Exception as e:
        analysis['analysis_error'] = str(e)
    
    return analysis


def execute_iterative_optimization_with_degradation_analysis(original_query: str, analysis_result: str, metrics: Dict[str, Any], max_optimization_attempts: int = 3) -> Dict[str, Any]:
    """
    Iterative optimization and performance degradation analysis
    Attempt re-optimization up to 3 times by analyzing degradation causes, use original query if no improvement
    """
    from datetime import datetime
    
    print(f"\nðŸš€ Starting iterative optimization process (maximum {max_optimization_attempts} improvement attempts)")
    print("ðŸŽ¯ Goal: Achieve 10%+ cost reduction | Select best result when maximum attempts reached")
    print("=" * 70)
    
    optimization_attempts = []
    original_query_for_explain = original_query  # å…ƒã‚¯ã‚¨ãƒªã®ä¿æŒ
    
    # ðŸš€ ãƒ™ã‚¹ãƒˆçµæžœè¿½è·¡ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ï¼šæœ€å¤§è©¦è¡Œå›žæ•°åˆ°é”æ™‚ã¯æœ€ã‚‚è‰¯ã„çµæžœã‚’é¸æŠžï¼‰
    best_result = {
        'attempt_num': 0,
        'query': original_query,
        'cost_ratio': 1.0,
        'memory_ratio': 1.0,
        'performance_comparison': None,
        'optimized_result': '',
        'status': 'baseline'
    }
    
    # ðŸš€ ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®EXPLAINçµæžœã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆé‡è¤‡å®Ÿè¡Œé˜²æ­¢ï¼‰
    original_explain_cost_result = None
    corrected_original_query = globals().get('original_query_corrected', original_query)
    
    for attempt_num in range(1, max_optimization_attempts + 1):
        print(f"\nðŸ”„ Optimization attempt {attempt_num}/{max_optimization_attempts}")
        print("-" * 50)
        
        # å‰å›žã®è©¦è¡Œçµæžœã«åŸºã¥ãä¿®æ­£æŒ‡ç¤ºã‚’ç”Ÿæˆ
        fix_instructions = ""
        if attempt_num > 1 and optimization_attempts:
            previous_attempt = optimization_attempts[-1]
            if previous_attempt.get('degradation_analysis'):
                degradation_analysis = previous_attempt['degradation_analysis']
                fix_instructions = "\n".join([
                    f"ã€å‰å›žã®æ‚ªåŒ–åŽŸå› : {degradation_analysis['primary_cause']}ã€‘",
                    f"ã€ä¿¡é ¼åº¦: {degradation_analysis['confidence_level']}ã€‘",
                    "ã€ä¿®æ­£æŒ‡ç¤ºã€‘"
                ] + degradation_analysis['fix_instructions'])
                
                print(f"ðŸ”§ Degradation cause analysis result: {degradation_analysis['primary_cause']}")
                print(f"ðŸ“Š Confidence level: {degradation_analysis['confidence_level']}")
                print(f"ðŸ’¡ Fix instructions: {len(degradation_analysis['fix_instructions'])} items")
        
        # æœ€é©åŒ–ã‚¯ã‚¨ãƒªç”Ÿæˆï¼ˆåˆå›ž or ä¿®æ­£ç‰ˆï¼‰
        if attempt_num == 1:
            print("ðŸ¤– Initial optimization query generation")
            optimized_query = generate_optimized_query_with_llm(original_query, analysis_result, metrics)
            # ðŸ› DEBUG: åˆå›žè©¦è¡Œã‚¯ã‚¨ãƒªã‚’ä¿å­˜
            if isinstance(optimized_query, str) and not optimized_query.startswith("LLM_ERROR:"):
                save_debug_query_trial(optimized_query, attempt_num, "initial")
        else:
            print(f"ðŸ”§ Corrected optimization query generation (attempt {attempt_num})")
            # ðŸš¨ ä¿®æ­£: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ‚ªåŒ–å°‚ç”¨é–¢æ•°ã‚’ä½¿ç”¨
            previous_attempt = optimization_attempts[-1] if optimization_attempts else {}
            degradation_analysis = previous_attempt.get('degradation_analysis', {})
            optimized_query = generate_improved_query_for_performance_degradation(
                original_query, 
                analysis_result, 
                metrics, 
                degradation_analysis, 
                previous_attempt.get('optimized_query', '')
            )
            # ðŸ› DEBUG: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ”¹å–„è©¦è¡Œã‚¯ã‚¨ãƒªã‚’ä¿å­˜
            if isinstance(optimized_query, str) and not optimized_query.startswith("LLM_ERROR:"):
                degradation_cause = degradation_analysis.get('primary_cause', 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ‚ªåŒ–')
                save_debug_query_trial(optimized_query, attempt_num, "performance_improvement", 
                                     error_info=f"å‰å›žæ‚ªåŒ–åŽŸå› : {degradation_cause}")
        
        # LLMã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        if isinstance(optimized_query, str) and optimized_query.startswith("LLM_ERROR:"):
            print(f"âŒ LLM error occurred in optimization attempt {attempt_num}")
            optimization_attempts.append({
                'attempt': attempt_num,
                'status': 'llm_error',
                'error': optimized_query[10:],
                'optimized_query': None
            })
            continue
        
        # ã‚¯ã‚¨ãƒªæŠ½å‡º
        if isinstance(optimized_query, list):
            optimized_query_str = extract_main_content_from_thinking_response(optimized_query)
        else:
            optimized_query_str = str(optimized_query)
        
        extracted_sql = extract_sql_from_llm_response(optimized_query_str)
        current_query = extracted_sql if extracted_sql else original_query
        
        # EXPLAINå®Ÿè¡Œã¨æ§‹æ–‡ãƒã‚§ãƒƒã‚¯
        explain_result = execute_explain_with_retry_logic(current_query, analysis_result, metrics, max_retries=MAX_RETRIES)
        
        if explain_result['final_status'] != 'success':
            print(f"âš ï¸ Attempt {attempt_num}: EXPLAIN execution failed")
            optimization_attempts.append({
                'attempt': attempt_num,
                'status': 'explain_failed',
                'error': explain_result.get('error_details', 'Unknown error'),
                'optimized_query': current_query
            })
            continue
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒå®Ÿè¡Œ
        print(f"ðŸ” Attempt {attempt_num}: Executing performance degradation detection")
        
        # ðŸŽ¯ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ï¼ˆé‡è¤‡å‡¦ç†é˜²æ­¢ï¼‰
        if corrected_original_query != original_query:
            print("ðŸ’¾ Using cached original query: Preventing duplicate processing")
        
        # ðŸš€ å…ƒã‚¯ã‚¨ãƒªã®EXPLAIN COSTå–å¾—ï¼ˆåˆå›žã®ã¿å®Ÿè¡Œã€ä»¥é™ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ï¼‰
        if original_explain_cost_result is None:
            print(f"ðŸ”„ Attempt {attempt_num}: Executing EXPLAIN COST for original query (first time only)")
            original_explain_cost_result = execute_explain_and_save_to_file(corrected_original_query, "original_performance_check")
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            globals()['cached_original_explain_cost_result'] = original_explain_cost_result
        else:
            print(f"ðŸ’¾ Attempt {attempt_num}: Using cached EXPLAIN COST result for original query (avoiding duplicate execution)")
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¾©å…ƒ
            original_explain_cost_result = globals().get('cached_original_explain_cost_result', original_explain_cost_result)
        
        # æœ€é©åŒ–ã‚¯ã‚¨ãƒªã®EXPLAIN COSTå–å¾—
        optimized_explain_cost_result = execute_explain_and_save_to_file(current_query, f"optimized_attempt_{attempt_num}")
        
        performance_comparison = None
        degradation_analysis = None
        
        # ðŸ” EXPLAIN COSTã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®æ”¹å–„
        original_cost_success = ('explain_cost_file' in original_explain_cost_result and 
                                'error_file' not in original_explain_cost_result)
        optimized_cost_success = ('explain_cost_file' in optimized_explain_cost_result and 
                                 'error_file' not in optimized_explain_cost_result)
        
        # ðŸš¨ ç·Šæ€¥ãƒ‡ãƒãƒƒã‚°: EXPLAIN COSTæˆåŠŸ/å¤±æ•—ã®è©³ç´°è¡¨ç¤º
        print(f"ðŸ” EXPLAIN COST success determination:")
        print(f"   ðŸ“Š Original query: {'âœ… Success' if original_cost_success else 'âŒ Failed'}")
        if not original_cost_success:
            print(f"      â€¢ explain_cost_file exists: {'explain_cost_file' in original_explain_cost_result}")
            print(f"      â€¢ error_file exists: {'error_file' in original_explain_cost_result}")
            print(f"      â€¢ Return keys: {list(original_explain_cost_result.keys())}")
        print(f"   ðŸ”§ Optimized query: {'âœ… Success' if optimized_cost_success else 'âŒ Failed'}")
        if not optimized_cost_success:
            print(f"      â€¢ explain_cost_file exists: {'explain_cost_file' in optimized_explain_cost_result}")
            print(f"      â€¢ error_file exists: {'error_file' in optimized_explain_cost_result}")
            print(f"      â€¢ Return keys: {list(optimized_explain_cost_result.keys())}")
        
        if not original_cost_success:
            print("âš ï¸ Original query EXPLAIN COST execution failed: Skipping performance comparison")
            if 'error_file' in original_explain_cost_result:
                print(f"ðŸ“„ Error details: {original_explain_cost_result['error_file']}")
        
        if not optimized_cost_success:
            print("âš ï¸ Optimized query EXPLAIN COST execution failed: Attempting error correction")
            if 'error_file' in optimized_explain_cost_result:
                print(f"ðŸ“„ Error details: {optimized_explain_cost_result['error_file']}")
                
                # ðŸš¨ CRITICAL FIX: ã‚¨ãƒ©ãƒ¼æ¤œå‡ºæ™‚ã¯å³åº§ã«LLMä¿®æ­£ã‚’å®Ÿè¡Œ
                print("ðŸ”§ Executing LLM-based error correction...")
                error_message = optimized_explain_cost_result.get('error_message', 'Unknown error')
                
                # ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã®ãŸã‚ã®LLMå‘¼ã³å‡ºã—
                corrected_query = generate_optimized_query_with_error_feedback(
                    original_query,
                    analysis_result, 
                    metrics,
                    error_message,
                    current_query  # ç¾åœ¨ã®ã‚¯ã‚¨ãƒªï¼ˆãƒ’ãƒ³ãƒˆä»˜ãï¼‰ã‚’æ¸¡ã™
                )
                
                # ðŸ› DEBUG: ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã‚¯ã‚¨ãƒªã‚’ä¿å­˜
                if isinstance(corrected_query, str) and not corrected_query.startswith("LLM_ERROR:"):
                    save_debug_query_trial(corrected_query, attempt_num, "error_correction", 
                                         error_info=f"ä¿®æ­£å¯¾è±¡ã‚¨ãƒ©ãƒ¼: {error_message[:100]}")
                
                # LLMã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
                if isinstance(corrected_query, str) and corrected_query.startswith("LLM_ERROR:"):
                    print("âŒ Error occurred in LLM correction: Executing fallback evaluation")
                else:
                    # thinking_enabledå¯¾å¿œ
                    if isinstance(corrected_query, list):
                        corrected_query_str = extract_main_content_from_thinking_response(corrected_query)
                    else:
                        corrected_query_str = str(corrected_query)
                    
                    # SQLã‚¯ã‚¨ãƒªéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
                    extracted_sql = extract_sql_from_llm_response(corrected_query_str)
                    if extracted_sql:
                        current_query = extracted_sql
                        print("âœ… LLM-based error correction completed, re-evaluating with corrected query")
                        
                        # ä¿®æ­£ã‚¯ã‚¨ãƒªã§å†åº¦EXPLAINå®Ÿè¡Œ
                        optimized_explain_cost_result = execute_explain_and_save_to_file(current_query, f"optimized_attempt_{attempt_num}_corrected")
                        optimized_cost_success = ('explain_cost_file' in optimized_explain_cost_result and 
                                                'error_file' not in optimized_explain_cost_result)
                        
                        if optimized_cost_success:
                            print("ðŸŽ¯ Corrected query EXPLAIN execution successful!")
                            # ðŸŽ¯ æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆæŠ½å‡ºãƒ»ä¿å­˜ï¼ˆEXPLAINæˆåŠŸæ™‚ã®ã¿ï¼‰
                            try:
                                optimization_point = extract_optimization_points_from_query(current_query, "error_correction", attempt_num)
                                save_trial_log(optimization_point)  # Log individual trial
                                save_optimization_points_summary(optimization_point)  # Keep existing functionality
                            except Exception as e:
                                print(f"âš ï¸ Optimization points extraction failed: {str(e)}")
                        else:
                            print("âš ï¸ Error occurred even with corrected query: Executing fallback evaluation")
                    else:
                        print("âŒ Failed to extract SQL query: Executing fallback evaluation")
            
            # ã‚¨ãƒ©ãƒ¼ä¿®æ­£å¾Œã‚‚ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡ã‚’å®Ÿè¡Œ
            if not optimized_cost_success:
                print("ðŸ”„ Executing fallback evaluation")
        
        # ðŸš¨ ç·Šæ€¥ä¿®æ­£: EXPLAIN COSTå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹è©•ä¾¡
        if not (original_cost_success and optimized_cost_success):
            print("ðŸ”„ Fallback: Executing simple performance evaluation using EXPLAIN results")
            
            # EXPLAINçµæžœãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡
            original_explain_success = ('explain_file' in original_explain_cost_result and 
                                       'error_file' not in original_explain_cost_result)
            optimized_explain_success = ('explain_file' in optimized_explain_cost_result and 
                                        'error_file' not in optimized_explain_cost_result)
            
            if original_explain_success and optimized_explain_success:
                try:
                    # EXPLAINçµæžœã‚’èª­ã¿è¾¼ã¿
                    with open(original_explain_cost_result['explain_file'], 'r', encoding='utf-8') as f:
                        original_explain_content = f.read()
                    
                    with open(optimized_explain_cost_result['explain_file'], 'r', encoding='utf-8') as f:
                        optimized_explain_content = f.read()
                    
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡å®Ÿè¡Œ
                    fallback_evaluation = fallback_performance_evaluation(original_explain_content, optimized_explain_content)
                    
                    print(f"ðŸ“Š Fallback evaluation result: {fallback_evaluation['summary']}")
                    print(f"   - Recommendation: {fallback_evaluation['recommendation']}")
                    print(f"   - Confidence: {fallback_evaluation['confidence']}")
                    
                    for detail in fallback_evaluation['details']:
                        print(f"   - {detail}")
                    
                    # performance_comparisonã®ä»£æ›¿ã¨ã—ã¦ä½¿ç”¨
                    performance_comparison = {
                        'is_optimization_beneficial': fallback_evaluation['recommendation'] == 'use_optimized',
                        'performance_degradation_detected': fallback_evaluation['overall_status'] == 'degradation_possible',
                        'significant_improvement_detected': fallback_evaluation['overall_status'] == 'clear_improvement',  # ðŸš¨ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡ã§ã‚‚æ˜Žç¢ºæ”¹å–„æ¤œå‡º
                        'recommendation': fallback_evaluation['recommendation'],
                        'evaluation_type': 'fallback_plan_analysis',
                        'details': fallback_evaluation['details'],
                        'fallback_evaluation': fallback_evaluation,
                        'total_cost_ratio': 1.0,  # EXPLAIN COSTãªã—ã®ãŸã‚æœªçŸ¥
                        'memory_usage_ratio': 1.0  # EXPLAIN COSTãªã—ã®ãŸã‚æœªçŸ¥
                    }
                    
                    print("âœ… Fallback performance evaluation completed")
                    
                    # ðŸš¨ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡ã§ã‚‚åŽ³æ ¼åˆ¤å®šé©ç”¨
                    if not performance_comparison.get('significant_improvement_detected', False):
                        if performance_comparison['performance_degradation_detected']:
                            print(f"ðŸš¨ Attempt {attempt_num}: Possibility of degradation in fallback evaluation")
                            status_reason = "fallback_degradation_detected"
                        else:
                            print(f"âš ï¸ Attempt {attempt_num}: Clear improvement not confirmed in fallback evaluation")
                            status_reason = "fallback_insufficient_improvement"
                        
                        optimization_attempts.append({
                            'attempt': attempt_num,
                            'status': status_reason,
                            'optimized_query': current_query,
                            'performance_comparison': performance_comparison,
                            'cost_ratio': performance_comparison['total_cost_ratio'],
                            'memory_ratio': performance_comparison['memory_usage_ratio']
                        })
                        
                        # ðŸš€ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡ã§ã‚‚ãƒ™ã‚¹ãƒˆçµæžœè¿½è·¡
                        current_cost_ratio = performance_comparison.get('total_cost_ratio', 1.0)
                        current_memory_ratio = performance_comparison.get('memory_usage_ratio', 1.0)
                        
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡ã§ã¯æ”¹å–„ã®å ´åˆã®ã¿ãƒ™ã‚¹ãƒˆæ›´æ–°ï¼ˆä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ï¼‰
                        if performance_comparison.get('significant_improvement_detected', False):
                            is_better_than_best = (
                                current_cost_ratio < best_result['cost_ratio'] or 
                                (current_cost_ratio == best_result['cost_ratio'] and current_memory_ratio < best_result['memory_ratio'])
                            )
                            
                            if is_better_than_best:
                                print(f"ðŸ† Attempt {attempt_num}: New best result in fallback evaluation!")
                                best_result.update({
                                    'attempt_num': attempt_num,
                                    'query': current_query,
                                    'cost_ratio': current_cost_ratio,
                                    'memory_ratio': current_memory_ratio,
                                    'performance_comparison': performance_comparison,
                                    'optimized_result': optimized_query_str,
                                    'status': 'fallback_improved'
                                })
                        
                        optimization_attempts.append({
                            'attempt': attempt_num,
                            'status': status_reason,
                            'optimized_query': current_query,
                            'performance_comparison': performance_comparison,
                            'cost_ratio': current_cost_ratio,
                            'memory_ratio': current_memory_ratio
                        })
                        
                        # ðŸš€ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡ã§ã¯å¤§å¹…æ”¹å–„åˆ¤å®šãŒå›°é›£ãªãŸã‚ã€è©¦è¡Œç¶™ç¶š
                        if attempt_num < max_optimization_attempts:
                            print(f"ðŸ”„ Aiming for more reliable improvement in attempt {attempt_num + 1} (fallback evaluation)")
                            continue
                        else:
                            print(f"â° Maximum attempts ({max_optimization_attempts}) reached â†’ Selecting best result")
                            break
                    
                except Exception as e:
                    print(f"âŒ Error in fallback evaluation as well: {str(e)}")
                    print(f"   ðŸ“Š Error details: {type(e).__name__}")
                    if hasattr(e, '__traceback__'):
                        import traceback
                        print(f"   ðŸ“„ Stack trace: {traceback.format_exc()}")
                    performance_comparison = None
            else:
                print("âŒ EXPLAIN results also insufficient, performance evaluation impossible")
                performance_comparison = None
        
        # ðŸš¨ ç·Šæ€¥ä¿®æ­£: ãƒ­ã‚¸ãƒƒã‚¯é †åºã‚’ä¿®æ­£ï¼ˆEXPLAIN COSTæˆåŠŸåˆ¤å®šã‚’å…ˆã«å®Ÿè¡Œï¼‰
        if (original_cost_success and optimized_cost_success):
            
            try:
                print(f"ðŸŽ¯ Both EXPLAIN COST successful â†’ Executing performance comparison")
                
                # EXPLAIN COSTå†…å®¹ã‚’èª­ã¿è¾¼ã¿
                with open(original_explain_cost_result['explain_cost_file'], 'r', encoding='utf-8') as f:
                    original_cost_content = f.read()
                
                with open(optimized_explain_cost_result['explain_cost_file'], 'r', encoding='utf-8') as f:
                    optimized_cost_content = f.read()
                
                print(f"   ðŸ“Š Original query COST content length: {len(original_cost_content)} characters")
                print(f"   ðŸ”§ Optimized query COST content length: {len(optimized_cost_content)} characters")
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒå®Ÿè¡Œ
                print(f"ðŸ” Executing compare_query_performance...")
                performance_comparison = compare_query_performance(original_cost_content, optimized_cost_content)
                print(f"âœ… compare_query_performance completed: {performance_comparison is not None}")
                
                if performance_comparison:
                    print(f"   ðŸ“Š significant_improvement_detected: {performance_comparison.get('significant_improvement_detected', 'UNKNOWN')}")
                    print(f"   ðŸ“Š performance_degradation_detected: {performance_comparison.get('performance_degradation_detected', 'UNKNOWN')}")
                    print(f"   ðŸ“Š is_optimization_beneficial: {performance_comparison.get('is_optimization_beneficial', 'UNKNOWN')}")
                else:
                    print(f"âŒ performance_comparison is None!")
                
                # ðŸš€ ãƒ™ã‚¹ãƒˆçµæžœæ›´æ–°åˆ¤å®šï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ï¼šå¸¸ã«æœ€è‰¯çµæžœã‚’è¿½è·¡ï¼‰
                current_cost_ratio = performance_comparison['total_cost_ratio']
                current_memory_ratio = performance_comparison['memory_usage_ratio']
                
                # ç¾åœ¨ã®çµæžœãŒãƒ™ã‚¹ãƒˆã‚’ä¸Šå›žã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆã‚³ã‚¹ãƒˆæ¯”çŽ‡ãŒä½Žã„ã»ã©è‰¯ã„ï¼‰
                is_better_than_best = (
                    current_cost_ratio < best_result['cost_ratio'] or 
                    (current_cost_ratio == best_result['cost_ratio'] and current_memory_ratio < best_result['memory_ratio'])
                )
                
                if is_better_than_best:
                    print(f"ðŸ† Attempt {attempt_num}: New best result recorded!")
                    print(f"   ðŸ“Š Cost ratio: {best_result['cost_ratio']:.3f} â†’ {current_cost_ratio:.3f}")
                    print(f"   ðŸ’¾ Memory ratio: {best_result['memory_ratio']:.3f} â†’ {current_memory_ratio:.3f}")
                    best_result.update({
                        'attempt_num': attempt_num,
                        'query': current_query,
                        'cost_ratio': current_cost_ratio,
                        'memory_ratio': current_memory_ratio,
                        'performance_comparison': performance_comparison,
                        'optimized_result': optimized_query_str,
                        'status': 'improved'
                    })
                
                # ðŸš€ å¤§å¹…æ”¹å–„ï¼ˆ10%ä»¥ä¸Šï¼‰é”æˆã§å³åº§ã«çµ‚äº†
                if performance_comparison.get('substantial_improvement_detected', False):
                    print(f"ðŸš€ Attempt {attempt_num}: Significant improvement achieved (10%+ reduction)! Optimization completed immediately")
                    optimization_attempts.append({
                        'attempt': attempt_num,
                        'status': 'substantial_success',
                        'optimized_query': current_query,
                        'performance_comparison': performance_comparison,
                        'cost_ratio': current_cost_ratio,
                        'memory_ratio': current_memory_ratio
                    })
                    
                    return {
                        'final_status': 'optimization_success',
                        'final_query': current_query,
                        'successful_attempt': attempt_num,
                        'total_attempts': attempt_num,
                        'optimization_attempts': optimization_attempts,
                        'performance_comparison': performance_comparison,
                        'optimized_result': optimized_query_str,
                        'saved_files': None,  # ãƒ¡ã‚¤ãƒ³å‡¦ç†ã§ä¿å­˜
                        'achievement_type': 'substantial_improvement'
                    }
                
                # ðŸš€ æ”¹å–„ã¯ã‚ã‚‹ãŒå¤§å¹…ã§ãªã„å ´åˆã®åˆ¤å®š
                elif performance_comparison.get('significant_improvement_detected', False):
                    print(f"âœ… Attempt {attempt_num}: Improvement confirmed (target 10% not reached)")
                    status_reason = "partial_improvement"
                else:
                    # æ”¹å–„ãªã—ã¾ãŸã¯æ‚ªåŒ–ã®å ´åˆ
                    if performance_comparison['performance_degradation_detected']:
                        print(f"ðŸš¨ Attempt {attempt_num}: Performance increase detected")
                        print(f"   ðŸ“Š Cost ratio: {current_cost_ratio:.3f}")
                        print(f"   ðŸ’¾ Memory ratio: {current_memory_ratio:.3f}")
                        # ã‚¹ãƒ”ãƒ«æŽ¨å®šå€¤ã‚‚è¡¨ç¤º
                        if 'estimated_spill_gb' in performance_comparison:
                            orig_spill = performance_comparison.get('original_estimated_spill_gb', 0)
                            opt_spill = performance_comparison.get('optimized_estimated_spill_gb', 0)
                            if orig_spill > 0 or opt_spill > 0:
                                print(f"   ðŸ’§ Estimated spill: {orig_spill:.2f}GB â†’ {opt_spill:.2f}GB")
                        status_reason = "performance_degraded"
                    else:
                        print(f"âš ï¸ Attempt {attempt_num}: Clear improvement cannot be confirmed")
                        print(f"   ðŸ“Š Cost ratio: {current_cost_ratio:.3f}")
                        print(f"   ðŸ’¾ Memory ratio: {current_memory_ratio:.3f}")
                        status_reason = "insufficient_improvement"
                
                # æ‚ªåŒ–åŽŸå› åˆ†æžï¼ˆæ”¹å–„ä¸è¶³ã®å ´åˆã‚‚å®Ÿè¡Œï¼‰
                degradation_analysis = analyze_degradation_causes(performance_comparison, original_cost_content, optimized_cost_content)
                
                print(f"   Details: {', '.join(performance_comparison.get('details', []))}")
                
                optimization_attempts.append({
                    'attempt': attempt_num,
                    'status': status_reason,
                    'optimized_query': current_query,
                    'performance_comparison': performance_comparison,
                    'degradation_analysis': degradation_analysis,
                    'cost_ratio': current_cost_ratio,
                    'memory_ratio': current_memory_ratio
                })
                
                # ðŸš€ æ–°åˆ¤å®š: å¤§å¹…æ”¹å–„ï¼ˆ10%ä»¥ä¸Šï¼‰ã§ãªã„é™ã‚Šè©¦è¡Œç¶™ç¶š
                if attempt_num < max_optimization_attempts:
                    print(f"ðŸ”„ Aiming for significant improvement (10%+ reduction) in attempt {attempt_num + 1}")
                    continue
                else:
                    print(f"â° Maximum attempts ({max_optimization_attempts}) reached â†’ Selecting best result")
                    break
            
            except Exception as e:
                print(f"âŒ Attempt {attempt_num}: Error in performance comparison: {str(e)}")
                print(f"   ðŸ“Š Error type: {type(e).__name__}")
                if hasattr(e, '__traceback__'):
                    import traceback
                    print(f"   ðŸ“„ Stack trace: {traceback.format_exc()}")
                print(f"ðŸš¨ This error is the cause of 'Performance evaluation impossible'!")
                optimization_attempts.append({
                    'attempt': attempt_num,
                    'status': 'comparison_error',
                    'error': str(e),
                    'optimized_query': current_query
                })
                continue
        
        # ðŸš¨ ç·Šæ€¥ä¿®æ­£: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹è©•ä¾¡ãŒå®Œå…¨ã«å¤±æ•—ã—ãŸå ´åˆã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆãƒ­ã‚¸ãƒƒã‚¯é †åºä¿®æ­£å¾Œï¼‰
        elif performance_comparison is None:
            print(f"ðŸš¨ Attempt {attempt_num}: Performance evaluation impossible, proceeding to next attempt")
            
            optimization_attempts.append({
                'attempt': attempt_num,
                'status': 'performance_evaluation_failed',
                'optimized_query': current_query,
                'performance_comparison': None,
                'error': 'EXPLAIN COSTå®Ÿè¡Œå¤±æ•—ã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡å¤±æ•—',
                'cost_ratio': None,
                'memory_ratio': None
            })
            
            # æœ€å¾Œã®è©¦è¡Œã§ãªã„å ´åˆã¯æ¬¡ã®æ”¹å–„ã‚’è©¦è¡Œ
            if attempt_num < max_optimization_attempts:
                print(f"ðŸ”„ Will retry performance evaluation in attempt {attempt_num + 1}")
                continue
            else:
                print(f"âŒ Maximum attempts ({max_optimization_attempts}) reached, using original query")
                break
        
        else:
            print(f"âš ï¸ Attempt {attempt_num}: EXPLAIN COST acquisition failed, using syntactically normal optimized query")
            optimization_attempts.append({
                'attempt': attempt_num,
                'status': 'explain_cost_failed',
                'optimized_query': current_query,
                'note': 'EXPLAIN COST comparison skipped due to execution failure'
            })
            
            # ðŸš¨ ä¿®æ­£: EXPLAIN COSTãŒå–å¾—ã§ããªã„å ´åˆã‚‚é‡è¤‡ä¿å­˜ã‚’é˜²æ­¢
            # saved_files = save_optimized_sql_files(...)  # â† é‡è¤‡é˜²æ­¢ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
            
            return {
                'final_status': 'partial_success',
                'final_query': current_query,
                'successful_attempt': attempt_num,
                'total_attempts': attempt_num,
                'optimization_attempts': optimization_attempts,
                'optimized_result': optimized_query_str,  # ðŸ”§ ãƒ¡ã‚¤ãƒ³å‡¦ç†ã§ã®ä¿å­˜ç”¨ã«è¿½åŠ 
                'saved_files': None,  # ðŸ”§ ãƒ¡ã‚¤ãƒ³å‡¦ç†ã§ä¿å­˜ã™ã‚‹ãŸã‚None
                'note': 'Performance comparison unavailable but query is syntactically valid'
            }
    
    # ðŸš€ æœ€å¤§è©¦è¡Œå›žæ•°åˆ°é”ï¼šãƒ™ã‚¹ãƒˆçµæžœã‚’æœ€çµ‚ã‚¯ã‚¨ãƒªã¨ã—ã¦é¸æŠž
    print(f"\nâ° All {max_optimization_attempts} optimization attempts completed")
    print("ðŸ† Selecting best result as final query")
    print("=" * 60)
    
    # ðŸ“Š æœ€é©åŒ–è©¦è¡Œçµæžœã‚µãƒžãƒªãƒ¼è¡¨ç¤º
    print(f"\nðŸ“Š Optimization attempt details: {len(optimization_attempts)} times")
    for i, attempt in enumerate(optimization_attempts, 1):
        status_symbol = {
            'llm_error': 'âŒ',
            'explain_failed': 'ðŸš«', 
            'insufficient_improvement': 'â“',
            'substantial_success': 'ðŸ†',
            'performance_degraded': 'â¬‡ï¸',
            'comparison_error': 'ðŸ’¥'
        }.get(attempt['status'], 'â“')
        
        status_details = ""
        if 'cost_ratio' in attempt and attempt['cost_ratio'] is not None:
            cost_ratio = attempt['cost_ratio']
            status_details = f"ðŸ’° Cost ratio: {cost_ratio:.2f}x"
        
        print(f"   {status_symbol} Attempt {i}: {attempt['status']}")
        if status_details:
            print(f"      {status_details}")
    
    print("=" * 60)
    
    # ãƒ™ã‚¹ãƒˆçµæžœã®è©³ç´°è¡¨ç¤º
    if best_result['attempt_num'] > 0:
        print(f"ðŸ¥‡ FINAL SELECTION: Attempt {best_result['attempt_num']} has been chosen as the optimized query")
        print(f"   ðŸ“Š Cost ratio: {best_result['cost_ratio']:.3f} (Improvement: {(1-best_result['cost_ratio'])*100:.1f}%)")
        print(f"   ðŸ’¾ Memory ratio: {best_result['memory_ratio']:.3f} (Improvement: {(1-best_result['memory_ratio'])*100:.1f}%)")
        
        # ã‚¹ãƒ”ãƒ«æŽ¨å®šå€¤ã®è¡¨ç¤ºï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if 'performance_comparison' in best_result and best_result['performance_comparison']:
            pc = best_result['performance_comparison']
            orig_spill = pc.get('original_estimated_spill_gb', 0)
            opt_spill = pc.get('optimized_estimated_spill_gb', 0)
            if orig_spill > 0 or opt_spill > 0:
                spill_improvement = orig_spill - opt_spill
                if spill_improvement > 0:
                    print(f"   ðŸ’§ Spill improvement: {spill_improvement:.2f}GB reduction ({orig_spill:.2f}GB â†’ {opt_spill:.2f}GB)")
                elif spill_improvement < 0:
                    print(f"   ðŸ’§ Spill increase: {-spill_improvement:.2f}GB increase ({orig_spill:.2f}GB â†’ {opt_spill:.2f}GB)")
                else:
                    print(f"   ðŸ’§ Spill estimation: {orig_spill:.2f}GB (no change)")
        
        print(f"   ðŸŽ¯ Selection reason: Best cost performance among all attempts")
        
        final_query = best_result['query']
        final_optimized_result = best_result['optimized_result']
        final_performance_comparison = best_result['performance_comparison']
        final_status = 'optimization_success'
        achievement_type = 'best_of_trials'
        
        print(f"âœ… CONFIRMED: Using Attempt {best_result['attempt_num']} optimized query for final report")
        
    else:
        print(f"âš ï¸ Using original query due to errors or evaluation failures in all attempts")
        
        # è©¦è¡Œçµæžœã‚µãƒžãƒªãƒ¼
        failure_summary = []
        for attempt in optimization_attempts:
            if attempt['status'] == 'performance_degraded':
                failure_summary.append(f"è©¦è¡Œ{attempt['attempt']}: {attempt.get('degradation_analysis', {}).get('primary_cause', 'unknown')} (ã‚³ã‚¹ãƒˆæ¯”: {attempt.get('cost_ratio', 'N/A')})")
            elif attempt['status'] == 'llm_error':
                failure_summary.append(f"è©¦è¡Œ{attempt['attempt']}: LLMã‚¨ãƒ©ãƒ¼")
            elif attempt['status'] == 'explain_failed':
                failure_summary.append(f"è©¦è¡Œ{attempt['attempt']}: EXPLAINå®Ÿè¡Œå¤±æ•—")
            elif attempt['status'] == 'comparison_error':
                failure_summary.append(f"è©¦è¡Œ{attempt['attempt']}: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒã‚¨ãƒ©ãƒ¼")
            else:
                failure_summary.append(f"è©¦è¡Œ{attempt['attempt']}: {attempt['status']}")
        
        failure_report = f"""# âš ï¸ å…¨æœ€é©åŒ–è©¦è¡Œå®Œäº†ã®ãŸã‚ã€å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨

## æœ€é©åŒ–è©¦è¡Œçµæžœ

{chr(10).join(failure_summary) if failure_summary else "å…¨ã¦ã®è©¦è¡Œã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ"}

## æœ€çµ‚åˆ¤æ–­

{max_optimization_attempts}å›žã®æœ€é©åŒ–è©¦è¡Œã‚’å®Ÿè¡Œã—ã¾ã—ãŸãŒã€10%ä»¥ä¸Šã®å¤§å¹…æ”¹å–„ã«ã¯åˆ°é”ã›ãšã€
ãƒ™ã‚¹ãƒˆçµæžœã‚‚å…ƒã‚¯ã‚¨ãƒªã‚’ä¸Šå›žã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚

## å…ƒã®ã‚¯ã‚¨ãƒª

```sql
{original_query}
```

## æŽ¨å¥¨äº‹é …

- ãƒ‡ãƒ¼ã‚¿é‡ã‚„ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆæƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„
- ã‚ˆã‚Šè©³ç´°ãªEXPLAINæƒ…å ±ã‚’å–å¾—ã—ã¦æ‰‹å‹•æœ€é©åŒ–ã‚’æ¤œè¨Žã—ã¦ãã ã•ã„  
- Liquid Clusteringã‚„ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆã®æ›´æ–°ã‚’æ¤œè¨Žã—ã¦ãã ã•ã„
"""
        
        final_query = original_query
        final_optimized_result = failure_report
        final_performance_comparison = None
        final_status = 'optimization_failed'
        achievement_type = 'no_improvement'
    
    return {
        'final_status': final_status,
        'final_query': final_query,
        'total_attempts': len(optimization_attempts),
        'optimization_attempts': optimization_attempts,
        'performance_comparison': final_performance_comparison,
        'optimized_result': final_optimized_result,
        'saved_files': None,  # ãƒ¡ã‚¤ãƒ³å‡¦ç†ã§ä¿å­˜
        'best_result': best_result,
        'achievement_type': achievement_type,
        'fallback_reason': f'Best result from {max_optimization_attempts} attempts selected' if best_result['attempt_num'] > 0 else 'All attempts failed or degraded'
    }


def execute_explain_with_retry_logic(original_query: str, analysis_result: str, metrics: Dict[str, Any], max_retries: int = 3) -> Dict[str, Any]:
    """
    EXPLAIN execution and error correction retry logic (syntax errors only)
    Attempts automatic correction up to max_retries times, uses original query on failure
    """
    from datetime import datetime
    
    print(f"\nðŸ”„ EXPLAIN execution and automatic error correction (max {max_retries} attempts)")
    print("=" * 60)
    
    # Initial optimization query generation
    print("ðŸ¤– Step 1: Initial optimization query generation")
    optimized_query = generate_optimized_query_with_llm(original_query, analysis_result, metrics)
    
    # ðŸ› DEBUG: å˜ä½“æœ€é©åŒ–ã‚¯ã‚¨ãƒªã‚’ä¿å­˜ï¼ˆåå¾©æœ€é©åŒ–ä»¥å¤–ã®ãƒ‘ã‚¹ï¼‰
    if isinstance(optimized_query, str) and not optimized_query.startswith("LLM_ERROR:"):
        save_debug_query_trial(optimized_query, 1, "single_optimization", query_id="direct_path")
    
    # LLMã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¦ï¼‰
    if isinstance(optimized_query, str) and optimized_query.startswith("LLM_ERROR:"):
        print("âŒ Error occurred in LLM optimization, using original query")
        print(f"ðŸ”§ Error details: {optimized_query[10:]}")  # Remove "LLM_ERROR:"
        
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ã—ã¦å³åº§ã«ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
        fallback_result = save_optimized_sql_files(
            original_query,
            f"# âŒ LLMæœ€é©åŒ–ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚ã€å…ƒã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨\n\n## ã‚¨ãƒ©ãƒ¼è©³ç´°\n{optimized_query[10:]}\n\n## å…ƒã®ã‚¯ã‚¨ãƒª\n```sql\n{original_query}\n```",
            metrics,
            analysis_result,
            "",  # llm_response
            None,  # performance_comparison
            None,  # best_attempt_number
            None,  # optimization_attempts
            False  # ðŸš€ æœ€é©åŒ–å¤±æ•—ï¼ˆLLMã‚¨ãƒ©ãƒ¼ï¼‰
        )
        
        return {
            'final_status': 'llm_error',
            'final_query': original_query,
            'total_attempts': 0,
            'all_attempts': [],
            'explain_result': None,
            'optimized_result': optimized_query,
            'error_details': optimized_query[10:]
        }
    
    # thinking_enabledå¯¾å¿œ: ãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆã¯ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
    if isinstance(optimized_query, list):
        optimized_query_str = extract_main_content_from_thinking_response(optimized_query)
    else:
        optimized_query_str = str(optimized_query)
    
    # SQLã‚¯ã‚¨ãƒªéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
    extracted_sql = extract_sql_from_llm_response(optimized_query_str)
    current_query = extracted_sql if extracted_sql else original_query
    
    retry_count = 0
    all_attempts = []  # å…¨è©¦è¡Œã®è¨˜éŒ²
    
    while retry_count <= max_retries:
        attempt_num = retry_count + 1
        print(f"\nðŸ” Attempt {attempt_num}/{max_retries + 1}: EXPLAIN execution")
        
        # EXPLAINå®Ÿè¡Œï¼ˆæœ€é©åŒ–å¾Œã‚¯ã‚¨ãƒªï¼‰
        explain_result = execute_explain_and_save_to_file(current_query, "optimized")
        
        # æˆåŠŸæ™‚ã®å‡¦ç†
        if 'explain_file' in explain_result and 'error_file' not in explain_result:
            print(f"âœ… Succeeded in attempt {attempt_num}!")
            
            # ðŸš¨ ä¿®æ­£ï¼šãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒã¯åå¾©æœ€é©åŒ–é–¢æ•°ã§ä¸€å…ƒåŒ–
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒã‚’ã“ã“ã§å®Ÿè¡Œã™ã‚‹ã¨äºŒé‡å®Ÿè¡Œã«ãªã‚‹ãŸã‚å‰Šé™¤
            
            # ðŸš¨ ä¿®æ­£ï¼šä»¥ä¸‹ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒã‚’ç„¡åŠ¹åŒ–ï¼ˆäºŒé‡å®Ÿè¡Œé˜²æ­¢ï¼‰
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒã¯ execute_iterative_optimization_with_degradation_analysis ã§ä¸€å…ƒåŒ–
            
            # ðŸ”§ æ§‹æ–‡ãƒã‚§ãƒƒã‚¯æˆåŠŸã®ãŸã‚ã€å³åº§ã« success ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã§æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸
            performance_comparison = None  # åå¾©æœ€é©åŒ–ã§è¨­å®šã•ã‚Œã‚‹
            
            if False:  # ðŸš¨ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒãƒ–ãƒ­ãƒƒã‚¯ã‚’ç„¡åŠ¹åŒ–
                
                try:
                    # EXPLAIN COSTãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿è¾¼ã¿
                    with open(original_explain_cost_result['explain_cost_file'], 'r', encoding='utf-8') as f:
                        original_cost_content = f.read()
                    
                    with open(optimized_explain_cost_result['explain_cost_file'], 'r', encoding='utf-8') as f:
                        optimized_cost_content = f.read()
                    
                    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒå®Ÿè¡Œ
                    performance_comparison = compare_query_performance(original_cost_content, optimized_cost_content)
                    
                    print(f"ðŸ“Š Performance comparison results:")
                    cost_ratio = performance_comparison.get('total_cost_ratio', 1.0) or 1.0
                    memory_ratio = performance_comparison.get('memory_usage_ratio', 1.0) or 1.0
                    print(f"   - Execution cost ratio: {cost_ratio:.2f}x")
                    print(f"   - Memory usage ratio: {memory_ratio:.2f}x")
                    print(f"   - Recommendation: {performance_comparison['recommendation']}")
                    
                    for detail in performance_comparison['details']:
                        print(f"   - {detail}")
                    
                    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ‚ªåŒ–ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆ
                    if performance_comparison['performance_degradation_detected']:
                        print("ðŸš¨ Performance degradation detected! Using original query")
                        
                        # å…ƒã‚¯ã‚¨ãƒªã§ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ‚ªåŒ–é˜²æ­¢ï¼‰
                        fallback_result = save_optimized_sql_files(
                            original_query,
                            f"# ðŸš¨ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ‚ªåŒ–æ¤œå‡ºã®ãŸã‚å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨\n\n## æ‚ªåŒ–è¦å› \n{'; '.join(performance_comparison['details'])}\n\n## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒçµæžœ\n- å®Ÿè¡Œã‚³ã‚¹ãƒˆæ¯”: {cost_ratio:.2f}å€\n- ãƒ¡ãƒ¢ãƒªä½¿ç”¨æ¯”: {memory_ratio:.2f}å€\n\n## å…ƒã®ã‚¯ã‚¨ãƒªï¼ˆæœ€é©åŒ–å‰ï¼‰\n```sql\n{original_query}\n```",
                            metrics,
                            analysis_result,
                            "",  # llm_response
                            performance_comparison,  # ðŸ” è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒçµæžœã‚’å«ã‚ã‚‹
                            None,  # best_attempt_number
                            None,  # optimization_attempts
                            False  # ðŸš€ æœ€é©åŒ–å¤±æ•—ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ‚ªåŒ–ï¼‰
                        )
                        
                        return {
                            'final_status': 'performance_degradation_detected',
                            'final_query': original_query,
                            'total_attempts': attempt_num,
                            'all_attempts': all_attempts,
                            'explain_result': original_explain_cost_result,
                            'optimized_result': optimized_query,
                            'performance_comparison': performance_comparison,
                            'fallback_reason': 'performance_degradation'
                        }
                    
                    else:
                        print("âœ… Performance improvement confirmed. Using optimized query")
                    
                except Exception as e:
                    print(f"âš ï¸ Error occurred in performance comparison: {str(e)}")
                    print("ðŸ”„ Using original query for safety")
                    
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚å®‰å…¨å´ã«å€’ã—ã¦å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨
                    fallback_result = save_optimized_sql_files(
                        original_query,
                        f"# âš ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒã‚¨ãƒ©ãƒ¼ã®ãŸã‚å®‰å…¨æ€§ã‚’å„ªå…ˆã—ã¦å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨\n\n## ã‚¨ãƒ©ãƒ¼è©³ç´°\n{str(e)}\n\n## å…ƒã®ã‚¯ã‚¨ãƒª\n```sql\n{original_query}\n```",
                        metrics,
                        analysis_result,
                        "",  # llm_response
                        None,  # performance_comparison
                        None,  # best_attempt_number
                        None,  # optimization_attempts
                        False  # ðŸš€ æœ€é©åŒ–å¤±æ•—ï¼ˆæ¯”è¼ƒã‚¨ãƒ©ãƒ¼ï¼‰
                    )
                    
                    return {
                        'final_status': 'performance_comparison_error',
                        'final_query': original_query,
                        'total_attempts': attempt_num,
                        'all_attempts': all_attempts,
                        'explain_result': explain_result,
                        'optimized_result': optimized_query,
                        'fallback_reason': 'performance_comparison_error',
                        'error_details': str(e)
                    }
            
            # ðŸš¨ ä¿®æ­£ï¼šelseéƒ¨åˆ†ã‚‚ç„¡åŠ¹åŒ–ï¼ˆäºŒé‡å®Ÿè¡Œé˜²æ­¢ï¼‰
            # else:
            #     print("âš ï¸ Skipping performance comparison due to EXPLAIN COST acquisition failure")
#     print("ðŸ”„ Using syntactically valid optimized query")
            
            # æˆåŠŸè¨˜éŒ²
            attempt_record = {
                'attempt': attempt_num,
                'status': 'success',
                'query': current_query,
                'explain_file': explain_result.get('explain_file'),
                'plan_lines': explain_result.get('plan_lines', 0),
                'performance_comparison': performance_comparison
            }
            all_attempts.append(attempt_record)
            
            # æœ€çµ‚çµæžœï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ‚ªåŒ–ãªã—ã®å ´åˆï¼‰
            return {
                'final_status': 'success',
                'final_query': current_query,
                'total_attempts': attempt_num,
                'all_attempts': all_attempts,
                'explain_result': explain_result,
                'optimized_result': optimized_query,  # å…ƒã®å®Œå…¨ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹
                'performance_comparison': performance_comparison
            }
        
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®å‡¦ç†
        elif 'error_file' in explain_result:
            error_message = explain_result.get('error_message', 'Unknown error')
            print(f"âŒ Error occurred in attempt {attempt_num}: {error_message}")
            
            # ã‚¨ãƒ©ãƒ¼è¨˜éŒ²
            attempt_record = {
                'attempt': attempt_num,
                'status': 'error',
                'query': current_query,
                'error_message': error_message,
                'error_file': explain_result.get('error_file')
            }
            all_attempts.append(attempt_record)
            
            # æœ€å¤§è©¦è¡Œå›žæ•°ã«é”ã—ãŸå ´åˆ
            if retry_count >= max_retries:
                print(f"ðŸš¨ Maximum number of attempts ({max_retries}) reached")
                print("ðŸ“‹ Using original working query")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…ƒã‚¯ã‚¨ãƒªã§ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
                fallback_result = save_optimized_sql_files(
                    original_query, 
                    f"# ðŸš¨ æœ€é©åŒ–ã‚¯ã‚¨ãƒªã®EXPLAINå®Ÿè¡ŒãŒ{max_retries}å›žã¨ã‚‚å¤±æ•—ã—ãŸãŸã‚ã€å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨\n\n## æœ€å¾Œã®ã‚¨ãƒ©ãƒ¼æƒ…å ±\n{error_message}\n\n## å…ƒã®ã‚¯ã‚¨ãƒª\n```sql\n{original_query}\n```",
                    metrics,
                    analysis_result,
                    "",  # llm_response
                    None,  # performance_comparison
                    None,  # best_attempt_number
                    None,  # optimization_attempts
                    False  # ðŸš€ æœ€é©åŒ–å¤±æ•—ï¼ˆæœ€å¤§è©¦è¡Œé”æˆï¼‰
                )
                
                # å¤±æ•—æ™‚ã®ãƒ­ã‚°è¨˜éŒ²
                timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                log_filename = f"output_optimization_failure_log_{timestamp}.txt"
                
                try:
                    with open(log_filename, 'w', encoding='utf-8') as f:
                        f.write(f"# æœ€é©åŒ–ã‚¯ã‚¨ãƒªç”Ÿæˆå¤±æ•—ãƒ­ã‚°\n")
                        f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                        f.write(f"æœ€å¤§è©¦è¡Œå›žæ•°: {max_retries}å›ž\n")
                        f.write(f"æœ€çµ‚çµæžœ: å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨\n\n")
                        
                        f.write("=" * 80 + "\n")
                        f.write("å…¨è©¦è¡Œã®è©³ç´°è¨˜éŒ²:\n")
                        f.write("=" * 80 + "\n\n")
                        
                        for attempt in all_attempts:
                            f.write(f"ã€è©¦è¡Œ {attempt['attempt']}ã€‘\n")
                            f.write(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {attempt['status']}\n")
                            if attempt['status'] == 'error':
                                f.write(f"ã‚¨ãƒ©ãƒ¼: {attempt['error_message']}\n")
                                f.write(f"ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«: {attempt.get('error_file', 'N/A')}\n")
                            f.write(f"ä½¿ç”¨ã‚¯ã‚¨ãƒªé•·: {len(attempt['query'])} æ–‡å­—\n\n")
                        
                        f.write("=" * 80 + "\n")
                        f.write("å…ƒã®ã‚¯ã‚¨ãƒªï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½¿ç”¨ï¼‰:\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(original_query)
                    
                    print(f"ðŸ“„ Saved failure log: {log_filename}")
                    
                except Exception as log_error:
                                            print(f"âŒ Failed to save failure log as well: {str(log_error)}")
                
                return {
                    'final_status': 'fallback_to_original',
                    'final_query': original_query,
                    'total_attempts': attempt_num,
                    'all_attempts': all_attempts,
                    'fallback_files': fallback_result,
                    'failure_log': log_filename
                }
            
            # å†è©¦è¡Œã™ã‚‹å ´åˆã®ã‚¨ãƒ©ãƒ¼ä¿®æ­£
            retry_count += 1
            print(f"ðŸ”§ Correcting error for attempt {retry_count + 1}...")
            
            # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å«ã‚ã¦å†ç”Ÿæˆï¼ˆåˆå›žæœ€é©åŒ–ã‚¯ã‚¨ãƒªã‚‚æ¸¡ã™ï¼‰
            corrected_query = generate_optimized_query_with_error_feedback(
                original_query, 
                analysis_result, 
                metrics, 
                error_message,
                current_query  # ðŸš€ åˆå›žæœ€é©åŒ–ã‚¯ã‚¨ãƒªï¼ˆãƒ’ãƒ³ãƒˆä»˜ãï¼‰ã‚’æ¸¡ã™
            )
            
            # ðŸ› DEBUG: å†è©¦è¡Œæ™‚ã®ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã‚¯ã‚¨ãƒªã‚’ä¿å­˜
            if isinstance(corrected_query, str) and not corrected_query.startswith("LLM_ERROR:"):
                save_debug_query_trial(corrected_query, retry_count + 1, "retry_error_correction", 
                                     query_id=f"retry_{retry_count + 1}", 
                                     error_info=f"å†è©¦è¡Œ{retry_count + 1}ã®ã‚¨ãƒ©ãƒ¼ä¿®æ­£: {error_message[:100]}")
            
            # LLMã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¨ãƒ©ãƒ¼ä¿®æ­£æ™‚ï¼‰
            if isinstance(corrected_query, str) and corrected_query.startswith("LLM_ERROR:"):
                print("âŒ LLM error occurred even in error correction, using original query")
                print(f"ðŸ”§ Error details: {corrected_query[10:]}")  # Remove "LLM_ERROR:"
                
                # å¤±æ•—è¨˜éŒ²
                attempt_record = {
                    'attempt': retry_count + 1,
                    'status': 'llm_error_correction_failed',
                    'query': current_query,
                    'error_message': f"ã‚¨ãƒ©ãƒ¼ä¿®æ­£æ™‚LLMã‚¨ãƒ©ãƒ¼: {corrected_query[10:]}",
                    'error_file': None
                }
                all_attempts.append(attempt_record)
                
                # å…ƒã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
                fallback_result = save_optimized_sql_files(
                    original_query,
                    f"# âŒ ã‚¨ãƒ©ãƒ¼ä¿®æ­£æ™‚ã‚‚LLMã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚ã€å…ƒã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨\n\n## ã‚¨ãƒ©ãƒ¼ä¿®æ­£æ™‚ã®ã‚¨ãƒ©ãƒ¼è©³ç´°\n{corrected_query[10:]}\n\n## å…ƒã®ã‚¯ã‚¨ãƒª\n```sql\n{original_query}\n```",
                    metrics,
                    analysis_result,
                    "",  # llm_response
                    None,  # performance_comparison
                    None,  # best_attempt_number
                    None,  # optimization_attempts
                    False  # ðŸš€ æœ€é©åŒ–å¤±æ•—ï¼ˆä¿®æ­£æ™‚LLMã‚¨ãƒ©ãƒ¼ï¼‰
                )
                
                return {
                    'final_status': 'llm_error_correction_failed',
                    'final_query': original_query,
                    'total_attempts': retry_count + 1,
                    'all_attempts': all_attempts,
                    'explain_result': None,
                    'optimized_result': corrected_query,
                    'error_details': corrected_query[10:]
                }
            
            # thinking_enabledå¯¾å¿œ
            if isinstance(corrected_query, list):
                corrected_query_str = extract_main_content_from_thinking_response(corrected_query)
            else:
                corrected_query_str = str(corrected_query)
            
            # SQLã‚¯ã‚¨ãƒªéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
            extracted_sql = extract_sql_from_llm_response(corrected_query_str)
            current_query = extracted_sql if extracted_sql else current_query
            
            print(f"âœ… Generated error correction query ({len(current_query)} characters)")
    
    # ã“ã“ã«ã¯åˆ°é”ã—ãªã„ã¯ãšã ãŒã€å®‰å…¨ã®ãŸã‚
    return {
        'final_status': 'unexpected_error',
        'final_query': original_query,
        'total_attempts': retry_count + 1,
        'all_attempts': all_attempts
    }


def extract_sql_from_llm_response(llm_response: str) -> str:
    """
    Extract only SQL query part from LLM response (Enhanced version)
    åˆ†æžãƒ†ã‚­ã‚¹ãƒˆã¨SQLã‚’æ­£ç¢ºã«åˆ†é›¢ã™ã‚‹æ”¹å–„ç‰ˆ
    """
    import re
    
    if not llm_response or not llm_response.strip():
        return ""
    
    # 1. SQLã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¤œç´¢ï¼ˆ```sql ... ```ï¼‰
    sql_pattern = r'```sql\s*(.*?)\s*```'
    matches = re.findall(sql_pattern, llm_response, re.DOTALL | re.IGNORECASE)
    
    if matches:
        # æœ€é•·ã®SQLãƒ–ãƒ­ãƒƒã‚¯ã‚’é¸æŠž
        sql_query = max(matches, key=len).strip()
        return clean_extracted_sql(sql_query)
    
    # 2. ä¸€èˆ¬çš„ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¤œç´¢ï¼ˆ```ã®ã¿ï¼‰
    code_pattern = r'```\s*(.*?)\s*```'
    matches = re.findall(code_pattern, llm_response, re.DOTALL)
    
    for match in matches:
        match = match.strip()
        # SQLã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§å§‹ã¾ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if re.match(r'^(SELECT|WITH|CREATE|INSERT|UPDATE|DELETE|EXPLAIN)', match, re.IGNORECASE):
            return clean_extracted_sql(match)
    
    # 3. SQLã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§å§‹ã¾ã‚‹è¡Œã‹ã‚‰åˆ†æžã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¾ã§ã‚’æŠ½å‡º
    lines = llm_response.split('\n')
    sql_lines = []
    in_sql = False
    
    for line in lines:
        line_stripped = line.strip()
        
        # SQLé–‹å§‹ã®æ¤œå‡ºï¼ˆã‚ˆã‚ŠåŽ³å¯†ï¼‰
        if re.match(r'^(WITH|SELECT|FROM|CREATE|INSERT|UPDATE|DELETE)\s', line_stripped, re.IGNORECASE):
            in_sql = True
        
        if in_sql:
            # åˆ†æžã‚»ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹ã§SQLçµ‚äº†ï¼ˆåŽ³å¯†ãªæ¤œå‡ºï¼‰
            if (line_stripped.startswith('##') or 
                line_stripped.startswith('**') and ('æ”¹å–„' in line_stripped or 'åŠ¹æžœ' in line_stripped or 'æ ¹æ‹ ' in line_stripped) or
                'æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ' in line_stripped or 
                'æœŸå¾…åŠ¹æžœ' in line_stripped or
                'JOINæœ€é©åŒ–ã®æ ¹æ‹ ' in line_stripped or
                'æœ€é©åŒ–æ‰‹æ³•' in line_stripped or
                'EXPLAIN COSTãƒ™ãƒ¼ã‚¹ã®' in line_stripped):
                break
            
            # æœ‰åŠ¹ãªSQLè¡Œã‚’è¿½åŠ 
            sql_lines.append(line)
    
    if sql_lines:
        return clean_extracted_sql('\n'.join(sql_lines).strip())
    
    # 4. ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒžãƒƒãƒã—ãªã„å ´åˆã¯å…ƒã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãã®ã¾ã¾è¿”ã™
    return llm_response.strip()


def clean_extracted_sql(sql_content: str) -> str:
    """
    æŠ½å‡ºã•ã‚ŒãŸSQLã‹ã‚‰ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤åŽ»
    """
    if not sql_content:
        return ""
    
    # åˆ†æžãƒ†ã‚­ã‚¹ãƒˆã®æ··å…¥ã‚’é™¤åŽ»
    lines = sql_content.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line_stripped = line.strip()
        
        # åˆ†æžãƒ†ã‚­ã‚¹ãƒˆã®é™¤åŽ»
        if (line_stripped.startswith('**') or
            line_stripped.startswith('##') or
            'æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ' in line_stripped or
            'æœŸå¾…åŠ¹æžœ' in line_stripped or
            'æœ€é©åŒ–æ‰‹æ³•' in line_stripped or
            'EXPLAIN COST' in line_stripped and 'ãƒ™ãƒ¼ã‚¹' in line_stripped):
            break
        
        # ç©ºè¡Œã§ãªã„ã€ã¾ãŸã¯æ„å‘³ã®ã‚ã‚‹è¡Œã®ã¿è¿½åŠ 
        if line_stripped or (cleaned_lines and not cleaned_lines[-1].strip()):
            cleaned_lines.append(line)
    
    return '\n'.join(cleaned_lines).strip()


def extract_analysis_content_from_llm_response(llm_response: str) -> str:
    """
    LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰åˆ†æžçµæžœéƒ¨åˆ†ã‚’æŠ½å‡º
    SQLã‚³ãƒ¼ãƒ‰ã¨åˆ†é›¢ã—ã¦ã€åˆ†æžãƒ¬ãƒãƒ¼ãƒˆç”¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
    """
    import re
    from datetime import datetime
    
    if not llm_response or not llm_response.strip():
        return ""
    
    # SQLã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤åŽ»ã—ãŸæ®‹ã‚Šã®éƒ¨åˆ†ã‚’æŠ½å‡º
    lines = llm_response.split('\n')
    analysis_lines = []
    in_sql_block = False
    sql_block_pattern = re.compile(r'```sql', re.IGNORECASE)
    sql_block_end_pattern = re.compile(r'```')
    
    for line in lines:
        line_stripped = line.strip()
        
        # SQLãƒ–ãƒ­ãƒƒã‚¯ã®é–‹å§‹ã‚’æ¤œå‡º
        if sql_block_pattern.search(line):
            in_sql_block = True
            continue
        
        # SQLãƒ–ãƒ­ãƒƒã‚¯ã®çµ‚äº†ã‚’æ¤œå‡º
        if in_sql_block and sql_block_end_pattern.search(line):
            in_sql_block = False
            continue
        
        # SQLãƒ–ãƒ­ãƒƒã‚¯å†…ã§ãªã„å ´åˆã¯åˆ†æžã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ã—ã¦è¿½åŠ 
        if not in_sql_block:
            # SQLæ–‡ã®è¡Œã‚‚é™¤å¤–ï¼ˆSQLãƒ–ãƒ­ãƒƒã‚¯å¤–ã«ã‚ã‚‹SQLæ–‡ï¼‰
            if not re.match(r'^(WITH|SELECT|FROM|WHERE|GROUP BY|ORDER BY|LIMIT|CREATE|INSERT|UPDATE|DELETE)\s', line_stripped, re.IGNORECASE):
                analysis_lines.append(line)
    
    # åˆ†æžã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ•´ç†
    analysis_content = '\n'.join(analysis_lines).strip()
    
    # ç©ºã®åˆ†æžã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å ´åˆã¯åŸºæœ¬çš„ãªæƒ…å ±ã‚’è¿½åŠ 
    if not analysis_content or len(analysis_content) < 100:
        analysis_content = f"""# SQLæœ€é©åŒ–åˆ†æžçµæžœ

## æ¦‚è¦
LLMã«ã‚ˆã‚‹SQLæœ€é©åŒ–ãŒå®Ÿè¡Œã•ã‚Œã¾ã—ãŸã€‚

## æœ€é©åŒ–å†…å®¹
- å®Ÿè¡Œãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã®æ”¹å–„
- SQLã‚¯ã‚¨ãƒªæ§‹é€ ã®æœ€é©åŒ–
- åŠ¹çŽ‡çš„ãªJOINå‡¦ç†ã®å®Ÿè£…

## æ³¨æ„äº‹é …
è©³ç´°ãªåˆ†æžçµæžœã¯æœ€é©åŒ–ã•ã‚ŒãŸSQLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚

ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    return analysis_content


def execute_explain_and_save_to_file(original_query: str, query_type: str = "original") -> Dict[str, str]:
    """
    Execute EXPLAIN and EXPLAIN COST statements for queries and save results to file based on EXPLAIN_ENABLED setting
    For CTAS, extract only the SELECT part and pass it to EXPLAIN statement
    
    Args:
        original_query: Query to execute EXPLAIN on
        query_type: "original" or "optimized" to identify filename
    """
    from datetime import datetime
    import os
    
    if not original_query or not original_query.strip():
        print("âŒ Query is empty")
        return {}
    
    # EXPLAIN_ENABLEDè¨­å®šã‚’ç¢ºèª
    explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
    debug_enabled = globals().get('DEBUG_ENABLED', 'N')
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆï¼ˆEXPLAIN_ENABLED=Yã®å ´åˆã®ã¿ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    if explain_enabled.upper() == 'Y':
        explain_filename = f"output_explain_{query_type}_{timestamp}.txt"
        explain_cost_filename = f"output_explain_cost_{query_type}_{timestamp}.txt"
    else:
        explain_filename = None
        explain_cost_filename = None
    
    # CTASã®å ´åˆã¯SELECTéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
    query_for_explain = extract_select_from_ctas(original_query)
    
    # EXPLAINæ–‡ã¨EXPLAIN COSTæ–‡ã®ç”Ÿæˆ
    explain_query = f"EXPLAIN {query_for_explain}"
    explain_cost_query = f"EXPLAIN COST {query_for_explain}"
    
    # ã‚«ã‚¿ãƒ­ã‚°ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®è¨­å®šã‚’å–å¾—
    catalog = globals().get('CATALOG', 'main')
    database = globals().get('DATABASE', 'default')
    
    print(f"ðŸ“‚ Using catalog: {catalog}")
    print(f"ðŸ—‚ï¸ Using database: {database}")
    
    # ã‚«ã‚¿ãƒ­ã‚°ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’è¨­å®š
    try:
        spark.sql(f"USE CATALOG {catalog}")
        spark.sql(f"USE DATABASE {database}")
    except Exception as e:
        print(f"âš ï¸ Catalog/database configuration error: {str(e)}")
    
    # EXPLAINæ–‡ã¨EXPLAIN COSTæ–‡ã®å®Ÿè¡Œ
    try:
        print("ðŸ”„ Executing EXPLAIN and EXPLAIN COST statements...")
        
        # 1. é€šå¸¸ã®EXPLAINå®Ÿè¡Œ
        print("   ðŸ“Š Executing EXPLAIN...")
        explain_result_spark = spark.sql(explain_query)
        explain_result = explain_result_spark.collect()
        
        # EXPLAINçµæžœã®å†…å®¹ã‚’ãƒã‚§ãƒƒã‚¯
        explain_content = ""
        for row in explain_result:
            explain_content += str(row[0]) + "\n"
        
        # 2. EXPLAIN COSTå®Ÿè¡Œ
        print("   ðŸ’° Executing EXPLAIN COST...")
        explain_cost_result_spark = spark.sql(explain_cost_query)
        explain_cost_result = explain_cost_result_spark.collect()
        
        # EXPLAIN COSTçµæžœã®å†…å®¹ã‚’ãƒã‚§ãƒƒã‚¯
        explain_cost_content = ""
        for row in explain_cost_result:
            explain_cost_content += str(row[0]) + "\n"
        
        # ðŸš¨ ç·Šæ€¥ä¿®æ­£: ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åŽ³å¯†åŒ–ï¼ˆèª¤æ¤œå‡ºé˜²æ­¢ï¼‰
        retryable_error_patterns = [
            "Error occurred during query planning",
            "error occurred during query planning", 
            "Query planning failed",
            "query planning failed",
            "Plan optimization failed",
            "plan optimization failed",
            "Failed to plan query",
            "failed to plan query",
            "Analysis exception",
            "analysis exception",
            "AMBIGUOUS_REFERENCE",
            "ambiguous_reference",
            "[AMBIGUOUS_REFERENCE]",
            # "Reference",  # ðŸš¨ é™¤åŽ»: éŽåº¦ã«ä¸€èˆ¬çš„ã€æ­£å¸¸çµæžœã‚‚èª¤æ¤œå‡º
            "reference is ambiguous",  # ã‚ˆã‚Šå…·ä½“çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¤‰æ›´
            # "is ambiguous",  # ðŸš¨ é™¤åŽ»: éŽåº¦ã«ä¸€èˆ¬çš„
            "ambiguous reference",  # ã‚ˆã‚Šå…·ä½“çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¤‰æ›´
            # "Ambiguous",  # ðŸš¨ é™¤åŽ»: éŽåº¦ã«ä¸€èˆ¬çš„
            "ParseException",
            "SemanticException", 
            "AnalysisException",
            "Syntax error",
            "syntax error",
            "PARSE_SYNTAX_ERROR",
            "INVALID_IDENTIFIER",
            "TABLE_OR_VIEW_NOT_FOUND",
            "COLUMN_NOT_FOUND",
            "UNRESOLVED_COLUMN",
            "[UNRESOLVED_COLUMN",
            "UNRESOLVED_COLUMN.WITH_SUGGESTION",
            "[UNRESOLVED_COLUMN.WITH_SUGGESTION]"
        ]
        
        # ðŸš¨ é‡è¦: EXPLAINçµæžœã¨EXPLAIN COSTçµæžœã®ä¸¡æ–¹ã‚’ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        detected_error = None
        error_source = None
        
        # ðŸš¨ ç·Šæ€¥ãƒ‡ãƒãƒƒã‚°: ã‚¨ãƒ©ãƒ¼æ¤œå‡ºãƒ—ãƒ­ã‚»ã‚¹ã®è©³ç´°è¡¨ç¤º
        print(f"ðŸ” Executing error pattern detection (patterns: {len(retryable_error_patterns)})")
        print(f"   ðŸ“Š EXPLAIN content length: {len(explain_content)} characters")
        print(f"   ðŸ’° EXPLAIN COST content length: {len(explain_cost_content)} characters")
        
        # 1. EXPLAINçµæžœã®ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        for pattern in retryable_error_patterns:
            if pattern in explain_content.lower():
                detected_error = pattern
                error_source = "EXPLAIN"
                print(f"âŒ Error pattern detected in EXPLAIN result: '{pattern}'")
                break
        
        # 2. EXPLAIN COSTçµæžœã®ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆEXPLAINã§ã‚¨ãƒ©ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã¿ï¼‰
        if not detected_error:
            for pattern in retryable_error_patterns:
                if pattern in explain_cost_content.lower():
                    detected_error = pattern
                    error_source = "EXPLAIN COST"
                    print(f"âŒ Error pattern detected in EXPLAIN COST result: '{pattern}'")
                    break
        
        if not detected_error:
            print("âœ… No error patterns detected: Processing as normal result")
        
        if detected_error:
            # ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦å‡¦ç†
            print(f"âŒ Error detected in {error_source} result: {detected_error}")
            
            # çµæžœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºï¼ˆã‚¨ãƒ©ãƒ¼ç”¨ï¼‰
            print(f"\nðŸ“‹ {error_source} result preview:")
            print("-" * 50)
            if error_source == "EXPLAIN":
                preview_lines = min(10, len(explain_result))
                for i, row in enumerate(explain_result[:preview_lines]):
                    print(f"{i+1:2d}: {str(row[0])[:100]}...")
            else:
                preview_lines = min(10, len(explain_cost_result))
                for i, row in enumerate(explain_cost_result[:preview_lines]):
                    print(f"{i+1:2d}: {str(row[0])[:100]}...")
            
            # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ï¼ˆEXPLAIN_ENABLED=Yã®å ´åˆã®ã¿ï¼‰
            error_filename = None
            error_cost_filename = None
            if explain_enabled.upper() == 'Y':
                # EXPLAINçµæžœã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«
                error_filename = f"output_explain_error_{query_type}_{timestamp}.txt"
                with open(error_filename, 'w', encoding='utf-8') as f:
                    f.write(f"# EXPLAINå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({query_type}ã‚¯ã‚¨ãƒª)\n")
                    f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—: {query_type}\n")
                    f.write(f"ã‚¨ãƒ©ãƒ¼æ¤œå‡ºå…ƒ: {error_source}\n")
                    f.write(f"æ¤œå‡ºã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³: {detected_error}\n")
                    f.write(f"ã‚¯ã‚¨ãƒªæ–‡å­—æ•°: {len(original_query):,}\n")
                    f.write("\n" + "=" * 80 + "\n")
                    f.write("EXPLAIN çµæžœ:\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(explain_content)
                    f.write("\n" + "=" * 80 + "\n")
                    f.write("EXPLAIN COST çµæžœ:\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(explain_cost_content)
                
                print(f"ðŸ“„ Saved error details: {error_filename}")
                if error_source == "EXPLAIN" and len(explain_result) > preview_lines:
                    print(f"... (Remaining {len(explain_result) - preview_lines} lines, see {error_filename})")
                elif error_source == "EXPLAIN COST" and len(explain_cost_result) > preview_lines:
                    print(f"... (Remaining {len(explain_cost_result) - preview_lines} lines, see {error_filename})")
            else:
                print("ðŸ’¡ Error file not saved because EXPLAIN_ENABLED=N")
                if error_source == "EXPLAIN" and len(explain_result) > preview_lines:
                    print(f"... (Remaining {len(explain_result) - preview_lines} lines)")
                elif error_source == "EXPLAIN COST" and len(explain_cost_result) > preview_lines:
                    print(f"... (Remaining {len(explain_cost_result) - preview_lines} lines)")
            
            print("-" * 50)
            
            result_dict = {
                'error_message': explain_content.strip() if error_source == "EXPLAIN" else explain_cost_content.strip(),
                'detected_pattern': detected_error,
                'error_source': error_source
            }
            if error_filename:
                result_dict['error_file'] = error_filename
            
            return result_dict
        
        # ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œãªã‹ã£ãŸå ´åˆã¯æˆåŠŸã¨ã—ã¦å‡¦ç†
        print(f"âœ… EXPLAIN & EXPLAIN COST execution successful")
        print(f"ðŸ“Š EXPLAIN execution plan lines: {len(explain_result):,}")
        print(f"ðŸ’° EXPLAIN COST statistics lines: {len(explain_cost_result):,}")
        
        # çµæžœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
        print("\nðŸ“‹ EXPLAIN results preview:")
        print("-" * 50)
        preview_lines = min(10, len(explain_result))
        for i, row in enumerate(explain_result[:preview_lines]):
            print(f"{i+1:2d}: {str(row[0])[:100]}...")
        
        print("\nðŸ’° EXPLAIN COST results preview:")
        print("-" * 50)
        cost_preview_lines = min(10, len(explain_cost_result))
        for i, row in enumerate(explain_cost_result[:cost_preview_lines]):
            print(f"{i+1:2d}: {str(row[0])[:100]}...")
        
        # çµæžœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆEXPLAIN_ENABLED=Yã®å ´åˆã®ã¿ï¼‰
        if explain_enabled.upper() == 'Y' and explain_filename and explain_cost_filename:
            # EXPLAINçµæžœãƒ•ã‚¡ã‚¤ãƒ«
            with open(explain_filename, 'w', encoding='utf-8') as f:
                f.write(f"# EXPLAINå®Ÿè¡Œçµæžœ ({query_type}ã‚¯ã‚¨ãƒª)\n")
                f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—: {query_type}\n")
                f.write(f"ã‚¯ã‚¨ãƒªæ–‡å­—æ•°: {len(original_query):,}\n")
                f.write("\n" + "=" * 80 + "\n")
                f.write("EXPLAINçµæžœ:\n")
                f.write("=" * 80 + "\n\n")
                f.write(explain_content)
            
            # EXPLAIN COSTçµæžœãƒ•ã‚¡ã‚¤ãƒ«
            with open(explain_cost_filename, 'w', encoding='utf-8') as f:
                f.write(f"# EXPLAIN COSTå®Ÿè¡Œçµæžœ ({query_type}ã‚¯ã‚¨ãƒª)\n")
                f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—: {query_type}\n")
                f.write(f"ã‚¯ã‚¨ãƒªæ–‡å­—æ•°: {len(original_query):,}\n")
                f.write("\n" + "=" * 80 + "\n")
                f.write("EXPLAIN COSTçµæžœï¼ˆçµ±è¨ˆæƒ…å ±ä»˜ãï¼‰:\n")
                f.write("=" * 80 + "\n\n")
                f.write(explain_cost_content)
            
            print(f"ðŸ“„ Saved EXPLAIN results: {explain_filename}")
            print(f"ðŸ’° Saved EXPLAIN COST results: {explain_cost_filename}")
            if len(explain_result) > preview_lines:
                print(f"... (Remaining {len(explain_result) - preview_lines} lines, see {explain_filename})")
            if len(explain_cost_result) > cost_preview_lines:
                print(f"... (Remaining {len(explain_cost_result) - cost_preview_lines} lines, see {explain_cost_filename})")
        else:
            print("ðŸ’¡ EXPLAIN result files not saved because EXPLAIN_ENABLED=N")
            if len(explain_result) > preview_lines:
                print(f"... (Remaining {len(explain_result) - preview_lines} lines)")
            if len(explain_cost_result) > cost_preview_lines:
                print(f"... (Remaining {len(explain_cost_result) - cost_preview_lines} lines)")
        
        print("-" * 50)
        
        result_dict = {
            'plan_lines': len(explain_result),
            'cost_lines': len(explain_cost_result)
        }
        if explain_filename and explain_enabled.upper() == 'Y':
            result_dict['explain_file'] = explain_filename
        if explain_cost_filename and explain_enabled.upper() == 'Y':
            result_dict['explain_cost_file'] = explain_cost_filename
        
        return result_dict
        
    except Exception as e:
        error_message = str(e)
        print(f"âŒ Failed to execute EXPLAIN or EXPLAIN COST statement: {error_message}")
        
        # çœŸã®è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ï¼ˆãƒªãƒˆãƒ©ã‚¤ä¸å¯èƒ½ãªã‚¨ãƒ©ãƒ¼ï¼‰ã®ãƒã‚§ãƒƒã‚¯
        truly_fatal_errors = [
            "Permission denied",
            "Access denied", 
            "Insufficient privileges",
            "Database not found",
            "Catalog not found",
            "Spark session is not active",
            "java.lang.OutOfMemoryError",
            "Driver killed",
            "Connection refused"
        ]
        
        # å†è©¦è¡Œå¯èƒ½ãªã‚¨ãƒ©ãƒ¼ï¼ˆLLMã§ä¿®æ­£å¯èƒ½ï¼‰
        retryable_error_patterns = [
            "Error occurred during query planning",
            "error occurred during query planning", 
            "Query planning failed",
            "query planning failed",
            "Plan optimization failed",
            "plan optimization failed",
            "Failed to plan query",
            "failed to plan query",
            "Analysis exception",
            "analysis exception",
            "AMBIGUOUS_REFERENCE",
            "ambiguous_reference",
            "[AMBIGUOUS_REFERENCE]",
            "Reference",
            "is ambiguous",
            "Ambiguous",
            "ParseException",
            "SemanticException",
            "AnalysisException",
            "Syntax error",
            "syntax error",
            "PARSE_SYNTAX_ERROR",
            "INVALID_IDENTIFIER",
            "TABLE_OR_VIEW_NOT_FOUND",
            "COLUMN_NOT_FOUND",
            "UNRESOLVED_COLUMN",
            "[UNRESOLVED_COLUMN",
            "UNRESOLVED_COLUMN.WITH_SUGGESTION",
            "[UNRESOLVED_COLUMN.WITH_SUGGESTION]"
        ]
        
        # çœŸã®è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ã‹ãƒã‚§ãƒƒã‚¯
        is_truly_fatal = any(pattern in error_message.lower() for pattern in truly_fatal_errors)
        
        # å†è©¦è¡Œå¯èƒ½ã‚¨ãƒ©ãƒ¼ã‹ãƒã‚§ãƒƒã‚¯
        is_retryable = any(pattern in error_message.lower() for pattern in retryable_error_patterns)
        
        if is_truly_fatal:
            print(f"ðŸš¨ FATAL: Unrecoverable error occurred")
            print(f"ðŸš¨ Error details: {error_message}")
            print(f"ðŸš¨ Terminating processing.")
            
            # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ï¼ˆEXPLAIN_ENABLED=Yã®å ´åˆã®ã¿ï¼‰
            if explain_enabled.upper() == 'Y':
                error_filename = f"output_explain_fatal_error_{query_type}_{timestamp}.txt"
                try:
                    with open(error_filename, 'w', encoding='utf-8') as f:
                        f.write(f"# FATAL EXPLAINå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ (å›žå¾©ä¸å¯èƒ½, {query_type}ã‚¯ã‚¨ãƒª)\n")
                        f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                        f.write(f"ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—: {query_type}\n")
                        f.write(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {error_message}\n")
                        f.write(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: FATAL - Unrecoverable Error\n")
                        f.write("\n" + "=" * 80 + "\n")
                        f.write("å®Ÿè¡Œã—ã‚ˆã†ã¨ã—ãŸEXPLAINæ–‡:\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(explain_query)
                        f.write("\n\n" + "=" * 80 + "\n")
                        f.write("å®Ÿè¡Œã—ã‚ˆã†ã¨ã—ãŸEXPLAIN COSTæ–‡:\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(explain_cost_query)
                    
                    print(f"ðŸ“„ Saved Fatal error details: {error_filename}")
                    
                except Exception as file_error:
                    print(f"âŒ Failed to save Fatal error file: {str(file_error)}")
            else:
                print("ðŸ’¡ Fatal error file not saved because EXPLAIN_ENABLED=N")
            
            # ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†
            import sys
            sys.exit(1)
        
        elif is_retryable:
            print(f"ðŸ”„ Detected retryable error: {error_message}")
            print(f"ðŸ’¡ This error is a candidate for LLM automatic correction")
        
        # éžè‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ã®å ´åˆã®å‡¦ç†
        error_filename = None
        if explain_enabled.upper() == 'Y':
            error_filename = f"output_explain_error_{query_type}_{timestamp}.txt"
            try:
                with open(error_filename, 'w', encoding='utf-8') as f:
                    f.write(f"# EXPLAINå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({query_type}ã‚¯ã‚¨ãƒª)\n")
                    f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—: {query_type}\n")
                    f.write(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {error_message}\n")
                    f.write("\n" + "=" * 80 + "\n")
                    f.write("å®Ÿè¡Œã—ã‚ˆã†ã¨ã—ãŸEXPLAINæ–‡:\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(explain_query)
                    f.write("\n\n" + "=" * 80 + "\n")
                    f.write("å®Ÿè¡Œã—ã‚ˆã†ã¨ã—ãŸEXPLAIN COSTæ–‡:\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(explain_cost_query)
                
                print(f"ðŸ“„ Saved error details: {error_filename}")
                
            except Exception as file_error:
                print(f"âŒ Failed to save error file: {str(file_error)}")
        else:
            print("ðŸ’¡ Error file not saved because EXPLAIN_ENABLED=N")
        
        result_dict = {
            'error_message': error_message
        }
        if error_filename:
            result_dict['error_file'] = error_filename
        
        return result_dict

# EXPLAINæ–‡å®Ÿè¡Œã®å®Ÿè¡Œ
print("\nðŸ” EXPLAIN statement execution processing")
print("-" * 40)

# ã‚»ãƒ«43ã§æŠ½å‡ºã—ãŸã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãŒå¤‰æ•°ã«æ®‹ã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
try:
    # original_queryãŒæ—¢ã«å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
    original_query_for_explain = original_query
    print(f"âœ… Retrieved original query ({len(original_query_for_explain)} characters)")
    
except NameError:
    print("âš ï¸ Original query variable not found in current session")
    print("   Attempting automatic extraction from profiler data...")
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å†æŠ½å‡º
    try:
        print("ðŸ”„ Extracting original query from profiler data...")
        original_query_for_explain = extract_original_query_from_profiler_data(profiler_data)
        
        if original_query_for_explain and original_query_for_explain.strip():
            print(f"âœ… Extraction successful ({len(original_query_for_explain)} characters)")
            print(f"ðŸ” Query preview: {original_query_for_explain[:200]}{'...' if len(original_query_for_explain) > 200 else ''}")
        else:
            print("âš ï¸ Query extraction from profiler data returned empty result")
            print("   Using default sample query for demonstration")
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã‚’æä¾›
            original_query_for_explain = """
            -- Sample query for demonstration (replace with actual query)
            SELECT 
                ss_customer_sk,
                ss_item_sk,
                SUM(ss_sales_price) as total_sales,
                COUNT(*) as transaction_count
            FROM store_sales 
            WHERE ss_sold_date_sk >= 2450815
            GROUP BY ss_customer_sk, ss_item_sk
            ORDER BY total_sales DESC
            LIMIT 100
            """
            print(f"ðŸ“ Default query has been set ({len(original_query_for_explain)} characters)")
            
    except Exception as e:
        print(f"âŒ Error during extraction: {str(e)}")
        print("   Using default sample query for demonstration")
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã‚‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¯ã‚¨ãƒªã‚’è¨­å®š
        original_query_for_explain = """
        -- Sample query for demonstration (replace with actual query)
        SELECT 
            ss_customer_sk,
            ss_item_sk,
            SUM(ss_sales_price) as total_sales,
            COUNT(*) as transaction_count
        FROM store_sales 
        WHERE ss_sold_date_sk >= 2450815
        GROUP BY ss_customer_sk, ss_item_sk
        ORDER BY total_sales DESC
        LIMIT 100
        """
        print(f"ðŸ“ Default query has been set ({len(original_query_for_explain)} characters)")

# EXPLAINå®Ÿè¡Œãƒ•ãƒ©ã‚°ã®ç¢ºèª
explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
print(f"ðŸ” EXPLAIN execution setting: {explain_enabled}")

if explain_enabled.upper() != 'Y':
    print("âš ï¸ EXPLAIN execution is disabled")
    print("   To execute EXPLAIN statements, set EXPLAIN_ENABLED = 'Y' in the first cell")
elif original_query_for_explain and original_query_for_explain.strip():
    print("\nðŸš€ Integrated SQL Optimization & EXPLAIN Execution (with automatic error correction)")
    
    # Sparkç’°å¢ƒã®ç¢ºèª
    try:
        spark_version = spark.version
        print(f"ðŸ“Š Spark environment: {spark_version}")
    except Exception as e:
        print(f"âŒ Failed to check Spark environment: {str(e)}")
        print("   Please execute in Databricks environment")
        spark = None
    
    if spark:
        # çµ±åˆå‡¦ç†: åˆ†æžçµæžœãŒå¿…è¦ãªã®ã§ç¢ºèª
        try:
            # analysis_resultãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if 'analysis_result' in globals():
                current_analysis_result = analysis_result
            else:
                print("âš ï¸ Analysis results not found. Executing simple analysis...")
                current_analysis_result = "åˆ†æžçµæžœãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€åŸºæœ¬çš„ãªæœ€é©åŒ–ã®ã¿å®Ÿè¡Œ"
            
            # extracted_metricsãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯  
            if 'extracted_metrics' in globals():
                current_metrics = extracted_metrics
            else:
                print("âš ï¸ Metrics not found. Executing with empty metrics...")
                current_metrics = {}
            
            # thinking_enabledå¯¾å¿œ
            if isinstance(current_analysis_result, list):
                analysis_result_str = extract_main_content_from_thinking_response(current_analysis_result)
            else:
                analysis_result_str = str(current_analysis_result)
            
            # ðŸ” Step 1: Original query EXPLAIN execution (with pre-correction)
            print("\nðŸ“‹ Step 1: Original query EXPLAIN execution (Photon compatibility analysis)")
            print("-" * 60)
            
            # ðŸŽ¯ Save the original query as-is (relying completely on LLM correction)
            print("ðŸ“‹ Using original query as-is: Relying on advanced LLM correction")
            original_query_validated = original_query_for_explain
            
            # ðŸŽ¯ å…ƒã‚¯ã‚¨ãƒªã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜ï¼ˆé‡è¤‡å‡¦ç†é˜²æ­¢ï¼‰
            globals()['original_query_corrected'] = original_query_validated
            print("ðŸ’¾ Caching original query: Preventing duplicate processing")
            
            original_explain_result = execute_explain_and_save_to_file(original_query_for_explain, "original")
            
            # ðŸš€ EXPLAINçµæžœã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆé‡è¤‡å®Ÿè¡Œé˜²æ­¢ï¼‰
            if original_explain_result and 'error_file' not in original_explain_result:
                globals()['cached_main_original_explain_result'] = original_explain_result
                print("ðŸ’¾ Caching main EXPLAIN results: Preventing duplicate processing")
            
            # ðŸš¨ å…ƒã‚¯ã‚¨ãƒªã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã®LLMä¿®æ­£
            if 'error_file' in original_explain_result:
                print(f"ðŸš¨ Detected syntax error in original query: {original_explain_result.get('error_file', 'unknown')}")
                print("ðŸ¤– Executing LLM-based original query correction...")
                
                # ã‚¨ãƒ©ãƒ¼å†…å®¹ã‚’èª­ã¿è¾¼ã¿
                error_message = ""
                if 'error_file' in original_explain_result:
                    try:
                        with open(original_explain_result['error_file'], 'r', encoding='utf-8') as f:
                            error_message = f.read()
                    except:
                        error_message = "ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—"
                
                # LLMã«ã‚ˆã‚‹å…ƒã‚¯ã‚¨ãƒªä¿®æ­£
                corrected_original_query = generate_optimized_query_with_error_feedback(
                    original_query_for_explain,
                    "å…ƒã®ã‚¯ã‚¨ãƒªã«æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ä¿®æ­£ãŒå¿…è¦ã§ã™ã€‚",
                    current_metrics,
                    error_message,
                    ""  # previous_optimized_queryã¯ç©º
                )
                
                # ðŸ› DEBUG: å…ƒã‚¯ã‚¨ãƒªã®ã‚¨ãƒ©ãƒ¼ä¿®æ­£çµæžœã‚’ä¿å­˜
                if isinstance(corrected_original_query, str) and not corrected_original_query.startswith("LLM_ERROR:"):
                    save_debug_query_trial(corrected_original_query, 0, "original_query_correction", 
                                         query_id="original_corrected", 
                                         error_info=f"å…ƒã‚¯ã‚¨ãƒªæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ä¿®æ­£: {error_message[:100] if error_message else 'unknown error'}")
                
                # ä¿®æ­£çµæžœã‚’ãƒã‚§ãƒƒã‚¯
                if isinstance(corrected_original_query, str) and not corrected_original_query.startswith("LLM_ERROR:"):
                    print("âœ… LLM-based original query correction completed")
                    
                    # ä¿®æ­£ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã‹ã‚‰SQLã‚’æŠ½å‡º
                    if isinstance(corrected_original_query, list):
                        corrected_query_str = extract_main_content_from_thinking_response(corrected_original_query)
                    else:
                        corrected_query_str = str(corrected_original_query)
                    
                    extracted_sql = extract_sql_from_llm_response(corrected_query_str)
                    if extracted_sql:
                        original_query_for_explain = extracted_sql
                        print("ðŸ”„ Re-executing EXPLAIN with corrected query")
                        
                        # ä¿®æ­£ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã§å†åº¦EXPLAINå®Ÿè¡Œ
                        original_explain_result = execute_explain_and_save_to_file(original_query_for_explain, "original_corrected")
                        
                        # ðŸš€ ä¿®æ­£å¾ŒEXPLAINçµæžœã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆé‡è¤‡å®Ÿè¡Œé˜²æ­¢ï¼‰
                        if original_explain_result and 'error_file' not in original_explain_result:
                            globals()['cached_corrected_original_explain_result'] = original_explain_result
                            print("ðŸ’¾ Caching corrected EXPLAIN results: Preventing duplicate processing")
                        
                        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚æ›´æ–°
                        globals()['original_query_corrected'] = original_query_for_explain
                        print("ðŸ’¾ Updating cache with corrected original query")
                    else:
                        print("âŒ Failed to extract SQL from corrected query")
                else:
                    print("âŒ LLM-based original query correction failed")
            
            if 'explain_file' in original_explain_result:
                print(f"âœ… Saved original query EXPLAIN result: {original_explain_result['explain_file']}")
            if 'plan_lines' in original_explain_result:
                print(f"ðŸ“Š Original query execution plan lines: {original_explain_result['plan_lines']:,}")
            
            # ðŸš€ Step 2: New iterative optimization process: up to 3 improvement attempts with degradation cause analysis
            print("\nðŸ“‹ Step 2: Iterative LLM optimization & performance degradation analysis (max 3 improvement attempts)")
            print("-" * 60)
            max_optimization_attempts = globals().get('MAX_OPTIMIZATION_ATTEMPTS', 3)
            retry_result = execute_iterative_optimization_with_degradation_analysis(
                original_query_for_explain, 
                analysis_result_str, 
                current_metrics, 
                max_optimization_attempts=max_optimization_attempts
            )            
            # çµæžœã®è¡¨ç¤º
            print(f"\nðŸ“Š Final result: {retry_result['final_status']}")
            print(f"ðŸ”„ Total attempts: {retry_result['total_attempts']}")
            
            # åå¾©æœ€é©åŒ–ã®è©¦è¡Œè©³ç´°è¡¨ç¤º
            if 'optimization_attempts' in retry_result:
                attempts = retry_result['optimization_attempts']
                print(f"ðŸ“ˆ Optimization attempt details: {len(attempts)} times")
                for attempt in attempts:
                    status_icon = {
                        'success': 'âœ…',
                        'performance_degraded': 'ðŸš¨',
                        'llm_error': 'âŒ',
                        'explain_failed': 'âš ï¸',
                        'comparison_error': 'ðŸ”§'
                    }.get(attempt['status'], 'â“')
                    print(f"   {status_icon} Attempt {attempt['attempt']}: {attempt['status']}")
                    if 'cost_ratio' in attempt and attempt['cost_ratio'] is not None:
                        print(f"      ðŸ’° Cost ratio: {attempt['cost_ratio']:.2f}x")
            
            if retry_result['final_status'] in ['optimization_success', 'partial_success']:
                print("âœ… Successfully executed EXPLAIN for optimized query!")
                
                # ðŸŽ¯ æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆæŠ½å‡ºãƒ»ä¿å­˜ï¼ˆEXPLAINæˆåŠŸæ™‚ã®ã¿ï¼‰
                try:
                    # optimized_queryã¯ã‚¹ã‚³ãƒ¼ãƒ—ã«å¿œã˜ã¦é©åˆ‡ãªå¤‰æ•°ã‚’ä½¿ç”¨
                    query_for_extraction = locals().get('optimized_query', locals().get('final_query', ''))
                    if query_for_extraction:
                        optimization_point = extract_optimization_points_from_query(query_for_extraction, "single_optimization", 1)
                        save_trial_log(optimization_point)  # Log individual trial
                        save_optimization_points_summary(optimization_point)  # Keep existing functionality
                except Exception as e:
                    print(f"âš ï¸ Optimization points extraction failed: {str(e)}")
                
                # æˆåŠŸæ™‚ã®ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±è¡¨ç¤º
                explain_result = retry_result.get('explain_result', {})
                if explain_result:
                    print("\nðŸ“ Generated files:")
                    if 'explain_file' in explain_result:
                        print(f"   ðŸ“„ EXPLAIN results: {explain_result['explain_file']}")
                    if 'plan_lines' in explain_result:
                        print(f"   ðŸ“Š Execution plan lines: {explain_result['plan_lines']:,}")
                
                # æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®ä¿å­˜
                optimized_result = retry_result.get('optimized_result', '')
                final_query = retry_result.get('final_query', original_query_for_explain)
                
                # File saving: final_query (successful query) to SQL file, optimized_result (original LLM response) to report
                performance_comparison = retry_result.get('performance_comparison')
                best_attempt_number = retry_result.get('best_result', {}).get('attempt_num')  # ðŸŽ¯ ãƒ™ã‚¹ãƒˆè©¦è¡Œç•ªå·ã‚’å–å¾—
                optimization_attempts = retry_result.get('optimization_attempts', [])  # ðŸŽ¯ æœ€é©åŒ–è©¦è¡Œè©³ç´°ã‚’å–å¾—
                saved_files = save_optimized_sql_files(
                    original_query_for_explain,
                    final_query,  # ðŸš€ æˆåŠŸã—ãŸã‚¯ã‚¨ãƒªï¼ˆãƒ’ãƒ³ãƒˆä»˜ãï¼‰ã‚’ä¿å­˜
                    current_metrics,
                    analysis_result_str,
                    optimized_result,  # ðŸ“Š å…ƒã®LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆãƒ¬ãƒãƒ¼ãƒˆç”¨ï¼‰
                    performance_comparison,  # ðŸ” ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒçµæžœ
                    best_attempt_number,  # ðŸŽ¯ ãƒ™ã‚¹ãƒˆè©¦è¡Œç•ªå·ï¼ˆãƒ¬ãƒãƒ¼ãƒˆç”¨ï¼‰
                    optimization_attempts,  # ðŸŽ¯ æœ€é©åŒ–è©¦è¡Œè©³ç´°ï¼ˆãƒ¬ãƒãƒ¼ãƒˆç”¨ï¼‰
                    True  # ðŸš€ æœ€é©åŒ–æˆåŠŸ
                )
                
                print("\nðŸ“ Optimization files:")
                for file_type, filename in saved_files.items():
                    print(f"   ðŸ“„ {file_type}: {filename}")
                    
            elif retry_result['final_status'] == 'optimization_failed':
                print("ðŸš¨ Using original query due to failure or degradation in all optimization attempts")
                fallback_reason = retry_result.get('fallback_reason', 'Unknown reason')
                print(f"ðŸ”§ Failure reason: {fallback_reason}")
                
                # å¤±æ•—è©³ç´°ã®è¡¨ç¤º
                if 'optimization_attempts' in retry_result:
                    attempts = retry_result['optimization_attempts']
                    degraded_count = sum(1 for a in attempts if a['status'] == 'performance_degraded')
                    error_count = sum(1 for a in attempts if a['status'] in ['llm_error', 'explain_failed'])
                    
                    if degraded_count > 0:
                        print(f"ðŸ“Š Performance degradation: {degraded_count} times")
                    if error_count > 0:
                        print(f"âŒ Errors occurred: {error_count} times")
                
                print("ðŸ’¡ Recommendations:")
                print("   - Consider updating table statistics")
                print("   - Consider manual optimization with more detailed EXPLAIN information")
                print("   - Please check data volume and query complexity")
                
                # ðŸš€ å¤±æ•—æ™‚ã§ã‚‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’å®Ÿè¡Œï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã«ã‚ˆã‚‹è¿½åŠ ï¼‰
                print("\nðŸ¤– Generating final report even though optimization failed...")
                fallback_query = retry_result.get('final_query', original_query_for_explain)
                fallback_result = retry_result.get('optimized_result', 'Optimization failed')
                optimization_attempts = retry_result.get('optimization_attempts', [])
                best_attempt_number = retry_result.get('best_result', {}).get('attempt_num', 1)
                
                saved_files = save_optimized_sql_files(
                    original_query_for_explain,
                    fallback_query,  # å…ƒã®ã‚¯ã‚¨ãƒªã¾ãŸã¯æœ€å¾Œã«æˆåŠŸã—ãŸã‚¯ã‚¨ãƒª
                    current_metrics,
                    analysis_result_str,
                    fallback_result,  # å¤±æ•—æƒ…å ±ã‚’å«ã‚€ãƒ¬ãƒãƒ¼ãƒˆ
                    None,  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒã¯å¤±æ•—
                    best_attempt_number,  # æœ€é©åŒ–è©¦è¡Œç•ªå·
                    optimization_attempts,  # æœ€é©åŒ–è©¦è¡Œè©³ç´°
                    False  # ðŸš€ æœ€é©åŒ–å¤±æ•—
                )
                
                print("\nðŸ“ Generated files (failure case):")
                for file_type, filename in saved_files.items():
                    print(f"   ðŸ“„ {file_type}: {filename}")
            
            elif retry_result['final_status'] == 'fallback_to_original':
                print("âš ï¸ Using original query due to persistent errors in optimized query")
            
            elif retry_result['final_status'] == 'llm_error':
                print("âŒ Using original query due to LLM API call error")
                error_details = retry_result.get('error_details', 'Unknown error')
                print(f"ðŸ”§ LLM error details: {error_details[:200]}...")
                print("ðŸ’¡ Solution: Reduce input data size or adjust LLM settings")
            
            elif retry_result['final_status'] == 'llm_error_correction_failed':
                print("âŒ Using original query due to LLM error even during error correction")
                error_details = retry_result.get('error_details', 'Unknown error')
                print(f"ðŸ”§ LLM error details: {error_details[:200]}...")
                print("ðŸ’¡ Solution: Execute manual SQL optimization or retry with simpler query")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã®ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±è¡¨ç¤º
                fallback_files = retry_result.get('fallback_files', {})
                failure_log = retry_result.get('failure_log', '')
                
                print("\nðŸ“ Generated files:")
                for file_type, filename in fallback_files.items():
                    print(f"   ðŸ“„ {file_type}: {filename}")
                if failure_log:
                    print(f"   ðŸ“„ Failure log: {failure_log}")
                    
            # å…¨è©¦è¡Œã®è©³ç´°è¡¨ç¤º
            print("\nðŸ“‹ Attempt details:")
            for attempt in retry_result.get('all_attempts', []):
                status_icon = "âœ…" if attempt['status'] == 'success' else "âŒ"
                print(f"   {status_icon} Attempt {attempt['attempt']}: {attempt['status']}")
                if attempt['status'] == 'error':
                    print(f"      Error: {attempt['error_message'][:100]}...")
                    
        except Exception as e:
            print(f"âŒ Error occurred during integrated processing: {str(e)}")
            print("ðŸš¨ Emergency error details:")
            import traceback
            traceback.print_exc()
            print("   Emergency fallback: Executing basic analysis and minimal file generation...")
            
            try:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®EXPLAINå®Ÿè¡Œï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªï¼‰
                # ðŸš€ æ—¢å­˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸEXPLAINçµæžœã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¤‡å®Ÿè¡Œé˜²æ­¢ï¼‰
                cached_original_result = globals().get('cached_original_explain_cost_result')
                if cached_original_result and 'explain_file' in cached_original_result:
                    print("ðŸ’¾ Using cached EXPLAIN results for fallback processing (avoiding duplicate execution)")
                    explain_results = cached_original_result
                else:
                    print("ðŸ”„ Executing EXPLAIN for original query (fallback processing)")
                    explain_results = execute_explain_and_save_to_file(original_query_for_explain, "original")
                
                if explain_results:
                    print("\nðŸ“ EXPLAIN results:")
                    for file_type, filename in explain_results.items():
                        if file_type == 'explain_file':
                            print(f"   ðŸ“„ EXPLAIN results: {filename}")
                        elif file_type == 'error_file':
                            print(f"   ðŸ“„ Error log: {filename}")
                        elif file_type == 'plan_lines':
                            print(f"   ðŸ“Š Execution plan lines: {filename}")
                        elif file_type == 'error_message':
                            print(f"   âŒ Error message: {filename}")
                
                # ðŸš¨ ç·Šæ€¥ä¿®æ­£: ã‚¨ãƒ©ãƒ¼æ™‚ã§ã‚‚ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¼·åˆ¶ç”Ÿæˆ
                print("ðŸš¨ Executing emergency report generation...")
                emergency_saved_files = save_optimized_sql_files(
                    original_query_for_explain,
                    original_query_for_explain,  # æœ€é©åŒ–å¤±æ•—æ™‚ã¯å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨
                    current_metrics if 'current_metrics' in locals() else {},
                    "ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: çµ±åˆå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚ã€åŸºæœ¬åˆ†æžã®ã¿å®Ÿè¡Œ",
                    f"ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†\n\nã‚¨ãƒ©ãƒ¼è©³ç´°:\n{str(e)}\n\nå…ƒã‚¯ã‚¨ãƒªã‚’ãã®ã¾ã¾ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚",
                    None,  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒçµæžœãªã—
                    None,  # best_attempt_number
                    None,  # optimization_attempts
                    False  # ðŸš€ æœ€é©åŒ–å¤±æ•—ï¼ˆç·Šæ€¥æ™‚ï¼‰
                )
                
                print("\nðŸ“ Emergency generated files:")
                for file_type, filename in emergency_saved_files.items():
                    print(f"   ðŸ“„ {file_type}: {filename}")
                    
            except Exception as emergency_error:
                print(f"ðŸš¨ Error even in emergency fallback processing: {str(emergency_error)}")
                print("âš ï¸ Please verify query manually")
        
        print("\nâœ… Integrated SQL optimization processing completed")
        
    else:
        print("âŒ EXPLAIN statements cannot be executed because Spark environment is not available")
        print("   Please execute in Databricks environment")
        
else:
    print("âŒ No executable original query available")
    print("   Note: Original query extraction from profiler data was unsuccessful")

print()



# COMMAND ----------

# MAGIC %md
# MAGIC ## ðŸ“ Report Formatting Process
# MAGIC
# MAGIC This cell performs the following processing:
# MAGIC - Search and load optimization report files
# MAGIC - Refine and improve report content using LLM
# MAGIC - Save refinement results and generate final report

# COMMAND ----------
# 
# ðŸ“ ãƒ¬ãƒãƒ¼ãƒˆæŽ¨æ•²å‡¦ç†ï¼ˆçµ±åˆå‡¦ç†ç”¨ï¼‰
print("\nðŸ“ Report refinement processing")
print("-" * 40)
# 
def find_latest_report_file() -> str:
    """Find the latest report file"""
    import os
    import glob
    
    # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ (è¨€èªžåˆ¥å¯¾å¿œ)
    language_suffix = 'en' if OUTPUT_LANGUAGE == 'en' else 'jp'
    pattern = f"output_optimization_report_{language_suffix}_*.md"
    report_files = glob.glob(pattern)
    
    if not report_files:
        return None
    
    # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é †ï¼‰
    latest_file = max(report_files, key=os.path.getctime)
    return latest_file
# 
def refine_report_content_with_llm(report_content: str) -> str:
    """Refine report using LLM"""
    
    # LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è¨­å®šç¢ºèª
    if not LLM_CONFIG or not LLM_CONFIG.get('provider'):
        print("âŒ LLM provider is not configured")
        return report_content
    
    # ðŸš¨ ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾ç­–: ãƒ¬ãƒãƒ¼ãƒˆã‚µã‚¤ã‚ºåˆ¶é™
    MAX_CONTENT_SIZE = 50000  # 50KBåˆ¶é™
    original_size = len(report_content)
    
    if original_size > MAX_CONTENT_SIZE:
        print(f"âš ï¸ Report size too large: {original_size:,} characters â†’ truncated to {MAX_CONTENT_SIZE:,} characters")
        # é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å„ªå…ˆçš„ã«ä¿æŒ
        truncated_content = report_content[:MAX_CONTENT_SIZE]
        truncated_content += f"\n\nâš ï¸ ãƒ¬ãƒãƒ¼ãƒˆãŒå¤§ãã™ãŽã‚‹ãŸã‚ã€{MAX_CONTENT_SIZE:,} æ–‡å­—ã«åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸï¼ˆå…ƒã‚µã‚¤ã‚º: {original_size:,} æ–‡å­—ï¼‰"
        report_content = truncated_content
    else:
        print(f"ðŸ“Š Report size: {original_size:,} characters (executing refinement)")
    
    # Photonåˆ©ç”¨çŽ‡ã®æŠ½å‡ºã¨è©•ä¾¡åˆ¤å®š
    import re
    photon_pattern = r'åˆ©ç”¨çŽ‡[ï¼š:]\s*(\d+(?:\.\d+)?)%'
    photon_match = re.search(photon_pattern, report_content)
    
    photon_evaluation_instruction = ""
    if photon_match:
        photon_utilization = float(photon_match.group(1))
        if OUTPUT_LANGUAGE == 'ja':
            if photon_utilization <= 80:
                photon_evaluation_instruction = """
ã€Photonåˆ©ç”¨çŽ‡è©•ä¾¡æŒ‡ç¤ºã€‘
- Photonåˆ©ç”¨çŽ‡ãŒ80%ä»¥ä¸‹ã®å ´åˆã¯ã€Œè¦æ”¹å–„ã€ã¾ãŸã¯ã€Œä¸è‰¯ã€ã®è©•ä¾¡ã‚’æ˜Žç¢ºã«è¡¨ç¤ºã—ã¦ãã ã•ã„
- 80%ä»¥ä¸‹ã®å ´åˆã¯ã€æ”¹å–„ã®å¿…è¦æ€§ã‚’å¼·èª¿ã—ã€å…·ä½“çš„ãªæ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æç¤ºã—ã¦ãã ã•ã„
- è©•ä¾¡ä¾‹: ã€ŒPhotonåˆ©ç”¨çŽ‡: XX% (è©•ä¾¡: è¦æ”¹å–„)ã€
"""
            else:
                photon_evaluation_instruction = """
ã€Photonåˆ©ç”¨çŽ‡è©•ä¾¡æŒ‡ç¤ºã€‘
- Photonåˆ©ç”¨çŽ‡ãŒ80%ä»¥ä¸Šã®å ´åˆã¯ã€Œè‰¯å¥½ã€ã®è©•ä¾¡ã‚’è¡¨ç¤ºã—ã¦ãã ã•ã„
- è©•ä¾¡ä¾‹: ã€ŒPhotonåˆ©ç”¨çŽ‡: XX% (è©•ä¾¡: è‰¯å¥½)ã€
"""
        else:
            if photon_utilization <= 80:
                photon_evaluation_instruction = """
ã€Photon Utilization Rate Evaluation Instructionsã€‘
- If Photon utilization rate is 80% or below, clearly display "Needs Improvement" or "Poor" evaluation
- For 80% or below, emphasize the need for improvement and provide specific improvement actions
- Example: "Photon Utilization Rate: XX% (Evaluation: Needs Improvement)"
"""
            else:
                photon_evaluation_instruction = """
ã€Photon Utilization Rate Evaluation Instructionsã€‘
- If Photon utilization rate is 80% or above, display "Good" evaluation
- Example: "Photon Utilization Rate: XX% (Evaluation: Good)"
"""
    
    # è¨€èªžã«å¿œã˜ã¦æŽ¨æ•²ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ‡ã‚Šæ›¿ãˆ
    if OUTPUT_LANGUAGE == 'ja':
        refinement_prompt = f"""ã‚ãªãŸã¯æŠ€è¡“æ–‡æ›¸ç·¨é›†è€…ã§ã™ã€‚ä»¥ä¸‹ã®Databricks SQL ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åˆ†æžãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿ã‚„ã™ãç°¡æ½”ã«æŽ¨æ•²ã—ã¦ãã ã•ã„ã€‚

ã€æŽ¨æ•²è¦ä»¶ã€‘
1. å…¨ä½“æ§‹æˆã‚’æ•´ç†ã—ã€è«–ç†çš„ã«æƒ…å ±ã‚’é…ç½®
2. å†—é•·ãªè¡¨ç¾ã‚’å‰Šé™¤ã—ã€ç°¡æ½”ã§ç†è§£ã—ã‚„ã™ã„è¡¨ç¾ã«ä¿®æ­£
3. é‡è¦ãªæƒ…å ±ãŒåŸ‹ã‚‚ã‚Œãªã„ã‚ˆã†é©åˆ‡ãªè¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«ã§æ§‹é€ åŒ–
4. æŠ€è¡“ç”¨èªžã‚’ä¿æŒã—ã¤ã¤ã€ç†è§£ã—ã‚„ã™ã„èª¬æ˜Žã‚’è¿½åŠ 
5. æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¿æŒ
6. å®Ÿç”¨çš„ãªæŽ¨å¥¨äº‹é …ã‚’æ˜Žç¢ºã«æç¤º

ã€ðŸš¨ å‰Šé™¤ãƒ»ä¿®æ­£ã—ã¦ã¯ã„ã‘ãªã„é‡è¦æƒ…å ±ã€‘
- **ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±**: "ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: XX" ã¾ãŸã¯ "è¨­å®šãªã—" ã®è¡¨ç¤º
- **ãƒ•ã‚£ãƒ«ã‚¿çŽ‡æƒ…å ±**: "ãƒ•ã‚£ãƒ«ã‚¿çŽ‡: X.X% (èª­ã¿è¾¼ã¿: XX.XXGB, ãƒ—ãƒ«ãƒ¼ãƒ³: XX.XXGB)" ã®å½¢å¼
- **ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—**: å„ãƒ—ãƒ­ã‚»ã‚¹ã® "å…¨ä½“ã®XX%" è¡¨ç¤ºï¼ˆä¸¦åˆ—å®Ÿè¡Œã‚’è€ƒæ…®ã—ãŸæ­£ç¢ºãªè¨ˆç®—ï¼‰
- **æŽ¨å¥¨vsç¾åœ¨ã®æ¯”è¼ƒåˆ†æž**: æŽ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã¨ç¾åœ¨ã®ã‚­ãƒ¼ã®æ¯”è¼ƒæƒ…å ±
- **å…·ä½“çš„ãªæ•°å€¤ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: å®Ÿè¡Œæ™‚é–“ã€ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é‡ã€ã‚¹ãƒ”ãƒ«é‡ã€åˆ©ç”¨çŽ‡ç­‰
- **SQLå®Ÿè£…ä¾‹**: ALTER TABLEæ§‹æ–‡ã€CLUSTER BYæ–‡ã€ãƒ’ãƒ³ãƒˆå¥ç­‰ã®å…·ä½“ä¾‹
- **ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥è©³ç´°æƒ…å ±**: å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒŽãƒ¼ãƒ‰æƒ…å ±ã€ãƒ•ã‚£ãƒ«ã‚¿åŠ¹çŽ‡ã€æŽ¨å¥¨äº‹é …

{photon_evaluation_instruction}

ã€ç¾åœ¨ã®ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã€‘
{report_content}

ã€å‡ºåŠ›è¦ä»¶ã€‘
- ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§æŽ¨æ•²ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›
- æŠ€è¡“æƒ…å ±ã‚’ä¿æŒã—ã¤ã¤å¯èª­æ€§ã‚’å‘ä¸Š
- é‡è¦ãƒã‚¤ãƒ³ãƒˆã®å¼·èª¿ã¨è¡Œå‹•è¨ˆç”»ã®æ˜Žç¢ºåŒ–
- Photonåˆ©ç”¨çŽ‡è©•ä¾¡ã®æ˜Žç¢ºãªè¡¨ç¤º
- **å¿…é ˆ**: ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã¨ãƒ•ã‚£ãƒ«ã‚¿çŽ‡æƒ…å ±ã®å®Œå…¨ä¿æŒ
- **å¿…é ˆ**: ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ã§ã¯å…ƒã®æ­£ç¢ºãªæ•°å€¤ã‚’ä½¿ç”¨
- **å¿…é ˆ**: ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥è©³ç´°åˆ†æžæƒ…å ±ï¼ˆç¾åœ¨ã‚­ãƒ¼ã€æŽ¨å¥¨ã‚­ãƒ¼ã€ãƒ•ã‚£ãƒ«ã‚¿çŽ‡ï¼‰ã‚’å‰Šé™¤ã—ãªã„
- **å¿…é ˆ**: SQLå®Ÿè£…ä¾‹ï¼ˆALTER TABLEã€CLUSTER BYç­‰ï¼‰ã‚’å®Œå…¨ãªå½¢ã§ä¿æŒ
"""
    else:
        refinement_prompt = f"""You are a technical document editor. Please refine the following Databricks SQL performance analysis report to make it readable and concise.

ã€Refinement Requirementsã€‘
1. Organize the overall structure and arrange information logically
2. Remove redundant expressions and modify to concise, understandable expressions
3. Structure with appropriate heading levels so important information doesn't get buried
4. Keep technical terms while adding understandable explanations
5. Preserve numerical data and metrics
6. Clearly present practical recommendations

ã€ðŸš¨ Critical Information That Must NOT Be Deleted or Modifiedã€‘
- **Current clustering key information**: Display "Current clustering key: XX" or "Not configured"
- **Filter rate information**: Format "Filter rate: X.X% (read: XX.XXGB, pruned: XX.XXGB)"
- **Percentage calculations**: Display "XX% of total" for each process (accurate calculations considering parallel execution)
- **Recommended vs current comparison analysis**: Comparison information between recommended clustering keys and current keys
- **Specific numerical metrics**: Execution time, data read volume, spill volume, utilization rates, etc.
- **SQL implementation examples**: Specific examples of ALTER TABLE syntax, CLUSTER BY statements, hint clauses, etc.
- **Table-specific detailed information**: Node information, filter efficiency, and recommendations for each table

{photon_evaluation_instruction}

ã€Current Report Contentã€‘
{report_content}

ã€Output Requirementsã€‘
- Output refined report in markdown format
- Maintain technical information while improving readability
- Emphasize important points and clarify action plans
- Clearly display Photon utilization rate evaluation
- **Required**: Completely preserve current clustering key information and filter rate information
- **Required**: Use original accurate numerical values for percentage calculations
- **Required**: Do not delete detailed analysis information by table (current key, recommended key, filter rate)
- **Required**: Preserve SQL implementation examples (ALTER TABLE, CLUSTER BY, etc.) in complete form
"""
    
    try:
        # è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«åŸºã¥ã„ã¦æŽ¨æ•²ã‚’å®Ÿè¡Œ
        provider = LLM_CONFIG.get('provider', 'databricks')
        
        if provider == 'databricks':
            refined_content = _call_databricks_llm(refinement_prompt)
        elif provider == 'openai':
            refined_content = _call_openai_llm(refinement_prompt)
        elif provider == 'azure_openai':
            refined_content = _call_azure_openai_llm(refinement_prompt)
        elif provider == 'anthropic':
            refined_content = _call_anthropic_llm(refinement_prompt)
        else:
            print(f"âŒ Unsupported LLM provider: {provider}")
            return report_content
        
        # ðŸš¨ LLMã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ¤œå‡ºï¼ˆç²¾å¯†åŒ–ï¼‰
        if isinstance(refined_content, str):
            # ã‚ˆã‚Šç²¾å¯†ãªã‚¨ãƒ©ãƒ¼æ¤œå‡ºï¼ˆãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã®çµµæ–‡å­—ã¨åŒºåˆ¥ï¼‰
            actual_error_indicators = [
                "APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰",
                "Input is too long for requested model",
                "Bad Request",
                "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼:",
                "APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼:",
                'ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {"error_code":',
                "âŒ APIã‚¨ãƒ©ãƒ¼:",
                "âš ï¸ APIã‚¨ãƒ©ãƒ¼:"
            ]
            
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®é–‹å§‹éƒ¨åˆ†ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚ŠåŽ³å¯†ï¼‰
            is_error_response = any(
                refined_content.strip().startswith(indicator) or 
                f"\n{indicator}" in refined_content[:500]  # å…ˆé ­500æ–‡å­—ä»¥å†…ã§ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                for indicator in actual_error_indicators
            )
            
            if is_error_response:
                print(f"âŒ Error detected in LLM report refinement: {refined_content[:200]}...")
                print("ðŸ“„ Returning original report")
                return report_content
        
        # thinking_enabledå¯¾å¿œ: çµæžœãŒãƒªã‚¹ãƒˆã®å ´åˆã®å‡¦ç†
        if isinstance(refined_content, list):
            refined_content = format_thinking_response(refined_content)
        
        print(f"âœ… LLM-based report refinement completed (Cell 46 independent processing)")
        return refined_content
        
    except Exception as e:
        print(f"âŒ Error occurred during LLM-based report refinement: {str(e)}")
        return report_content
# 
def save_refined_report(refined_content: str, original_filename: str) -> str:
    """Save refined report"""
    from datetime import datetime
    
    # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆè¨€èªžåˆ¥å¯¾å¿œï¼‰
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    language_suffix = 'en' if OUTPUT_LANGUAGE == 'en' else 'jp'
    refined_filename = f"output_final_report_{language_suffix}_{timestamp}.md"
    
    try:
        with open(refined_filename, 'w', encoding='utf-8') as f:
            f.write(refined_content)
        
        print(f"âœ… Saved final report: {refined_filename}")
        return refined_filename
        
    except Exception as e:
        print(f"âŒ Error during refined report saving: {str(e)}")
        return None
# 
def finalize_report_files(original_filename: str, refined_filename: str) -> str:
    """Execute file processing based on DEBUG_ENABLED setting"""
    import os
    
    # DEBUG_ENABLEDè¨­å®šã‚’ç¢ºèª
    debug_enabled = globals().get('DEBUG_ENABLED', 'N')
    
    try:
        if debug_enabled.upper() == 'Y':
            # DEBUG_ENABLED=Y: å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åç§°å¤‰æ›´ã—ã¦ä¿æŒ
            if os.path.exists(original_filename):
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆå…ƒãƒ•ã‚¡ã‚¤ãƒ«åã« _raw ã‚’è¿½åŠ ï¼‰
                backup_filename = original_filename.replace('.md', '_raw.md')
                
                os.rename(original_filename, backup_filename)
                print(f"ðŸ“ Preserving original file: {original_filename} â†’ {backup_filename}")
            else:
                print(f"âš ï¸ Original file not found: {original_filename}")
        else:
            # DEBUG_ENABLED=N: å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            if os.path.exists(original_filename):
                os.remove(original_filename)
                print(f"ðŸ—‘ï¸ Deleted original file: {original_filename}")
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆoutput_final_report_*ï¼‰ã¯ãƒªãƒãƒ¼ãƒ ã›ãšãã®ã¾ã¾ä¿æŒ
        if os.path.exists(refined_filename):
            print(f"âœ… Preserving final report file: {refined_filename}")
            return refined_filename
        else:
            print(f"âŒ Refined version file not found: {refined_filename}")
            return None
            
    except Exception as e:
        print(f"âŒ Error during file operations: {str(e)}")
        return None
# 
# 
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
try:
    # æœ€æ–°ã®ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    latest_report = find_latest_report_file()
    
    if not latest_report:
        print("âŒ Report file not found")
        print("âš ï¸ No analysis report files were found in the current directory")
        print()
        print("ðŸ” Detailed troubleshooting:")
        print("1. Please confirm that the main analysis processing completed normally")
        print("2. Please check if any error messages are displayed in previous cells")
        print("3. Please check if variables like current_analysis_result and extracted_metrics are defined")
        print("4. Emergency fallback processing may have been executed")
        print("5. You may need to re-run the main analysis cells to generate reports")
        
        # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        import glob
        sql_files = glob.glob("output_optimized_query_*.sql")
        original_files = glob.glob("output_original_query_*.sql")
        all_reports = glob.glob("output_optimization_report*.md")
        
        # ç¾åœ¨ã®è¨€èªžè¨­å®šã«å¯¾å¿œã™ã‚‹ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
        language_suffix = 'en' if OUTPUT_LANGUAGE == 'en' else 'jp'
        current_lang_reports = glob.glob(f"output_optimization_report_{language_suffix}_*.md")
        
        print(f"\nðŸ“ Current file status:")
        print(f"   ðŸ“„ Optimized query files: {len(sql_files)} files")
        print(f"   ðŸ“„ Original query files: {len(original_files)} files")
        print(f"   ðŸ“„ Report files ({language_suffix.upper()}): {len(current_lang_reports)} files")
        print(f"   ðŸ“„ Report files (total): {len(all_reports)} files")
        
        if all_reports:
            print(f"   ðŸ“‹ Detected report files:")
            for report in all_reports:
                print(f"      - {report}")
            print("   âš ï¸ Files exist but not detected by find_latest_report_file()")
            print("   ðŸ’¡ Please check filenames manually - possible pattern matching issue")
        
        if not sql_files and not original_files:
            print("   ðŸš¨ Important: Cell 43 processing may not have been executed at all")
            print("   ðŸ“‹ Solution: Re-execute Cell 43 from the beginning")
    else:
        print(f"ðŸ“„ Target report file: {latest_report}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿è¾¼ã¿
        with open(latest_report, 'r', encoding='utf-8') as f:
            original_content = f.read()
        
        print(f"ðŸ“Š Original report size: {len(original_content):,} characters")
        
        # ðŸš¨ é‡è¤‡æŽ¨æ•²é˜²æ­¢: æ—¢ã«æŽ¨æ•²æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
        refinement_indicators = [
            "ðŸ“Š **æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ**",  # æŽ¨æ•²å¾Œã®å…¸åž‹çš„ãªãƒ˜ãƒƒãƒ€ãƒ¼
            "ðŸš€ **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ”¹å–„çµæžœ**",  # æŽ¨æ•²å¾Œã®å…¸åž‹çš„ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³
            "âœ… **æŽ¨å¥¨äº‹é …**",  # æŽ¨æ•²å¾Œã®ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ
            "LLMã«ã‚ˆã‚‹æŽ¨æ•²ã‚’å®Ÿè¡Œä¸­",  # æŽ¨æ•²ãƒ—ãƒ­ã‚»ã‚¹ä¸­ã«å«ã¾ã‚Œã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            "æŽ¨æ•²ç‰ˆãƒ¬ãƒãƒ¼ãƒˆ:",  # æŽ¨æ•²æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        ]
        
        already_refined = any(indicator in original_content for indicator in refinement_indicators)
        
        if already_refined:
            print(f"âœ… Report already refined (avoiding duplicate processing): {latest_report}")
            print("ðŸ“‹ Using refined report as is")
            refined_content = original_content
        else:
            print(f"ðŸ¤– Executing LLM-based refinement (target: {latest_report})...")
            refined_content = refine_report_content_with_llm(original_content)
        
        if refined_content != original_content:
            print(f"ðŸ“Š Post-refinement size: {len(refined_content):,} characters")
            
            # æŽ¨æ•²ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
            refined_filename = save_refined_report(refined_content, latest_report)
            
            if refined_filename:
                print(f"ðŸ“„ Refined report: {refined_filename}")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèª
                import os
                if os.path.exists(refined_filename):
                    file_size = os.path.getsize(refined_filename)
                    print(f"ðŸ“ Refined file size: {file_size:,} bytes")
                
                # å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã€æŽ¨æ•²ç‰ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«åã«ãƒªãƒãƒ¼ãƒ 
                final_filename = finalize_report_files(latest_report, refined_filename)
                
                if final_filename:
                    print(f"ðŸ“„ Final report file: {final_filename}")
                    
                    # æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèª
                    if os.path.exists(final_filename):
                        final_file_size = os.path.getsize(final_filename)
                        print(f"ðŸ“ Final file size: {final_file_size:,} bytes")
                
                print(f"âœ… Report refinement processing completed: {final_filename}")
                
                # æŽ¨æ•²ã®çµæžœã‚’è¡¨ç¤ºï¼ˆæœ€åˆã®1000æ–‡å­—ï¼‰
                print("\nðŸ“‹ Refinement result preview:")
                print("-" * 50)
                preview = refined_content[:1000]
                print(preview)
                if len(refined_content) > 1000:
                    print(f"\n... (remaining {len(refined_content) - 1000} characters see {final_filename or latest_report})")
                print("-" * 50)
            else:
                print("âŒ Failed to save refined report")
        else:
            print("ðŸ“‹ Report is already in optimal state (refinement processing skipped)")
            print("âœ… Using existing report file as is")
            
            # æ—¢ã«æŽ¨æ•²æ¸ˆã¿ã®å ´åˆã‚‚ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤º
            print("\nðŸ“‹ Report content preview:")
            print("-" * 50)
            preview = refined_content[:1000]
            print(preview)
            if len(refined_content) > 1000:
                print(f"\n... (remaining {len(refined_content) - 1000} characters see {latest_report})")
            print("-" * 50)
            
except Exception as e:
    print(f"âŒ Error occurred during report refinement processing: {str(e)}")
    import traceback
    traceback.print_exc()
# 
print()
# 
# # ðŸ§¹ ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤å‡¦ç†ï¼ˆDEBUG_ENABLEDãƒ•ãƒ©ã‚°ã«åŸºã¥ãï¼‰
debug_enabled = globals().get('DEBUG_ENABLED', 'N')
explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')

if debug_enabled.upper() == 'Y':
    print("\nðŸ› Debug mode enabled: Preserving intermediate files")
    print("-" * 40)
    print("ðŸ’¡ All intermediate files are preserved because DEBUG_ENABLED=Y")
    print("ðŸ“ The following files are preserved:")
    
    import glob
    import os
    
    # ä¿æŒã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
    if explain_enabled.upper() == 'Y':
        original_files = glob.glob("output_explain_original_*.txt")
        optimized_files = glob.glob("output_explain_optimized_*.txt")
        cost_original_files = glob.glob("output_explain_cost_original_*.txt")
        cost_optimized_files = glob.glob("output_explain_cost_optimized_*.txt")
        error_files = glob.glob("output_explain_error_*.txt")
        all_files = original_files + optimized_files + cost_original_files + cost_optimized_files + error_files
        
        if all_files:
            print(f"   ðŸ” EXPLAIN result files:")
            print(f"      ðŸ“Š EXPLAIN: Original {len(original_files)} files, Post-optimization {len(optimized_files)} files")
            print(f"      ðŸ’° EXPLAIN COST: Original {len(cost_original_files)} files, Post-optimization {len(cost_optimized_files)} files")
            print(f"      âŒ Errors: {len(error_files)} files")
            for file_path in all_files[:3]:  # æœ€å¤§3å€‹ã¾ã§è¡¨ç¤º
                print(f"      ðŸ“„ {file_path}")
            if len(all_files) > 3:
                print(f"      ... and {len(all_files) - 3} other files")
    
    print("âœ… Debug mode: Skipped file deletion processing")
else:
    print("\nðŸ§¹ Intermediate file deletion processing")
    print("-" * 40)
    print("ðŸ’¡ Deleting intermediate files because DEBUG_ENABLED=N")
    language_suffix = 'en' if OUTPUT_LANGUAGE == 'en' else 'jp'
    print(f"ðŸ“ Files to be kept: output_original_query_*.sql, output_optimization_report_{language_suffix}_*.md, output_optimized_query_*.sql")
    
    import glob
    import os
    
    if explain_enabled.upper() == 'Y':
        # EXPLAINçµæžœãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆæ–°ãƒ‘ã‚¿ãƒ¼ãƒ³ + æ—§ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        original_files = glob.glob("output_explain_original_*.txt")
        optimized_files = glob.glob("output_explain_optimized_*.txt")
        cost_original_files = glob.glob("output_explain_cost_original_*.txt")
        cost_optimized_files = glob.glob("output_explain_cost_optimized_*.txt")
        error_original_files = glob.glob("output_explain_error_original_*.txt")
        error_optimized_files = glob.glob("output_explain_error_optimized_*.txt")
        
        # æ—§ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å‰Šé™¤å¯¾è±¡ã«å«ã‚ã‚‹ï¼ˆä¸‹ä½äº’æ›æ€§ï¼‰
        old_explain_files = glob.glob("output_explain_plan_*.txt")
        old_error_files = glob.glob("output_explain_error_*.txt")
        
        # ðŸš¨ æ–°è¦è¿½åŠ : DEBUGç”¨ã®å®Œå…¨æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å‰Šé™¤å¯¾è±¡ã«å«ã‚ã‚‹
        full_plan_files = glob.glob("output_physical_plan_full_*.txt")
        full_stats_files = glob.glob("output_explain_cost_statistics_full_*.txt")
        extracted_stats_files = glob.glob("output_explain_cost_statistics_extracted_*.json")
        structured_plan_files = glob.glob("output_physical_plan_structured_*.json")
        structured_cost_files = glob.glob("output_explain_cost_structured_*.json")
        
        all_temp_files = (original_files + optimized_files + cost_original_files + cost_optimized_files + 
                         error_original_files + error_optimized_files + old_explain_files + old_error_files +
                         full_plan_files + full_stats_files + extracted_stats_files + 
                         structured_plan_files + structured_cost_files)
        
        explain_files = original_files + optimized_files + old_explain_files
        cost_files = cost_original_files + cost_optimized_files
        error_files = error_original_files + error_optimized_files + old_error_files
        debug_files = full_plan_files + full_stats_files + extracted_stats_files + structured_plan_files + structured_cost_files
        
        if all_temp_files:
            print(f"ðŸ“ Files to be deleted:")
            print(f"   ðŸ“Š EXPLAIN results: {len(explain_files)} files")
            print(f"   ðŸ’° EXPLAIN COST results: {len(cost_files)} files")
            print(f"   âŒ Error files: {len(error_files)} files")
            print(f"   ðŸ”§ DEBUG complete information: {len(debug_files)} files")
            print("ðŸ’¡ Note: These files should not have been created because DEBUG_ENABLED=N")
            
            # ðŸ”§ å¤‰æ•°ã®åˆæœŸåŒ–ã‚’ã‚ˆã‚Šå®‰å…¨ã«å®Ÿè¡Œ
            deleted_count = 0
            for file_path in all_temp_files:
                try:
                    os.remove(file_path)
                    print(f"âœ… Deletion completed: {file_path}")
                    deleted_count += 1
                except Exception as e:
                    print(f"âŒ Deletion failed: {file_path} - {str(e)}")
            
            print(f"ðŸ—‘ï¸ Deletion completed: {deleted_count}/{len(all_temp_files)} files")
            print("ðŸ’¡ EXPLAIN/EXPLAIN COST results and error files deleted as they were already used by LLM optimization processing")
        else:
            print("ðŸ“ No EXPLAIN/EXPLAIN COST results or error files found for deletion")
    else:
        print("âš ï¸ Skipped EXPLAIN result file deletion processing because EXPLAIN execution is disabled")

print()

print("ðŸŽ‰ All processing completed!")
print("ðŸ“ Please check the generated files and utilize the analysis results.")
